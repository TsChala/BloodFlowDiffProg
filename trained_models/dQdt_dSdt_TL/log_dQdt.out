Size of 1D data matrix:(300, 110, 47, 4)
Shape: [timesteps,spatial locations,waveforms,variables]
Number of parameters in neural network: 2330
optimizer 1 is ADAM
optimizer 2 is BFGS optimizer
ODE Time integrator selected:RK4
Transfer learning - loaded weights from file
Batch size:10
Start training epoch 1
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-92.83048
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 48.82933329669796
Iteration: 2 || Loss: 47.541480914209096
Iteration: 3 || Loss: 47.874642524423614
Iteration: 4 || Loss: 47.55294665453491
Iteration: 5 || Loss: 46.91376172619791
Iteration: 6 || Loss: 46.91376172619791
saving ADAM checkpoint...
Sum of params:-92.77799
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 46.91376172619791
Iteration: 2 || Loss: 46.8429026433554
Iteration: 3 || Loss: 46.75168860417003
Iteration: 4 || Loss: 44.82363702497978
Iteration: 5 || Loss: 43.18396639680985
Iteration: 6 || Loss: 39.48965803119587
Iteration: 7 || Loss: 38.04169899926455
Iteration: 8 || Loss: 36.70267991929178
Iteration: 9 || Loss: 35.167457515951796
Iteration: 10 || Loss: 34.13165644303574
Iteration: 11 || Loss: 31.84420574220278
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-92.96367
Epoch 1 loss:31.84420574220278
MSE loss S0.9886856408960027
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-92.96367
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 127.86858377174237
Iteration: 2 || Loss: 128.33370824210263
Iteration: 3 || Loss: 127.20549523432368
Iteration: 4 || Loss: 126.91570712626829
Iteration: 5 || Loss: 126.84428137925372
Iteration: 6 || Loss: 126.84428137925372
saving ADAM checkpoint...
Sum of params:-92.98126
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 126.84428137925372
Iteration: 2 || Loss: 126.52681417501516
Iteration: 3 || Loss: 126.44929377357965
Iteration: 4 || Loss: 125.96214811749026
Iteration: 5 || Loss: 123.3920799223431
Iteration: 6 || Loss: 117.90094469049572
Iteration: 7 || Loss: 115.02796948420388
Iteration: 8 || Loss: 108.80705300411375
Iteration: 9 || Loss: 106.58086617778054
Iteration: 10 || Loss: 102.38842672716308
Iteration: 11 || Loss: 101.11594641725574
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.56047
Epoch 1 loss:101.11594641725574
MSE loss S1.499911884044272
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-93.56047
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 689.8615133694562
Iteration: 2 || Loss: 685.6865282408472
Iteration: 3 || Loss: 683.3997001308163
Iteration: 4 || Loss: 681.3517688378885
Iteration: 5 || Loss: 679.3272733408763
Iteration: 6 || Loss: 679.3272733408763
saving ADAM checkpoint...
Sum of params:-93.68053
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 679.3272733408763
Iteration: 2 || Loss: 678.4654205341823
Iteration: 3 || Loss: 666.7593424033082
Iteration: 4 || Loss: 601.467183519146
Iteration: 5 || Loss: 588.8266794899574
Iteration: 6 || Loss: 579.6063457338805
Iteration: 7 || Loss: 573.4339255608945
Iteration: 8 || Loss: 567.8017204298192
Iteration: 9 || Loss: 555.9022997543555
Iteration: 10 || Loss: 532.2207996428411
Iteration: 11 || Loss: 524.5830735041196
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.20888
Epoch 1 loss:524.5830735041196
MSE loss S8.335906317958107
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:144.68728844874514
MSE loss S4.09832077506312
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:707.8272538835521
MSE loss S13.15143342783052
waveform batch: 2/2
Test loss - extrapolation:526.8077541320022
MSE loss S8.806481926851033
Epoch 1 mean train loss:22.673904333226833
Epoch 1 mean test loss - interpolation:24.114548074790857
Epoch 1 mean test loss - extrapolation:102.88625066796287
Start training epoch 2
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.20888
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 55.878947245154706
Iteration: 2 || Loss: 48.44759225829667
Iteration: 3 || Loss: 47.148921604119884
Iteration: 4 || Loss: 47.66765438557772
Iteration: 5 || Loss: 46.986233276998675
Iteration: 6 || Loss: 46.986233276998675
saving ADAM checkpoint...
Sum of params:-95.17498
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 46.986233276998675
Iteration: 2 || Loss: 44.92623843772711
Iteration: 3 || Loss: 44.3517362241312
Iteration: 4 || Loss: 41.6376588493719
Iteration: 5 || Loss: 32.33612469901426
Iteration: 6 || Loss: 31.001946433084527
Iteration: 7 || Loss: 30.329333068619942
Iteration: 8 || Loss: 28.05562501621511
Iteration: 9 || Loss: 26.87085690120799
Iteration: 10 || Loss: 26.65245428709363
Iteration: 11 || Loss: 26.536345222944476
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.877396
Epoch 2 loss:26.536345222944476
MSE loss S0.511589735233027
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-93.877396
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 106.6561393721317
Iteration: 2 || Loss: 107.87724639434077
Iteration: 3 || Loss: 106.21908907928847
Iteration: 4 || Loss: 106.00886553651901
Iteration: 5 || Loss: 106.39265752325849
Iteration: 6 || Loss: 106.00886553651901
saving ADAM checkpoint...
Sum of params:-93.85493
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 106.00886553651901
Iteration: 2 || Loss: 105.7924197303856
Iteration: 3 || Loss: 105.73730021649811
Iteration: 4 || Loss: 103.09627565361967
Iteration: 5 || Loss: 100.4932545452095
Iteration: 6 || Loss: 98.27226757529066
Iteration: 7 || Loss: 95.17933357994885
Iteration: 8 || Loss: 92.15554650852792
Iteration: 9 || Loss: 89.57835533443976
Iteration: 10 || Loss: 88.64916201023857
Iteration: 11 || Loss: 87.43397138169468
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-92.55711
Epoch 2 loss:87.43397138169468
MSE loss S1.0082220110822437
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-92.55711
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 623.1687646056447
Iteration: 2 || Loss: 620.962269361729
Iteration: 3 || Loss: 617.9810703821286
Iteration: 4 || Loss: 616.131173437341
Iteration: 5 || Loss: 615.0704583163536
Iteration: 6 || Loss: 615.0704583163536
saving ADAM checkpoint...
Sum of params:-92.75282
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 615.0704583163536
Iteration: 2 || Loss: 614.347939208567
Iteration: 3 || Loss: 609.1582632921708
Iteration: 4 || Loss: 541.0825895611715
Iteration: 5 || Loss: 533.9086606920102
Iteration: 6 || Loss: 529.5941749277887
Iteration: 7 || Loss: 509.24682280843683
Iteration: 8 || Loss: 494.4734103604988
Iteration: 9 || Loss: 487.6944806561619
Iteration: 10 || Loss: 477.7695646990391
Iteration: 11 || Loss: 475.66079396594216
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.619354
Epoch 2 loss:475.66079396594216
MSE loss S6.311895973249388
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:136.32546004802398
MSE loss S2.8105796378851275
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:667.379654410399
MSE loss S12.173448546409428
waveform batch: 2/2
Test loss - extrapolation:493.4965557718805
MSE loss S8.911999865486328
Epoch 2 mean train loss:20.332107261054528
Epoch 2 mean test loss - interpolation:22.720910008003997
Epoch 2 mean test loss - extrapolation:96.73968418185662
Start training epoch 3
Changing learning rate to:1.0e-5
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.619354
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 61.732649294490926
Iteration: 2 || Loss: 60.21817833121861
Iteration: 3 || Loss: 58.77967364906107
Iteration: 4 || Loss: 57.41731018106718
Iteration: 5 || Loss: 56.13261876271237
Iteration: 6 || Loss: 56.13261876271237
saving ADAM checkpoint...
Sum of params:-95.62199
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.13261876271237
Iteration: 2 || Loss: 45.641805128065776
Iteration: 3 || Loss: 42.546285976814715
Iteration: 4 || Loss: 37.40380969734493
Iteration: 5 || Loss: 31.709538204901136
Iteration: 6 || Loss: 30.916732657482584
Iteration: 7 || Loss: 30.32058506184125
Iteration: 8 || Loss: 29.0944773419498
Iteration: 9 || Loss: 26.996602008358202
Iteration: 10 || Loss: 26.480109913605872
Iteration: 11 || Loss: 26.081580525026794
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.892296
Epoch 3 loss:26.081580525026794
MSE loss S0.45839443570464333
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-93.892296
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 99.39838852408839
Iteration: 2 || Loss: 99.17742255875163
Iteration: 3 || Loss: 99.02455622772423
Iteration: 4 || Loss: 98.93401052681348
Iteration: 5 || Loss: 98.8921925187414
Iteration: 6 || Loss: 98.8921925187414
saving ADAM checkpoint...
Sum of params:-93.8953
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 98.8921925187414
Iteration: 2 || Loss: 98.81351396810753
Iteration: 3 || Loss: 98.69601030200353
Iteration: 4 || Loss: 98.2954054438512
Iteration: 5 || Loss: 94.29183205317439
Iteration: 6 || Loss: 89.98542254979563
Iteration: 7 || Loss: 87.44178271859565
Iteration: 8 || Loss: 85.01498251412303
Iteration: 9 || Loss: 83.24586542782332
Iteration: 10 || Loss: 81.51918609766528
Iteration: 11 || Loss: 81.0593513876094
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-92.52146
Epoch 3 loss:81.0593513876094
MSE loss S0.914578510520824
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-92.52146
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 564.2075916119356
Iteration: 2 || Loss: 563.7563531831581
Iteration: 3 || Loss: 563.3550600111855
Iteration: 4 || Loss: 563.0035325780483
Iteration: 5 || Loss: 562.6999073436207
Iteration: 6 || Loss: 562.6999073436207
saving ADAM checkpoint...
Sum of params:-92.537575
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 562.6999073436207
Iteration: 2 || Loss: 561.4280796862474
Iteration: 3 || Loss: 560.1407847127927
Iteration: 4 || Loss: 508.49539139153546
Iteration: 5 || Loss: 498.0969393385171
Iteration: 6 || Loss: 492.1615354233332
Iteration: 7 || Loss: 471.4416813089833
Iteration: 8 || Loss: 461.08157921741554
Iteration: 9 || Loss: 454.70502436164605
Iteration: 10 || Loss: 449.53910634491467
Iteration: 11 || Loss: 447.4994161908216
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.76993
Epoch 3 loss:447.4994161908216
MSE loss S6.385972645093394
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:123.83252057549092
MSE loss S2.6923093018468345
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:639.9302126428947
MSE loss S12.348260560338716
waveform batch: 2/2
Test loss - extrapolation:480.07011827800824
MSE loss S9.100282315068048
Epoch 3 mean train loss:19.12552924494682
Epoch 3 mean test loss - interpolation:20.638753429248485
Epoch 3 mean test loss - extrapolation:93.33336091007526
Start training epoch 4
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-94.76993
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 54.16011551658677
Iteration: 2 || Loss: 52.93854174168777
Iteration: 3 || Loss: 51.78973286402651
Iteration: 4 || Loss: 50.714207392496675
Iteration: 5 || Loss: 49.71274047400702
Iteration: 6 || Loss: 49.71274047400702
saving ADAM checkpoint...
Sum of params:-94.77178
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.71274047400702
Iteration: 2 || Loss: 42.863067223738355
Iteration: 3 || Loss: 39.56590845767227
Iteration: 4 || Loss: 38.201018671303736
Iteration: 5 || Loss: 30.889087181229016
Iteration: 6 || Loss: 30.404100002050303
Iteration: 7 || Loss: 27.665361218504042
Iteration: 8 || Loss: 26.95387180901134
Iteration: 9 || Loss: 25.497878155333925
Iteration: 10 || Loss: 25.31079736771726
Iteration: 11 || Loss: 24.565838835909418
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.0733
Epoch 4 loss:24.565838835909418
MSE loss S0.5493016138894851
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-94.0733
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 93.6607588478782
Iteration: 2 || Loss: 93.52950645387234
Iteration: 3 || Loss: 93.46364806856585
Iteration: 4 || Loss: 93.42132210468215
Iteration: 5 || Loss: 93.37344673069327
Iteration: 6 || Loss: 93.37344673069327
saving ADAM checkpoint...
Sum of params:-94.06552
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 93.37344673069327
Iteration: 2 || Loss: 93.29787941169877
Iteration: 3 || Loss: 93.20349918871374
Iteration: 4 || Loss: 92.63227134906144
Iteration: 5 || Loss: 88.07156602793403
Iteration: 6 || Loss: 83.56939695095481
Iteration: 7 || Loss: 80.82855072054288
Iteration: 8 || Loss: 79.88542720954862
Iteration: 9 || Loss: 77.39893522528244
Iteration: 10 || Loss: 76.32845208880191
Iteration: 11 || Loss: 75.47486234017872
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-92.8035
Epoch 4 loss:75.47486234017872
MSE loss S1.06095246852753
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-92.8035
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 526.0717714923417
Iteration: 2 || Loss: 525.3413913027719
Iteration: 3 || Loss: 524.6659903438724
Iteration: 4 || Loss: 524.0453117569764
Iteration: 5 || Loss: 523.4799913483218
Iteration: 6 || Loss: 523.4799913483218
saving ADAM checkpoint...
Sum of params:-92.817314
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 523.4799913483218
Iteration: 2 || Loss: 521.7726883426076
Iteration: 3 || Loss: 519.9769063251616
Iteration: 4 || Loss: 482.9790303960475
Iteration: 5 || Loss: 469.74019062222334
Iteration: 6 || Loss: 463.97195332344523
Iteration: 7 || Loss: 449.4713332192829
Iteration: 8 || Loss: 437.17765613189505
Iteration: 9 || Loss: 431.2203847014798
Iteration: 10 || Loss: 425.9642435162377
Iteration: 11 || Loss: 424.52828995670137
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.81194
Epoch 4 loss:424.52828995670137
MSE loss S5.885062609150353
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:118.53462257896359
MSE loss S2.406392517699694
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:614.9505287307152
MSE loss S11.672250125182899
waveform batch: 2/2
Test loss - extrapolation:460.9004843247767
MSE loss S8.654031955081964
Epoch 4 mean train loss:18.088585901130674
Epoch 4 mean test loss - interpolation:19.755770429827265
Epoch 4 mean test loss - extrapolation:89.65425108795766
Start training epoch 5
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-94.81194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.839074756761676
Iteration: 2 || Loss: 48.65770397948479
Iteration: 3 || Loss: 47.550112041474364
Iteration: 4 || Loss: 46.51730559799363
Iteration: 5 || Loss: 45.55978552420294
Iteration: 6 || Loss: 45.55978552420294
saving ADAM checkpoint...
Sum of params:-94.81384
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.55978552420294
Iteration: 2 || Loss: 39.33164407342896
Iteration: 3 || Loss: 36.25148452379539
Iteration: 4 || Loss: 35.24736179283208
Iteration: 5 || Loss: 28.6850162120486
Iteration: 6 || Loss: 27.033650373426354
Iteration: 7 || Loss: 24.242845905414587
Iteration: 8 || Loss: 23.598269557541123
Iteration: 9 || Loss: 23.292711598138432
Iteration: 10 || Loss: 22.95233051790415
Iteration: 11 || Loss: 21.792750467944487
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.01098
Epoch 5 loss:21.792750467944487
MSE loss S0.5892335831284564
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-94.01098
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 88.90027219430006
Iteration: 2 || Loss: 88.78719290471359
Iteration: 3 || Loss: 88.68379894035705
Iteration: 4 || Loss: 88.59027691204844
Iteration: 5 || Loss: 88.50628853767796
Iteration: 6 || Loss: 88.50628853767796
saving ADAM checkpoint...
Sum of params:-94.00256
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 88.50628853767796
Iteration: 2 || Loss: 88.3779211092647
Iteration: 3 || Loss: 88.31741323989635
Iteration: 4 || Loss: 83.78888140146384
Iteration: 5 || Loss: 82.41894794118967
Iteration: 6 || Loss: 79.1958213451346
Iteration: 7 || Loss: 75.9986260732914
Iteration: 8 || Loss: 73.7123486295516
Iteration: 9 || Loss: 72.69355707938864
Iteration: 10 || Loss: 71.74338457643515
Iteration: 11 || Loss: 70.93745465119737
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.1487
Epoch 5 loss:70.93745465119737
MSE loss S0.9648485546455086
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-93.1487
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 500.819791562907
Iteration: 2 || Loss: 500.16357884258525
Iteration: 3 || Loss: 499.561769315804
Iteration: 4 || Loss: 499.0138279091485
Iteration: 5 || Loss: 498.5197616374542
Iteration: 6 || Loss: 498.5197616374542
saving ADAM checkpoint...
Sum of params:-93.161514
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 498.5197616374542
Iteration: 2 || Loss: 497.26714063069295
Iteration: 3 || Loss: 495.2014577748991
Iteration: 4 || Loss: 454.3156294224292
Iteration: 5 || Loss: 446.63265045673813
Iteration: 6 || Loss: 442.79159072036356
Iteration: 7 || Loss: 425.8520163551255
Iteration: 8 || Loss: 415.3109871165425
Iteration: 9 || Loss: 409.06752651644297
Iteration: 10 || Loss: 406.30349572194024
Iteration: 11 || Loss: 405.2490759396144
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.081726
Epoch 5 loss:405.2490759396144
MSE loss S5.300346433799952
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:114.69930011398426
MSE loss S2.196855327173041
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:598.7586491317496
MSE loss S10.992812468794337
waveform batch: 2/2
Test loss - extrapolation:445.30027985630244
MSE loss S8.052076098662583
Epoch 5 mean train loss:17.171699346853664
Epoch 5 mean test loss - interpolation:19.116550018997376
Epoch 5 mean test loss - extrapolation:87.00491074900434
Start training epoch 6
Changing learning rate to:1.0000000000000002e-6
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.081726
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.3269799003178
Iteration: 2 || Loss: 50.17967677862364
Iteration: 3 || Loss: 50.03345098328713
Iteration: 4 || Loss: 49.88763180046303
Iteration: 5 || Loss: 49.74269565349297
Iteration: 6 || Loss: 49.74269565349297
saving ADAM checkpoint...
Sum of params:-95.08191
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.74269565349297
Iteration: 2 || Loss: 36.11206302728675
Iteration: 3 || Loss: 32.7330103610822
Iteration: 4 || Loss: 31.966673708964898
Iteration: 5 || Loss: 26.346167386094002
Iteration: 6 || Loss: 25.679022263672998
Iteration: 7 || Loss: 25.130289804312657
Iteration: 8 || Loss: 21.354673796013273
Iteration: 9 || Loss: 20.8324984226878
Iteration: 10 || Loss: 20.74299626558276
Iteration: 11 || Loss: 20.043842518753845
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.44525
Epoch 6 loss:20.043842518753845
MSE loss S0.4776826810395528
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-94.44525
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 81.91132925397405
Iteration: 2 || Loss: 81.90059851886417
Iteration: 3 || Loss: 81.89046516591243
Iteration: 4 || Loss: 81.88136366320941
Iteration: 5 || Loss: 81.87266579554246
Iteration: 6 || Loss: 81.87266579554246
saving ADAM checkpoint...
Sum of params:-94.44516
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 81.87266579554246
Iteration: 2 || Loss: 81.78994203010704
Iteration: 3 || Loss: 81.67814628477852
Iteration: 4 || Loss: 80.92022646154396
Iteration: 5 || Loss: 78.70054152325905
Iteration: 6 || Loss: 76.17006126104681
Iteration: 7 || Loss: 72.86944968564585
Iteration: 8 || Loss: 70.88603684475855
Iteration: 9 || Loss: 69.34305914813783
Iteration: 10 || Loss: 68.64975093013662
Iteration: 11 || Loss: 67.73615830994473
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.487656
Epoch 6 loss:67.73615830994473
MSE loss S0.9039361014216944
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-93.487656
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 478.7411005409958
Iteration: 2 || Loss: 478.6813000016159
Iteration: 3 || Loss: 478.62216610450207
Iteration: 4 || Loss: 478.56325272806845
Iteration: 5 || Loss: 478.50545280563847
Iteration: 6 || Loss: 478.50545280563847
saving ADAM checkpoint...
Sum of params:-93.488945
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 478.50545280563847
Iteration: 2 || Loss: 476.68235596646673
Iteration: 3 || Loss: 474.5387075811388
Iteration: 4 || Loss: 438.6276581553631
Iteration: 5 || Loss: 427.4955842289488
Iteration: 6 || Loss: 422.15366699166555
Iteration: 7 || Loss: 416.9608313409991
Iteration: 8 || Loss: 404.0306259103404
Iteration: 9 || Loss: 398.8857807159944
Iteration: 10 || Loss: 394.35242297354023
Iteration: 11 || Loss: 390.7387044672041
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.28656
Epoch 6 loss:390.7387044672041
MSE loss S5.1890529968476695
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:108.8070101052294
MSE loss S2.1084288038277954
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:586.2467929291344
MSE loss S10.82038316009196
waveform batch: 2/2
Test loss - extrapolation:438.0384946089461
MSE loss S7.9519212726619415
Epoch 6 mean train loss:16.50064501020354
Epoch 6 mean test loss - interpolation:18.1345016842049
Epoch 6 mean test loss - extrapolation:85.35710729484003
Start training epoch 7
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.28656
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.09220198132373
Iteration: 2 || Loss: 44.96210138441731
Iteration: 3 || Loss: 44.8328535910776
Iteration: 4 || Loss: 44.704248233978895
Iteration: 5 || Loss: 44.576451478690885
Iteration: 6 || Loss: 44.576451478690885
saving ADAM checkpoint...
Sum of params:-95.28686
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.576451478690885
Iteration: 2 || Loss: 33.91622410218225
Iteration: 3 || Loss: 30.609072547499174
Iteration: 4 || Loss: 29.732673666513758
Iteration: 5 || Loss: 24.117229934546177
Iteration: 6 || Loss: 23.643330340560365
Iteration: 7 || Loss: 22.97805998160231
Iteration: 8 || Loss: 20.01810785061828
Iteration: 9 || Loss: 19.253425007843713
Iteration: 10 || Loss: 19.20454380892881
Iteration: 11 || Loss: 18.836880670775724
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.654854
Epoch 7 loss:18.836880670775724
MSE loss S0.435676288781237
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-94.654854
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 78.30456759712567
Iteration: 2 || Loss: 78.26347376667013
Iteration: 3 || Loss: 78.22334600986193
Iteration: 4 || Loss: 78.18409882083668
Iteration: 5 || Loss: 78.14596993766993
Iteration: 6 || Loss: 78.14596993766993
saving ADAM checkpoint...
Sum of params:-94.65492
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 78.14596993766993
Iteration: 2 || Loss: 77.31339618490895
Iteration: 3 || Loss: 77.13970588703786
Iteration: 4 || Loss: 76.49517197534159
Iteration: 5 || Loss: 73.93723920657142
Iteration: 6 || Loss: 72.67711167977775
Iteration: 7 || Loss: 70.5641927058937
Iteration: 8 || Loss: 68.72324602577413
Iteration: 9 || Loss: 67.09037639530115
Iteration: 10 || Loss: 66.60656323585972
Iteration: 11 || Loss: 66.34732783545805
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.67798
Epoch 7 loss:66.34732783545805
MSE loss S0.9375231556020507
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-93.67798
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 457.41084317024234
Iteration: 2 || Loss: 457.3510254204679
Iteration: 3 || Loss: 457.2916909645194
Iteration: 4 || Loss: 457.2331085281315
Iteration: 5 || Loss: 457.1748666185528
Iteration: 6 || Loss: 457.1748666185528
saving ADAM checkpoint...
Sum of params:-93.679085
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 457.1748666185528
Iteration: 2 || Loss: 455.42285102135764
Iteration: 3 || Loss: 453.4791470093655
Iteration: 4 || Loss: 419.0570920626637
Iteration: 5 || Loss: 409.1841128336798
Iteration: 6 || Loss: 404.36340570261814
Iteration: 7 || Loss: 396.37029525858276
Iteration: 8 || Loss: 389.30269210897774
Iteration: 9 || Loss: 385.99150745708437
Iteration: 10 || Loss: 381.5702542202802
Iteration: 11 || Loss: 380.547312894807
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.40976
Epoch 7 loss:380.547312894807
MSE loss S4.1018665419941165
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:103.32639646997875
MSE loss S1.4172686788328581
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:573.7001731801669
MSE loss S9.508108212326233
waveform batch: 2/2
Test loss - extrapolation:430.01890142107726
MSE loss S7.130897033581613
Epoch 7 mean train loss:16.05970763451865
Epoch 7 mean test loss - interpolation:17.22106607832979
Epoch 7 mean test loss - extrapolation:83.64325621677034
Start training epoch 8
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.40976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.91732490026941
Iteration: 2 || Loss: 39.780294468026675
Iteration: 3 || Loss: 39.64381290916277
Iteration: 4 || Loss: 39.50822309029107
Iteration: 5 || Loss: 39.37322700010921
Iteration: 6 || Loss: 39.37322700010921
saving ADAM checkpoint...
Sum of params:-95.41012
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.37322700010921
Iteration: 2 || Loss: 27.781602788623097
Iteration: 3 || Loss: 26.43345447400876
Iteration: 4 || Loss: 25.87826720414895
Iteration: 5 || Loss: 22.60878938834372
Iteration: 6 || Loss: 21.13023244173865
Iteration: 7 || Loss: 20.806677671874407
Iteration: 8 || Loss: 20.655345815146546
Iteration: 9 || Loss: 19.475009297869413
Iteration: 10 || Loss: 18.207583165996944
Iteration: 11 || Loss: 17.77277394989627
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.63971
Epoch 8 loss:17.77277394989627
MSE loss S0.47458267812450916
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-94.63971
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.20854387736475
Iteration: 2 || Loss: 74.18470786927733
Iteration: 3 || Loss: 74.16163474805045
Iteration: 4 || Loss: 74.13942113312123
Iteration: 5 || Loss: 74.11795036240272
Iteration: 6 || Loss: 74.11795036240272
saving ADAM checkpoint...
Sum of params:-94.63899
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.11795036240272
Iteration: 2 || Loss: 73.80820791036294
Iteration: 3 || Loss: 73.58563288345273
Iteration: 4 || Loss: 72.90534707462358
Iteration: 5 || Loss: 69.74455190919875
Iteration: 6 || Loss: 66.86546731734441
Iteration: 7 || Loss: 65.5633500769607
Iteration: 8 || Loss: 64.83459060714546
Iteration: 9 || Loss: 64.35226168257574
Iteration: 10 || Loss: 63.66488621611781
Iteration: 11 || Loss: 63.23204845394146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.78022
Epoch 8 loss:63.23204845394146
MSE loss S0.8299497652859864
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-93.78022
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 439.6797294681409
Iteration: 2 || Loss: 439.6105378108012
Iteration: 3 || Loss: 439.54177850553714
Iteration: 4 || Loss: 439.4737015845325
Iteration: 5 || Loss: 439.40621968006167
Iteration: 6 || Loss: 439.40621968006167
saving ADAM checkpoint...
Sum of params:-93.78216
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 439.40621968006167
Iteration: 2 || Loss: 437.0403421248646
Iteration: 3 || Loss: 435.5388810758218
Iteration: 4 || Loss: 409.951411152448
Iteration: 5 || Loss: 398.8002912750632
Iteration: 6 || Loss: 394.46004822103555
Iteration: 7 || Loss: 388.75810224376744
Iteration: 8 || Loss: 375.23707202910924
Iteration: 9 || Loss: 371.65748190900473
Iteration: 10 || Loss: 369.8211387277823
Iteration: 11 || Loss: 367.512045923352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.63492
Epoch 8 loss:367.512045923352
MSE loss S4.55650897966102
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:103.5943854499354
MSE loss S1.72115484201278
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:556.1250062642665
MSE loss S9.822313238915562
waveform batch: 2/2
Test loss - extrapolation:415.04124027206734
MSE loss S7.250695974118442
Epoch 8 mean train loss:15.466098907834128
Epoch 8 mean test loss - interpolation:17.265730908322567
Epoch 8 mean test loss - extrapolation:80.93052054469449
Start training epoch 9
Changing learning rate to:1.0000000000000002e-7
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.63492
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.655473558876196
Iteration: 2 || Loss: 40.64232931071004
Iteration: 3 || Loss: 40.62918892878607
Iteration: 4 || Loss: 40.61603926062791
Iteration: 5 || Loss: 40.60299892560113
Iteration: 6 || Loss: 40.60299892560113
saving ADAM checkpoint...
Sum of params:-95.63492
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.60299892560113
Iteration: 2 || Loss: 29.245919669620207
Iteration: 3 || Loss: 26.992696753975206
Iteration: 4 || Loss: 26.506707795202296
Iteration: 5 || Loss: 23.04102660301096
Iteration: 6 || Loss: 21.06358047369768
Iteration: 7 || Loss: 20.86937047420762
Iteration: 8 || Loss: 20.530548293499887
Iteration: 9 || Loss: 18.686315751755586
Iteration: 10 || Loss: 18.055911561569864
Iteration: 11 || Loss: 17.998432185491104
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.968575
Epoch 9 loss:17.998432185491104
MSE loss S0.4817161814334288
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-94.968575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.18056692599029
Iteration: 2 || Loss: 74.17756292902216
Iteration: 3 || Loss: 74.17455435614737
Iteration: 4 || Loss: 74.17162886014461
Iteration: 5 || Loss: 74.1686595251018
Iteration: 6 || Loss: 74.1686595251018
saving ADAM checkpoint...
Sum of params:-94.96854
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.1686595251018
Iteration: 2 || Loss: 73.64801643708505
Iteration: 3 || Loss: 73.46169711773035
Iteration: 4 || Loss: 72.81416076219274
Iteration: 5 || Loss: 69.67448136604463
Iteration: 6 || Loss: 64.63415497846398
Iteration: 7 || Loss: 63.75955485954761
Iteration: 8 || Loss: 63.31615956951808
Iteration: 9 || Loss: 61.72136514293718
Iteration: 10 || Loss: 61.20445637837031
Iteration: 11 || Loss: 60.80676402154198
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.02908
Epoch 9 loss:60.80676402154198
MSE loss S0.7951049297095778
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.02908
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 428.4965382009888
Iteration: 2 || Loss: 428.49018181571836
Iteration: 3 || Loss: 428.48380185329603
Iteration: 4 || Loss: 428.4776094162938
Iteration: 5 || Loss: 428.4713631522637
Iteration: 6 || Loss: 428.4713631522637
saving ADAM checkpoint...
Sum of params:-94.029274
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 428.4713631522637
Iteration: 2 || Loss: 426.3752508098004
Iteration: 3 || Loss: 424.64641585155806
Iteration: 4 || Loss: 393.1652795654761
Iteration: 5 || Loss: 382.4779575997014
Iteration: 6 || Loss: 378.8896084147072
Iteration: 7 || Loss: 372.888032290604
Iteration: 8 || Loss: 362.53181463812365
Iteration: 9 || Loss: 359.39594317605633
Iteration: 10 || Loss: 356.844639451603
Iteration: 11 || Loss: 356.1472690908979
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.70044
Epoch 9 loss:356.1472690908979
MSE loss S4.046903528337013
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:98.17141110934865
MSE loss S1.432051713698879
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:545.2245426379561
MSE loss S9.300029057493523
waveform batch: 2/2
Test loss - extrapolation:408.1447830210115
MSE loss S6.847670471440027
Epoch 9 mean train loss:14.998360872342447
Epoch 9 mean test loss - interpolation:16.36190185155811
Epoch 9 mean test loss - extrapolation:79.44744380491397
Start training epoch 10
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.70044
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.10049352500193
Iteration: 2 || Loss: 37.08735555514143
Iteration: 3 || Loss: 37.07437228125702
Iteration: 4 || Loss: 37.06148027017268
Iteration: 5 || Loss: 37.048485447125216
Iteration: 6 || Loss: 37.048485447125216
saving ADAM checkpoint...
Sum of params:-95.70046
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.048485447125216
Iteration: 2 || Loss: 26.035892135263875
Iteration: 3 || Loss: 24.605906905068682
Iteration: 4 || Loss: 24.209720156629057
Iteration: 5 || Loss: 20.99321993431947
Iteration: 6 || Loss: 19.58086655082506
Iteration: 7 || Loss: 19.227457627152063
Iteration: 8 || Loss: 18.995578949304843
Iteration: 9 || Loss: 18.102557534406728
Iteration: 10 || Loss: 17.000648892824767
Iteration: 11 || Loss: 16.81324753765311
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.016075
Epoch 10 loss:16.81324753765311
MSE loss S0.4536816802894791
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.016075
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.6396814883694
Iteration: 2 || Loss: 69.63701352026972
Iteration: 3 || Loss: 69.63439230178749
Iteration: 4 || Loss: 69.6317614108551
Iteration: 5 || Loss: 69.62913667458047
Iteration: 6 || Loss: 69.62913667458047
saving ADAM checkpoint...
Sum of params:-95.01598
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.62913667458047
Iteration: 2 || Loss: 69.21153570128202
Iteration: 3 || Loss: 69.0460838635288
Iteration: 4 || Loss: 68.59453724776077
Iteration: 5 || Loss: 66.02946726931278
Iteration: 6 || Loss: 62.91286991379673
Iteration: 7 || Loss: 61.64016987528306
Iteration: 8 || Loss: 60.6927097194428
Iteration: 9 || Loss: 60.22854950088727
Iteration: 10 || Loss: 59.55113351379271
Iteration: 11 || Loss: 59.172952592651285
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.14413
Epoch 10 loss:59.172952592651285
MSE loss S0.8310790197752456
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.14413
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 411.54502383265736
Iteration: 2 || Loss: 411.53786741376916
Iteration: 3 || Loss: 411.5307077343021
Iteration: 4 || Loss: 411.52363970278253
Iteration: 5 || Loss: 411.51671923242503
Iteration: 6 || Loss: 411.51671923242503
saving ADAM checkpoint...
Sum of params:-94.144264
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 411.51671923242503
Iteration: 2 || Loss: 408.85428157621465
Iteration: 3 || Loss: 407.39228145582086
Iteration: 4 || Loss: 384.2376922442282
Iteration: 5 || Loss: 371.6003637444999
Iteration: 6 || Loss: 367.89937214924595
Iteration: 7 || Loss: 362.46742701568206
Iteration: 8 || Loss: 350.1841932143887
Iteration: 9 || Loss: 349.3279690931092
Iteration: 10 || Loss: 347.84761786176523
Iteration: 11 || Loss: 346.7219258502887
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.76586
Epoch 10 loss:346.7219258502887
MSE loss S4.0727639209726405
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:90.49118179458547
MSE loss S1.1579365263547734
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:530.4792146931715
MSE loss S9.145712036219088
waveform batch: 2/2
Test loss - extrapolation:404.0135362686125
MSE loss S6.974023452073758
Epoch 10 mean train loss:14.576142275192865
Epoch 10 mean test loss - interpolation:15.081863632430911
Epoch 10 mean test loss - extrapolation:77.87439591348199
Start training epoch 11
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.76586
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.684982280473278
Iteration: 2 || Loss: 28.675730171359717
Iteration: 3 || Loss: 28.666539058300337
Iteration: 4 || Loss: 28.657308992589222
Iteration: 5 || Loss: 28.648007056365902
Iteration: 6 || Loss: 28.648007056365902
saving ADAM checkpoint...
Sum of params:-95.765854
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.648007056365902
Iteration: 2 || Loss: 23.068793048766825
Iteration: 3 || Loss: 22.40126158639908
Iteration: 4 || Loss: 21.64239596503164
Iteration: 5 || Loss: 19.578252081979635
Iteration: 6 || Loss: 18.067062826468543
Iteration: 7 || Loss: 17.66549887160448
Iteration: 8 || Loss: 17.448545647399676
Iteration: 9 || Loss: 16.849714180726046
Iteration: 10 || Loss: 16.192750911327998
Iteration: 11 || Loss: 16.133713712486546
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.18009
Epoch 11 loss:16.133713712486546
MSE loss S0.4316599059016847
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.18009
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.89173656319745
Iteration: 2 || Loss: 66.88934311089659
Iteration: 3 || Loss: 66.88696811945356
Iteration: 4 || Loss: 66.88460392642715
Iteration: 5 || Loss: 66.88226837325128
Iteration: 6 || Loss: 66.88226837325128
saving ADAM checkpoint...
Sum of params:-95.180016
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.88226837325128
Iteration: 2 || Loss: 66.54067370107359
Iteration: 3 || Loss: 66.40440876422298
Iteration: 4 || Loss: 66.09349339819137
Iteration: 5 || Loss: 63.588966527023786
Iteration: 6 || Loss: 60.24703064692884
Iteration: 7 || Loss: 59.11529289386091
Iteration: 8 || Loss: 58.77164652194581
Iteration: 9 || Loss: 58.05838259123349
Iteration: 10 || Loss: 57.511263715125324
Iteration: 11 || Loss: 57.06825949810111
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.21271
Epoch 11 loss:57.06825949810111
MSE loss S0.8572508997578907
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.21271
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 409.2157335224841
Iteration: 2 || Loss: 409.20856040677376
Iteration: 3 || Loss: 409.20149466638037
Iteration: 4 || Loss: 409.19449676064465
Iteration: 5 || Loss: 409.18761974271524
Iteration: 6 || Loss: 409.18761974271524
saving ADAM checkpoint...
Sum of params:-94.21283
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 409.18761974271524
Iteration: 2 || Loss: 406.5365278048882
Iteration: 3 || Loss: 404.6295062831547
Iteration: 4 || Loss: 376.71616940531686
Iteration: 5 || Loss: 362.2317215983815
Iteration: 6 || Loss: 358.7787347410907
Iteration: 7 || Loss: 354.400008752964
Iteration: 8 || Loss: 339.62325158609565
Iteration: 9 || Loss: 338.85252103828844
Iteration: 10 || Loss: 337.9003188765109
Iteration: 11 || Loss: 334.7016487393886
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.92573
Epoch 11 loss:334.7016487393886
MSE loss S4.099211870014711
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:88.53679029066816
MSE loss S1.1801187989913964
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:516.344252196827
MSE loss S9.189301807487267
waveform batch: 2/2
Test loss - extrapolation:392.5604734707352
MSE loss S6.85184854183554
Epoch 11 mean train loss:14.065642136206078
Epoch 11 mean test loss - interpolation:14.75613171511136
Epoch 11 mean test loss - extrapolation:75.74206047229684
Start training epoch 12
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.92573
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.379744230029598
Iteration: 2 || Loss: 27.37064552861056
Iteration: 3 || Loss: 27.361656508944574
Iteration: 4 || Loss: 27.352680669731814
Iteration: 5 || Loss: 27.34366811656267
Iteration: 6 || Loss: 27.34366811656267
saving ADAM checkpoint...
Sum of params:-95.925735
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.34366811656267
Iteration: 2 || Loss: 22.116301221829612
Iteration: 3 || Loss: 21.551538576292792
Iteration: 4 || Loss: 20.00492820052324
Iteration: 5 || Loss: 18.53408423158981
Iteration: 6 || Loss: 17.350591999690156
Iteration: 7 || Loss: 17.11253892127247
Iteration: 8 || Loss: 16.882445115171123
Iteration: 9 || Loss: 16.213398574001786
Iteration: 10 || Loss: 15.656754106719886
Iteration: 11 || Loss: 15.613111830553096
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.40271
Epoch 12 loss:15.613111830553096
MSE loss S0.4563445451755753
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.40271
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 65.18823888778485
Iteration: 2 || Loss: 65.18522936114802
Iteration: 3 || Loss: 65.1822719348848
Iteration: 4 || Loss: 65.17931486170077
Iteration: 5 || Loss: 65.17632955805212
Iteration: 6 || Loss: 65.17632955805212
saving ADAM checkpoint...
Sum of params:-95.40271
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 65.17632955805212
Iteration: 2 || Loss: 64.65784217967442
Iteration: 3 || Loss: 64.53275926450544
Iteration: 4 || Loss: 64.14514045065388
Iteration: 5 || Loss: 62.04588412934408
Iteration: 6 || Loss: 59.21728742649226
Iteration: 7 || Loss: 57.896248080335376
Iteration: 8 || Loss: 57.04336469149303
Iteration: 9 || Loss: 55.99783943444283
Iteration: 10 || Loss: 55.86861664137195
Iteration: 11 || Loss: 55.466172114607666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.59871
Epoch 12 loss:55.466172114607666
MSE loss S0.9171759390076651
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.59871
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 391.8003584492692
Iteration: 2 || Loss: 391.7929589821633
Iteration: 3 || Loss: 391.7855881516553
Iteration: 4 || Loss: 391.77819166836565
Iteration: 5 || Loss: 391.7707721459915
Iteration: 6 || Loss: 391.7707721459915
saving ADAM checkpoint...
Sum of params:-94.59884
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 391.7707721459915
Iteration: 2 || Loss: 388.93187948791746
Iteration: 3 || Loss: 386.80679348288237
Iteration: 4 || Loss: 360.6882098567991
Iteration: 5 || Loss: 347.25876098394485
Iteration: 6 || Loss: 344.6517572148519
Iteration: 7 || Loss: 341.5610769586138
Iteration: 8 || Loss: 329.45313339881136
Iteration: 9 || Loss: 328.05487997129114
Iteration: 10 || Loss: 326.4824118557792
Iteration: 11 || Loss: 325.8350696151148
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.053276
Epoch 12 loss:325.8350696151148
MSE loss S3.6488606138893864
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:85.72782102247933
MSE loss S1.029157058055156
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:507.70759106383497
MSE loss S8.605468126783776
waveform batch: 2/2
Test loss - extrapolation:384.86187673922655
MSE loss S6.479738181441477
Epoch 12 mean train loss:13.686701846906054
Epoch 12 mean test loss - interpolation:14.287970170413223
Epoch 12 mean test loss - extrapolation:74.38078898358846
Start training epoch 13
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.053276
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.069589422506407
Iteration: 2 || Loss: 26.06044332118996
Iteration: 3 || Loss: 26.05138259325983
Iteration: 4 || Loss: 26.04217779812479
Iteration: 5 || Loss: 26.033201802009255
Iteration: 6 || Loss: 26.033201802009255
saving ADAM checkpoint...
Sum of params:-96.05328
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.033201802009255
Iteration: 2 || Loss: 20.576995159842447
Iteration: 3 || Loss: 20.073849503264746
Iteration: 4 || Loss: 18.78817541100898
Iteration: 5 || Loss: 17.57362463274987
Iteration: 6 || Loss: 16.114460570500647
Iteration: 7 || Loss: 15.811725873575227
Iteration: 8 || Loss: 15.655907810831101
Iteration: 9 || Loss: 15.124615929687952
Iteration: 10 || Loss: 14.824889276533238
Iteration: 11 || Loss: 14.737180090367449
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.44036
Epoch 13 loss:14.737180090367449
MSE loss S0.4087196769033554
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.44036
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 61.46682651540224
Iteration: 2 || Loss: 61.46466132193096
Iteration: 3 || Loss: 61.46249821834666
Iteration: 4 || Loss: 61.46033930418492
Iteration: 5 || Loss: 61.45822355122793
Iteration: 6 || Loss: 61.45822355122793
saving ADAM checkpoint...
Sum of params:-95.44027
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 61.45822355122793
Iteration: 2 || Loss: 61.18086200974953
Iteration: 3 || Loss: 61.07448084775941
Iteration: 4 || Loss: 60.82905589412352
Iteration: 5 || Loss: 58.91770745542443
Iteration: 6 || Loss: 56.05999353541409
Iteration: 7 || Loss: 55.23849040862528
Iteration: 8 || Loss: 54.940868722092034
Iteration: 9 || Loss: 54.40253396504283
Iteration: 10 || Loss: 54.10386014172104
Iteration: 11 || Loss: 53.5582422859711
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.4863
Epoch 13 loss:53.5582422859711
MSE loss S0.8200533805411826
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.4863
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 387.37016369621745
Iteration: 2 || Loss: 387.3625604177833
Iteration: 3 || Loss: 387.35497597139874
Iteration: 4 || Loss: 387.3475058042867
Iteration: 5 || Loss: 387.3396718828375
Iteration: 6 || Loss: 387.3396718828375
saving ADAM checkpoint...
Sum of params:-94.48643
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 387.3396718828375
Iteration: 2 || Loss: 384.2561234832961
Iteration: 3 || Loss: 382.4683910830023
Iteration: 4 || Loss: 358.23090160746517
Iteration: 5 || Loss: 341.6458945551841
Iteration: 6 || Loss: 338.3543072663409
Iteration: 7 || Loss: 334.4436535951482
Iteration: 8 || Loss: 320.4638598124588
Iteration: 9 || Loss: 319.3971597861683
Iteration: 10 || Loss: 318.9093730761398
Iteration: 11 || Loss: 316.0792044367917
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.17885
Epoch 13 loss:316.0792044367917
MSE loss S3.9492282728157067
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:87.36926969024773
MSE loss S1.2995434153364493
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:496.03062455783686
MSE loss S8.902100553495654
waveform batch: 2/2
Test loss - extrapolation:373.8893642214636
MSE loss S6.569772646825134
Epoch 13 mean train loss:13.254297476314836
Epoch 13 mean test loss - interpolation:14.561544948374623
Epoch 13 mean test loss - extrapolation:72.49333239827503
Start training epoch 14
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.17885
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.72305742254706
Iteration: 2 || Loss: 26.715296609449712
Iteration: 3 || Loss: 26.707539099759103
Iteration: 4 || Loss: 26.69974316608254
Iteration: 5 || Loss: 26.691999574646843
Iteration: 6 || Loss: 26.691999574646843
saving ADAM checkpoint...
Sum of params:-96.17885
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.691999574646843
Iteration: 2 || Loss: 22.640312525514474
Iteration: 3 || Loss: 21.440069974649013
Iteration: 4 || Loss: 20.093647813587182
Iteration: 5 || Loss: 17.81754371519949
Iteration: 6 || Loss: 16.00263875275191
Iteration: 7 || Loss: 15.819480837532218
Iteration: 8 || Loss: 15.263504278677045
Iteration: 9 || Loss: 14.693945417948822
Iteration: 10 || Loss: 14.650720884337789
Iteration: 11 || Loss: 14.38685810882093
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.71719
Epoch 14 loss:14.38685810882093
MSE loss S0.39623366776481184
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.71719
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 60.8945238427019
Iteration: 2 || Loss: 60.891679985827764
Iteration: 3 || Loss: 60.88877183170991
Iteration: 4 || Loss: 60.88591005537429
Iteration: 5 || Loss: 60.883061023585434
Iteration: 6 || Loss: 60.883061023585434
saving ADAM checkpoint...
Sum of params:-95.71709
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 60.883061023585434
Iteration: 2 || Loss: 60.41739137109233
Iteration: 3 || Loss: 60.30928385218196
Iteration: 4 || Loss: 58.76748117275437
Iteration: 5 || Loss: 58.54689646851474
Iteration: 6 || Loss: 57.10930242585585
Iteration: 7 || Loss: 54.83086925104242
Iteration: 8 || Loss: 53.97760054116911
Iteration: 9 || Loss: 53.22263089153708
Iteration: 10 || Loss: 52.79667072402631
Iteration: 11 || Loss: 52.655942238436324
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.88171
Epoch 14 loss:52.655942238436324
MSE loss S0.7911841991685592
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.88171
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 366.18160865821403
Iteration: 2 || Loss: 366.17429307116294
Iteration: 3 || Loss: 366.1670194151046
Iteration: 4 || Loss: 366.15964365411213
Iteration: 5 || Loss: 366.1523219769416
Iteration: 6 || Loss: 366.1523219769416
saving ADAM checkpoint...
Sum of params:-94.88183
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 366.1523219769416
Iteration: 2 || Loss: 363.42934329660824
Iteration: 3 || Loss: 361.92828201751246
Iteration: 4 || Loss: 339.36744196464065
Iteration: 5 || Loss: 327.40907686341586
Iteration: 6 || Loss: 323.93571313207343
Iteration: 7 || Loss: 320.57589464866993
Iteration: 8 || Loss: 310.52175017312265
Iteration: 9 || Loss: 309.9722823724017
Iteration: 10 || Loss: 308.61079118187746
Iteration: 11 || Loss: 307.9899438762759
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.27542
Epoch 14 loss:307.9899438762759
MSE loss S3.5846629160750743
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:78.8552483073606
MSE loss S0.9504167111179509
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:488.0062365234705
MSE loss S8.431468387316162
waveform batch: 2/2
Test loss - extrapolation:371.5527274648698
MSE loss S6.338020410271507
Epoch 14 mean train loss:12.932163593914936
Epoch 14 mean test loss - interpolation:13.142541384560099
Epoch 14 mean test loss - extrapolation:71.62991366569503
Start training epoch 15
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.27542
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.974759507840584
Iteration: 2 || Loss: 21.966904815255674
Iteration: 3 || Loss: 21.9591773177442
Iteration: 4 || Loss: 21.951394168039435
Iteration: 5 || Loss: 21.943692778889016
Iteration: 6 || Loss: 21.943692778889016
saving ADAM checkpoint...
Sum of params:-96.275406
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.943692778889016
Iteration: 2 || Loss: 18.028530381443105
Iteration: 3 || Loss: 17.63004947279286
Iteration: 4 || Loss: 17.34707589818514
Iteration: 5 || Loss: 15.878261854007032
Iteration: 6 || Loss: 14.785144373999485
Iteration: 7 || Loss: 14.485975904694733
Iteration: 8 || Loss: 14.38518056453477
Iteration: 9 || Loss: 14.256975769527022
Iteration: 10 || Loss: 13.75586895956265
Iteration: 11 || Loss: 13.643109919336341
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.77566
Epoch 15 loss:13.643109919336341
MSE loss S0.4092317491610329
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.77566
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 57.41995442380464
Iteration: 2 || Loss: 57.418287195863535
Iteration: 3 || Loss: 57.41666883663381
Iteration: 4 || Loss: 57.41514097292515
Iteration: 5 || Loss: 57.41354070333632
Iteration: 6 || Loss: 57.41354070333632
saving ADAM checkpoint...
Sum of params:-95.77558
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 57.41354070333632
Iteration: 2 || Loss: 57.2548983005984
Iteration: 3 || Loss: 57.161481505915646
Iteration: 4 || Loss: 56.91476735127121
Iteration: 5 || Loss: 55.43652943964696
Iteration: 6 || Loss: 53.260604719637186
Iteration: 7 || Loss: 52.5251597827682
Iteration: 8 || Loss: 52.04326666171219
Iteration: 9 || Loss: 51.52454460486173
Iteration: 10 || Loss: 51.18669397379161
Iteration: 11 || Loss: 50.821745324908804
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.75884
Epoch 15 loss:50.821745324908804
MSE loss S0.7431643325066011
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.75884
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 367.5879355026156
Iteration: 2 || Loss: 367.5799635833414
Iteration: 3 || Loss: 367.57242005618644
Iteration: 4 || Loss: 367.5647077589235
Iteration: 5 || Loss: 367.5568856552318
Iteration: 6 || Loss: 367.5568856552318
saving ADAM checkpoint...
Sum of params:-94.75897
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 367.5568856552318
Iteration: 2 || Loss: 364.4232575588027
Iteration: 3 || Loss: 362.783644868572
Iteration: 4 || Loss: 338.3514917562228
Iteration: 5 || Loss: 322.28588033093615
Iteration: 6 || Loss: 319.48636865490766
Iteration: 7 || Loss: 316.03539218078174
Iteration: 8 || Loss: 303.2744867654132
Iteration: 9 || Loss: 302.5739893966475
Iteration: 10 || Loss: 301.44142604619196
Iteration: 11 || Loss: 300.0265753098856
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.19365
Epoch 15 loss:300.0265753098856
MSE loss S3.5214651997895485
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:78.58228423619545
MSE loss S1.0040559410237977
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:479.9157767175791
MSE loss S8.275080196199138
waveform batch: 2/2
Test loss - extrapolation:363.6333808292293
MSE loss S6.178446992654699
Epoch 15 mean train loss:12.568670019107955
Epoch 15 mean test loss - interpolation:13.097047372699242
Epoch 15 mean test loss - extrapolation:70.2957631289007
Start training epoch 16
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.19365
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.25697805301621
Iteration: 2 || Loss: 21.250533900615537
Iteration: 3 || Loss: 21.24404385964487
Iteration: 4 || Loss: 21.237553901106416
Iteration: 5 || Loss: 21.23100927357285
Iteration: 6 || Loss: 21.23100927357285
saving ADAM checkpoint...
Sum of params:-96.193634
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.23100927357285
Iteration: 2 || Loss: 18.39495148017433
Iteration: 3 || Loss: 17.84826094495136
Iteration: 4 || Loss: 16.847943337834376
Iteration: 5 || Loss: 15.026747473603427
Iteration: 6 || Loss: 14.00768571918841
Iteration: 7 || Loss: 13.830404402361987
Iteration: 8 || Loss: 13.68836656269733
Iteration: 9 || Loss: 13.356081691013099
Iteration: 10 || Loss: 13.308003094035943
Iteration: 11 || Loss: 13.091366019623425
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.760956
Epoch 16 loss:13.091366019623425
MSE loss S0.3772203048960047
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.760956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 55.41051621965096
Iteration: 2 || Loss: 55.40834064605012
Iteration: 3 || Loss: 55.40620470897793
Iteration: 4 || Loss: 55.40406926665487
Iteration: 5 || Loss: 55.401961790980096
Iteration: 6 || Loss: 55.401961790980096
saving ADAM checkpoint...
Sum of params:-95.76095
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 55.401961790980096
Iteration: 2 || Loss: 55.130365539970725
Iteration: 3 || Loss: 55.04689916767326
Iteration: 4 || Loss: 54.81300981540048
Iteration: 5 || Loss: 53.73325036947819
Iteration: 6 || Loss: 52.12130438451455
Iteration: 7 || Loss: 50.78677840091156
Iteration: 8 || Loss: 50.485192603299225
Iteration: 9 || Loss: 50.12086188514708
Iteration: 10 || Loss: 49.7518244896304
Iteration: 11 || Loss: 49.59835338152918
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.891815
Epoch 16 loss:49.59835338152918
MSE loss S0.7864658783493077
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.891815
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 353.49618023066904
Iteration: 2 || Loss: 353.4882102613137
Iteration: 3 || Loss: 353.480255992945
Iteration: 4 || Loss: 353.47224328740646
Iteration: 5 || Loss: 353.4642570701647
Iteration: 6 || Loss: 353.4642570701647
saving ADAM checkpoint...
Sum of params:-94.89194
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 353.4642570701647
Iteration: 2 || Loss: 350.1506328766234
Iteration: 3 || Loss: 348.6008493307336
Iteration: 4 || Loss: 328.3674398428628
Iteration: 5 || Loss: 312.2896750624676
Iteration: 6 || Loss: 309.4541242006862
Iteration: 7 || Loss: 306.7912992170834
Iteration: 8 || Loss: 296.0808627982512
Iteration: 9 || Loss: 295.18393364637166
Iteration: 10 || Loss: 294.58136553437146
Iteration: 11 || Loss: 292.55741074576105
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.363785
Epoch 16 loss:292.55741074576105
MSE loss S3.4912277512384495
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:77.73032887325205
MSE loss S1.0187990662324542
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:470.56855265386747
MSE loss S8.214824904211458
waveform batch: 2/2
Test loss - extrapolation:355.28631258043106
MSE loss S6.085859973304645
Epoch 16 mean train loss:12.249901039548748
Epoch 16 mean test loss - interpolation:12.955054812208674
Epoch 16 mean test loss - extrapolation:68.82123876952488
Start training epoch 17
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.363785
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.820769316115832
Iteration: 2 || Loss: 20.81439861186279
Iteration: 3 || Loss: 20.808088686189024
Iteration: 4 || Loss: 20.801724112594343
Iteration: 5 || Loss: 20.795335564717163
Iteration: 6 || Loss: 20.795335564717163
saving ADAM checkpoint...
Sum of params:-96.36379
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.795335564717163
Iteration: 2 || Loss: 18.084050668833566
Iteration: 3 || Loss: 17.459439391374712
Iteration: 4 || Loss: 16.93952240474885
Iteration: 5 || Loss: 15.114387517194933
Iteration: 6 || Loss: 13.790190618970462
Iteration: 7 || Loss: 13.654885173051989
Iteration: 8 || Loss: 13.501605004000309
Iteration: 9 || Loss: 13.05107520911233
Iteration: 10 || Loss: 13.017801854758943
Iteration: 11 || Loss: 12.78755773535648
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.97418
Epoch 17 loss:12.78755773535648
MSE loss S0.37378903788906287
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.97418
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 54.586989822003964
Iteration: 2 || Loss: 54.58500297960673
Iteration: 3 || Loss: 54.583027517942924
Iteration: 4 || Loss: 54.58109509108589
Iteration: 5 || Loss: 54.579101891812286
Iteration: 6 || Loss: 54.579101891812286
saving ADAM checkpoint...
Sum of params:-95.9742
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 54.579101891812286
Iteration: 2 || Loss: 54.35020806570462
Iteration: 3 || Loss: 54.25746103572587
Iteration: 4 || Loss: 52.96640887525391
Iteration: 5 || Loss: 52.685310410206654
Iteration: 6 || Loss: 51.85382551865934
Iteration: 7 || Loss: 49.81194191474848
Iteration: 8 || Loss: 49.49527209937982
Iteration: 9 || Loss: 49.17223087571707
Iteration: 10 || Loss: 48.90978025195904
Iteration: 11 || Loss: 48.47058635523791
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.99527
Epoch 17 loss:48.47058635523791
MSE loss S0.7952399588893351
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.99527
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 341.5402356787013
Iteration: 2 || Loss: 341.5329053237301
Iteration: 3 || Loss: 341.5254551794527
Iteration: 4 || Loss: 341.51813569043696
Iteration: 5 || Loss: 341.51063406645034
Iteration: 6 || Loss: 341.51063406645034
saving ADAM checkpoint...
Sum of params:-94.99539
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 341.51063406645034
Iteration: 2 || Loss: 338.66039156081087
Iteration: 3 || Loss: 337.26242390318555
Iteration: 4 || Loss: 317.540973149067
Iteration: 5 || Loss: 303.378247930976
Iteration: 6 || Loss: 301.1393093855839
Iteration: 7 || Loss: 298.1389522757268
Iteration: 8 || Loss: 288.90667055091336
Iteration: 9 || Loss: 288.12826420024754
Iteration: 10 || Loss: 286.6612668219081
Iteration: 11 || Loss: 285.75714056031103
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.36237
Epoch 17 loss:285.75714056031103
MSE loss S3.3183008954834903
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:73.84495342071835
MSE loss S0.8849527920458906
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:462.60834532455743
MSE loss S7.945159211468898
waveform batch: 2/2
Test loss - extrapolation:350.27781624173855
MSE loss S5.936224013924782
Epoch 17 mean train loss:11.966044298307084
Epoch 17 mean test loss - interpolation:12.307492236786393
Epoch 17 mean test loss - extrapolation:67.740513463858
Start training epoch 18
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.36237
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.531526658575313
Iteration: 2 || Loss: 18.525937607402103
Iteration: 3 || Loss: 18.52039333059705
Iteration: 4 || Loss: 18.514892942737053
Iteration: 5 || Loss: 18.509352615211736
Iteration: 6 || Loss: 18.509352615211736
saving ADAM checkpoint...
Sum of params:-96.3624
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.509352615211736
Iteration: 2 || Loss: 16.468250992813356
Iteration: 3 || Loss: 16.129112454789976
Iteration: 4 || Loss: 15.525323254369185
Iteration: 5 || Loss: 13.89704148269564
Iteration: 6 || Loss: 13.159510463853954
Iteration: 7 || Loss: 13.003232324345323
Iteration: 8 || Loss: 12.86601727311852
Iteration: 9 || Loss: 12.615601243816213
Iteration: 10 || Loss: 12.566480087027719
Iteration: 11 || Loss: 12.323236338164232
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.94539
Epoch 18 loss:12.323236338164232
MSE loss S0.3652082380048106
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.94539
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 52.548934483981526
Iteration: 2 || Loss: 52.5469570035371
Iteration: 3 || Loss: 52.544911100712206
Iteration: 4 || Loss: 52.54294761065235
Iteration: 5 || Loss: 52.540944524114344
Iteration: 6 || Loss: 52.540944524114344
saving ADAM checkpoint...
Sum of params:-95.9454
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 52.540944524114344
Iteration: 2 || Loss: 52.3118348732786
Iteration: 3 || Loss: 52.24106471355443
Iteration: 4 || Loss: 51.89677822287355
Iteration: 5 || Loss: 50.99537582283813
Iteration: 6 || Loss: 49.884023687782175
Iteration: 7 || Loss: 48.56175364092081
Iteration: 8 || Loss: 48.260539424726694
Iteration: 9 || Loss: 47.92016358777513
Iteration: 10 || Loss: 47.59690149828411
Iteration: 11 || Loss: 47.420226969749514
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.032196
Epoch 18 loss:47.420226969749514
MSE loss S0.7536429954427152
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.032196
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 335.0718429342107
Iteration: 2 || Loss: 335.0638645442418
Iteration: 3 || Loss: 335.0558912683165
Iteration: 4 || Loss: 335.04794394371083
Iteration: 5 || Loss: 335.0399928752727
Iteration: 6 || Loss: 335.0399928752727
saving ADAM checkpoint...
Sum of params:-95.03235
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 335.0399928752727
Iteration: 2 || Loss: 331.75946827339766
Iteration: 3 || Loss: 330.3723811807908
Iteration: 4 || Loss: 313.2326320792308
Iteration: 5 || Loss: 296.7988375377023
Iteration: 6 || Loss: 294.74540339722097
Iteration: 7 || Loss: 291.84784491283835
Iteration: 8 || Loss: 283.2414467270535
Iteration: 9 || Loss: 282.41145174102525
Iteration: 10 || Loss: 281.03187200821463
Iteration: 11 || Loss: 279.9112996118083
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.391266
Epoch 18 loss:279.9112996118083
MSE loss S3.1965967967560194
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:71.85959907999685
MSE loss S0.8308785415543234
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:456.1346236433162
MSE loss S7.779543615104867
waveform batch: 2/2
Test loss - extrapolation:344.77931176824654
MSE loss S5.783623161179534
Epoch 18 mean train loss:11.712233204128347
Epoch 18 mean test loss - interpolation:11.976599846666142
Epoch 18 mean test loss - extrapolation:66.74282795096356
Start training epoch 19
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.391266
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.80548693884695
Iteration: 2 || Loss: 17.799808174722966
Iteration: 3 || Loss: 17.794007199014587
Iteration: 4 || Loss: 17.788336100883633
Iteration: 5 || Loss: 17.782570172483144
Iteration: 6 || Loss: 17.782570172483144
saving ADAM checkpoint...
Sum of params:-96.391266
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.782570172483144
Iteration: 2 || Loss: 15.60109329296022
Iteration: 3 || Loss: 15.33616375022335
Iteration: 4 || Loss: 15.069228917698192
Iteration: 5 || Loss: 13.487587454806407
Iteration: 6 || Loss: 12.696172036914573
Iteration: 7 || Loss: 12.617314379886809
Iteration: 8 || Loss: 12.47270241478109
Iteration: 9 || Loss: 12.275248179097328
Iteration: 10 || Loss: 12.20869998679094
Iteration: 11 || Loss: 11.998055601980639
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.98844
Epoch 19 loss:11.998055601980639
MSE loss S0.3780235415371511
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.98844
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 51.16896581883924
Iteration: 2 || Loss: 51.16825073794548
Iteration: 3 || Loss: 51.16747594474645
Iteration: 4 || Loss: 51.166780994813635
Iteration: 5 || Loss: 51.16608404374276
Iteration: 6 || Loss: 51.16608404374276
saving ADAM checkpoint...
Sum of params:-95.98836
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 51.16608404374276
Iteration: 2 || Loss: 51.11835120414371
Iteration: 3 || Loss: 51.05525426211336
Iteration: 4 || Loss: 50.36674260857571
Iteration: 5 || Loss: 49.846051959023505
Iteration: 6 || Loss: 48.26967634920185
Iteration: 7 || Loss: 47.334453929011545
Iteration: 8 || Loss: 47.03778865898018
Iteration: 9 || Loss: 46.60351677723761
Iteration: 10 || Loss: 46.24980352607815
Iteration: 11 || Loss: 45.987087291703446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.90224
Epoch 19 loss:45.987087291703446
MSE loss S0.6789136199931685
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.90224
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 334.07333446348673
Iteration: 2 || Loss: 334.0656953491525
Iteration: 3 || Loss: 334.05792128922195
Iteration: 4 || Loss: 334.05028429265997
Iteration: 5 || Loss: 334.0428479496607
Iteration: 6 || Loss: 334.0428479496607
saving ADAM checkpoint...
Sum of params:-94.90239
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 334.0428479496607
Iteration: 2 || Loss: 330.980113613359
Iteration: 3 || Loss: 329.57836486554623
Iteration: 4 || Loss: 309.7198452939796
Iteration: 5 || Loss: 293.04950362510374
Iteration: 6 || Loss: 291.5023333254129
Iteration: 7 || Loss: 285.6051270395205
Iteration: 8 || Loss: 278.86217141058523
Iteration: 9 || Loss: 277.74012210067303
Iteration: 10 || Loss: 275.0389102549748
Iteration: 11 || Loss: 274.1590984194839
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.286705
Epoch 19 loss:274.1590984194839
MSE loss S3.1672993988665556
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:69.46445788871577
MSE loss S0.790914449406894
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:449.7573931444971
MSE loss S7.700366180765679
waveform batch: 2/2
Test loss - extrapolation:339.5370254882229
MSE loss S5.736623062608485
Epoch 19 mean train loss:11.453249700454068
Epoch 19 mean test loss - interpolation:11.577409648119295
Epoch 19 mean test loss - extrapolation:65.77453488606
Start training epoch 20
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.286705
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 16.66618467790755
Iteration: 2 || Loss: 16.660889744475632
Iteration: 3 || Loss: 16.655750382405188
Iteration: 4 || Loss: 16.650534166368637
Iteration: 5 || Loss: 16.64532869465128
Iteration: 6 || Loss: 16.64532869465128
saving ADAM checkpoint...
Sum of params:-96.2867
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 16.64532869465128
Iteration: 2 || Loss: 14.870191015283899
Iteration: 3 || Loss: 14.68961500007997
Iteration: 4 || Loss: 13.697442484798174
Iteration: 5 || Loss: 12.806328599904807
Iteration: 6 || Loss: 12.389183891955163
Iteration: 7 || Loss: 12.28259349011802
Iteration: 8 || Loss: 12.054031691296505
Iteration: 9 || Loss: 11.938952535686374
Iteration: 10 || Loss: 11.887284408233485
Iteration: 11 || Loss: 11.571521184461794
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.88086
Epoch 20 loss:11.571521184461794
MSE loss S0.3616433967026126
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.88086
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.64716515881127
Iteration: 2 || Loss: 49.64570431553446
Iteration: 3 || Loss: 49.64428168915966
Iteration: 4 || Loss: 49.64280184087437
Iteration: 5 || Loss: 49.641266702393736
Iteration: 6 || Loss: 49.641266702393736
saving ADAM checkpoint...
Sum of params:-95.88087
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.641266702393736
Iteration: 2 || Loss: 49.50872283779783
Iteration: 3 || Loss: 49.433946316297224
Iteration: 4 || Loss: 49.028581466404205
Iteration: 5 || Loss: 48.102320124935034
Iteration: 6 || Loss: 47.22554375199185
Iteration: 7 || Loss: 45.913398469887504
Iteration: 8 || Loss: 45.71121792712848
Iteration: 9 || Loss: 45.25012727755024
Iteration: 10 || Loss: 44.95659669225195
Iteration: 11 || Loss: 44.74614915368582
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.94844
Epoch 20 loss:44.74614915368582
MSE loss S0.7158689269488515
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.94844
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 322.94084133135806
Iteration: 2 || Loss: 322.93261207151664
Iteration: 3 || Loss: 322.9243956407859
Iteration: 4 || Loss: 322.9162465069059
Iteration: 5 || Loss: 322.90799881615345
Iteration: 6 || Loss: 322.90799881615345
saving ADAM checkpoint...
Sum of params:-94.9486
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 322.90799881615345
Iteration: 2 || Loss: 319.3742336195923
Iteration: 3 || Loss: 317.95461510285884
Iteration: 4 || Loss: 302.2410806222437
Iteration: 5 || Loss: 284.7529758948836
Iteration: 6 || Loss: 283.51869961148924
Iteration: 7 || Loss: 278.9883020803901
Iteration: 8 || Loss: 272.80115296105566
Iteration: 9 || Loss: 271.91053177264183
Iteration: 10 || Loss: 269.5544980567928
Iteration: 11 || Loss: 268.55161073316003
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.280525
Epoch 20 loss:268.55161073316003
MSE loss S3.082673671238897
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:67.83791714694357
MSE loss S0.7624608909664583
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:444.480477495639
MSE loss S7.596968773510177
waveform batch: 2/2
Test loss - extrapolation:334.30541990482976
MSE loss S5.600323431827125
Epoch 20 mean train loss:11.202389002458883
Epoch 20 mean test loss - interpolation:11.306319524490595
Epoch 20 mean test loss - extrapolation:64.89882478337239
Start training epoch 21
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.280525
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.801046250943356
Iteration: 2 || Loss: 15.796343124897227
Iteration: 3 || Loss: 15.79164698578243
Iteration: 4 || Loss: 15.786918768850494
Iteration: 5 || Loss: 15.782196265687741
Iteration: 6 || Loss: 15.782196265687741
saving ADAM checkpoint...
Sum of params:-96.28052
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.782196265687741
Iteration: 2 || Loss: 14.3275587700241
Iteration: 3 || Loss: 14.165221286245114
Iteration: 4 || Loss: 13.935922210193976
Iteration: 5 || Loss: 12.421760123065049
Iteration: 6 || Loss: 11.979783912579192
Iteration: 7 || Loss: 11.866517108809703
Iteration: 8 || Loss: 11.619341174407863
Iteration: 9 || Loss: 11.555863737592414
Iteration: 10 || Loss: 11.400344336111166
Iteration: 11 || Loss: 11.214574770760295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.885056
Epoch 21 loss:11.214574770760295
MSE loss S0.33845398482213024
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.885056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 48.207479088907085
Iteration: 2 || Loss: 48.206250464980315
Iteration: 3 || Loss: 48.20505341391113
Iteration: 4 || Loss: 48.20385161992387
Iteration: 5 || Loss: 48.202606513861454
Iteration: 6 || Loss: 48.202606513861454
saving ADAM checkpoint...
Sum of params:-95.88507
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 48.202606513861454
Iteration: 2 || Loss: 48.104401635070644
Iteration: 3 || Loss: 48.034155305844514
Iteration: 4 || Loss: 46.985311867327326
Iteration: 5 || Loss: 46.79916365925676
Iteration: 6 || Loss: 45.75147930493065
Iteration: 7 || Loss: 44.733733774643376
Iteration: 8 || Loss: 44.47564130888932
Iteration: 9 || Loss: 44.14587573109467
Iteration: 10 || Loss: 43.984059868017916
Iteration: 11 || Loss: 43.64141326037931
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.9259
Epoch 21 loss:43.64141326037931
MSE loss S0.7100919513851537
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.9259
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 317.201104877975
Iteration: 2 || Loss: 317.1928234201834
Iteration: 3 || Loss: 317.18458319448865
Iteration: 4 || Loss: 317.1762710033369
Iteration: 5 || Loss: 317.1679768270246
Iteration: 6 || Loss: 317.1679768270246
saving ADAM checkpoint...
Sum of params:-94.926056
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 317.1679768270246
Iteration: 2 || Loss: 313.5461183201458
Iteration: 3 || Loss: 312.20553620353706
Iteration: 4 || Loss: 298.28948821271877
Iteration: 5 || Loss: 280.36029483132756
Iteration: 6 || Loss: 279.1138564436309
Iteration: 7 || Loss: 273.5688180487577
Iteration: 8 || Loss: 268.17616510697087
Iteration: 9 || Loss: 267.273393266791
Iteration: 10 || Loss: 264.28134649560127
Iteration: 11 || Loss: 263.53948295561895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.26256
Epoch 21 loss:263.53948295561895
MSE loss S2.988410958503081
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:66.21676265653022
MSE loss S0.7072610349854134
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:439.009973680509
MSE loss S7.470592693894918
waveform batch: 2/2
Test loss - extrapolation:329.6488035915925
MSE loss S5.516223719167702
Epoch 21 mean train loss:10.979154171957191
Epoch 21 mean test loss - interpolation:11.036127109421704
Epoch 21 mean test loss - extrapolation:64.05489810600847
Start training epoch 22
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.26256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.290359307788053
Iteration: 2 || Loss: 15.285852320520126
Iteration: 3 || Loss: 15.281295212947029
Iteration: 4 || Loss: 15.27678617176126
Iteration: 5 || Loss: 15.272309618557296
Iteration: 6 || Loss: 15.272309618557296
saving ADAM checkpoint...
Sum of params:-96.262566
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.272309618557296
Iteration: 2 || Loss: 13.928389678401212
Iteration: 3 || Loss: 13.806319738033478
Iteration: 4 || Loss: 13.678133672268743
Iteration: 5 || Loss: 12.079796523069003
Iteration: 6 || Loss: 11.748034005275553
Iteration: 7 || Loss: 11.464525043522803
Iteration: 8 || Loss: 11.294810929614162
Iteration: 9 || Loss: 11.25944352959529
Iteration: 10 || Loss: 11.070523720682406
Iteration: 11 || Loss: 10.822710187952874
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.8706
Epoch 22 loss:10.822710187952874
MSE loss S0.3494138549387089
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.8706
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 47.064043486459504
Iteration: 2 || Loss: 47.06287186628775
Iteration: 3 || Loss: 47.06166711450751
Iteration: 4 || Loss: 47.060522152892176
Iteration: 5 || Loss: 47.059371476512226
Iteration: 6 || Loss: 47.059371476512226
saving ADAM checkpoint...
Sum of params:-95.870575
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 47.059371476512226
Iteration: 2 || Loss: 46.97777732013899
Iteration: 3 || Loss: 46.91336540723135
Iteration: 4 || Loss: 45.38175234542594
Iteration: 5 || Loss: 45.32484501590076
Iteration: 6 || Loss: 44.4923781369653
Iteration: 7 || Loss: 43.61745458864999
Iteration: 8 || Loss: 43.31348936296755
Iteration: 9 || Loss: 43.05548002926935
Iteration: 10 || Loss: 42.87462033066561
Iteration: 11 || Loss: 42.60617816822385
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.93617
Epoch 22 loss:42.60617816822385
MSE loss S0.7027009100283175
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.93617
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 312.5411018191643
Iteration: 2 || Loss: 312.5330928651134
Iteration: 3 || Loss: 312.5252945418795
Iteration: 4 || Loss: 312.51735531517784
Iteration: 5 || Loss: 312.5094637941258
Iteration: 6 || Loss: 312.5094637941258
saving ADAM checkpoint...
Sum of params:-94.936295
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 312.5094637941258
Iteration: 2 || Loss: 309.2449425638069
Iteration: 3 || Loss: 307.8017467549492
Iteration: 4 || Loss: 291.7773702871123
Iteration: 5 || Loss: 274.72212603101326
Iteration: 6 || Loss: 273.5341443484144
Iteration: 7 || Loss: 267.15878817792714
Iteration: 8 || Loss: 263.2099766031062
Iteration: 9 || Loss: 262.2211817751333
Iteration: 10 || Loss: 259.17279687717087
Iteration: 11 || Loss: 258.64317420083813
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.26957
Epoch 22 loss:258.64317420083813
MSE loss S2.9410413014088452
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:64.641990911335
MSE loss S0.6829157979942064
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:433.8039016452587
MSE loss S7.399199355543123
waveform batch: 2/2
Test loss - extrapolation:324.86142522869403
MSE loss S5.445790095771709
Epoch 22 mean train loss:10.761105605414306
Epoch 22 mean test loss - interpolation:10.773665151889167
Epoch 22 mean test loss - extrapolation:63.22211057282939
Start training epoch 23
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.26957
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.50474760594034
Iteration: 2 || Loss: 14.500643206305469
Iteration: 3 || Loss: 14.496565306543568
Iteration: 4 || Loss: 14.49245798224268
Iteration: 5 || Loss: 14.48839009575531
Iteration: 6 || Loss: 14.48839009575531
saving ADAM checkpoint...
Sum of params:-96.26965
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.48839009575531
Iteration: 2 || Loss: 13.396546449547472
Iteration: 3 || Loss: 13.300804885129121
Iteration: 4 || Loss: 13.142325472980454
Iteration: 5 || Loss: 11.634327784329157
Iteration: 6 || Loss: 11.308859067968163
Iteration: 7 || Loss: 11.194962786348983
Iteration: 8 || Loss: 10.945895555086642
Iteration: 9 || Loss: 10.909603680852946
Iteration: 10 || Loss: 10.505565358611427
Iteration: 11 || Loss: 10.448509172356019
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.871864
Epoch 23 loss:10.448509172356019
MSE loss S0.3360568527355086
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.871864
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.81145214369132
Iteration: 2 || Loss: 45.81006033575818
Iteration: 3 || Loss: 45.8085910633428
Iteration: 4 || Loss: 45.8071308839299
Iteration: 5 || Loss: 45.80567232757197
Iteration: 6 || Loss: 45.80567232757197
saving ADAM checkpoint...
Sum of params:-95.87178
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.80567232757197
Iteration: 2 || Loss: 45.66240798431087
Iteration: 3 || Loss: 45.55888373829162
Iteration: 4 || Loss: 44.253965490090444
Iteration: 5 || Loss: 43.988689937300364
Iteration: 6 || Loss: 43.51190941967115
Iteration: 7 || Loss: 42.59203899882188
Iteration: 8 || Loss: 42.39102815547171
Iteration: 9 || Loss: 42.10582806113247
Iteration: 10 || Loss: 41.93651282651487
Iteration: 11 || Loss: 41.8319070817222
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.02754
Epoch 23 loss:41.8319070817222
MSE loss S0.6588924032374321
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.02754
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 305.1841033818722
Iteration: 2 || Loss: 305.176285793997
Iteration: 3 || Loss: 305.16837769510863
Iteration: 4 || Loss: 305.1606318469166
Iteration: 5 || Loss: 305.152657337279
Iteration: 6 || Loss: 305.152657337279
saving ADAM checkpoint...
Sum of params:-95.027664
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 305.152657337279
Iteration: 2 || Loss: 302.04365614290174
Iteration: 3 || Loss: 300.72699051434654
Iteration: 4 || Loss: 284.78877139585103
Iteration: 5 || Loss: 268.9411394563817
Iteration: 6 || Loss: 267.74156848970057
Iteration: 7 || Loss: 262.1969527903585
Iteration: 8 || Loss: 258.1274034267591
Iteration: 9 || Loss: 257.2655845801299
Iteration: 10 || Loss: 254.7311862254831
Iteration: 11 || Loss: 254.17571931192643
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.32814
Epoch 23 loss:254.17571931192643
MSE loss S2.863963911917093
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:63.185055946888035
MSE loss S0.6469905799051885
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:428.4989739506826
MSE loss S7.28445649578612
waveform batch: 2/2
Test loss - extrapolation:320.6711011336175
MSE loss S5.370057543677705
Epoch 23 mean train loss:10.567452950551885
Epoch 23 mean test loss - interpolation:10.530842657814672
Epoch 23 mean test loss - extrapolation:62.430839590358346
Start training epoch 24
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.32814
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.909675437291208
Iteration: 2 || Loss: 13.90572391250463
Iteration: 3 || Loss: 13.901803840621232
Iteration: 4 || Loss: 13.897855693491147
Iteration: 5 || Loss: 13.89398565697259
Iteration: 6 || Loss: 13.89398565697259
saving ADAM checkpoint...
Sum of params:-96.32824
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.89398565697259
Iteration: 2 || Loss: 12.901217884453555
Iteration: 3 || Loss: 12.829209577850232
Iteration: 4 || Loss: 11.681382974325109
Iteration: 5 || Loss: 11.248320970235048
Iteration: 6 || Loss: 10.994081914408284
Iteration: 7 || Loss: 10.843047808138946
Iteration: 8 || Loss: 10.692863025499593
Iteration: 9 || Loss: 10.652089348653947
Iteration: 10 || Loss: 10.390004631726537
Iteration: 11 || Loss: 10.307743057773965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.924446
Epoch 24 loss:10.307743057773965
MSE loss S0.3254773446701927
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.924446
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.79803062829256
Iteration: 2 || Loss: 44.79701867783963
Iteration: 3 || Loss: 44.79607574078226
Iteration: 4 || Loss: 44.79507211201692
Iteration: 5 || Loss: 44.79408009873369
Iteration: 6 || Loss: 44.79408009873369
saving ADAM checkpoint...
Sum of params:-95.92444
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.79408009873369
Iteration: 2 || Loss: 44.74106179048692
Iteration: 3 || Loss: 44.69868696440226
Iteration: 4 || Loss: 43.94199461535942
Iteration: 5 || Loss: 43.40930605422855
Iteration: 6 || Loss: 42.52285368255354
Iteration: 7 || Loss: 41.81131562497685
Iteration: 8 || Loss: 41.589197685208745
Iteration: 9 || Loss: 41.32647625271159
Iteration: 10 || Loss: 41.108880129304104
Iteration: 11 || Loss: 40.787057537656054
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.88118
Epoch 24 loss:40.787057537656054
MSE loss S0.6556954484604371
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.88118
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 304.0199905056836
Iteration: 2 || Loss: 304.01166806874375
Iteration: 3 || Loss: 304.0035374381083
Iteration: 4 || Loss: 303.99521606724954
Iteration: 5 || Loss: 303.98692168829245
Iteration: 6 || Loss: 303.98692168829245
saving ADAM checkpoint...
Sum of params:-94.881294
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 303.98692168829245
Iteration: 2 || Loss: 300.42503269874476
Iteration: 3 || Loss: 299.02769204726695
Iteration: 4 || Loss: 284.6686421548506
Iteration: 5 || Loss: 266.8595758801472
Iteration: 6 || Loss: 264.82483557275464
Iteration: 7 || Loss: 257.1441598510537
Iteration: 8 || Loss: 255.42287297410985
Iteration: 9 || Loss: 251.44965575703503
Iteration: 10 || Loss: 250.15014292858544
Iteration: 11 || Loss: 249.85219752811673
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.25625
Epoch 24 loss:249.85219752811673
MSE loss S2.764319020583914
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:63.15213363375999
MSE loss S0.6424668663564497
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:423.4128262851242
MSE loss S7.149727056145242
waveform batch: 2/2
Test loss - extrapolation:314.40040038379755
MSE loss S5.205157429866495
Epoch 24 mean train loss:10.377482693915404
Epoch 24 mean test loss - interpolation:10.525355605626665
Epoch 24 mean test loss - extrapolation:61.48443555574348
Start training epoch 25
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.25625
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.520110734177612
Iteration: 2 || Loss: 14.515335712876027
Iteration: 3 || Loss: 14.51050604773469
Iteration: 4 || Loss: 14.505703616263009
Iteration: 5 || Loss: 14.50092945362811
Iteration: 6 || Loss: 14.50092945362811
saving ADAM checkpoint...
Sum of params:-96.25626
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.50092945362811
Iteration: 2 || Loss: 13.00852289578433
Iteration: 3 || Loss: 12.922837345676152
Iteration: 4 || Loss: 12.795787363010563
Iteration: 5 || Loss: 11.25612640357539
Iteration: 6 || Loss: 10.964719791216018
Iteration: 7 || Loss: 10.607067418051342
Iteration: 8 || Loss: 10.508488653628925
Iteration: 9 || Loss: 10.482300378705169
Iteration: 10 || Loss: 10.270480683503472
Iteration: 11 || Loss: 10.16637631273151
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.88105
Epoch 25 loss:10.16637631273151
MSE loss S0.32445350128475803
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.88105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.13248242508689
Iteration: 2 || Loss: 44.130994403047865
Iteration: 3 || Loss: 44.12948417266403
Iteration: 4 || Loss: 44.12799739069402
Iteration: 5 || Loss: 44.12649981410008
Iteration: 6 || Loss: 44.12649981410008
saving ADAM checkpoint...
Sum of params:-95.88106
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.12649981410008
Iteration: 2 || Loss: 44.007084305811205
Iteration: 3 || Loss: 43.95993235188823
Iteration: 4 || Loss: 43.134122331932254
Iteration: 5 || Loss: 42.66331180416024
Iteration: 6 || Loss: 41.61977271598909
Iteration: 7 || Loss: 40.81965920150958
Iteration: 8 || Loss: 40.67924038462635
Iteration: 9 || Loss: 40.41646587899493
Iteration: 10 || Loss: 40.27609715903393
Iteration: 11 || Loss: 40.053561822340605
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.979675
Epoch 25 loss:40.053561822340605
MSE loss S0.6088779490975774
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.979675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 295.52095474652447
Iteration: 2 || Loss: 295.5131386189661
Iteration: 3 || Loss: 295.5052520210178
Iteration: 4 || Loss: 295.497346173343
Iteration: 5 || Loss: 295.4895255812562
Iteration: 6 || Loss: 295.4895255812562
saving ADAM checkpoint...
Sum of params:-94.97981
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 295.4895255812562
Iteration: 2 || Loss: 292.40205777451354
Iteration: 3 || Loss: 291.1471431303077
Iteration: 4 || Loss: 276.3324643496928
Iteration: 5 || Loss: 260.8363781905219
Iteration: 6 || Loss: 259.031977579895
Iteration: 7 || Loss: 251.53007936609637
Iteration: 8 || Loss: 249.91964941397313
Iteration: 9 || Loss: 246.29365787790823
Iteration: 10 || Loss: 245.72303167922308
Iteration: 11 || Loss: 245.33645651122563
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.289474
Epoch 25 loss:245.33645651122563
MSE loss S2.621611322266433
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:61.71661359269244
MSE loss S0.5811220046172851
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:418.2155495432615
MSE loss S6.98468542114275
waveform batch: 2/2
Test loss - extrapolation:310.2923595884908
MSE loss S5.083238329971913
Epoch 25 mean train loss:10.191599815389578
Epoch 25 mean test loss - interpolation:10.28610226544874
Epoch 25 mean test loss - extrapolation:60.708992427646024
Start training epoch 26
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.289474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.870543235699191
Iteration: 2 || Loss: 13.865915279497452
Iteration: 3 || Loss: 13.861257532871505
Iteration: 4 || Loss: 13.856582616237969
Iteration: 5 || Loss: 13.852095434459809
Iteration: 6 || Loss: 13.852095434459809
saving ADAM checkpoint...
Sum of params:-96.28952
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.852095434459809
Iteration: 2 || Loss: 12.465109897869322
Iteration: 3 || Loss: 12.448070128007558
Iteration: 4 || Loss: 12.08782111401495
Iteration: 5 || Loss: 10.820720802149449
Iteration: 6 || Loss: 10.581114343994322
Iteration: 7 || Loss: 10.441527966212647
Iteration: 8 || Loss: 10.207054872734101
Iteration: 9 || Loss: 10.179524438946373
Iteration: 10 || Loss: 9.927460484666009
Iteration: 11 || Loss: 9.866778520486998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.91924
Epoch 26 loss:9.866778520486998
MSE loss S0.32553648874915186
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.91924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.05042008219982
Iteration: 2 || Loss: 43.0489852691113
Iteration: 3 || Loss: 43.04757649694191
Iteration: 4 || Loss: 43.0461468886715
Iteration: 5 || Loss: 43.04472520748411
Iteration: 6 || Loss: 43.04472520748411
saving ADAM checkpoint...
Sum of params:-95.919235
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.04472520748411
Iteration: 2 || Loss: 42.92728235693721
Iteration: 3 || Loss: 42.8592642755204
Iteration: 4 || Loss: 42.12538335848677
Iteration: 5 || Loss: 41.63903021900691
Iteration: 6 || Loss: 40.87480237014097
Iteration: 7 || Loss: 40.11097575560258
Iteration: 8 || Loss: 39.977521070136866
Iteration: 9 || Loss: 39.69321794583813
Iteration: 10 || Loss: 39.591102016929575
Iteration: 11 || Loss: 39.27099339316493
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.97393
Epoch 26 loss:39.27099339316493
MSE loss S0.6383400413384095
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.97393
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 290.0496523236653
Iteration: 2 || Loss: 290.04151181172733
Iteration: 3 || Loss: 290.0333904053426
Iteration: 4 || Loss: 290.0253610719909
Iteration: 5 || Loss: 290.0171271988258
Iteration: 6 || Loss: 290.0171271988258
saving ADAM checkpoint...
Sum of params:-94.97405
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 290.0171271988258
Iteration: 2 || Loss: 286.62735853919526
Iteration: 3 || Loss: 285.2777909949741
Iteration: 4 || Loss: 272.58658359119045
Iteration: 5 || Loss: 256.66424181887095
Iteration: 6 || Loss: 253.88167203506308
Iteration: 7 || Loss: 247.6258093025077
Iteration: 8 || Loss: 246.20528261050296
Iteration: 9 || Loss: 244.33694258897987
Iteration: 10 || Loss: 241.5412451756445
Iteration: 11 || Loss: 241.12110984076415
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.29063
Epoch 26 loss:241.12110984076415
MSE loss S2.6093552975545893
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:59.578647072816764
MSE loss S0.5244321186454134
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:413.28670584824357
MSE loss S6.9513825652417625
waveform batch: 2/2
Test loss - extrapolation:307.9067435624966
MSE loss S5.142558447676649
Epoch 26 mean train loss:10.00892695704883
Epoch 26 mean test loss - interpolation:9.929774512136127
Epoch 26 mean test loss - extrapolation:60.099454117561685
Start training epoch 27
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.29063
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.74425413721727
Iteration: 2 || Loss: 12.741407699571921
Iteration: 3 || Loss: 12.738515120574892
Iteration: 4 || Loss: 12.735727348528151
Iteration: 5 || Loss: 12.732866690438156
Iteration: 6 || Loss: 12.732866690438156
saving ADAM checkpoint...
Sum of params:-96.2907
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.732866690438156
Iteration: 2 || Loss: 12.208447228271519
Iteration: 3 || Loss: 12.168333785691193
Iteration: 4 || Loss: 12.077173650561623
Iteration: 5 || Loss: 10.628565812959588
Iteration: 6 || Loss: 10.425254724839668
Iteration: 7 || Loss: 10.094601060554591
Iteration: 8 || Loss: 10.051755078044653
Iteration: 9 || Loss: 9.880003331156587
Iteration: 10 || Loss: 9.72189685060388
Iteration: 11 || Loss: 9.537178310881085
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.95987
Epoch 27 loss:9.537178310881085
MSE loss S0.288021816167493
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.95987
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 42.29657994443936
Iteration: 2 || Loss: 42.29440275935761
Iteration: 3 || Loss: 42.2922822625717
Iteration: 4 || Loss: 42.29019331624663
Iteration: 5 || Loss: 42.288049088306785
Iteration: 6 || Loss: 42.288049088306785
saving ADAM checkpoint...
Sum of params:-95.95979
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 42.288049088306785
Iteration: 2 || Loss: 42.03740387176824
Iteration: 3 || Loss: 41.99647299267994
Iteration: 4 || Loss: 40.636830323346665
Iteration: 5 || Loss: 40.514925956006934
Iteration: 6 || Loss: 40.179224674954085
Iteration: 7 || Loss: 39.423976666568564
Iteration: 8 || Loss: 39.19574215538977
Iteration: 9 || Loss: 39.02350963969772
Iteration: 10 || Loss: 38.851656712305825
Iteration: 11 || Loss: 38.75613998167656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.14039
Epoch 27 loss:38.75613998167656
MSE loss S0.6145548651010109
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.14039
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 281.9480285151576
Iteration: 2 || Loss: 281.9401532330939
Iteration: 3 || Loss: 281.932501181719
Iteration: 4 || Loss: 281.9246950452411
Iteration: 5 || Loss: 281.91686499790745
Iteration: 6 || Loss: 281.91686499790745
saving ADAM checkpoint...
Sum of params:-95.14051
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 281.91686499790745
Iteration: 2 || Loss: 278.98089698143264
Iteration: 3 || Loss: 277.64630895486505
Iteration: 4 || Loss: 264.06930928861095
Iteration: 5 || Loss: 250.09558886817987
Iteration: 6 || Loss: 247.9086793952601
Iteration: 7 || Loss: 241.97172617609476
Iteration: 8 || Loss: 240.71859964495425
Iteration: 9 || Loss: 238.92847868268242
Iteration: 10 || Loss: 237.3638446680701
Iteration: 11 || Loss: 237.0080551473851
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.42049
Epoch 27 loss:237.0080551473851
MSE loss S2.539803414813157
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:58.93849946018868
MSE loss S0.5483814647560834
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:408.17352268201097
MSE loss S6.834371369619836
waveform batch: 2/2
Test loss - extrapolation:302.86410339317644
MSE loss S4.992163192124895
Epoch 27 mean train loss:9.837978394480784
Epoch 27 mean test loss - interpolation:9.82308324336478
Epoch 27 mean test loss - extrapolation:59.253135506265615
Start training epoch 28
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.42049
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.744562709911577
Iteration: 2 || Loss: 12.740538996459085
Iteration: 3 || Loss: 12.736519773027705
Iteration: 4 || Loss: 12.732481489618202
Iteration: 5 || Loss: 12.728428867617787
Iteration: 6 || Loss: 12.728428867617787
saving ADAM checkpoint...
Sum of params:-96.420586
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.728428867617787
Iteration: 2 || Loss: 11.68084422831048
Iteration: 3 || Loss: 11.623516966194671
Iteration: 4 || Loss: 11.521801212285832
Iteration: 5 || Loss: 10.340281898830547
Iteration: 6 || Loss: 10.175823497561659
Iteration: 7 || Loss: 10.000081516076802
Iteration: 8 || Loss: 9.872613970542536
Iteration: 9 || Loss: 9.809441879632583
Iteration: 10 || Loss: 9.568446570996812
Iteration: 11 || Loss: 9.428039907233343
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.08266
Epoch 28 loss:9.428039907233343
MSE loss S0.3046380960466665
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.08266
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.641609892232665
Iteration: 2 || Loss: 41.640210109453974
Iteration: 3 || Loss: 41.63891895826378
Iteration: 4 || Loss: 41.63759910949717
Iteration: 5 || Loss: 41.6363453465624
Iteration: 6 || Loss: 41.6363453465624
saving ADAM checkpoint...
Sum of params:-96.082565
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.6363453465624
Iteration: 2 || Loss: 41.5173911746805
Iteration: 3 || Loss: 41.42504375457016
Iteration: 4 || Loss: 40.54801257526446
Iteration: 5 || Loss: 40.03908243123913
Iteration: 6 || Loss: 39.645297702925994
Iteration: 7 || Loss: 38.90024460893371
Iteration: 8 || Loss: 38.691374426294516
Iteration: 9 || Loss: 38.48429445396521
Iteration: 10 || Loss: 38.33286753871821
Iteration: 11 || Loss: 38.22104206065624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.20213
Epoch 28 loss:38.22104206065624
MSE loss S0.6253289612617774
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.20213
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 276.194171328269
Iteration: 2 || Loss: 276.1862662007198
Iteration: 3 || Loss: 276.1782465614842
Iteration: 4 || Loss: 276.1703177230927
Iteration: 5 || Loss: 276.1624657738568
Iteration: 6 || Loss: 276.1624657738568
saving ADAM checkpoint...
Sum of params:-95.20229
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 276.1624657738568
Iteration: 2 || Loss: 273.1461010083023
Iteration: 3 || Loss: 271.77947202549507
Iteration: 4 || Loss: 259.49025181674466
Iteration: 5 || Loss: 245.71423668828572
Iteration: 6 || Loss: 243.6130166435208
Iteration: 7 || Loss: 238.0235433206235
Iteration: 8 || Loss: 236.83699564421948
Iteration: 9 || Loss: 235.06899142223557
Iteration: 10 || Loss: 233.4460714666808
Iteration: 11 || Loss: 233.05962876393556
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.478096
Epoch 28 loss:233.05962876393556
MSE loss S2.4894965662547577
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:58.08919925503111
MSE loss S0.5307773510173842
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:402.62208690751555
MSE loss S6.744086680774638
waveform batch: 2/2
Test loss - extrapolation:298.31026111387786
MSE loss S4.933817164718887
Epoch 28 mean train loss:9.679610714890522
Epoch 28 mean test loss - interpolation:9.681533209171851
Epoch 28 mean test loss - extrapolation:58.41102900178279
Start training epoch 29
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.478096
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.354454804788674
Iteration: 2 || Loss: 12.350830907854386
Iteration: 3 || Loss: 12.34722939654921
Iteration: 4 || Loss: 12.343730626807154
Iteration: 5 || Loss: 12.340132351276079
Iteration: 6 || Loss: 12.340132351276079
saving ADAM checkpoint...
Sum of params:-96.47817
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.340132351276079
Iteration: 2 || Loss: 11.522337576098161
Iteration: 3 || Loss: 11.504056665774458
Iteration: 4 || Loss: 11.346065703328343
Iteration: 5 || Loss: 10.191180429328067
Iteration: 6 || Loss: 10.04838255620368
Iteration: 7 || Loss: 9.803541280400076
Iteration: 8 || Loss: 9.72378173433464
Iteration: 9 || Loss: 9.601131455707176
Iteration: 10 || Loss: 9.432375623778258
Iteration: 11 || Loss: 9.250772469222104
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.15794
Epoch 29 loss:9.250772469222104
MSE loss S0.2908028430994058
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.15794
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.10013274705884
Iteration: 2 || Loss: 41.09863388516309
Iteration: 3 || Loss: 41.097146966926935
Iteration: 4 || Loss: 41.09559335284869
Iteration: 5 || Loss: 41.0941307822078
Iteration: 6 || Loss: 41.0941307822078
saving ADAM checkpoint...
Sum of params:-96.15784
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.0941307822078
Iteration: 2 || Loss: 40.960339500868315
Iteration: 3 || Loss: 40.882232142241875
Iteration: 4 || Loss: 40.234721716403826
Iteration: 5 || Loss: 39.426524527990196
Iteration: 6 || Loss: 39.09220608448055
Iteration: 7 || Loss: 38.462746451972556
Iteration: 8 || Loss: 38.191607560033255
Iteration: 9 || Loss: 37.99502311050553
Iteration: 10 || Loss: 37.8383948811807
Iteration: 11 || Loss: 37.736509378746355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.26308
Epoch 29 loss:37.736509378746355
MSE loss S0.5946663926908069
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.26308
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 271.17515016710325
Iteration: 2 || Loss: 271.1673251147239
Iteration: 3 || Loss: 271.1595300045992
Iteration: 4 || Loss: 271.15159737294084
Iteration: 5 || Loss: 271.1438009265034
Iteration: 6 || Loss: 271.1438009265034
saving ADAM checkpoint...
Sum of params:-95.2632
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 271.1438009265034
Iteration: 2 || Loss: 268.2221890330936
Iteration: 3 || Loss: 266.87417207610486
Iteration: 4 || Loss: 255.08496129993554
Iteration: 5 || Loss: 241.84211542823553
Iteration: 6 || Loss: 239.3859322463102
Iteration: 7 || Loss: 234.15473832303766
Iteration: 8 || Loss: 232.99564254029417
Iteration: 9 || Loss: 231.44966129035177
Iteration: 10 || Loss: 229.70144300004284
Iteration: 11 || Loss: 229.35078044509706
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.473724
Epoch 29 loss:229.35078044509706
MSE loss S2.4482529175837433
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:56.47356716823556
MSE loss S0.49668513076922394
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:399.14747755140763
MSE loss S6.706415925258101
waveform batch: 2/2
Test loss - extrapolation:295.71145404715946
MSE loss S4.912195702128759
Epoch 29 mean train loss:9.528898699760878
Epoch 29 mean test loss - interpolation:9.412261194705927
Epoch 29 mean test loss - extrapolation:57.904910966547256
Start training epoch 30
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.473724
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.051380660377761
Iteration: 2 || Loss: 12.047699462576853
Iteration: 3 || Loss: 12.043877623793426
Iteration: 4 || Loss: 12.040197350183567
Iteration: 5 || Loss: 12.036435514950085
Iteration: 6 || Loss: 12.036435514950085
saving ADAM checkpoint...
Sum of params:-96.473816
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.036435514950085
Iteration: 2 || Loss: 11.137721910615397
Iteration: 3 || Loss: 11.091433821910831
Iteration: 4 || Loss: 11.011128975032582
Iteration: 5 || Loss: 9.879916333434334
Iteration: 6 || Loss: 9.757560921034607
Iteration: 7 || Loss: 9.498639431195855
Iteration: 8 || Loss: 9.472890281791514
Iteration: 9 || Loss: 9.34042601591516
Iteration: 10 || Loss: 9.245005080897947
Iteration: 11 || Loss: 9.091968786981871
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.185974
Epoch 30 loss:9.091968786981871
MSE loss S0.27054355369883815
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.185974
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.40623649655354
Iteration: 2 || Loss: 40.40419629413233
Iteration: 3 || Loss: 40.402185429590034
Iteration: 4 || Loss: 40.40022093163052
Iteration: 5 || Loss: 40.398184963272975
Iteration: 6 || Loss: 40.398184963272975
saving ADAM checkpoint...
Sum of params:-96.18599
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.398184963272975
Iteration: 2 || Loss: 40.18062607506686
Iteration: 3 || Loss: 40.15104761587533
Iteration: 4 || Loss: 38.89006099585203
Iteration: 5 || Loss: 38.84700585286119
Iteration: 6 || Loss: 38.57754388741402
Iteration: 7 || Loss: 37.96099711862447
Iteration: 8 || Loss: 37.649937772731136
Iteration: 9 || Loss: 37.53720319386183
Iteration: 10 || Loss: 37.366772406030535
Iteration: 11 || Loss: 37.267535040826175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.35706
Epoch 30 loss:37.267535040826175
MSE loss S0.6000287050929926
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.35706
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 263.22782455430337
Iteration: 2 || Loss: 263.22049928409245
Iteration: 3 || Loss: 263.2134436816783
Iteration: 4 || Loss: 263.2063979675499
Iteration: 5 || Loss: 263.199355524928
Iteration: 6 || Loss: 263.199355524928
saving ADAM checkpoint...
Sum of params:-95.357185
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 263.199355524928
Iteration: 2 || Loss: 260.8291515099584
Iteration: 3 || Loss: 259.46495527959115
Iteration: 4 || Loss: 247.407912905322
Iteration: 5 || Loss: 236.9692555956956
Iteration: 6 || Loss: 234.97529022466549
Iteration: 7 || Loss: 229.81707748336865
Iteration: 8 || Loss: 228.8031664294908
Iteration: 9 || Loss: 226.64325675476925
Iteration: 10 || Loss: 226.09172634970272
Iteration: 11 || Loss: 225.6783147055079
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.487595
Epoch 30 loss:225.6783147055079
MSE loss S2.5680580494275147
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:54.43080620750816
MSE loss S0.5223688406315574
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:396.21982028071614
MSE loss S6.82169644596272
waveform batch: 2/2
Test loss - extrapolation:294.28885422901385
MSE loss S5.024927345319424
Epoch 30 mean train loss:9.38061443218331
Epoch 30 mean test loss - interpolation:9.071801034584693
Epoch 30 mean test loss - extrapolation:57.5423895424775
Start training epoch 31
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.487595
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.232655295156752
Iteration: 2 || Loss: 11.23006462327109
Iteration: 3 || Loss: 11.22734115729594
Iteration: 4 || Loss: 11.224714313177484
Iteration: 5 || Loss: 11.22205000025054
Iteration: 6 || Loss: 11.22205000025054
saving ADAM checkpoint...
Sum of params:-96.487686
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.22205000025054
Iteration: 2 || Loss: 10.808673900020649
Iteration: 3 || Loss: 10.678831476655974
Iteration: 4 || Loss: 10.558378109808823
Iteration: 5 || Loss: 9.618679417324918
Iteration: 6 || Loss: 9.40449553519202
Iteration: 7 || Loss: 9.274724554717116
Iteration: 8 || Loss: 9.163189144110808
Iteration: 9 || Loss: 8.995901779859064
Iteration: 10 || Loss: 8.900348100767431
Iteration: 11 || Loss: 8.798895623119238
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.18459
Epoch 31 loss:8.798895623119238
MSE loss S0.2750910302499521
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.18459
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.43547190965485
Iteration: 2 || Loss: 39.43434841168828
Iteration: 3 || Loss: 39.43324589892653
Iteration: 4 || Loss: 39.43211452614529
Iteration: 5 || Loss: 39.430944221298866
Iteration: 6 || Loss: 39.430944221298866
saving ADAM checkpoint...
Sum of params:-96.18454
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.430944221298866
Iteration: 2 || Loss: 39.365150569400384
Iteration: 3 || Loss: 39.33556373869347
Iteration: 4 || Loss: 38.584171527550374
Iteration: 5 || Loss: 37.97736055241597
Iteration: 6 || Loss: 37.776218337562305
Iteration: 7 || Loss: 37.28664236791945
Iteration: 8 || Loss: 36.996180703933895
Iteration: 9 || Loss: 36.81796197608793
Iteration: 10 || Loss: 36.65664425948859
Iteration: 11 || Loss: 36.56864039029862
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.17278
Epoch 31 loss:36.56864039029862
MSE loss S0.574712846079665
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.17278
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 265.5616173623248
Iteration: 2 || Loss: 265.5535247868032
Iteration: 3 || Loss: 265.5453195416457
Iteration: 4 || Loss: 265.53721682144214
Iteration: 5 || Loss: 265.52910730489066
Iteration: 6 || Loss: 265.52910730489066
saving ADAM checkpoint...
Sum of params:-95.17292
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 265.52910730489066
Iteration: 2 || Loss: 262.3037056203522
Iteration: 3 || Loss: 261.00246046507436
Iteration: 4 || Loss: 250.69130409810595
Iteration: 5 || Loss: 236.82544525259402
Iteration: 6 || Loss: 232.7250974969646
Iteration: 7 || Loss: 228.47647109875555
Iteration: 8 || Loss: 227.27344004890062
Iteration: 9 || Loss: 223.52146631982743
Iteration: 10 || Loss: 223.1736208796718
Iteration: 11 || Loss: 222.85484909602647
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.5131
Epoch 31 loss:222.85484909602647
MSE loss S2.656534635790946
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:55.290049694954206
MSE loss S0.6023801712247248
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:388.99397948648715
MSE loss S6.7975752164083545
waveform batch: 2/2
Test loss - extrapolation:286.643845348095
MSE loss S4.981421296054707
Epoch 31 mean train loss:9.249047762394632
Epoch 31 mean test loss - interpolation:9.215008282492368
Epoch 31 mean test loss - extrapolation:56.30315206954851
Start training epoch 32
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.5131
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.899459254724077
Iteration: 2 || Loss: 11.895628222200411
Iteration: 3 || Loss: 11.891859139949025
Iteration: 4 || Loss: 11.888102926400249
Iteration: 5 || Loss: 11.884307354243004
Iteration: 6 || Loss: 11.884307354243004
saving ADAM checkpoint...
Sum of params:-96.51315
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.884307354243004
Iteration: 2 || Loss: 10.990846956858203
Iteration: 3 || Loss: 10.88025322940793
Iteration: 4 || Loss: 10.79030778748593
Iteration: 5 || Loss: 9.766391072439028
Iteration: 6 || Loss: 9.621974611799295
Iteration: 7 || Loss: 9.343400519132572
Iteration: 8 || Loss: 9.306694544689316
Iteration: 9 || Loss: 9.134921627453156
Iteration: 10 || Loss: 9.061009498802928
Iteration: 11 || Loss: 8.761660980587536
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.13968
Epoch 32 loss:8.761660980587536
MSE loss S0.3026698728246867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.13968
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.88060400228646
Iteration: 2 || Loss: 38.87962865987591
Iteration: 3 || Loss: 38.87865638850462
Iteration: 4 || Loss: 38.87767333471896
Iteration: 5 || Loss: 38.87673681802593
Iteration: 6 || Loss: 38.87673681802593
saving ADAM checkpoint...
Sum of params:-96.1396
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.87673681802593
Iteration: 2 || Loss: 38.86555010896242
Iteration: 3 || Loss: 38.81741139125978
Iteration: 4 || Loss: 38.256210713982604
Iteration: 5 || Loss: 37.44394845399903
Iteration: 6 || Loss: 37.21346693984504
Iteration: 7 || Loss: 36.63543062817824
Iteration: 8 || Loss: 36.39583853524885
Iteration: 9 || Loss: 36.21636162336097
Iteration: 10 || Loss: 36.112238208913936
Iteration: 11 || Loss: 35.97477005865279
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.30286
Epoch 32 loss:35.97477005865279
MSE loss S0.5714342076093455
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.30286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 255.71926911507956
Iteration: 2 || Loss: 255.71187572349137
Iteration: 3 || Loss: 255.70473364376087
Iteration: 4 || Loss: 255.69742805208716
Iteration: 5 || Loss: 255.69011000322746
Iteration: 6 || Loss: 255.69011000322746
saving ADAM checkpoint...
Sum of params:-95.30301
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 255.69011000322746
Iteration: 2 || Loss: 253.21217334723696
Iteration: 3 || Loss: 251.99184337842826
Iteration: 4 || Loss: 240.8519226626553
Iteration: 5 || Loss: 230.4477267090224
Iteration: 6 || Loss: 227.838008952829
Iteration: 7 || Loss: 223.40454771710222
Iteration: 8 || Loss: 222.33645348219804
Iteration: 9 || Loss: 219.84026701972812
Iteration: 10 || Loss: 219.41510311100993
Iteration: 11 || Loss: 219.21276091797398
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.5421
Epoch 32 loss:219.21276091797398
MSE loss S2.4307385471112077
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:54.49968661914245
MSE loss S0.5207822745941195
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:384.63303899613777
MSE loss S6.563099881868408
waveform batch: 2/2
Test loss - extrapolation:282.22248677691726
MSE loss S4.771781477383152
Epoch 32 mean train loss:9.101696274386699
Epoch 32 mean test loss - interpolation:9.083281103190409
Epoch 32 mean test loss - extrapolation:55.571293814421246
Start training epoch 33
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.5421
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.433048657220361
Iteration: 2 || Loss: 11.429526411524337
Iteration: 3 || Loss: 11.426028505221787
Iteration: 4 || Loss: 11.42247503055939
Iteration: 5 || Loss: 11.418946640476001
Iteration: 6 || Loss: 11.418946640476001
saving ADAM checkpoint...
Sum of params:-96.54219
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.418946640476001
Iteration: 2 || Loss: 10.665111317791066
Iteration: 3 || Loss: 10.611231931273771
Iteration: 4 || Loss: 10.512247150001341
Iteration: 5 || Loss: 9.51641640275653
Iteration: 6 || Loss: 9.367466523731105
Iteration: 7 || Loss: 9.11500739764255
Iteration: 8 || Loss: 9.086313137022634
Iteration: 9 || Loss: 8.940592148678473
Iteration: 10 || Loss: 8.873088494150453
Iteration: 11 || Loss: 8.61702812600114
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.22504
Epoch 33 loss:8.61702812600114
MSE loss S0.26791871607095263
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.22504
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.718092716668146
Iteration: 2 || Loss: 38.71615474445604
Iteration: 3 || Loss: 38.71412543095098
Iteration: 4 || Loss: 38.712119775610404
Iteration: 5 || Loss: 38.710144978640294
Iteration: 6 || Loss: 38.710144978640294
saving ADAM checkpoint...
Sum of params:-96.22494
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.710144978640294
Iteration: 2 || Loss: 38.49887935998194
Iteration: 3 || Loss: 38.45973730783313
Iteration: 4 || Loss: 37.06680113658492
Iteration: 5 || Loss: 36.93747869940453
Iteration: 6 || Loss: 36.69927892903017
Iteration: 7 || Loss: 36.30608824978674
Iteration: 8 || Loss: 35.98071193277019
Iteration: 9 || Loss: 35.86649580658082
Iteration: 10 || Loss: 35.704381587188024
Iteration: 11 || Loss: 35.625440010558634
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.4169
Epoch 33 loss:35.625440010558634
MSE loss S0.5749362518843044
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.4169
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 252.03897835307328
Iteration: 2 || Loss: 252.03147400174086
Iteration: 3 || Loss: 252.02394148352434
Iteration: 4 || Loss: 252.01642672881462
Iteration: 5 || Loss: 252.00883704444936
Iteration: 6 || Loss: 252.00883704444936
saving ADAM checkpoint...
Sum of params:-95.41702
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 252.00883704444936
Iteration: 2 || Loss: 249.45454099614403
Iteration: 3 || Loss: 247.84054487186162
Iteration: 4 || Loss: 237.35278710109634
Iteration: 5 || Loss: 226.77627984164488
Iteration: 6 || Loss: 224.6074001985471
Iteration: 7 || Loss: 219.77640928331746
Iteration: 8 || Loss: 218.83438195034026
Iteration: 9 || Loss: 216.77283007083182
Iteration: 10 || Loss: 216.2719959192407
Iteration: 11 || Loss: 216.08674828039725
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.5569
Epoch 33 loss:216.08674828039725
MSE loss S2.4165671080181648
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:52.77255427444972
MSE loss S0.49249814645918943
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:382.2259660210843
MSE loss S6.558909956956281
waveform batch: 2/2
Test loss - extrapolation:281.23722922602803
MSE loss S4.7978856792879725
Epoch 33 mean train loss:8.976869531619208
Epoch 33 mean test loss - interpolation:8.795425712408287
Epoch 33 mean test loss - extrapolation:55.28859960392602
Start training epoch 34
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.5569
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.979092872372288
Iteration: 2 || Loss: 10.97581802660346
Iteration: 3 || Loss: 10.972578414885133
Iteration: 4 || Loss: 10.969353777312637
Iteration: 5 || Loss: 10.966106965403224
Iteration: 6 || Loss: 10.966106965403224
saving ADAM checkpoint...
Sum of params:-96.55698
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.966106965403224
Iteration: 2 || Loss: 10.33861447952762
Iteration: 3 || Loss: 10.171020859436972
Iteration: 4 || Loss: 10.127911831869278
Iteration: 5 || Loss: 9.235624844110966
Iteration: 6 || Loss: 9.091120217097936
Iteration: 7 || Loss: 8.903824390146298
Iteration: 8 || Loss: 8.87179634476478
Iteration: 9 || Loss: 8.715580388538253
Iteration: 10 || Loss: 8.625114096941926
Iteration: 11 || Loss: 8.460268026255433
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.27145
Epoch 34 loss:8.460268026255433
MSE loss S0.280126525585628
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.27145
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.02829136597532
Iteration: 2 || Loss: 38.0269793853516
Iteration: 3 || Loss: 38.02555242419184
Iteration: 4 || Loss: 38.02425489529934
Iteration: 5 || Loss: 38.02293498626973
Iteration: 6 || Loss: 38.02293498626973
saving ADAM checkpoint...
Sum of params:-96.27138
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.02293498626973
Iteration: 2 || Loss: 37.92577523130899
Iteration: 3 || Loss: 37.88362617948834
Iteration: 4 || Loss: 37.16199511318009
Iteration: 5 || Loss: 36.41568948189584
Iteration: 6 || Loss: 36.234764444494665
Iteration: 7 || Loss: 35.85889048434877
Iteration: 8 || Loss: 35.557411454935206
Iteration: 9 || Loss: 35.440488516700384
Iteration: 10 || Loss: 35.2596363656908
Iteration: 11 || Loss: 35.182487921789715
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.37725
Epoch 34 loss:35.182487921789715
MSE loss S0.5418287232199891
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.37725
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 249.4592039653046
Iteration: 2 || Loss: 249.45169799913694
Iteration: 3 || Loss: 249.4442597786267
Iteration: 4 || Loss: 249.43671993501405
Iteration: 5 || Loss: 249.4291618365587
Iteration: 6 || Loss: 249.4291618365587
saving ADAM checkpoint...
Sum of params:-95.37738
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 249.4291618365587
Iteration: 2 || Loss: 246.78803326233034
Iteration: 3 || Loss: 245.51966351367764
Iteration: 4 || Loss: 235.70029129130756
Iteration: 5 || Loss: 224.9732382265466
Iteration: 6 || Loss: 222.57927955358772
Iteration: 7 || Loss: 217.33447963854923
Iteration: 8 || Loss: 216.32927382453616
Iteration: 9 || Loss: 214.52251227386967
Iteration: 10 || Loss: 213.53442832268124
Iteration: 11 || Loss: 213.2084597605797
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.617134
Epoch 34 loss:213.2084597605797
MSE loss S2.3489298984726217
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:52.738612671809804
MSE loss S0.4897888832566666
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:376.5396351131042
MSE loss S6.429144012315779
waveform batch: 2/2
Test loss - extrapolation:276.44853165453316
MSE loss S4.701414753035223
Epoch 34 mean train loss:8.856938472711203
Epoch 34 mean test loss - interpolation:8.789768778634967
Epoch 34 mean test loss - extrapolation:54.41568056396978
Start training epoch 35
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.617134
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.138437126312992
Iteration: 2 || Loss: 11.134877114255044
Iteration: 3 || Loss: 11.131371461970037
Iteration: 4 || Loss: 11.127842588903672
Iteration: 5 || Loss: 11.124361835216181
Iteration: 6 || Loss: 11.124361835216181
saving ADAM checkpoint...
Sum of params:-96.617226
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.124361835216181
Iteration: 2 || Loss: 10.375286274473522
Iteration: 3 || Loss: 10.230005597153319
Iteration: 4 || Loss: 10.15339108320402
Iteration: 5 || Loss: 9.260559098046086
Iteration: 6 || Loss: 9.134576052244585
Iteration: 7 || Loss: 8.905195321980731
Iteration: 8 || Loss: 8.879698015274062
Iteration: 9 || Loss: 8.698052986155979
Iteration: 10 || Loss: 8.624445452733198
Iteration: 11 || Loss: 8.40641738314373
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.328575
Epoch 35 loss:8.40641738314373
MSE loss S0.28002529188867703
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.328575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.77870999312159
Iteration: 2 || Loss: 37.77706203831535
Iteration: 3 || Loss: 37.77554832483695
Iteration: 4 || Loss: 37.773900205359034
Iteration: 5 || Loss: 37.77235382177558
Iteration: 6 || Loss: 37.77235382177558
saving ADAM checkpoint...
Sum of params:-96.32851
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.77235382177558
Iteration: 2 || Loss: 37.63841253762788
Iteration: 3 || Loss: 37.59570026386445
Iteration: 4 || Loss: 36.78572914089228
Iteration: 5 || Loss: 36.0582981752333
Iteration: 6 || Loss: 35.88009732839189
Iteration: 7 || Loss: 35.50028119986874
Iteration: 8 || Loss: 35.186719990938016
Iteration: 9 || Loss: 35.08092278223208
Iteration: 10 || Loss: 34.885815893701924
Iteration: 11 || Loss: 34.8214618145618
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.492775
Epoch 35 loss:34.8214618145618
MSE loss S0.5356788708564552
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.492775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 242.97992153461928
Iteration: 2 || Loss: 242.97264907278537
Iteration: 3 || Loss: 242.96539571842547
Iteration: 4 || Loss: 242.9582632654473
Iteration: 5 || Loss: 242.9510388445065
Iteration: 6 || Loss: 242.9510388445065
saving ADAM checkpoint...
Sum of params:-95.4929
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 242.9510388445065
Iteration: 2 || Loss: 240.58777058071396
Iteration: 3 || Loss: 239.29940461202702
Iteration: 4 || Loss: 229.7698472542714
Iteration: 5 || Loss: 220.57272603817924
Iteration: 6 || Loss: 218.5723422520411
Iteration: 7 || Loss: 213.50923670492296
Iteration: 8 || Loss: 212.5350314158615
Iteration: 9 || Loss: 210.5052943830877
Iteration: 10 || Loss: 210.28889747805383
Iteration: 11 || Loss: 209.72274846551804
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.75075
Epoch 35 loss:209.72274846551804
MSE loss S2.518906814395741
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:51.797900327973835
MSE loss S0.5618136379818499
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:370.0399568360338
MSE loss S6.523363291031131
waveform batch: 2/2
Test loss - extrapolation:272.2416064516652
MSE loss S4.8116297861761606
Epoch 35 mean train loss:8.722435436662881
Epoch 35 mean test loss - interpolation:8.632983387995639
Epoch 35 mean test loss - extrapolation:53.52346360730825
Start training epoch 36
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.75075
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.381769324832725
Iteration: 2 || Loss: 10.37991870828153
Iteration: 3 || Loss: 10.37803207652624
Iteration: 4 || Loss: 10.37612809442127
Iteration: 5 || Loss: 10.374279952417966
Iteration: 6 || Loss: 10.374279952417966
saving ADAM checkpoint...
Sum of params:-96.750824
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.374279952417966
Iteration: 2 || Loss: 10.172087715320272
Iteration: 3 || Loss: 10.110219367855505
Iteration: 4 || Loss: 10.026161473029198
Iteration: 5 || Loss: 9.193017016121736
Iteration: 6 || Loss: 8.882360328829483
Iteration: 7 || Loss: 8.835275774344469
Iteration: 8 || Loss: 8.615204646270094
Iteration: 9 || Loss: 8.45180604186606
Iteration: 10 || Loss: 8.299178847859766
Iteration: 11 || Loss: 8.142651940320414
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.34951
Epoch 36 loss:8.142651940320414
MSE loss S0.24480765806650445
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.34951
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.059216441059974
Iteration: 2 || Loss: 37.057827255586034
Iteration: 3 || Loss: 37.05641558594677
Iteration: 4 || Loss: 37.05504756998424
Iteration: 5 || Loss: 37.05367486432269
Iteration: 6 || Loss: 37.05367486432269
saving ADAM checkpoint...
Sum of params:-96.34942
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.05367486432269
Iteration: 2 || Loss: 36.96074502015203
Iteration: 3 || Loss: 36.92851320043885
Iteration: 4 || Loss: 36.47681569140526
Iteration: 5 || Loss: 35.50274954880723
Iteration: 6 || Loss: 35.337928793614815
Iteration: 7 || Loss: 34.99787431718546
Iteration: 8 || Loss: 34.62808303029357
Iteration: 9 || Loss: 34.549767064940625
Iteration: 10 || Loss: 34.41034667438053
Iteration: 11 || Loss: 34.30607808472686
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.44708
Epoch 36 loss:34.30607808472686
MSE loss S0.5546522913842016
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.44708
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 239.66373205289318
Iteration: 2 || Loss: 239.65597541176868
Iteration: 3 || Loss: 239.64831603646158
Iteration: 4 || Loss: 239.64065998389694
Iteration: 5 || Loss: 239.6330506579143
Iteration: 6 || Loss: 239.6330506579143
saving ADAM checkpoint...
Sum of params:-95.447205
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 239.6330506579143
Iteration: 2 || Loss: 236.9200123462671
Iteration: 3 || Loss: 235.3893675126254
Iteration: 4 || Loss: 227.2250336785158
Iteration: 5 || Loss: 217.41662613626733
Iteration: 6 || Loss: 215.18673763309155
Iteration: 7 || Loss: 210.70082771726481
Iteration: 8 || Loss: 209.77353895442155
Iteration: 9 || Loss: 207.38754941576923
Iteration: 10 || Loss: 206.99009025912545
Iteration: 11 || Loss: 206.7524991316306
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.67776
Epoch 36 loss:206.7524991316306
MSE loss S2.365141139250712
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:51.12079917236429
MSE loss S0.49711820590046485
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:367.288346832517
MSE loss S6.373846944615359
waveform batch: 2/2
Test loss - extrapolation:268.86537562578656
MSE loss S4.675343767704911
Epoch 36 mean train loss:8.593145832988892
Epoch 36 mean test loss - interpolation:8.520133195394049
Epoch 36 mean test loss - extrapolation:53.012810204858624
Start training epoch 37
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.67776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.53313695440921
Iteration: 2 || Loss: 10.530375608532466
Iteration: 3 || Loss: 10.527659529311888
Iteration: 4 || Loss: 10.52485939383959
Iteration: 5 || Loss: 10.522079770203122
Iteration: 6 || Loss: 10.522079770203122
saving ADAM checkpoint...
Sum of params:-96.67786
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.522079770203122
Iteration: 2 || Loss: 10.084876672753774
Iteration: 3 || Loss: 9.926713782449085
Iteration: 4 || Loss: 9.86526486196214
Iteration: 5 || Loss: 9.006045718137566
Iteration: 6 || Loss: 8.778502088953646
Iteration: 7 || Loss: 8.663059445806438
Iteration: 8 || Loss: 8.575309244415596
Iteration: 9 || Loss: 8.422318181102963
Iteration: 10 || Loss: 8.252892781206405
Iteration: 11 || Loss: 8.172106545513724
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.389084
Epoch 37 loss:8.172106545513724
MSE loss S0.2737602421354199
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.389084
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.68092180064057
Iteration: 2 || Loss: 36.6795043570502
Iteration: 3 || Loss: 36.67805096854068
Iteration: 4 || Loss: 36.67664156543398
Iteration: 5 || Loss: 36.67522645203779
Iteration: 6 || Loss: 36.67522645203779
saving ADAM checkpoint...
Sum of params:-96.38902
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.67522645203779
Iteration: 2 || Loss: 36.57274266316293
Iteration: 3 || Loss: 36.53898522458427
Iteration: 4 || Loss: 35.722301086793856
Iteration: 5 || Loss: 35.076898625068885
Iteration: 6 || Loss: 34.89620212211986
Iteration: 7 || Loss: 34.57650746231584
Iteration: 8 || Loss: 34.25615167675007
Iteration: 9 || Loss: 34.15035931118164
Iteration: 10 || Loss: 33.99995640373252
Iteration: 11 || Loss: 33.93576837296256
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.54163
Epoch 37 loss:33.93576837296256
MSE loss S0.5265559315536177
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.54163
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 235.56695827272532
Iteration: 2 || Loss: 235.55958415322672
Iteration: 3 || Loss: 235.5522394848814
Iteration: 4 || Loss: 235.5450949112593
Iteration: 5 || Loss: 235.53776559514318
Iteration: 6 || Loss: 235.53776559514318
saving ADAM checkpoint...
Sum of params:-95.54179
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 235.53776559514318
Iteration: 2 || Loss: 233.12792050602224
Iteration: 3 || Loss: 231.6075909234518
Iteration: 4 || Loss: 223.20350387662816
Iteration: 5 || Loss: 214.24376755055846
Iteration: 6 || Loss: 212.64461153866839
Iteration: 7 || Loss: 207.23998541470047
Iteration: 8 || Loss: 206.2888570315256
Iteration: 9 || Loss: 204.37014273193518
Iteration: 10 || Loss: 204.0657335400342
Iteration: 11 || Loss: 203.51223573510737
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.74622
Epoch 37 loss:203.51223573510737
MSE loss S2.366275687972779
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:49.903405304265014
MSE loss S0.4697787749670663
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:361.87463259888676
MSE loss S6.334269659631431
waveform batch: 2/2
Test loss - extrapolation:265.6039651465063
MSE loss S4.718290308043907
Epoch 37 mean train loss:8.469658988054608
Epoch 37 mean test loss - interpolation:8.317234217377502
Epoch 37 mean test loss - extrapolation:52.289883145449416
Start training epoch 38
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.74622
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.880796112264687
Iteration: 2 || Loss: 9.88029727714859
Iteration: 3 || Loss: 9.879778280186216
Iteration: 4 || Loss: 9.879255254802304
Iteration: 5 || Loss: 9.878681338213172
Iteration: 6 || Loss: 9.878681338213172
saving ADAM checkpoint...
Sum of params:-96.746254
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.878681338213172
Iteration: 2 || Loss: 9.869145023401085
Iteration: 3 || Loss: 9.790557646385976
Iteration: 4 || Loss: 9.75769249212755
Iteration: 5 || Loss: 8.891384752638883
Iteration: 6 || Loss: 8.583554472897328
Iteration: 7 || Loss: 8.501479608256844
Iteration: 8 || Loss: 8.240806963553357
Iteration: 9 || Loss: 8.134253538316075
Iteration: 10 || Loss: 7.957358076657196
Iteration: 11 || Loss: 7.898942792189502
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.38507
Epoch 38 loss:7.898942792189502
MSE loss S0.23039783166503208
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.38507
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.11316168605483
Iteration: 2 || Loss: 36.111585406248516
Iteration: 3 || Loss: 36.11005453587939
Iteration: 4 || Loss: 36.108465618080714
Iteration: 5 || Loss: 36.10696027709537
Iteration: 6 || Loss: 36.10696027709537
saving ADAM checkpoint...
Sum of params:-96.385
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.10696027709537
Iteration: 2 || Loss: 35.986525354960214
Iteration: 3 || Loss: 35.956942305404894
Iteration: 4 || Loss: 35.49912677408733
Iteration: 5 || Loss: 34.55461534275749
Iteration: 6 || Loss: 34.37882567085258
Iteration: 7 || Loss: 34.15436540995038
Iteration: 8 || Loss: 33.773925475948275
Iteration: 9 || Loss: 33.70081320143098
Iteration: 10 || Loss: 33.58854467998566
Iteration: 11 || Loss: 33.434609081003025
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.44591
Epoch 38 loss:33.434609081003025
MSE loss S0.5465776812073327
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.44591
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 234.1284465769628
Iteration: 2 || Loss: 234.1206837725027
Iteration: 3 || Loss: 234.1129394027159
Iteration: 4 || Loss: 234.10516287637483
Iteration: 5 || Loss: 234.09735939597206
Iteration: 6 || Loss: 234.09735939597206
saving ADAM checkpoint...
Sum of params:-95.44602
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 234.09735939597206
Iteration: 2 || Loss: 231.3983227788348
Iteration: 3 || Loss: 229.44309726269736
Iteration: 4 || Loss: 221.27095627846654
Iteration: 5 || Loss: 211.4393141853233
Iteration: 6 || Loss: 209.5133594258512
Iteration: 7 || Loss: 204.84529186740014
Iteration: 8 || Loss: 203.94076046029704
Iteration: 9 || Loss: 201.88795023518662
Iteration: 10 || Loss: 201.08346210717556
Iteration: 11 || Loss: 200.55213791886507
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.7443
Epoch 38 loss:200.55213791886507
MSE loss S2.1819695332236253
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:50.24325879681915
MSE loss S0.4393337293512456
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:357.0911861975358
MSE loss S6.090771737789548
waveform batch: 2/2
Test loss - extrapolation:260.1470167325862
MSE loss S4.476043233673371
Epoch 38 mean train loss:8.340885854898538
Epoch 38 mean test loss - interpolation:8.373876466136524
Epoch 38 mean test loss - extrapolation:51.4365169108435
Start training epoch 39
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.7443
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.04946146582392
Iteration: 2 || Loss: 10.047989829399883
Iteration: 3 || Loss: 10.046641451404396
Iteration: 4 || Loss: 10.045192615469654
Iteration: 5 || Loss: 10.043761131629266
Iteration: 6 || Loss: 10.043761131629266
saving ADAM checkpoint...
Sum of params:-96.74428
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.043761131629266
Iteration: 2 || Loss: 9.918814288626496
Iteration: 3 || Loss: 9.8870770757091
Iteration: 4 || Loss: 9.281243805358999
Iteration: 5 || Loss: 8.857698032182535
Iteration: 6 || Loss: 8.522922801347898
Iteration: 7 || Loss: 8.46408042351207
Iteration: 8 || Loss: 8.28763127735663
Iteration: 9 || Loss: 8.127344733549009
Iteration: 10 || Loss: 8.02634962240146
Iteration: 11 || Loss: 7.900309291715859
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.436325
Epoch 39 loss:7.900309291715859
MSE loss S0.23458394597267818
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.436325
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.96343120028904
Iteration: 2 || Loss: 35.96125658468207
Iteration: 3 || Loss: 35.959075697741184
Iteration: 4 || Loss: 35.95697678274555
Iteration: 5 || Loss: 35.954802210762345
Iteration: 6 || Loss: 35.954802210762345
saving ADAM checkpoint...
Sum of params:-96.43626
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.954802210762345
Iteration: 2 || Loss: 35.71257341444228
Iteration: 3 || Loss: 35.68041657968699
Iteration: 4 || Loss: 34.130117986887534
Iteration: 5 || Loss: 34.10893727752523
Iteration: 6 || Loss: 33.949111608739805
Iteration: 7 || Loss: 33.79639836694757
Iteration: 8 || Loss: 33.39601846904557
Iteration: 9 || Loss: 33.294992016189866
Iteration: 10 || Loss: 33.19629846284312
Iteration: 11 || Loss: 33.1229610473876
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.699776
Epoch 39 loss:33.1229610473876
MSE loss S0.5462142756111313
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.699776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 225.82048393816763
Iteration: 2 || Loss: 225.8131181073482
Iteration: 3 || Loss: 225.80578421115018
Iteration: 4 || Loss: 225.79843055872936
Iteration: 5 || Loss: 225.7910955571783
Iteration: 6 || Loss: 225.7910955571783
saving ADAM checkpoint...
Sum of params:-95.69989
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 225.7910955571783
Iteration: 2 || Loss: 223.43301077632324
Iteration: 3 || Loss: 219.6066807539157
Iteration: 4 || Loss: 214.23501160811628
Iteration: 5 || Loss: 206.2496293829267
Iteration: 6 || Loss: 205.11973880132578
Iteration: 7 || Loss: 200.26600222317438
Iteration: 8 || Loss: 199.62506897513728
Iteration: 9 || Loss: 198.54821378725296
Iteration: 10 || Loss: 197.6784591714075
Iteration: 11 || Loss: 197.3642218952579
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.74751
Epoch 39 loss:197.3642218952579
MSE loss S2.370796192305103
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:48.71926629076521
MSE loss S0.5263365269564649
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:356.62063828124747
MSE loss S6.339016392141365
waveform batch: 2/2
Test loss - extrapolation:259.3520238464394
MSE loss S4.59843607004458
Epoch 39 mean train loss:8.220258352909012
Epoch 39 mean test loss - interpolation:8.119877715127535
Epoch 39 mean test loss - extrapolation:51.331055177307235
Start training epoch 40
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.74751
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.251842095064038
Iteration: 2 || Loss: 10.248724931473706
Iteration: 3 || Loss: 10.245545212614383
Iteration: 4 || Loss: 10.242447851502154
Iteration: 5 || Loss: 10.239320096223125
Iteration: 6 || Loss: 10.239320096223125
saving ADAM checkpoint...
Sum of params:-96.74757
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.239320096223125
Iteration: 2 || Loss: 9.696195920506739
Iteration: 3 || Loss: 9.518457564568259
Iteration: 4 || Loss: 9.253631525672223
Iteration: 5 || Loss: 8.498801675191725
Iteration: 6 || Loss: 8.300065129852891
Iteration: 7 || Loss: 8.211118375253573
Iteration: 8 || Loss: 8.125571071200307
Iteration: 9 || Loss: 8.030766321003698
Iteration: 10 || Loss: 7.916647567214679
Iteration: 11 || Loss: 7.8734881486306785
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.54264
Epoch 40 loss:7.8734881486306785
MSE loss S0.25910730897961054
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.54264
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.37145574160168
Iteration: 2 || Loss: 35.36982300405552
Iteration: 3 || Loss: 35.36828213212462
Iteration: 4 || Loss: 35.36662303898609
Iteration: 5 || Loss: 35.36506491632527
Iteration: 6 || Loss: 35.36506491632527
saving ADAM checkpoint...
Sum of params:-96.54259
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.36506491632527
Iteration: 2 || Loss: 35.23385737728748
Iteration: 3 || Loss: 35.202945596726266
Iteration: 4 || Loss: 34.51233474964512
Iteration: 5 || Loss: 33.86764108726729
Iteration: 6 || Loss: 33.7065507933565
Iteration: 7 || Loss: 33.47626190642426
Iteration: 8 || Loss: 33.14853407579268
Iteration: 9 || Loss: 33.07042449659712
Iteration: 10 || Loss: 32.94101173081549
Iteration: 11 || Loss: 32.86769193738065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.70657
Epoch 40 loss:32.86769193738065
MSE loss S0.5134224690397898
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.70657
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 222.7402370142037
Iteration: 2 || Loss: 222.73312792545403
Iteration: 3 || Loss: 222.7259401110143
Iteration: 4 || Loss: 222.7189157580098
Iteration: 5 || Loss: 222.71177399380406
Iteration: 6 || Loss: 222.71177399380406
saving ADAM checkpoint...
Sum of params:-95.706696
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 222.71177399380406
Iteration: 2 || Loss: 220.51117010652916
Iteration: 3 || Loss: 218.9344377118294
Iteration: 4 || Loss: 211.70735060011518
Iteration: 5 || Loss: 204.23241984274344
Iteration: 6 || Loss: 203.15440499823626
Iteration: 7 || Loss: 198.01510616131893
Iteration: 8 || Loss: 197.25845221788015
Iteration: 9 || Loss: 196.31478890759067
Iteration: 10 || Loss: 195.24504829938314
Iteration: 11 || Loss: 194.8293123196175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.80318
Epoch 40 loss:194.8293123196175
MSE loss S2.1013367664775586
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:47.38963415118781
MSE loss S0.39375227925836087
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:351.8416473200593
MSE loss S5.986574896164685
waveform batch: 2/2
Test loss - extrapolation:257.26225621136433
MSE loss S4.427819066065581
Epoch 40 mean train loss:8.123120427780304
Epoch 40 mean test loss - interpolation:7.898272358531302
Epoch 40 mean test loss - extrapolation:50.75865862761864
Start training epoch 41
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.80318
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.569894259649924
Iteration: 2 || Loss: 9.568457371529629
Iteration: 3 || Loss: 9.567054561335622
Iteration: 4 || Loss: 9.565608059642626
Iteration: 5 || Loss: 9.564182253187031
Iteration: 6 || Loss: 9.564182253187031
saving ADAM checkpoint...
Sum of params:-96.80319
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.564182253187031
Iteration: 2 || Loss: 9.366901938370294
Iteration: 3 || Loss: 9.19982816579457
Iteration: 4 || Loss: 9.13772578908746
Iteration: 5 || Loss: 8.497888874398033
Iteration: 6 || Loss: 8.261130737857233
Iteration: 7 || Loss: 8.208151850232884
Iteration: 8 || Loss: 8.011654653383431
Iteration: 9 || Loss: 7.943372979578211
Iteration: 10 || Loss: 7.798552058693634
Iteration: 11 || Loss: 7.728123299318985
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.57044
Epoch 41 loss:7.728123299318985
MSE loss S0.23695607917864753
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.57044
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.18839186632187
Iteration: 2 || Loss: 35.18670006066624
Iteration: 3 || Loss: 35.18513077146567
Iteration: 4 || Loss: 35.1834660536533
Iteration: 5 || Loss: 35.18188939718987
Iteration: 6 || Loss: 35.18188939718987
saving ADAM checkpoint...
Sum of params:-96.5704
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.18188939718987
Iteration: 2 || Loss: 35.041613762999276
Iteration: 3 || Loss: 34.997782909181666
Iteration: 4 || Loss: 34.202074704001234
Iteration: 5 || Loss: 33.47724800580433
Iteration: 6 || Loss: 33.34356071195011
Iteration: 7 || Loss: 33.19340215812689
Iteration: 8 || Loss: 32.78903626290442
Iteration: 9 || Loss: 32.71986326866835
Iteration: 10 || Loss: 32.61840623884906
Iteration: 11 || Loss: 32.56377277277159
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.78757
Epoch 41 loss:32.56377277277159
MSE loss S0.5219748562388997
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.78757
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 217.86734069467468
Iteration: 2 || Loss: 217.8600336339543
Iteration: 3 || Loss: 217.85287330802524
Iteration: 4 || Loss: 217.8455371668854
Iteration: 5 || Loss: 217.83836672399045
Iteration: 6 || Loss: 217.83836672399045
saving ADAM checkpoint...
Sum of params:-95.78769
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 217.83836672399045
Iteration: 2 || Loss: 215.57183848751274
Iteration: 3 || Loss: 213.48284330631276
Iteration: 4 || Loss: 207.60632094284338
Iteration: 5 || Loss: 200.570326503633
Iteration: 6 || Loss: 199.33597402271374
Iteration: 7 || Loss: 194.87237448020335
Iteration: 8 || Loss: 194.05149506928274
Iteration: 9 || Loss: 192.93482581108051
Iteration: 10 || Loss: 192.41510768271664
Iteration: 11 || Loss: 191.68413639207859
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.88033
Epoch 41 loss:191.68413639207859
MSE loss S2.068394137221363
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:47.01513966430769
MSE loss S0.39574832351751826
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:346.59886874944516
MSE loss S5.926535909226513
waveform batch: 2/2
Test loss - extrapolation:252.2919725761857
MSE loss S4.370223576712401
Epoch 41 mean train loss:7.999173533247213
Epoch 41 mean test loss - interpolation:7.835856610717948
Epoch 41 mean test loss - extrapolation:49.90757011046924
Start training epoch 42
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.88033
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.449978856336593
Iteration: 2 || Loss: 9.449098353188518
Iteration: 3 || Loss: 9.448124602221583
Iteration: 4 || Loss: 9.447248095233412
Iteration: 5 || Loss: 9.446326318459475
Iteration: 6 || Loss: 9.446326318459475
saving ADAM checkpoint...
Sum of params:-96.88046
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.446326318459475
Iteration: 2 || Loss: 9.211899731490861
Iteration: 3 || Loss: 9.201400290244685
Iteration: 4 || Loss: 9.102171706631555
Iteration: 5 || Loss: 8.38326246098941
Iteration: 6 || Loss: 8.146702506751417
Iteration: 7 || Loss: 8.073260885051802
Iteration: 8 || Loss: 7.8887450496552685
Iteration: 9 || Loss: 7.78676624583842
Iteration: 10 || Loss: 7.658273577578568
Iteration: 11 || Loss: 7.606719850164368
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.62349
Epoch 42 loss:7.606719850164368
MSE loss S0.22748266429112962
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.62349
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.86043161613865
Iteration: 2 || Loss: 34.85866268951628
Iteration: 3 || Loss: 34.856989163747976
Iteration: 4 || Loss: 34.85529305916451
Iteration: 5 || Loss: 34.85357062786341
Iteration: 6 || Loss: 34.85357062786341
saving ADAM checkpoint...
Sum of params:-96.62342
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.85357062786341
Iteration: 2 || Loss: 34.70667566795408
Iteration: 3 || Loss: 34.674217325186575
Iteration: 4 || Loss: 34.209443303901175
Iteration: 5 || Loss: 33.20180992648084
Iteration: 6 || Loss: 33.041436145471884
Iteration: 7 || Loss: 32.88478392811761
Iteration: 8 || Loss: 32.48699935839145
Iteration: 9 || Loss: 32.42829002246613
Iteration: 10 || Loss: 32.33607870897157
Iteration: 11 || Loss: 32.22649530571735
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.774055
Epoch 42 loss:32.22649530571735
MSE loss S0.5224499513670648
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.774055
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 215.6243344691392
Iteration: 2 || Loss: 215.6169682661802
Iteration: 3 || Loss: 215.60966877874387
Iteration: 4 || Loss: 215.60247837920045
Iteration: 5 || Loss: 215.5952408674512
Iteration: 6 || Loss: 215.5952408674512
saving ADAM checkpoint...
Sum of params:-95.77417
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 215.5952408674512
Iteration: 2 || Loss: 213.32119683557843
Iteration: 3 || Loss: 210.53123904365066
Iteration: 4 || Loss: 204.93136994430293
Iteration: 5 || Loss: 197.69685648262148
Iteration: 6 || Loss: 196.04294046695466
Iteration: 7 || Loss: 192.4166841790337
Iteration: 8 || Loss: 191.84570384490937
Iteration: 9 || Loss: 190.2823081919558
Iteration: 10 || Loss: 189.56802500513382
Iteration: 11 || Loss: 189.3949542189651
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.81087
Epoch 42 loss:189.3949542189651
MSE loss S2.168033816896746
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:46.04654239620808
MSE loss S0.4258231643261733
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:346.12810429023335
MSE loss S6.017355564081508
waveform batch: 2/2
Test loss - extrapolation:251.67843533699016
MSE loss S4.430298603624411
Epoch 42 mean train loss:7.904419633615407
Epoch 42 mean test loss - interpolation:7.674423732701347
Epoch 42 mean test loss - extrapolation:49.81721163560196
Start training epoch 43
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.81087
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.36620580839107
Iteration: 2 || Loss: 9.363523834709342
Iteration: 3 || Loss: 9.360892834172446
Iteration: 4 || Loss: 9.358245212102402
Iteration: 5 || Loss: 9.355624077198408
Iteration: 6 || Loss: 9.355624077198408
saving ADAM checkpoint...
Sum of params:-96.810974
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.355624077198408
Iteration: 2 || Loss: 8.954697662681848
Iteration: 3 || Loss: 8.929682730495978
Iteration: 4 || Loss: 8.871056212573798
Iteration: 5 || Loss: 8.256946853426964
Iteration: 6 || Loss: 8.039226179335316
Iteration: 7 || Loss: 7.9944375334233
Iteration: 8 || Loss: 7.895198908721694
Iteration: 9 || Loss: 7.836366239246342
Iteration: 10 || Loss: 7.694253954025759
Iteration: 11 || Loss: 7.636382194103039
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.64425
Epoch 43 loss:7.636382194103039
MSE loss S0.2474766259506473
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.64425
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.46483225487956
Iteration: 2 || Loss: 34.46273545214685
Iteration: 3 || Loss: 34.4605972230331
Iteration: 4 || Loss: 34.458502906074415
Iteration: 5 || Loss: 34.45645540155119
Iteration: 6 || Loss: 34.45645540155119
saving ADAM checkpoint...
Sum of params:-96.644165
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.45645540155119
Iteration: 2 || Loss: 34.22334961121897
Iteration: 3 || Loss: 34.178451483223625
Iteration: 4 || Loss: 33.638381880413014
Iteration: 5 || Loss: 32.7933622711926
Iteration: 6 || Loss: 32.66239610987663
Iteration: 7 || Loss: 32.536983406130446
Iteration: 8 || Loss: 32.21327771739597
Iteration: 9 || Loss: 32.13808566551566
Iteration: 10 || Loss: 32.044145131492854
Iteration: 11 || Loss: 31.987478740216208
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.933105
Epoch 43 loss:31.987478740216208
MSE loss S0.4983586794520353
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.933105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 209.91286228243345
Iteration: 2 || Loss: 209.9063800247702
Iteration: 3 || Loss: 209.8998207033471
Iteration: 4 || Loss: 209.89338004841517
Iteration: 5 || Loss: 209.88685765038824
Iteration: 6 || Loss: 209.88685765038824
saving ADAM checkpoint...
Sum of params:-95.93324
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 209.88685765038824
Iteration: 2 || Loss: 208.133826250958
Iteration: 3 || Loss: 206.3849462455689
Iteration: 4 || Loss: 200.10497139938084
Iteration: 5 || Loss: 194.7322682268497
Iteration: 6 || Loss: 193.71461750065095
Iteration: 7 || Loss: 189.36690031441984
Iteration: 8 || Loss: 188.78983177042653
Iteration: 9 || Loss: 188.08597913371457
Iteration: 10 || Loss: 187.2919228973692
Iteration: 11 || Loss: 186.87084628638203
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.91455
Epoch 43 loss:186.87084628638203
MSE loss S2.049972498207951
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:45.54037497039194
MSE loss S0.3840682480611805
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:341.0967893223738
MSE loss S5.854957461361781
waveform batch: 2/2
Test loss - extrapolation:247.85052186208523
MSE loss S4.3304027348063725
Epoch 43 mean train loss:7.810162317955217
Epoch 43 mean test loss - interpolation:7.590062495065323
Epoch 43 mean test loss - extrapolation:49.07894259870492
Start training epoch 44
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.91455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.058676434724049
Iteration: 2 || Loss: 9.05788005995722
Iteration: 3 || Loss: 9.057125777368128
Iteration: 4 || Loss: 9.05633010605704
Iteration: 5 || Loss: 9.055549464139744
Iteration: 6 || Loss: 9.055549464139744
saving ADAM checkpoint...
Sum of params:-96.9146
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.055549464139744
Iteration: 2 || Loss: 9.003873593242357
Iteration: 3 || Loss: 8.899623474061096
Iteration: 4 || Loss: 8.81754461183267
Iteration: 5 || Loss: 8.293622096329873
Iteration: 6 || Loss: 8.049912473202541
Iteration: 7 || Loss: 7.902707705979503
Iteration: 8 || Loss: 7.8133476648228575
Iteration: 9 || Loss: 7.692514395205626
Iteration: 10 || Loss: 7.547383141954345
Iteration: 11 || Loss: 7.494115469203362
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.71613
Epoch 44 loss:7.494115469203362
MSE loss S0.23259657007242926
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.71613
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.288588194348776
Iteration: 2 || Loss: 34.287076103016176
Iteration: 3 || Loss: 34.285493476940694
Iteration: 4 || Loss: 34.28389455283747
Iteration: 5 || Loss: 34.28238543946641
Iteration: 6 || Loss: 34.28238543946641
saving ADAM checkpoint...
Sum of params:-96.71606
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.28238543946641
Iteration: 2 || Loss: 34.15683412058369
Iteration: 3 || Loss: 34.116121279875095
Iteration: 4 || Loss: 33.60112866076557
Iteration: 5 || Loss: 32.563921008934734
Iteration: 6 || Loss: 32.42068212870787
Iteration: 7 || Loss: 32.29834187711175
Iteration: 8 || Loss: 31.922383022191454
Iteration: 9 || Loss: 31.85799113878916
Iteration: 10 || Loss: 31.776020917400437
Iteration: 11 || Loss: 31.71563465300175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.92485
Epoch 44 loss:31.71563465300175
MSE loss S0.5065580811953729
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.92485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 208.0148108204651
Iteration: 2 || Loss: 208.00782931128214
Iteration: 3 || Loss: 208.0008300542604
Iteration: 4 || Loss: 207.9937779866764
Iteration: 5 || Loss: 207.98686437178
Iteration: 6 || Loss: 207.98686437178
saving ADAM checkpoint...
Sum of params:-95.924965
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 207.98686437178
Iteration: 2 || Loss: 205.92538174184017
Iteration: 3 || Loss: 203.11174040275682
Iteration: 4 || Loss: 198.37107655883506
Iteration: 5 || Loss: 192.2806595495753
Iteration: 6 || Loss: 191.0753256529251
Iteration: 7 || Loss: 187.20875064178222
Iteration: 8 || Loss: 186.72238730607287
Iteration: 9 || Loss: 185.2123403788322
Iteration: 10 || Loss: 184.84242963282838
Iteration: 11 || Loss: 184.25598321936366
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.922104
Epoch 44 loss:184.25598321936366
MSE loss S2.4221996850773975
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:44.33530319336348
MSE loss S0.5484387550581933
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:340.046301759501
MSE loss S6.228699006771354
waveform batch: 2/2
Test loss - extrapolation:247.13014319381813
MSE loss S4.55490996923307
Epoch 44 mean train loss:7.705714942812716
Epoch 44 mean test loss - interpolation:7.389217198893913
Epoch 44 mean test loss - extrapolation:48.931370412776594
Start training epoch 45
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.922104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.945707609934153
Iteration: 2 || Loss: 8.944145865102442
Iteration: 3 || Loss: 8.942604230719812
Iteration: 4 || Loss: 8.941026527820158
Iteration: 5 || Loss: 8.939536074184907
Iteration: 6 || Loss: 8.939536074184907
saving ADAM checkpoint...
Sum of params:-96.92218
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.939536074184907
Iteration: 2 || Loss: 8.775996971491532
Iteration: 3 || Loss: 8.653908464484207
Iteration: 4 || Loss: 8.59499056741799
Iteration: 5 || Loss: 8.03499554455442
Iteration: 6 || Loss: 7.870309108715717
Iteration: 7 || Loss: 7.807673591035971
Iteration: 8 || Loss: 7.6703707427806815
Iteration: 9 || Loss: 7.6160963708185605
Iteration: 10 || Loss: 7.5345851126951295
Iteration: 11 || Loss: 7.438279686173366
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.76709
Epoch 45 loss:7.438279686173366
MSE loss S0.2315348069558198
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.76709
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.045486916166425
Iteration: 2 || Loss: 34.043885526493256
Iteration: 3 || Loss: 34.042224913018934
Iteration: 4 || Loss: 34.04059809605256
Iteration: 5 || Loss: 34.03895022563455
Iteration: 6 || Loss: 34.03895022563455
saving ADAM checkpoint...
Sum of params:-96.76708
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.03895022563455
Iteration: 2 || Loss: 33.904388354076175
Iteration: 3 || Loss: 33.86508218368028
Iteration: 4 || Loss: 33.35937746273556
Iteration: 5 || Loss: 32.26334455198722
Iteration: 6 || Loss: 32.11895628504767
Iteration: 7 || Loss: 32.00176086796198
Iteration: 8 || Loss: 31.64308757048973
Iteration: 9 || Loss: 31.586838257411124
Iteration: 10 || Loss: 31.50742457632977
Iteration: 11 || Loss: 31.4571492139189
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.07029
Epoch 45 loss:31.4571492139189
MSE loss S0.5280687163328222
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.07029
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 202.92997237973705
Iteration: 2 || Loss: 202.923195083635
Iteration: 3 || Loss: 202.9164813105716
Iteration: 4 || Loss: 202.90968564593547
Iteration: 5 || Loss: 202.90301293368694
Iteration: 6 || Loss: 202.90301293368694
saving ADAM checkpoint...
Sum of params:-96.07041
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 202.90301293368694
Iteration: 2 || Loss: 201.00830137454759
Iteration: 3 || Loss: 196.75146063614625
Iteration: 4 || Loss: 194.11076632235267
Iteration: 5 || Loss: 188.8476595972111
Iteration: 6 || Loss: 188.22509213173612
Iteration: 7 || Loss: 184.61368415092107
Iteration: 8 || Loss: 183.78957426099302
Iteration: 9 || Loss: 183.3407944147994
Iteration: 10 || Loss: 182.34279208232908
Iteration: 11 || Loss: 182.15268029459656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.0037
Epoch 45 loss:182.15268029459656
MSE loss S2.2472676473768547
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:44.35217418993791
MSE loss S0.4708353300625169
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:334.844484980521
MSE loss S6.010555834666587
waveform batch: 2/2
Test loss - extrapolation:242.45501911797797
MSE loss S4.424178117666543
Epoch 45 mean train loss:7.6223485929203045
Epoch 45 mean test loss - interpolation:7.392029031656318
Epoch 45 mean test loss - extrapolation:48.108292008208245
Start training epoch 46
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.0037
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.79707553248242
Iteration: 2 || Loss: 8.795685406774654
Iteration: 3 || Loss: 8.794372981845571
Iteration: 4 || Loss: 8.793006567347613
Iteration: 5 || Loss: 8.791658375231977
Iteration: 6 || Loss: 8.791658375231977
saving ADAM checkpoint...
Sum of params:-97.003784
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.791658375231977
Iteration: 2 || Loss: 8.696279427725814
Iteration: 3 || Loss: 8.65191663933197
Iteration: 4 || Loss: 8.576750720981986
Iteration: 5 || Loss: 8.01190469476129
Iteration: 6 || Loss: 7.789262703251937
Iteration: 7 || Loss: 7.692515456303326
Iteration: 8 || Loss: 7.599402338627124
Iteration: 9 || Loss: 7.496956535402227
Iteration: 10 || Loss: 7.403028943761809
Iteration: 11 || Loss: 7.36929529625594
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.80037
Epoch 46 loss:7.36929529625594
MSE loss S0.22985146593835593
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.80037
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.690970267936905
Iteration: 2 || Loss: 33.68950802933639
Iteration: 3 || Loss: 33.68803949093016
Iteration: 4 || Loss: 33.68650587987952
Iteration: 5 || Loss: 33.685053940679346
Iteration: 6 || Loss: 33.685053940679346
saving ADAM checkpoint...
Sum of params:-96.80028
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.685053940679346
Iteration: 2 || Loss: 33.57611009498768
Iteration: 3 || Loss: 33.539551470074294
Iteration: 4 || Loss: 33.08322418078739
Iteration: 5 || Loss: 31.993979973183134
Iteration: 6 || Loss: 31.84644553953775
Iteration: 7 || Loss: 31.729259591878662
Iteration: 8 || Loss: 31.40427274580188
Iteration: 9 || Loss: 31.33164495938022
Iteration: 10 || Loss: 31.27096420686671
Iteration: 11 || Loss: 31.213383859415938
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.1206
Epoch 46 loss:31.213383859415938
MSE loss S0.5261584535078323
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.1206
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 200.10220395412944
Iteration: 2 || Loss: 200.09567907220773
Iteration: 3 || Loss: 200.08896570030063
Iteration: 4 || Loss: 200.08238497570238
Iteration: 5 || Loss: 200.07589448744187
Iteration: 6 || Loss: 200.07589448744187
saving ADAM checkpoint...
Sum of params:-96.12072
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 200.07589448744187
Iteration: 2 || Loss: 198.30240160407948
Iteration: 3 || Loss: 194.77627331757603
Iteration: 4 || Loss: 191.39803850495335
Iteration: 5 || Loss: 186.56151395246525
Iteration: 6 || Loss: 186.00537439156093
Iteration: 7 || Loss: 182.54271891399577
Iteration: 8 || Loss: 181.7251447052086
Iteration: 9 || Loss: 181.21537582532397
Iteration: 10 || Loss: 180.31544086085745
Iteration: 11 || Loss: 180.1321706124295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.04976
Epoch 46 loss:180.1321706124295
MSE loss S2.21026897954022
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:43.75953093826314
MSE loss S0.45365259681646813
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:331.6860921667134
MSE loss S5.940399090235059
waveform batch: 2/2
Test loss - extrapolation:239.90072228028896
MSE loss S4.389510692849175
Epoch 46 mean train loss:7.541891371313841
Epoch 46 mean test loss - interpolation:7.29325515637719
Epoch 46 mean test loss - extrapolation:47.63223453725019
Start training epoch 47
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.04976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.618954371592261
Iteration: 2 || Loss: 8.618230289751377
Iteration: 3 || Loss: 8.617550291394128
Iteration: 4 || Loss: 8.616918786859262
Iteration: 5 || Loss: 8.616181208838885
Iteration: 6 || Loss: 8.616181208838885
saving ADAM checkpoint...
Sum of params:-97.049835
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.616181208838885
Iteration: 2 || Loss: 8.593460406342938
Iteration: 3 || Loss: 8.568353448929285
Iteration: 4 || Loss: 8.510474733745383
Iteration: 5 || Loss: 7.95344621380271
Iteration: 6 || Loss: 7.7360669570316505
Iteration: 7 || Loss: 7.609427049351336
Iteration: 8 || Loss: 7.508625094044957
Iteration: 9 || Loss: 7.342683734327077
Iteration: 10 || Loss: 7.305123431477165
Iteration: 11 || Loss: 7.275763757896668
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.78412
Epoch 47 loss:7.275763757896668
MSE loss S0.2176853160195244
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.78412
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.99620718212019
Iteration: 2 || Loss: 32.99448806926909
Iteration: 3 || Loss: 32.99281590294027
Iteration: 4 || Loss: 32.99111217958746
Iteration: 5 || Loss: 32.98949349261923
Iteration: 6 || Loss: 32.98949349261923
saving ADAM checkpoint...
Sum of params:-96.78413
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.98949349261923
Iteration: 2 || Loss: 32.84801453418019
Iteration: 3 || Loss: 32.81964520624698
Iteration: 4 || Loss: 32.595105487768244
Iteration: 5 || Loss: 31.759655349519825
Iteration: 6 || Loss: 31.57395820875994
Iteration: 7 || Loss: 31.4209811564313
Iteration: 8 || Loss: 31.148822052145047
Iteration: 9 || Loss: 31.100671761688623
Iteration: 10 || Loss: 31.04938284549638
Iteration: 11 || Loss: 30.989033640937777
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.23466
Epoch 47 loss:30.989033640937777
MSE loss S0.5298174248620551
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.23466
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 195.82152946034108
Iteration: 2 || Loss: 195.8152601349139
Iteration: 3 || Loss: 195.80907294336632
Iteration: 4 || Loss: 195.8029068075706
Iteration: 5 || Loss: 195.7968063711791
Iteration: 6 || Loss: 195.7968063711791
saving ADAM checkpoint...
Sum of params:-96.23478
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 195.7968063711791
Iteration: 2 || Loss: 194.24922501654297
Iteration: 3 || Loss: 188.77594978896803
Iteration: 4 || Loss: 187.7767612502536
Iteration: 5 || Loss: 183.81740204758296
Iteration: 6 || Loss: 181.77319073972893
Iteration: 7 || Loss: 179.52093350053823
Iteration: 8 || Loss: 179.18307575379498
Iteration: 9 || Loss: 178.63208206060224
Iteration: 10 || Loss: 178.23213249095454
Iteration: 11 || Loss: 177.61739856493122
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.11262
Epoch 47 loss:177.61739856493122
MSE loss S1.922922150034553
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:44.64970841247745
MSE loss S0.4131441102776755
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:327.40280747096574
MSE loss S5.613699921861497
waveform batch: 2/2
Test loss - extrapolation:233.33247508755346
MSE loss S4.021415382231359
Epoch 47 mean train loss:7.444213653922954
Epoch 47 mean test loss - interpolation:7.441618068746242
Epoch 47 mean test loss - extrapolation:46.72794021320993
Start training epoch 48
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.11262
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.834827320742441
Iteration: 2 || Loss: 8.83235342430809
Iteration: 3 || Loss: 8.829937569948587
Iteration: 4 || Loss: 8.827591366899716
Iteration: 5 || Loss: 8.8252076005152
Iteration: 6 || Loss: 8.8252076005152
saving ADAM checkpoint...
Sum of params:-97.11269
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.8252076005152
Iteration: 2 || Loss: 8.503327694566547
Iteration: 3 || Loss: 8.491600093577992
Iteration: 4 || Loss: 8.330571143471909
Iteration: 5 || Loss: 7.820812177030808
Iteration: 6 || Loss: 7.6223047145830405
Iteration: 7 || Loss: 7.6006817557070665
Iteration: 8 || Loss: 7.497435425769896
Iteration: 9 || Loss: 7.472152436673565
Iteration: 10 || Loss: 7.382727917255654
Iteration: 11 || Loss: 7.314187294121082
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.976036
Epoch 48 loss:7.314187294121082
MSE loss S0.22509348668008072
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.976036
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.56237610957992
Iteration: 2 || Loss: 33.56015465774489
Iteration: 3 || Loss: 33.55797943117557
Iteration: 4 || Loss: 33.555807423331544
Iteration: 5 || Loss: 33.55362238785906
Iteration: 6 || Loss: 33.55362238785906
saving ADAM checkpoint...
Sum of params:-96.97595
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.55362238785906
Iteration: 2 || Loss: 33.30454440858001
Iteration: 3 || Loss: 33.26444483394201
Iteration: 4 || Loss: 32.402379503187106
Iteration: 5 || Loss: 31.59106644617699
Iteration: 6 || Loss: 31.47564312682583
Iteration: 7 || Loss: 31.362684013788503
Iteration: 8 || Loss: 31.075967160572727
Iteration: 9 || Loss: 30.963315023013426
Iteration: 10 || Loss: 30.884733789202926
Iteration: 11 || Loss: 30.8419568848011
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.30686
Epoch 48 loss:30.8419568848011
MSE loss S0.49978387614406367
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.30686
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 193.2960276074102
Iteration: 2 || Loss: 193.28979985486367
Iteration: 3 || Loss: 193.28349023616246
Iteration: 4 || Loss: 193.2772160686661
Iteration: 5 || Loss: 193.27095760613204
Iteration: 6 || Loss: 193.27095760613204
saving ADAM checkpoint...
Sum of params:-96.30699
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 193.27095760613204
Iteration: 2 || Loss: 191.65442701913497
Iteration: 3 || Loss: 187.60359181686937
Iteration: 4 || Loss: 185.66887812977768
Iteration: 5 || Loss: 181.60888056806508
Iteration: 6 || Loss: 180.4520054090995
Iteration: 7 || Loss: 177.4789548385258
Iteration: 8 || Loss: 176.91947844719968
Iteration: 9 || Loss: 176.57631564785947
Iteration: 10 || Loss: 175.87004365162525
Iteration: 11 || Loss: 175.59534021424255
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.17557
Epoch 48 loss:175.59534021424255
MSE loss S2.1073746498309514
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:43.24816577591336
MSE loss S0.43982509071227255
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:324.66621241860446
MSE loss S5.779455162247232
waveform batch: 2/2
Test loss - extrapolation:233.22531584672825
MSE loss S4.241504198524568
Epoch 48 mean train loss:7.370740841143611
Epoch 48 mean test loss - interpolation:7.2080276293188925
Epoch 48 mean test loss - extrapolation:46.490960688777726
Start training epoch 49
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.17557
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.483277856239587
Iteration: 2 || Loss: 8.482438918529068
Iteration: 3 || Loss: 8.481555101668588
Iteration: 4 || Loss: 8.480726060892009
Iteration: 5 || Loss: 8.479948532101634
Iteration: 6 || Loss: 8.479948532101634
saving ADAM checkpoint...
Sum of params:-97.17553
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.479948532101634
Iteration: 2 || Loss: 8.447346695384214
Iteration: 3 || Loss: 8.43513168574071
Iteration: 4 || Loss: 8.341280375841375
Iteration: 5 || Loss: 7.81898738894548
Iteration: 6 || Loss: 7.6060946178392745
Iteration: 7 || Loss: 7.512229108674102
Iteration: 8 || Loss: 7.429514391128493
Iteration: 9 || Loss: 7.283983892789849
Iteration: 10 || Loss: 7.249258427685154
Iteration: 11 || Loss: 7.208960796271878
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.96256
Epoch 49 loss:7.208960796271878
MSE loss S0.21984793403573905
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.96256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.74065600046586
Iteration: 2 || Loss: 32.7389457199801
Iteration: 3 || Loss: 32.737244610635784
Iteration: 4 || Loss: 32.73557044980411
Iteration: 5 || Loss: 32.73394718623166
Iteration: 6 || Loss: 32.73394718623166
saving ADAM checkpoint...
Sum of params:-96.96258
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.73394718623166
Iteration: 2 || Loss: 32.59504013397727
Iteration: 3 || Loss: 32.56115409510816
Iteration: 4 || Loss: 32.28571296839586
Iteration: 5 || Loss: 31.37088614642633
Iteration: 6 || Loss: 31.20467207766236
Iteration: 7 || Loss: 31.075796996858866
Iteration: 8 || Loss: 30.796291683431
Iteration: 9 || Loss: 30.74023991904145
Iteration: 10 || Loss: 30.68692906059505
Iteration: 11 || Loss: 30.64269854828075
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.38065
Epoch 49 loss:30.64269854828075
MSE loss S0.5218433463575654
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.38065
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 190.11273989724353
Iteration: 2 || Loss: 190.10645475964904
Iteration: 3 || Loss: 190.10017175082197
Iteration: 4 || Loss: 190.09404081882874
Iteration: 5 || Loss: 190.08779777443684
Iteration: 6 || Loss: 190.08779777443684
saving ADAM checkpoint...
Sum of params:-96.38077
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 190.08779777443684
Iteration: 2 || Loss: 188.51823132591886
Iteration: 3 || Loss: 183.12787732335153
Iteration: 4 || Loss: 182.78811287746808
Iteration: 5 || Loss: 179.08062848420252
Iteration: 6 || Loss: 177.3438182653658
Iteration: 7 || Loss: 175.04982440548022
Iteration: 8 || Loss: 174.76873529431293
Iteration: 9 || Loss: 174.15710800138166
Iteration: 10 || Loss: 173.8741878585723
Iteration: 11 || Loss: 173.65414428918908
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.2316
Epoch 49 loss:173.65414428918908
MSE loss S1.9742607499894302
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:42.506930572278776
MSE loss S0.40369188999733896
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:322.2758528599516
MSE loss S5.59620044150018
waveform batch: 2/2
Test loss - extrapolation:231.47732292577953
MSE loss S4.099525337132752
Epoch 49 mean train loss:7.293303573577301
Epoch 49 mean test loss - interpolation:7.084488428713129
Epoch 49 mean test loss - extrapolation:46.14609798214426
Start training epoch 50
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.2316
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.464587851208272
Iteration: 2 || Loss: 8.463484680202395
Iteration: 3 || Loss: 8.462409504922814
Iteration: 4 || Loss: 8.461372412298378
Iteration: 5 || Loss: 8.460358864528185
Iteration: 6 || Loss: 8.460358864528185
saving ADAM checkpoint...
Sum of params:-97.23159
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.460358864528185
Iteration: 2 || Loss: 8.39816119453316
Iteration: 3 || Loss: 8.367209764011935
Iteration: 4 || Loss: 8.310997381959853
Iteration: 5 || Loss: 7.781644202921246
Iteration: 6 || Loss: 7.57970258926483
Iteration: 7 || Loss: 7.45732969353533
Iteration: 8 || Loss: 7.404695921237802
Iteration: 9 || Loss: 7.2512129625265445
Iteration: 10 || Loss: 7.212646497458669
Iteration: 11 || Loss: 7.182896292744344
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.05961
Epoch 50 loss:7.182896292744344
MSE loss S0.22217204877294713
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.05961
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.62574162796767
Iteration: 2 || Loss: 32.624159834990756
Iteration: 3 || Loss: 32.62256673430926
Iteration: 4 || Loss: 32.62103485187974
Iteration: 5 || Loss: 32.61949828692476
Iteration: 6 || Loss: 32.61949828692476
saving ADAM checkpoint...
Sum of params:-97.059555
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.61949828692476
Iteration: 2 || Loss: 32.49808275880879
Iteration: 3 || Loss: 32.465668275392595
Iteration: 4 || Loss: 32.107492809894744
Iteration: 5 || Loss: 31.174206613321203
Iteration: 6 || Loss: 31.027214035485827
Iteration: 7 || Loss: 30.914325982863684
Iteration: 8 || Loss: 30.595906942882127
Iteration: 9 || Loss: 30.539039256650415
Iteration: 10 || Loss: 30.484144962878556
Iteration: 11 || Loss: 30.441663892127963
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.41719
Epoch 50 loss:30.441663892127963
MSE loss S0.5217719555617416
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.41719
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 188.39390266835127
Iteration: 2 || Loss: 188.38762265321407
Iteration: 3 || Loss: 188.38136277364063
Iteration: 4 || Loss: 188.3751133666674
Iteration: 5 || Loss: 188.36886703423656
Iteration: 6 || Loss: 188.36886703423656
saving ADAM checkpoint...
Sum of params:-96.417305
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 188.36886703423656
Iteration: 2 || Loss: 186.81728841880653
Iteration: 3 || Loss: 181.57487784596847
Iteration: 4 || Loss: 180.98098991271678
Iteration: 5 || Loss: 177.2557259533351
Iteration: 6 || Loss: 176.8854998248404
Iteration: 7 || Loss: 173.98006906176377
Iteration: 8 || Loss: 172.9689331283297
Iteration: 9 || Loss: 172.642375797161
Iteration: 10 || Loss: 172.022953844655
Iteration: 11 || Loss: 171.83652944533029
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.31934
Epoch 50 loss:171.83652944533029
MSE loss S1.978340503684682
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:42.41986295940355
MSE loss S0.41088550878116237
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:317.89340118828676
MSE loss S5.560342827194258
waveform batch: 2/2
Test loss - extrapolation:227.87134984107934
MSE loss S4.083325580759836
Epoch 50 mean train loss:7.222796194144917
Epoch 50 mean test loss - interpolation:7.069977159900592
Epoch 50 mean test loss - extrapolation:45.480395919113846
Start training epoch 51
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.31934
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.459032433135253
Iteration: 2 || Loss: 8.458214824464424
Iteration: 3 || Loss: 8.45746366057148
Iteration: 4 || Loss: 8.456695810405256
Iteration: 5 || Loss: 8.455953452160141
Iteration: 6 || Loss: 8.455953452160141
saving ADAM checkpoint...
Sum of params:-97.31941
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.455953452160141
Iteration: 2 || Loss: 8.424885472599271
Iteration: 3 || Loss: 8.393825026786859
Iteration: 4 || Loss: 8.330343756991054
Iteration: 5 || Loss: 7.791646582598693
Iteration: 6 || Loss: 7.586257327054645
Iteration: 7 || Loss: 7.423661303888738
Iteration: 8 || Loss: 7.314164019845481
Iteration: 9 || Loss: 7.192180576309463
Iteration: 10 || Loss: 7.154011417968537
Iteration: 11 || Loss: 7.135657563269529
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.08107
Epoch 51 loss:7.135657563269529
MSE loss S0.21472837228876507
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.08107
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.440705784777776
Iteration: 2 || Loss: 32.43895666262634
Iteration: 3 || Loss: 32.43722849359467
Iteration: 4 || Loss: 32.43548629147972
Iteration: 5 || Loss: 32.43378294069535
Iteration: 6 || Loss: 32.43378294069535
saving ADAM checkpoint...
Sum of params:-97.08108
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.43378294069535
Iteration: 2 || Loss: 32.28698346253564
Iteration: 3 || Loss: 32.24795937621918
Iteration: 4 || Loss: 31.96004697463037
Iteration: 5 || Loss: 31.001589655390486
Iteration: 6 || Loss: 30.833510758732718
Iteration: 7 || Loss: 30.712105633291397
Iteration: 8 || Loss: 30.425628754524293
Iteration: 9 || Loss: 30.375092908618388
Iteration: 10 || Loss: 30.323855550335992
Iteration: 11 || Loss: 30.280938561431576
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.542915
Epoch 51 loss:30.280938561431576
MSE loss S0.5154347074369654
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.542915
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 184.53191644500893
Iteration: 2 || Loss: 184.52588589465603
Iteration: 3 || Loss: 184.51991982032985
Iteration: 4 || Loss: 184.51398393593317
Iteration: 5 || Loss: 184.50808427183622
Iteration: 6 || Loss: 184.50808427183622
saving ADAM checkpoint...
Sum of params:-96.54303
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 184.50808427183622
Iteration: 2 || Loss: 183.10339124242802
Iteration: 3 || Loss: 178.5518288098193
Iteration: 4 || Loss: 177.82742634490864
Iteration: 5 || Loss: 174.69871410137426
Iteration: 6 || Loss: 172.91310013690273
Iteration: 7 || Loss: 171.03312877842137
Iteration: 8 || Loss: 170.70690898903968
Iteration: 9 || Loss: 170.3637787157172
Iteration: 10 || Loss: 169.99105179562775
Iteration: 11 || Loss: 169.763427639681
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.33733
Epoch 51 loss:169.763427639681
MSE loss S2.022393852679758
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:41.768519979174116
MSE loss S0.42473837075124926
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:316.1923471249575
MSE loss S5.6024317197578295
waveform batch: 2/2
Test loss - extrapolation:226.27898585880422
MSE loss S4.112472363699022
Epoch 51 mean train loss:7.144138750495935
Epoch 51 mean test loss - interpolation:6.9614199965290195
Epoch 51 mean test loss - extrapolation:45.205944415313475
Start training epoch 52
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.33733
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.295967168371211
Iteration: 2 || Loss: 8.295626996328505
Iteration: 3 || Loss: 8.29532045906279
Iteration: 4 || Loss: 8.295034348219472
Iteration: 5 || Loss: 8.294793379786256
Iteration: 6 || Loss: 8.294793379786256
saving ADAM checkpoint...
Sum of params:-97.33741
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.294793379786256
Iteration: 2 || Loss: 8.292707864801308
Iteration: 3 || Loss: 8.235296385141345
Iteration: 4 || Loss: 8.213389845955446
Iteration: 5 || Loss: 7.677677209578861
Iteration: 6 || Loss: 7.472213059262622
Iteration: 7 || Loss: 7.3570479924908385
Iteration: 8 || Loss: 7.2815279571699865
Iteration: 9 || Loss: 7.174024616282004
Iteration: 10 || Loss: 7.113755040837238
Iteration: 11 || Loss: 7.093661772847752
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.15005
Epoch 52 loss:7.093661772847752
MSE loss S0.21269681659340178
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.15005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.25761402739235
Iteration: 2 || Loss: 32.25582455716421
Iteration: 3 || Loss: 32.25399991051986
Iteration: 4 || Loss: 32.25213575733038
Iteration: 5 || Loss: 32.25035843448043
Iteration: 6 || Loss: 32.25035843448043
saving ADAM checkpoint...
Sum of params:-97.150055
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.25035843448043
Iteration: 2 || Loss: 32.084854635270844
Iteration: 3 || Loss: 32.0512071745909
Iteration: 4 || Loss: 31.75940816773775
Iteration: 5 || Loss: 30.822696378023668
Iteration: 6 || Loss: 30.658625187118098
Iteration: 7 || Loss: 30.546989866298446
Iteration: 8 || Loss: 30.24794188015922
Iteration: 9 || Loss: 30.205874684726155
Iteration: 10 || Loss: 30.158571788185444
Iteration: 11 || Loss: 30.117194425299527
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.61438
Epoch 52 loss:30.117194425299527
MSE loss S0.5118593820848211
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.61438
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 181.9883557583965
Iteration: 2 || Loss: 181.98248351534318
Iteration: 3 || Loss: 181.97666863589555
Iteration: 4 || Loss: 181.9708777968021
Iteration: 5 || Loss: 181.9649993771309
Iteration: 6 || Loss: 181.9649993771309
saving ADAM checkpoint...
Sum of params:-96.61453
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 181.9649993771309
Iteration: 2 || Loss: 180.6218414076528
Iteration: 3 || Loss: 176.53191924131437
Iteration: 4 || Loss: 175.56241452929527
Iteration: 5 || Loss: 172.6483558723373
Iteration: 6 || Loss: 171.10876599594596
Iteration: 7 || Loss: 169.0454714876653
Iteration: 8 || Loss: 168.75156140399466
Iteration: 9 || Loss: 168.46345211059545
Iteration: 10 || Loss: 168.04608181882074
Iteration: 11 || Loss: 167.6273579376423
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.38126
Epoch 52 loss:167.6273579376423
MSE loss S1.9394269900819294
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:41.38162593741677
MSE loss S0.40136350529081544
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:313.1660098545616
MSE loss S5.496248025462487
waveform batch: 2/2
Test loss - extrapolation:223.3381554049643
MSE loss S4.025534721685072
Epoch 52 mean train loss:7.063386694337572
Epoch 52 mean test loss - interpolation:6.896937656236129
Epoch 52 mean test loss - extrapolation:44.70868043829383
Start training epoch 53
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.38126
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.25149662962163
Iteration: 2 || Loss: 8.251145567263102
Iteration: 3 || Loss: 8.250795783111768
Iteration: 4 || Loss: 8.250436421812402
Iteration: 5 || Loss: 8.250054873893484
Iteration: 6 || Loss: 8.250054873893484
saving ADAM checkpoint...
Sum of params:-97.38138
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.250054873893484
Iteration: 2 || Loss: 8.22252202962984
Iteration: 3 || Loss: 8.213618053798504
Iteration: 4 || Loss: 8.152781322610466
Iteration: 5 || Loss: 7.591865005591096
Iteration: 6 || Loss: 7.404212095912894
Iteration: 7 || Loss: 7.3383840553342035
Iteration: 8 || Loss: 7.268121053537972
Iteration: 9 || Loss: 7.150177674628315
Iteration: 10 || Loss: 7.101064110757648
Iteration: 11 || Loss: 7.072485703856155
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.24803
Epoch 53 loss:7.072485703856155
MSE loss S0.21737016194307035
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.24803
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.269684658012046
Iteration: 2 || Loss: 32.267915558462384
Iteration: 3 || Loss: 32.26614241947204
Iteration: 4 || Loss: 32.264369316522895
Iteration: 5 || Loss: 32.26260332275631
Iteration: 6 || Loss: 32.26260332275631
saving ADAM checkpoint...
Sum of params:-97.24804
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.26260332275631
Iteration: 2 || Loss: 32.10664458250457
Iteration: 3 || Loss: 32.07009258084265
Iteration: 4 || Loss: 31.70927807600301
Iteration: 5 || Loss: 30.671393362790777
Iteration: 6 || Loss: 30.51517107842726
Iteration: 7 || Loss: 30.413931275529208
Iteration: 8 || Loss: 30.115665488587577
Iteration: 9 || Loss: 30.06240023268826
Iteration: 10 || Loss: 30.004788738105216
Iteration: 11 || Loss: 29.96963202428483
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.69258
Epoch 53 loss:29.96963202428483
MSE loss S0.5073509233876081
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.69258
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 179.3683326974151
Iteration: 2 || Loss: 179.36261865016075
Iteration: 3 || Loss: 179.3568466907679
Iteration: 4 || Loss: 179.35102832175667
Iteration: 5 || Loss: 179.34532869876568
Iteration: 6 || Loss: 179.34532869876568
saving ADAM checkpoint...
Sum of params:-96.69275
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 179.34532869876568
Iteration: 2 || Loss: 178.03121603914474
Iteration: 3 || Loss: 173.86687164631076
Iteration: 4 || Loss: 173.22968391809312
Iteration: 5 || Loss: 170.4651534166133
Iteration: 6 || Loss: 169.38625256962138
Iteration: 7 || Loss: 167.2010484737293
Iteration: 8 || Loss: 166.72803140969506
Iteration: 9 || Loss: 166.4935014075032
Iteration: 10 || Loss: 166.0177661358872
Iteration: 11 || Loss: 165.44475767766778
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.47085
Epoch 53 loss:165.44475767766778
MSE loss S1.908436385159804
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:41.59548515038986
MSE loss S0.4113692589679148
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:308.811362012742
MSE loss S5.446692583381289
waveform batch: 2/2
Test loss - extrapolation:218.98990918271076
MSE loss S3.9505020866413725
Epoch 53 mean train loss:6.982306048476165
Epoch 53 mean test loss - interpolation:6.93258085839831
Epoch 53 mean test loss - extrapolation:43.983439266287725
Start training epoch 54
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.47085
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.228867886523906
Iteration: 2 || Loss: 8.228516210846822
Iteration: 3 || Loss: 8.228168764849121
Iteration: 4 || Loss: 8.227854693328938
Iteration: 5 || Loss: 8.227538288010233
Iteration: 6 || Loss: 8.227538288010233
saving ADAM checkpoint...
Sum of params:-97.47084
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.227538288010233
Iteration: 2 || Loss: 8.225705488429643
Iteration: 3 || Loss: 8.206166227145562
Iteration: 4 || Loss: 8.155339621916003
Iteration: 5 || Loss: 7.564196979726408
Iteration: 6 || Loss: 7.373073425119783
Iteration: 7 || Loss: 7.312053355161499
Iteration: 8 || Loss: 7.246019758894666
Iteration: 9 || Loss: 7.136487317778899
Iteration: 10 || Loss: 7.064559994523297
Iteration: 11 || Loss: 7.037965383418823
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.33178
Epoch 54 loss:7.037965383418823
MSE loss S0.21046105917667723
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.33178
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.106081372015105
Iteration: 2 || Loss: 32.104237847840785
Iteration: 3 || Loss: 32.10230742383253
Iteration: 4 || Loss: 32.10047204797045
Iteration: 5 || Loss: 32.09861915700879
Iteration: 6 || Loss: 32.09861915700879
saving ADAM checkpoint...
Sum of params:-97.331795
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.09861915700879
Iteration: 2 || Loss: 31.922880379068115
Iteration: 3 || Loss: 31.883084443759387
Iteration: 4 || Loss: 31.5517147580246
Iteration: 5 || Loss: 30.54293735851115
Iteration: 6 || Loss: 30.381852145601147
Iteration: 7 || Loss: 30.263940305244777
Iteration: 8 || Loss: 29.96065812415966
Iteration: 9 || Loss: 29.918387385048238
Iteration: 10 || Loss: 29.87386169996184
Iteration: 11 || Loss: 29.832816111051567
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.786285
Epoch 54 loss:29.832816111051567
MSE loss S0.510690467113093
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.786285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 176.38421134932446
Iteration: 2 || Loss: 176.37845766393693
Iteration: 3 || Loss: 176.37279520171543
Iteration: 4 || Loss: 176.36707657459712
Iteration: 5 || Loss: 176.36142241428018
Iteration: 6 || Loss: 176.36142241428018
saving ADAM checkpoint...
Sum of params:-96.786415
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 176.36142241428018
Iteration: 2 || Loss: 175.09310943904535
Iteration: 3 || Loss: 172.0484121170136
Iteration: 4 || Loss: 170.67178510271296
Iteration: 5 || Loss: 168.09984986647888
Iteration: 6 || Loss: 167.45388168713123
Iteration: 7 || Loss: 165.50884263034592
Iteration: 8 || Loss: 164.48601170389796
Iteration: 9 || Loss: 164.33057538412652
Iteration: 10 || Loss: 163.82034046951958
Iteration: 11 || Loss: 163.52884608877798
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.5383
Epoch 54 loss:163.52884608877798
MSE loss S1.983744026267214
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:40.64121791652685
MSE loss S0.43156170276418465
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:306.4303804486035
MSE loss S5.478000138267129
waveform batch: 2/2
Test loss - extrapolation:217.8652728617975
MSE loss S4.001594414287192
Epoch 54 mean train loss:6.910331985629255
Epoch 54 mean test loss - interpolation:6.773536319421141
Epoch 54 mean test loss - extrapolation:43.691304442533415
Start training epoch 55
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.5383
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.14049756825369
Iteration: 2 || Loss: 8.14018031677621
Iteration: 3 || Loss: 8.139885671146063
Iteration: 4 || Loss: 8.13960758813792
Iteration: 5 || Loss: 8.139327793014415
Iteration: 6 || Loss: 8.139327793014415
saving ADAM checkpoint...
Sum of params:-97.538315
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.139327793014415
Iteration: 2 || Loss: 8.13325441642194
Iteration: 3 || Loss: 8.117161585944794
Iteration: 4 || Loss: 8.065378917528605
Iteration: 5 || Loss: 7.5333142049234425
Iteration: 6 || Loss: 7.347796236659595
Iteration: 7 || Loss: 7.2783134484629155
Iteration: 8 || Loss: 7.2087259845938885
Iteration: 9 || Loss: 7.090173734386949
Iteration: 10 || Loss: 7.031559860003095
Iteration: 11 || Loss: 7.012492900866513
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.39632
Epoch 55 loss:7.012492900866513
MSE loss S0.2125029382992767
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.39632
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.989755244310196
Iteration: 2 || Loss: 31.98789654202325
Iteration: 3 || Loss: 31.98609930925021
Iteration: 4 || Loss: 31.984222090002
Iteration: 5 || Loss: 31.982437279593114
Iteration: 6 || Loss: 31.982437279593114
saving ADAM checkpoint...
Sum of params:-97.39634
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.982437279593114
Iteration: 2 || Loss: 31.81559650290222
Iteration: 3 || Loss: 31.761757767201352
Iteration: 4 || Loss: 31.44417160498503
Iteration: 5 || Loss: 30.35943983584906
Iteration: 6 || Loss: 30.208051041938024
Iteration: 7 || Loss: 30.111033602643225
Iteration: 8 || Loss: 29.822668147682425
Iteration: 9 || Loss: 29.771189812070396
Iteration: 10 || Loss: 29.719745060145605
Iteration: 11 || Loss: 29.684963339112528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.86346
Epoch 55 loss:29.684963339112528
MSE loss S0.5064794163675735
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.86346
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 174.15777960310146
Iteration: 2 || Loss: 174.15211758481732
Iteration: 3 || Loss: 174.14645018579768
Iteration: 4 || Loss: 174.1408427139195
Iteration: 5 || Loss: 174.13523625704576
Iteration: 6 || Loss: 174.13523625704576
saving ADAM checkpoint...
Sum of params:-96.863594
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 174.13523625704576
Iteration: 2 || Loss: 172.89824796398153
Iteration: 3 || Loss: 170.29625480752046
Iteration: 4 || Loss: 168.64154387115227
Iteration: 5 || Loss: 166.19808035979733
Iteration: 6 || Loss: 165.64799910252412
Iteration: 7 || Loss: 163.66027690549578
Iteration: 8 || Loss: 162.6551628771399
Iteration: 9 || Loss: 162.51038232971564
Iteration: 10 || Loss: 162.01023055813297
Iteration: 11 || Loss: 161.7385944374937
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.61504
Epoch 55 loss:161.7385944374937
MSE loss S1.9555703725887077
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:40.295834313200295
MSE loss S0.4279481486398117
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:303.2627055553737
MSE loss S5.411399863737241
waveform batch: 2/2
Test loss - extrapolation:215.30589477958253
MSE loss S3.9552519840545557
Epoch 55 mean train loss:6.842622437154232
Epoch 55 mean test loss - interpolation:6.715972385533383
Epoch 55 mean test loss - extrapolation:43.214050027913025
Start training epoch 56
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.61504
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.135357966245024
Iteration: 2 || Loss: 8.135104478771947
Iteration: 3 || Loss: 8.13486852366379
Iteration: 4 || Loss: 8.134657371842131
Iteration: 5 || Loss: 8.134425532344522
Iteration: 6 || Loss: 8.134425532344522
saving ADAM checkpoint...
Sum of params:-97.614975
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.134425532344522
Iteration: 2 || Loss: 8.115409920913066
Iteration: 3 || Loss: 8.09915859221404
Iteration: 4 || Loss: 8.055556287750802
Iteration: 5 || Loss: 7.512557855069592
Iteration: 6 || Loss: 7.326625093647294
Iteration: 7 || Loss: 7.251447794250296
Iteration: 8 || Loss: 7.1846706211574665
Iteration: 9 || Loss: 7.0647225870619215
Iteration: 10 || Loss: 7.012112496330437
Iteration: 11 || Loss: 6.991251636012274
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.48585
Epoch 56 loss:6.991251636012274
MSE loss S0.21541792325700163
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.48585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.864902457649315
Iteration: 2 || Loss: 31.86318302996224
Iteration: 3 || Loss: 31.861459755128884
Iteration: 4 || Loss: 31.859745950101644
Iteration: 5 || Loss: 31.858046361972235
Iteration: 6 || Loss: 31.858046361972235
saving ADAM checkpoint...
Sum of params:-97.48588
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.858046361972235
Iteration: 2 || Loss: 31.714848781638786
Iteration: 3 || Loss: 31.670647811140483
Iteration: 4 || Loss: 31.309542662282897
Iteration: 5 || Loss: 30.207476716535457
Iteration: 6 || Loss: 30.06254725248305
Iteration: 7 || Loss: 29.968417753455164
Iteration: 8 || Loss: 29.689178884491724
Iteration: 9 || Loss: 29.631997373196324
Iteration: 10 || Loss: 29.576453802291063
Iteration: 11 || Loss: 29.54219792397154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.92576
Epoch 56 loss:29.54219792397154
MSE loss S0.5030187299265192
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.92576
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 172.27006090454572
Iteration: 2 || Loss: 172.26443975054215
Iteration: 3 || Loss: 172.25876105517284
Iteration: 4 || Loss: 172.25309509440234
Iteration: 5 || Loss: 172.24739405795646
Iteration: 6 || Loss: 172.24739405795646
saving ADAM checkpoint...
Sum of params:-96.92587
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 172.24739405795646
Iteration: 2 || Loss: 171.00035534332292
Iteration: 3 || Loss: 168.70512130309916
Iteration: 4 || Loss: 166.8764956679997
Iteration: 5 || Loss: 164.44382613966093
Iteration: 6 || Loss: 163.94727340117367
Iteration: 7 || Loss: 161.82949953453968
Iteration: 8 || Loss: 160.90906797636404
Iteration: 9 || Loss: 160.75610841165593
Iteration: 10 || Loss: 160.26886388015097
Iteration: 11 || Loss: 160.05530078615635
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.68681
Epoch 56 loss:160.05530078615635
MSE loss S1.9275651647049212
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:39.98386933514012
MSE loss S0.4185951190210754
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:300.13831896074197
MSE loss S5.350509756503753
waveform batch: 2/2
Test loss - extrapolation:212.84443476997532
MSE loss S3.9184865131215423
Epoch 56 mean train loss:6.778922425728971
Epoch 56 mean test loss - interpolation:6.663978222523354
Epoch 56 mean test loss - extrapolation:42.748562810893105
Start training epoch 57
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.68681
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.086581006789908
Iteration: 2 || Loss: 8.086371763341202
Iteration: 3 || Loss: 8.086132431087744
Iteration: 4 || Loss: 8.085939930821706
Iteration: 5 || Loss: 8.08570990236517
Iteration: 6 || Loss: 8.08570990236517
saving ADAM checkpoint...
Sum of params:-97.68683
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.08570990236517
Iteration: 2 || Loss: 8.080074863611873
Iteration: 3 || Loss: 8.051326568972009
Iteration: 4 || Loss: 8.016362414680403
Iteration: 5 || Loss: 7.4914708055876025
Iteration: 6 || Loss: 7.307839446735748
Iteration: 7 || Loss: 7.223718489420984
Iteration: 8 || Loss: 7.157405628721805
Iteration: 9 || Loss: 7.038014470944411
Iteration: 10 || Loss: 6.979754170137628
Iteration: 11 || Loss: 6.960471529590991
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.5357
Epoch 57 loss:6.960471529590991
MSE loss S0.20983585412563005
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.5357
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.702302212056136
Iteration: 2 || Loss: 31.700459149559947
Iteration: 3 || Loss: 31.698630740443015
Iteration: 4 || Loss: 31.69685492020434
Iteration: 5 || Loss: 31.695015247454535
Iteration: 6 || Loss: 31.695015247454535
saving ADAM checkpoint...
Sum of params:-97.53572
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.695015247454535
Iteration: 2 || Loss: 31.535343549348443
Iteration: 3 || Loss: 31.4797734900923
Iteration: 4 || Loss: 31.122296617609887
Iteration: 5 || Loss: 30.0682634518156
Iteration: 6 || Loss: 29.916094774921948
Iteration: 7 || Loss: 29.82285932402891
Iteration: 8 || Loss: 29.545375239759398
Iteration: 9 || Loss: 29.49345429886806
Iteration: 10 || Loss: 29.444731127287298
Iteration: 11 || Loss: 29.41141118837038
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.01998
Epoch 57 loss:29.41141118837038
MSE loss S0.5038857834210144
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.01998
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 169.82747625153934
Iteration: 2 || Loss: 169.82195956736575
Iteration: 3 || Loss: 169.81651971315577
Iteration: 4 || Loss: 169.81116835366737
Iteration: 5 || Loss: 169.80561902722042
Iteration: 6 || Loss: 169.80561902722042
saving ADAM checkpoint...
Sum of params:-97.02007
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 169.80561902722042
Iteration: 2 || Loss: 168.65837685901906
Iteration: 3 || Loss: 167.03753236451774
Iteration: 4 || Loss: 164.69019825261378
Iteration: 5 || Loss: 162.53225043512032
Iteration: 6 || Loss: 162.05997793236082
Iteration: 7 || Loss: 159.88439910234078
Iteration: 8 || Loss: 159.1363841779088
Iteration: 9 || Loss: 159.00660667439524
Iteration: 10 || Loss: 158.59205510535367
Iteration: 11 || Loss: 158.32673983775203
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.7693
Epoch 57 loss:158.32673983775203
MSE loss S1.9356644429184149
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:39.70020560091644
MSE loss S0.43212851067526664
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:297.1834223770156
MSE loss S5.325498011142936
waveform batch: 2/2
Test loss - extrapolation:210.4667052218584
MSE loss S3.8970892259741605
Epoch 57 mean train loss:6.713745605369428
Epoch 57 mean test loss - interpolation:6.616700933486073
Epoch 57 mean test loss - extrapolation:42.304177299906165
Start training epoch 58
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.7693
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.128612675559264
Iteration: 2 || Loss: 8.128125504974792
Iteration: 3 || Loss: 8.127550922591208
Iteration: 4 || Loss: 8.127021949718742
Iteration: 5 || Loss: 8.126463671829036
Iteration: 6 || Loss: 8.126463671829036
saving ADAM checkpoint...
Sum of params:-97.7693
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.126463671829036
Iteration: 2 || Loss: 8.121002547467608
Iteration: 3 || Loss: 8.10877898251325
Iteration: 4 || Loss: 8.027444957015557
Iteration: 5 || Loss: 7.485486894338474
Iteration: 6 || Loss: 7.293063192038115
Iteration: 7 || Loss: 7.205761731524779
Iteration: 8 || Loss: 7.143069275550579
Iteration: 9 || Loss: 7.039108194812749
Iteration: 10 || Loss: 6.961998460673574
Iteration: 11 || Loss: 6.93837859873927
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.63221
Epoch 58 loss:6.93837859873927
MSE loss S0.2118367382667878
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.63221
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.585506692829675
Iteration: 2 || Loss: 31.583738190019396
Iteration: 3 || Loss: 31.581960314103327
Iteration: 4 || Loss: 31.580226522983672
Iteration: 5 || Loss: 31.57845260605124
Iteration: 6 || Loss: 31.57845260605124
saving ADAM checkpoint...
Sum of params:-97.632225
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.57845260605124
Iteration: 2 || Loss: 31.42624354821656
Iteration: 3 || Loss: 31.379800877237756
Iteration: 4 || Loss: 31.004159894084818
Iteration: 5 || Loss: 29.942131673585386
Iteration: 6 || Loss: 29.795421691162538
Iteration: 7 || Loss: 29.704383774465104
Iteration: 8 || Loss: 29.421844625131598
Iteration: 9 || Loss: 29.36871939744733
Iteration: 10 || Loss: 29.31834502470905
Iteration: 11 || Loss: 29.286138058370888
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.097435
Epoch 58 loss:29.286138058370888
MSE loss S0.5010463364058639
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.097435
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 167.77786357191576
Iteration: 2 || Loss: 167.77228386511192
Iteration: 3 || Loss: 167.76679351470364
Iteration: 4 || Loss: 167.76150310824391
Iteration: 5 || Loss: 167.75602527215824
Iteration: 6 || Loss: 167.75602527215824
saving ADAM checkpoint...
Sum of params:-97.09755
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 167.75602527215824
Iteration: 2 || Loss: 166.621130600521
Iteration: 3 || Loss: 164.98262420631735
Iteration: 4 || Loss: 162.81647766725106
Iteration: 5 || Loss: 160.72959258547567
Iteration: 6 || Loss: 160.26757874472085
Iteration: 7 || Loss: 158.32987588664483
Iteration: 8 || Loss: 157.39824903129812
Iteration: 9 || Loss: 157.27179505107006
Iteration: 10 || Loss: 156.83367588658277
Iteration: 11 || Loss: 156.58604614760466
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.837166
Epoch 58 loss:156.58604614760466
MSE loss S1.926114228190411
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:39.37469813968754
MSE loss S0.4326016778379037
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:294.35771263844293
MSE loss S5.288393813545277
waveform batch: 2/2
Test loss - extrapolation:208.12792110589015
MSE loss S3.8678221503088475
Epoch 58 mean train loss:6.6486400967143044
Epoch 58 mean test loss - interpolation:6.562449689947923
Epoch 58 mean test loss - extrapolation:41.873802812027755
Start training epoch 59
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.837166
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.084650967466132
Iteration: 2 || Loss: 8.084168145022094
Iteration: 3 || Loss: 8.083753985693463
Iteration: 4 || Loss: 8.083245803692977
Iteration: 5 || Loss: 8.082804215580396
Iteration: 6 || Loss: 8.082804215580396
saving ADAM checkpoint...
Sum of params:-97.83718
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.082804215580396
Iteration: 2 || Loss: 8.077098955574243
Iteration: 3 || Loss: 8.061375000438568
Iteration: 4 || Loss: 8.002607926206457
Iteration: 5 || Loss: 7.445545008431883
Iteration: 6 || Loss: 7.257699160343816
Iteration: 7 || Loss: 7.182397041446757
Iteration: 8 || Loss: 7.1153372114049045
Iteration: 9 || Loss: 7.009068751240607
Iteration: 10 || Loss: 6.94045314567549
Iteration: 11 || Loss: 6.917284925852769
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.70714
Epoch 59 loss:6.917284925852769
MSE loss S0.21002830988524557
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.70714
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.51420765022125
Iteration: 2 || Loss: 31.512379423540953
Iteration: 3 || Loss: 31.5105538024536
Iteration: 4 || Loss: 31.508763226790194
Iteration: 5 || Loss: 31.506979001599884
Iteration: 6 || Loss: 31.506979001599884
saving ADAM checkpoint...
Sum of params:-97.70706
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.506979001599884
Iteration: 2 || Loss: 31.3480468449532
Iteration: 3 || Loss: 31.29185020393007
Iteration: 4 || Loss: 30.89438663220711
Iteration: 5 || Loss: 29.815461029869887
Iteration: 6 || Loss: 29.667250722002166
Iteration: 7 || Loss: 29.579892942267982
Iteration: 8 || Loss: 29.30540150181669
Iteration: 9 || Loss: 29.249944316165102
Iteration: 10 || Loss: 29.197776546115527
Iteration: 11 || Loss: 29.16692371663151
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.18727
Epoch 59 loss:29.16692371663151
MSE loss S0.5005092548484635
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.18727
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 165.5845602171797
Iteration: 2 || Loss: 165.57917633676396
Iteration: 3 || Loss: 165.57372136351015
Iteration: 4 || Loss: 165.56844624946345
Iteration: 5 || Loss: 165.56304372296353
Iteration: 6 || Loss: 165.56304372296353
saving ADAM checkpoint...
Sum of params:-97.18737
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 165.56304372296353
Iteration: 2 || Loss: 164.47760052403152
Iteration: 3 || Loss: 163.1780529694351
Iteration: 4 || Loss: 160.82292214508345
Iteration: 5 || Loss: 158.88027682494902
Iteration: 6 || Loss: 158.3971708010557
Iteration: 7 || Loss: 156.20767772718187
Iteration: 8 || Loss: 155.6470148019772
Iteration: 9 || Loss: 155.52266569038753
Iteration: 10 || Loss: 155.153764781809
Iteration: 11 || Loss: 154.87048454411283
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.914444
Epoch 59 loss:154.87048454411283
MSE loss S1.9067796999777162
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:39.039660316614444
MSE loss S0.4304812714170556
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:291.5590519666291
MSE loss S5.238604674605089
waveform batch: 2/2
Test loss - extrapolation:205.97206120152214
MSE loss S3.834250947889844
Epoch 59 mean train loss:6.584644592641279
Epoch 59 mean test loss - interpolation:6.506610052769074
Epoch 59 mean test loss - extrapolation:41.46092609734594
Start training epoch 60
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.914444
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.09050061522232
Iteration: 2 || Loss: 8.089765095925921
Iteration: 3 || Loss: 8.089168542584016
Iteration: 4 || Loss: 8.088488496292037
Iteration: 5 || Loss: 8.08782496404217
Iteration: 6 || Loss: 8.08782496404217
saving ADAM checkpoint...
Sum of params:-97.91445
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.08782496404217
Iteration: 2 || Loss: 8.079027274077355
Iteration: 3 || Loss: 8.067897301506205
Iteration: 4 || Loss: 7.950370054837753
Iteration: 5 || Loss: 7.4317761532205475
Iteration: 6 || Loss: 7.239663348848284
Iteration: 7 || Loss: 7.171001234233621
Iteration: 8 || Loss: 7.09852835874717
Iteration: 9 || Loss: 7.015365399021678
Iteration: 10 || Loss: 6.9363112968130025
Iteration: 11 || Loss: 6.899977744858826
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.80342
Epoch 60 loss:6.899977744858826
MSE loss S0.213234450499906
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.80342
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.414828057931594
Iteration: 2 || Loss: 31.413055261596316
Iteration: 3 || Loss: 31.41129075439076
Iteration: 4 || Loss: 31.409610494160525
Iteration: 5 || Loss: 31.407907818084965
Iteration: 6 || Loss: 31.407907818084965
saving ADAM checkpoint...
Sum of params:-97.80342
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.407907818084965
Iteration: 2 || Loss: 31.26775202240359
Iteration: 3 || Loss: 31.21558442684939
Iteration: 4 || Loss: 30.807989001032006
Iteration: 5 || Loss: 29.690377338157628
Iteration: 6 || Loss: 29.5470964723356
Iteration: 7 || Loss: 29.461630870445553
Iteration: 8 || Loss: 29.19391940496434
Iteration: 9 || Loss: 29.133427921534533
Iteration: 10 || Loss: 29.077736648475383
Iteration: 11 || Loss: 29.047144587593426
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.26078
Epoch 60 loss:29.047144587593426
MSE loss S0.4976302813401291
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.26078
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 163.72035436266964
Iteration: 2 || Loss: 163.7149738492444
Iteration: 3 || Loss: 163.70964459382523
Iteration: 4 || Loss: 163.7042517393244
Iteration: 5 || Loss: 163.69890203629524
Iteration: 6 || Loss: 163.69890203629524
saving ADAM checkpoint...
Sum of params:-97.26087
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 163.69890203629524
Iteration: 2 || Loss: 162.61734475857347
Iteration: 3 || Loss: 161.24804019679144
Iteration: 4 || Loss: 159.08304655093247
Iteration: 5 || Loss: 157.17068120431094
Iteration: 6 || Loss: 156.6872493688504
Iteration: 7 || Loss: 154.2291787383697
Iteration: 8 || Loss: 153.90679167689825
Iteration: 9 || Loss: 153.70561482901434
Iteration: 10 || Loss: 153.36294975135604
Iteration: 11 || Loss: 153.21057779338912
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.96153
Epoch 60 loss:153.21057779338912
MSE loss S1.8970803962361025
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:38.462229837406205
MSE loss S0.4197666218694449
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:289.4910323254721
MSE loss S5.215839690936288
waveform batch: 2/2
Test loss - extrapolation:204.693969733304
MSE loss S3.8316407520430933
Epoch 60 mean train loss:6.522679314684185
Epoch 60 mean test loss - interpolation:6.410371639567701
Epoch 60 mean test loss - extrapolation:41.182083504898
Start training epoch 61
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.96153
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.024828327610534
Iteration: 2 || Loss: 8.024168122069058
Iteration: 3 || Loss: 8.023397392329638
Iteration: 4 || Loss: 8.022771181087112
Iteration: 5 || Loss: 8.022098904129493
Iteration: 6 || Loss: 8.022098904129493
saving ADAM checkpoint...
Sum of params:-97.96153
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.022098904129493
Iteration: 2 || Loss: 8.014622283519412
Iteration: 3 || Loss: 7.952756098211005
Iteration: 4 || Loss: 7.920090567612659
Iteration: 5 || Loss: 7.380202872764946
Iteration: 6 || Loss: 7.195890255660131
Iteration: 7 || Loss: 7.131242139356531
Iteration: 8 || Loss: 7.0645494370220785
Iteration: 9 || Loss: 6.974320363241946
Iteration: 10 || Loss: 6.896591543105538
Iteration: 11 || Loss: 6.872776525468694
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.86476
Epoch 61 loss:6.872776525468694
MSE loss S0.2104197751301304
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.86476
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.265853034966277
Iteration: 2 || Loss: 31.26407178784955
Iteration: 3 || Loss: 31.262280128308387
Iteration: 4 || Loss: 31.260504044594363
Iteration: 5 || Loss: 31.258725168261527
Iteration: 6 || Loss: 31.258725168261527
saving ADAM checkpoint...
Sum of params:-97.864746
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.258725168261527
Iteration: 2 || Loss: 31.104961132018914
Iteration: 3 || Loss: 31.057197790203332
Iteration: 4 || Loss: 30.672627939085206
Iteration: 5 || Loss: 29.55823380493698
Iteration: 6 || Loss: 29.41619076869152
Iteration: 7 || Loss: 29.334943731050544
Iteration: 8 || Loss: 29.06660213314206
Iteration: 9 || Loss: 29.01183446820155
Iteration: 10 || Loss: 28.960456571851743
Iteration: 11 || Loss: 28.930739557872016
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.34575
Epoch 61 loss:28.930739557872016
MSE loss S0.49690979392004525
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.34575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 161.65093918655694
Iteration: 2 || Loss: 161.6456887748018
Iteration: 3 || Loss: 161.64046357194786
Iteration: 4 || Loss: 161.63523765590924
Iteration: 5 || Loss: 161.6298880341375
Iteration: 6 || Loss: 161.6298880341375
saving ADAM checkpoint...
Sum of params:-97.345856
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 161.6298880341375
Iteration: 2 || Loss: 160.60105294529913
Iteration: 3 || Loss: 159.4196393318795
Iteration: 4 || Loss: 157.19949015610965
Iteration: 5 || Loss: 155.42476196414296
Iteration: 6 || Loss: 154.93964508860404
Iteration: 7 || Loss: 153.20148477529602
Iteration: 8 || Loss: 152.326882966228
Iteration: 9 || Loss: 152.1010268044259
Iteration: 10 || Loss: 151.76677380043952
Iteration: 11 || Loss: 151.5369572710927
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.03155
Epoch 61 loss:151.5369572710927
MSE loss S1.8869671414991471
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:38.32813123050102
MSE loss S0.4300585850847629
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:286.8010585249987
MSE loss S5.188629631231687
waveform batch: 2/2
Test loss - extrapolation:202.24284597974415
MSE loss S3.787244259227033
Epoch 61 mean train loss:6.46001632256667
Epoch 61 mean test loss - interpolation:6.38802187175017
Epoch 61 mean test loss - extrapolation:40.75365870872857
Start training epoch 62
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.03155
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.052828819138695
Iteration: 2 || Loss: 8.052216053923113
Iteration: 3 || Loss: 8.051671978408706
Iteration: 4 || Loss: 8.051061524481137
Iteration: 5 || Loss: 8.050504170827425
Iteration: 6 || Loss: 8.050504170827425
saving ADAM checkpoint...
Sum of params:-98.03155
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.050504170827425
Iteration: 2 || Loss: 8.04075822253338
Iteration: 3 || Loss: 7.975078562291791
Iteration: 4 || Loss: 7.928986822646052
Iteration: 5 || Loss: 7.350481949785046
Iteration: 6 || Loss: 7.167093353283181
Iteration: 7 || Loss: 7.1163811892443904
Iteration: 8 || Loss: 7.044629503281046
Iteration: 9 || Loss: 6.994305530946618
Iteration: 10 || Loss: 6.917356670423264
Iteration: 11 || Loss: 6.868864661579303
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.9784
Epoch 62 loss:6.868864661579303
MSE loss S0.21703811292073055
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.9784
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.235832261965626
Iteration: 2 || Loss: 31.234271875898624
Iteration: 3 || Loss: 31.2327971966415
Iteration: 4 || Loss: 31.231333996904286
Iteration: 5 || Loss: 31.22986622340745
Iteration: 6 || Loss: 31.22986622340745
saving ADAM checkpoint...
Sum of params:-97.97835
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.22986622340745
Iteration: 2 || Loss: 31.125691177370232
Iteration: 3 || Loss: 31.08362570183852
Iteration: 4 || Loss: 30.533320941899994
Iteration: 5 || Loss: 29.442410228173987
Iteration: 6 || Loss: 29.300642597889528
Iteration: 7 || Loss: 29.217326168302293
Iteration: 8 || Loss: 28.95509530013075
Iteration: 9 || Loss: 28.889593341938323
Iteration: 10 || Loss: 28.833979480876742
Iteration: 11 || Loss: 28.796054561763686
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.391754
Epoch 62 loss:28.796054561763686
MSE loss S0.5023201353646412
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.391754
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 160.3561239774481
Iteration: 2 || Loss: 160.35082825466546
Iteration: 3 || Loss: 160.34542952669432
Iteration: 4 || Loss: 160.34012680882117
Iteration: 5 || Loss: 160.33480691743284
Iteration: 6 || Loss: 160.33480691743284
saving ADAM checkpoint...
Sum of params:-97.39186
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 160.33480691743284
Iteration: 2 || Loss: 159.2820380430127
Iteration: 3 || Loss: 158.14939826663337
Iteration: 4 || Loss: 155.80210804722296
Iteration: 5 || Loss: 153.93016615308792
Iteration: 6 || Loss: 153.48376413433624
Iteration: 7 || Loss: 152.32307643059724
Iteration: 8 || Loss: 150.7847466665391
Iteration: 9 || Loss: 150.60423460181292
Iteration: 10 || Loss: 150.2695941637635
Iteration: 11 || Loss: 150.05671572789217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.158264
Epoch 62 loss:150.05671572789217
MSE loss S1.817755220579686
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:38.17075437837565
MSE loss S0.41138065445676464
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:282.6086673724872
MSE loss S5.03687889434718
waveform batch: 2/2
Test loss - extrapolation:199.03929638965045
MSE loss S3.7024415276318843
Epoch 62 mean train loss:6.404194308663281
Epoch 62 mean test loss - interpolation:6.361792396395941
Epoch 62 mean test loss - extrapolation:40.13733031351148
Start training epoch 63
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.158264
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.17533731004475
Iteration: 2 || Loss: 8.174121384327822
Iteration: 3 || Loss: 8.17294047191555
Iteration: 4 || Loss: 8.17177690335907
Iteration: 5 || Loss: 8.170621715915042
Iteration: 6 || Loss: 8.170621715915042
saving ADAM checkpoint...
Sum of params:-98.15823
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.170621715915042
Iteration: 2 || Loss: 8.133606081410441
Iteration: 3 || Loss: 8.108820585490044
Iteration: 4 || Loss: 8.027751425570479
Iteration: 5 || Loss: 7.442933790082275
Iteration: 6 || Loss: 7.292801914060447
Iteration: 7 || Loss: 7.181544462928745
Iteration: 8 || Loss: 7.081886112824663
Iteration: 9 || Loss: 7.030978034962679
Iteration: 10 || Loss: 6.932774991063061
Iteration: 11 || Loss: 6.881255946590922
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.084076
Epoch 63 loss:6.881255946590922
MSE loss S0.2218133992394139
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.084076
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.135294869209954
Iteration: 2 || Loss: 31.13382548978747
Iteration: 3 || Loss: 31.132330728579735
Iteration: 4 || Loss: 31.130933048126444
Iteration: 5 || Loss: 31.129549536861262
Iteration: 6 || Loss: 31.129549536861262
saving ADAM checkpoint...
Sum of params:-98.08397
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.129549536861262
Iteration: 2 || Loss: 31.033537491407834
Iteration: 3 || Loss: 30.989951466744092
Iteration: 4 || Loss: 30.380924523387478
Iteration: 5 || Loss: 29.32213556086758
Iteration: 6 || Loss: 29.19341846274335
Iteration: 7 || Loss: 29.10910007732099
Iteration: 8 || Loss: 28.840566110698436
Iteration: 9 || Loss: 28.781103193576755
Iteration: 10 || Loss: 28.715129651392644
Iteration: 11 || Loss: 28.685571312535632
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.43035
Epoch 63 loss:28.685571312535632
MSE loss S0.472706608742853
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.43035
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 159.10595338332558
Iteration: 2 || Loss: 159.10049340163317
Iteration: 3 || Loss: 159.09507144574616
Iteration: 4 || Loss: 159.08965964603001
Iteration: 5 || Loss: 159.08421196799418
Iteration: 6 || Loss: 159.08421196799418
saving ADAM checkpoint...
Sum of params:-97.430435
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 159.08421196799418
Iteration: 2 || Loss: 157.9908144758299
Iteration: 3 || Loss: 154.71689916926954
Iteration: 4 || Loss: 154.5604862374431
Iteration: 5 || Loss: 152.62057748484585
Iteration: 6 || Loss: 150.5978735395056
Iteration: 7 || Loss: 149.41752779849546
Iteration: 8 || Loss: 149.24169525023686
Iteration: 9 || Loss: 148.81809058824913
Iteration: 10 || Loss: 148.6842556467476
Iteration: 11 || Loss: 148.21148995572116
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.09863
Epoch 63 loss:148.21148995572116
MSE loss S1.7612827735071828
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:37.361851727606734
MSE loss S0.38412423783258687
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:282.8909767128257
MSE loss S5.010503474948579
waveform batch: 2/2
Test loss - extrapolation:199.06667944423637
MSE loss S3.6454347086041263
Epoch 63 mean train loss:6.337183352236129
Epoch 63 mean test loss - interpolation:6.226975287934455
Epoch 63 mean test loss - extrapolation:40.163138013088506
Start training epoch 64
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.09863
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.862991094218416
Iteration: 2 || Loss: 7.861624255320047
Iteration: 3 || Loss: 7.860185319574127
Iteration: 4 || Loss: 7.858794180627115
Iteration: 5 || Loss: 7.857389273369785
Iteration: 6 || Loss: 7.857389273369785
saving ADAM checkpoint...
Sum of params:-98.09863
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.857389273369785
Iteration: 2 || Loss: 7.757559195840524
Iteration: 3 || Loss: 7.742885925694561
Iteration: 4 || Loss: 7.685517746934366
Iteration: 5 || Loss: 7.217994847435039
Iteration: 6 || Loss: 7.0627900923417535
Iteration: 7 || Loss: 7.019996444449223
Iteration: 8 || Loss: 6.979213895321606
Iteration: 9 || Loss: 6.949837498295306
Iteration: 10 || Loss: 6.890780493393573
Iteration: 11 || Loss: 6.866303783906081
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.12954
Epoch 64 loss:6.866303783906081
MSE loss S0.22194117283918097
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.12954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.852676808343578
Iteration: 2 || Loss: 30.851195103244354
Iteration: 3 || Loss: 30.849833041032056
Iteration: 4 || Loss: 30.84842857450672
Iteration: 5 || Loss: 30.846997349260917
Iteration: 6 || Loss: 30.846997349260917
saving ADAM checkpoint...
Sum of params:-98.12945
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.846997349260917
Iteration: 2 || Loss: 30.75467442104268
Iteration: 3 || Loss: 30.71700402733184
Iteration: 4 || Loss: 29.97142066249868
Iteration: 5 || Loss: 29.147503264344344
Iteration: 6 || Loss: 29.019363127851932
Iteration: 7 || Loss: 28.944334362004234
Iteration: 8 || Loss: 28.706779359518634
Iteration: 9 || Loss: 28.64924587184062
Iteration: 10 || Loss: 28.586792333363405
Iteration: 11 || Loss: 28.554453340301617
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.45421
Epoch 64 loss:28.554453340301617
MSE loss S0.46835683773684433
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.45421
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 157.78809099143822
Iteration: 2 || Loss: 157.7825532649886
Iteration: 3 || Loss: 157.7770913658777
Iteration: 4 || Loss: 157.77161431047188
Iteration: 5 || Loss: 157.7661300932527
Iteration: 6 || Loss: 157.7661300932527
saving ADAM checkpoint...
Sum of params:-97.4543
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 157.7661300932527
Iteration: 2 || Loss: 156.63046453035105
Iteration: 3 || Loss: 153.92588717215511
Iteration: 4 || Loss: 153.35299797041532
Iteration: 5 || Loss: 151.28385563265417
Iteration: 6 || Loss: 150.77072959265954
Iteration: 7 || Loss: 148.5685115497601
Iteration: 8 || Loss: 147.83779634643452
Iteration: 9 || Loss: 147.46770785806856
Iteration: 10 || Loss: 147.1880065965854
Iteration: 11 || Loss: 147.03332987825902
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.22639
Epoch 64 loss:147.03332987825902
MSE loss S1.8174645843688275
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:37.36520095145113
MSE loss S0.402212893952576
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:278.2239078467258
MSE loss S4.990158530589246
waveform batch: 2/2
Test loss - extrapolation:195.78751032538005
MSE loss S3.675834537023609
Epoch 64 mean train loss:6.2915202414643705
Epoch 64 mean test loss - interpolation:6.227533491908521
Epoch 64 mean test loss - extrapolation:39.500951514342155
Start training epoch 65
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.22639
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.919634054181794
Iteration: 2 || Loss: 7.919396251298316
Iteration: 3 || Loss: 7.919094030124503
Iteration: 4 || Loss: 7.918836814182424
Iteration: 5 || Loss: 7.918585057039007
Iteration: 6 || Loss: 7.918585057039007
saving ADAM checkpoint...
Sum of params:-98.22644
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.918585057039007
Iteration: 2 || Loss: 7.911968115903723
Iteration: 3 || Loss: 7.889054054916661
Iteration: 4 || Loss: 7.838653778935405
Iteration: 5 || Loss: 7.298917725111583
Iteration: 6 || Loss: 7.128102690568405
Iteration: 7 || Loss: 7.069510233375144
Iteration: 8 || Loss: 6.981323546614351
Iteration: 9 || Loss: 6.870460220292743
Iteration: 10 || Loss: 6.810452465107901
Iteration: 11 || Loss: 6.794425970796293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.116684
Epoch 65 loss:6.794425970796293
MSE loss S0.2113490844184074
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.116684
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.63903347810705
Iteration: 2 || Loss: 30.63733095709188
Iteration: 3 || Loss: 30.635618533381713
Iteration: 4 || Loss: 30.63389339691851
Iteration: 5 || Loss: 30.63221963508321
Iteration: 6 || Loss: 30.63221963508321
saving ADAM checkpoint...
Sum of params:-98.116714
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.63221963508321
Iteration: 2 || Loss: 30.49409685233357
Iteration: 3 || Loss: 30.410879801141444
Iteration: 4 || Loss: 30.108253639242072
Iteration: 5 || Loss: 29.026445295046123
Iteration: 6 || Loss: 28.886101129298357
Iteration: 7 || Loss: 28.813413593665913
Iteration: 8 || Loss: 28.580624517597695
Iteration: 9 || Loss: 28.524664409312063
Iteration: 10 || Loss: 28.472971335700862
Iteration: 11 || Loss: 28.447848034844167
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.63257
Epoch 65 loss:28.447848034844167
MSE loss S0.49209942206598273
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.63257
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 154.65899485103677
Iteration: 2 || Loss: 154.65403335049575
Iteration: 3 || Loss: 154.64913907121903
Iteration: 4 || Loss: 154.6441682594262
Iteration: 5 || Loss: 154.6390889676356
Iteration: 6 || Loss: 154.6390889676356
saving ADAM checkpoint...
Sum of params:-97.632645
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 154.6390889676356
Iteration: 2 || Loss: 153.74856033087636
Iteration: 3 || Loss: 152.98388661010267
Iteration: 4 || Loss: 150.496501422448
Iteration: 5 || Loss: 149.028114096186
Iteration: 6 || Loss: 148.8083832628974
Iteration: 7 || Loss: 147.7711734815578
Iteration: 8 || Loss: 146.1635418995712
Iteration: 9 || Loss: 146.0370427554364
Iteration: 10 || Loss: 145.68205546591034
Iteration: 11 || Loss: 145.42151677658458
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.349045
Epoch 65 loss:145.42151677658458
MSE loss S1.8158600789076442
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:37.204450597112256
MSE loss S0.4240797980259555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:275.5579574734501
MSE loss S4.957020383504572
waveform batch: 2/2
Test loss - extrapolation:193.36827476659113
MSE loss S3.6284713629836465
Epoch 65 mean train loss:6.229785889042243
Epoch 65 mean test loss - interpolation:6.200741766185376
Epoch 65 mean test loss - extrapolation:39.07718602000343
Start training epoch 66
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.349045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.093781762151206
Iteration: 2 || Loss: 8.092453325741452
Iteration: 3 || Loss: 8.091255255698897
Iteration: 4 || Loss: 8.090024672576984
Iteration: 5 || Loss: 8.088753852911333
Iteration: 6 || Loss: 8.088753852911333
saving ADAM checkpoint...
Sum of params:-98.349045
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.088753852911333
Iteration: 2 || Loss: 8.05928260247172
Iteration: 3 || Loss: 7.981942843925598
Iteration: 4 || Loss: 7.9174298610844795
Iteration: 5 || Loss: 7.3631317850680285
Iteration: 6 || Loss: 7.186122278803355
Iteration: 7 || Loss: 7.081762877038831
Iteration: 8 || Loss: 7.0249350829388755
Iteration: 9 || Loss: 6.9641782109174954
Iteration: 10 || Loss: 6.890976969828243
Iteration: 11 || Loss: 6.834464105108342
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.31098
Epoch 66 loss:6.834464105108342
MSE loss S0.22235771452865652
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.31098
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.704885318736842
Iteration: 2 || Loss: 30.70302998727414
Iteration: 3 || Loss: 30.7011703581612
Iteration: 4 || Loss: 30.69933192666947
Iteration: 5 || Loss: 30.697429394899647
Iteration: 6 || Loss: 30.697429394899647
saving ADAM checkpoint...
Sum of params:-98.31092
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.697429394899647
Iteration: 2 || Loss: 30.5344064575147
Iteration: 3 || Loss: 30.478895631349193
Iteration: 4 || Loss: 30.07518064407646
Iteration: 5 || Loss: 28.978732022952265
Iteration: 6 || Loss: 28.854528030701484
Iteration: 7 || Loss: 28.787055848227027
Iteration: 8 || Loss: 28.534024904321033
Iteration: 9 || Loss: 28.477978769150454
Iteration: 10 || Loss: 28.39221428636291
Iteration: 11 || Loss: 28.358359479014165
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.70561
Epoch 66 loss:28.358359479014165
MSE loss S0.4956092650192958
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.70561
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 153.46141995844755
Iteration: 2 || Loss: 153.45610412420854
Iteration: 3 || Loss: 153.45074183861823
Iteration: 4 || Loss: 153.4454609889462
Iteration: 5 || Loss: 153.4401321563897
Iteration: 6 || Loss: 153.4401321563897
saving ADAM checkpoint...
Sum of params:-97.70571
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 153.4401321563897
Iteration: 2 || Loss: 152.42711363074199
Iteration: 3 || Loss: 151.6928243567658
Iteration: 4 || Loss: 149.34612751040166
Iteration: 5 || Loss: 147.6159362290174
Iteration: 6 || Loss: 147.05731152368014
Iteration: 7 || Loss: 145.3504982411086
Iteration: 8 || Loss: 144.66445385496067
Iteration: 9 || Loss: 144.503650937263
Iteration: 10 || Loss: 144.16758498951808
Iteration: 11 || Loss: 143.87973212535988
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.38283
Epoch 66 loss:143.87973212535988
MSE loss S1.9017365016557084
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:36.89668711550793
MSE loss S0.4524163541360679
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:273.5474011201819
MSE loss S5.048224560088056
waveform batch: 2/2
Test loss - extrapolation:191.52866355753025
MSE loss S3.6736367778293606
Epoch 66 mean train loss:6.1749157141200826
Epoch 66 mean test loss - interpolation:6.149447852584655
Epoch 66 mean test loss - extrapolation:38.75633872314268
Start training epoch 67
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.38283
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.880187509733264
Iteration: 2 || Loss: 7.879568374267166
Iteration: 3 || Loss: 7.879017425672023
Iteration: 4 || Loss: 7.878450578353678
Iteration: 5 || Loss: 7.877879982997421
Iteration: 6 || Loss: 7.877879982997421
saving ADAM checkpoint...
Sum of params:-98.38275
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.877879982997421
Iteration: 2 || Loss: 7.8426950879203545
Iteration: 3 || Loss: 7.80248304670914
Iteration: 4 || Loss: 7.740544601605459
Iteration: 5 || Loss: 7.209692772496309
Iteration: 6 || Loss: 7.040299382722662
Iteration: 7 || Loss: 6.991829818120635
Iteration: 8 || Loss: 6.9268736563720275
Iteration: 9 || Loss: 6.859829562423028
Iteration: 10 || Loss: 6.810540825972654
Iteration: 11 || Loss: 6.766324080811397
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.3198
Epoch 67 loss:6.766324080811397
MSE loss S0.21127658624727935
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.3198
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.51988703920902
Iteration: 2 || Loss: 30.51805424819658
Iteration: 3 || Loss: 30.516263539653668
Iteration: 4 || Loss: 30.514464184479323
Iteration: 5 || Loss: 30.512638020164097
Iteration: 6 || Loss: 30.512638020164097
saving ADAM checkpoint...
Sum of params:-98.319824
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.512638020164097
Iteration: 2 || Loss: 30.35839559416045
Iteration: 3 || Loss: 30.291694068496877
Iteration: 4 || Loss: 29.942707676403522
Iteration: 5 || Loss: 28.84171625486213
Iteration: 6 || Loss: 28.705497839691084
Iteration: 7 || Loss: 28.63940301916163
Iteration: 8 || Loss: 28.41209881127871
Iteration: 9 || Loss: 28.35314797450111
Iteration: 10 || Loss: 28.29462049331683
Iteration: 11 || Loss: 28.268431096072035
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.815544
Epoch 67 loss:28.268431096072035
MSE loss S0.49111490984621303
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.815544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 151.00083217231926
Iteration: 2 || Loss: 150.99600590083963
Iteration: 3 || Loss: 150.9909486459841
Iteration: 4 || Loss: 150.98602461625592
Iteration: 5 || Loss: 150.98114630869892
Iteration: 6 || Loss: 150.98114630869892
saving ADAM checkpoint...
Sum of params:-97.81562
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 150.98114630869892
Iteration: 2 || Loss: 150.10065284426264
Iteration: 3 || Loss: 149.29968951215676
Iteration: 4 || Loss: 147.2056820145105
Iteration: 5 || Loss: 145.82662172226492
Iteration: 6 || Loss: 145.61491668665627
Iteration: 7 || Loss: 144.71833844770288
Iteration: 8 || Loss: 143.14716246883688
Iteration: 9 || Loss: 143.03695520683414
Iteration: 10 || Loss: 142.68230014024292
Iteration: 11 || Loss: 142.42984584327712
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.50488
Epoch 67 loss:142.42984584327712
MSE loss S1.7883865291118033
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:36.62824957532914
MSE loss S0.42166344808070366
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:270.61536568553964
MSE loss S4.8740951449599255
waveform batch: 2/2
Test loss - extrapolation:189.36577537992
MSE loss S3.5599647144685553
Epoch 67 mean train loss:6.119469000695192
Epoch 67 mean test loss - interpolation:6.104708262554857
Epoch 67 mean test loss - extrapolation:38.33176175545497
Start training epoch 68
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.50488
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.043692750808008
Iteration: 2 || Loss: 8.042526927303474
Iteration: 3 || Loss: 8.041344497603546
Iteration: 4 || Loss: 8.040160523662246
Iteration: 5 || Loss: 8.039016525716688
Iteration: 6 || Loss: 8.039016525716688
saving ADAM checkpoint...
Sum of params:-98.50488
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.039016525716688
Iteration: 2 || Loss: 8.012317045491072
Iteration: 3 || Loss: 7.947305598563025
Iteration: 4 || Loss: 7.922143606783061
Iteration: 5 || Loss: 7.289702431646097
Iteration: 6 || Loss: 7.107469558709037
Iteration: 7 || Loss: 7.02602951155976
Iteration: 8 || Loss: 6.949596398380049
Iteration: 9 || Loss: 6.906724230158503
Iteration: 10 || Loss: 6.829764885668551
Iteration: 11 || Loss: 6.7883934177092184
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.48025
Epoch 68 loss:6.7883934177092184
MSE loss S0.2200737784732495
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.48025
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.55745177574137
Iteration: 2 || Loss: 30.555808506738966
Iteration: 3 || Loss: 30.554199756892825
Iteration: 4 || Loss: 30.552502412382612
Iteration: 5 || Loss: 30.55087637590218
Iteration: 6 || Loss: 30.55087637590218
saving ADAM checkpoint...
Sum of params:-98.48015
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.55087637590218
Iteration: 2 || Loss: 30.42817886898633
Iteration: 3 || Loss: 30.36413902823891
Iteration: 4 || Loss: 29.816930075162418
Iteration: 5 || Loss: 28.775716322358242
Iteration: 6 || Loss: 28.646621385373713
Iteration: 7 || Loss: 28.57761448195588
Iteration: 8 || Loss: 28.334671046702272
Iteration: 9 || Loss: 28.27519385544889
Iteration: 10 || Loss: 28.20339687347386
Iteration: 11 || Loss: 28.179608900213456
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.85867
Epoch 68 loss:28.179608900213456
MSE loss S0.46649937921238693
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.85867
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 149.8795660324288
Iteration: 2 || Loss: 149.87450867084638
Iteration: 3 || Loss: 149.8695450341631
Iteration: 4 || Loss: 149.8645773447242
Iteration: 5 || Loss: 149.8596729133499
Iteration: 6 || Loss: 149.8596729133499
saving ADAM checkpoint...
Sum of params:-97.85878
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 149.8596729133499
Iteration: 2 || Loss: 148.94712385503723
Iteration: 3 || Loss: 146.22071988275587
Iteration: 4 || Loss: 146.1033204833229
Iteration: 5 || Loss: 144.71287608592198
Iteration: 6 || Loss: 144.46209992743357
Iteration: 7 || Loss: 142.24515816926757
Iteration: 8 || Loss: 141.75515167869716
Iteration: 9 || Loss: 141.42224410219305
Iteration: 10 || Loss: 141.25423503818388
Iteration: 11 || Loss: 140.8440072355238
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.5968
Epoch 68 loss:140.8440072355238
MSE loss S1.7039159036078042
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:36.19220657747097
MSE loss S0.405047391482754
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:268.0546233185868
MSE loss S4.739089510994814
waveform batch: 2/2
Test loss - extrapolation:187.4016356831359
MSE loss S3.449013743693863
Epoch 68 mean train loss:6.062483088049879
Epoch 68 mean test loss - interpolation:6.032034429578495
Epoch 68 mean test loss - extrapolation:37.954688250143555
Start training epoch 69
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.5968
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.191095449370945
Iteration: 2 || Loss: 8.189858660064985
Iteration: 3 || Loss: 8.188653399157738
Iteration: 4 || Loss: 8.187394868133339
Iteration: 5 || Loss: 8.186176756538345
Iteration: 6 || Loss: 8.186176756538345
saving ADAM checkpoint...
Sum of params:-98.596825
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.186176756538345
Iteration: 2 || Loss: 8.13992675670008
Iteration: 3 || Loss: 8.089992740786805
Iteration: 4 || Loss: 8.003701151982161
Iteration: 5 || Loss: 7.336586074990402
Iteration: 6 || Loss: 7.144921829520467
Iteration: 7 || Loss: 7.025967442846519
Iteration: 8 || Loss: 6.970510241676215
Iteration: 9 || Loss: 6.924952563072964
Iteration: 10 || Loss: 6.874864100104611
Iteration: 11 || Loss: 6.81811572973976
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.630264
Epoch 69 loss:6.81811572973976
MSE loss S0.23443923742346834
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.630264
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.506061747612158
Iteration: 2 || Loss: 30.50465943858483
Iteration: 3 || Loss: 30.503343230702495
Iteration: 4 || Loss: 30.501916658629433
Iteration: 5 || Loss: 30.500486348609332
Iteration: 6 || Loss: 30.500486348609332
saving ADAM checkpoint...
Sum of params:-98.6302
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.500486348609332
Iteration: 2 || Loss: 30.414902522849335
Iteration: 3 || Loss: 30.3634589970369
Iteration: 4 || Loss: 29.99467889745967
Iteration: 5 || Loss: 28.742992426870345
Iteration: 6 || Loss: 28.612490312055407
Iteration: 7 || Loss: 28.523693020776857
Iteration: 8 || Loss: 28.25819836004414
Iteration: 9 || Loss: 28.189442460092145
Iteration: 10 || Loss: 28.12773889689859
Iteration: 11 || Loss: 28.074737610153765
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.8689
Epoch 69 loss:28.074737610153765
MSE loss S0.46896827270964536
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.8689
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 149.42703671194425
Iteration: 2 || Loss: 149.42154928988268
Iteration: 3 || Loss: 149.41617720554714
Iteration: 4 || Loss: 149.4107329271527
Iteration: 5 || Loss: 149.40530853033724
Iteration: 6 || Loss: 149.40530853033724
saving ADAM checkpoint...
Sum of params:-97.86901
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 149.40530853033724
Iteration: 2 || Loss: 148.3436031393456
Iteration: 3 || Loss: 147.24895063499577
Iteration: 4 || Loss: 145.41496293772684
Iteration: 5 || Loss: 143.55623576274132
Iteration: 6 || Loss: 143.2320627435043
Iteration: 7 || Loss: 142.41195236700514
Iteration: 8 || Loss: 140.46579169655826
Iteration: 9 || Loss: 140.25322368932555
Iteration: 10 || Loss: 139.78611959075678
Iteration: 11 || Loss: 139.64167138943438
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.56655
Epoch 69 loss:139.64167138943438
MSE loss S1.7502044107142682
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.916829991440395
MSE loss S0.38799142061283215
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:265.9943648178716
MSE loss S4.788147713377054
waveform batch: 2/2
Test loss - extrapolation:186.13592020566563
MSE loss S3.525444824407571
Epoch 69 mean train loss:6.018431887218203
Epoch 69 mean test loss - interpolation:5.986138331906733
Epoch 69 mean test loss - extrapolation:37.67752375196144
Start training epoch 70
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.56655
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.740928028993774
Iteration: 2 || Loss: 7.7404246221713535
Iteration: 3 || Loss: 7.739886870686025
Iteration: 4 || Loss: 7.739339773308809
Iteration: 5 || Loss: 7.738791321960587
Iteration: 6 || Loss: 7.738791321960587
saving ADAM checkpoint...
Sum of params:-98.566475
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.738791321960587
Iteration: 2 || Loss: 7.734780576233361
Iteration: 3 || Loss: 7.704417410913841
Iteration: 4 || Loss: 7.657366161599139
Iteration: 5 || Loss: 7.156382361289927
Iteration: 6 || Loss: 7.001818096482971
Iteration: 7 || Loss: 6.952131552472049
Iteration: 8 || Loss: 6.872682087729165
Iteration: 9 || Loss: 6.803069821280019
Iteration: 10 || Loss: 6.7485738164696105
Iteration: 11 || Loss: 6.711815532856437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.49891
Epoch 70 loss:6.711815532856437
MSE loss S0.21239502469901891
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.49891
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.010532895081813
Iteration: 2 || Loss: 30.00876809912157
Iteration: 3 || Loss: 30.00708319059259
Iteration: 4 || Loss: 30.005360490665044
Iteration: 5 || Loss: 30.003668325378413
Iteration: 6 || Loss: 30.003668325378413
saving ADAM checkpoint...
Sum of params:-98.49892
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.003668325378413
Iteration: 2 || Loss: 29.86670911599702
Iteration: 3 || Loss: 29.808357028603904
Iteration: 4 || Loss: 29.519454959344156
Iteration: 5 || Loss: 28.473933630694585
Iteration: 6 || Loss: 28.34368886935342
Iteration: 7 || Loss: 28.286581603656867
Iteration: 8 || Loss: 28.081079905062897
Iteration: 9 || Loss: 28.028125494564375
Iteration: 10 || Loss: 27.974085740971944
Iteration: 11 || Loss: 27.952629790618534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.03402
Epoch 70 loss:27.952629790618534
MSE loss S0.4848123582792739
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.03402
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 146.17793496058275
Iteration: 2 || Loss: 146.1731918026714
Iteration: 3 || Loss: 146.16846978251598
Iteration: 4 || Loss: 146.16380451972086
Iteration: 5 || Loss: 146.15910219736907
Iteration: 6 || Loss: 146.15910219736907
saving ADAM checkpoint...
Sum of params:-98.03411
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 146.15910219736907
Iteration: 2 || Loss: 145.37327789671795
Iteration: 3 || Loss: 144.77851330064564
Iteration: 4 || Loss: 142.64363998154616
Iteration: 5 || Loss: 141.53390326214208
Iteration: 6 || Loss: 141.26942410168007
Iteration: 7 || Loss: 140.29653486389586
Iteration: 8 || Loss: 139.0315605694503
Iteration: 9 || Loss: 138.85907596760828
Iteration: 10 || Loss: 138.54174714870234
Iteration: 11 || Loss: 138.2801312855542
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.678116
Epoch 70 loss:138.2801312855542
MSE loss S1.7509788855634851
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:36.03602240479987
MSE loss S0.4174803452029748
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:263.58129068996266
MSE loss S4.768575535512104
waveform batch: 2/2
Test loss - extrapolation:183.46349621168864
MSE loss S3.4605122462708477
Epoch 70 mean train loss:5.9636060899665235
Epoch 70 mean test loss - interpolation:6.006003734133312
Epoch 70 mean test loss - extrapolation:37.25373224180427
Start training epoch 71
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.678116
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.788793790832081
Iteration: 2 || Loss: 7.78850450611247
Iteration: 3 || Loss: 7.788206582895226
Iteration: 4 || Loss: 7.787924203776101
Iteration: 5 || Loss: 7.7875679925276735
Iteration: 6 || Loss: 7.7875679925276735
saving ADAM checkpoint...
Sum of params:-98.67803
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.7875679925276735
Iteration: 2 || Loss: 7.783088202799823
Iteration: 3 || Loss: 7.773225358335349
Iteration: 4 || Loss: 7.519754462574419
Iteration: 5 || Loss: 7.216676426447918
Iteration: 6 || Loss: 6.965666448593723
Iteration: 7 || Loss: 6.921674154261243
Iteration: 8 || Loss: 6.8571081700609895
Iteration: 9 || Loss: 6.789075780674051
Iteration: 10 || Loss: 6.708750676498815
Iteration: 11 || Loss: 6.690482406644183
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.623764
Epoch 71 loss:6.690482406644183
MSE loss S0.20696452547705174
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.623764
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.10576702300747
Iteration: 2 || Loss: 30.10398286761547
Iteration: 3 || Loss: 30.10218777608792
Iteration: 4 || Loss: 30.100445198519477
Iteration: 5 || Loss: 30.098628470060248
Iteration: 6 || Loss: 30.098628470060248
saving ADAM checkpoint...
Sum of params:-98.6238
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.098628470060248
Iteration: 2 || Loss: 29.949019595045524
Iteration: 3 || Loss: 29.876133481859284
Iteration: 4 || Loss: 29.575673158017082
Iteration: 5 || Loss: 28.445671285399865
Iteration: 6 || Loss: 28.309597417137063
Iteration: 7 || Loss: 28.243303086545197
Iteration: 8 || Loss: 28.00724983984436
Iteration: 9 || Loss: 27.956758989509982
Iteration: 10 || Loss: 27.910133277913015
Iteration: 11 || Loss: 27.887965057840496
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.1559
Epoch 71 loss:27.887965057840496
MSE loss S0.4883614921952377
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.1559
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 144.21148258348293
Iteration: 2 || Loss: 144.20681476085238
Iteration: 3 || Loss: 144.20223669500152
Iteration: 4 || Loss: 144.1976926707933
Iteration: 5 || Loss: 144.1931074058331
Iteration: 6 || Loss: 144.1931074058331
saving ADAM checkpoint...
Sum of params:-98.15599
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 144.1931074058331
Iteration: 2 || Loss: 143.4496717290809
Iteration: 3 || Loss: 142.93569378066655
Iteration: 4 || Loss: 140.92348357955234
Iteration: 5 || Loss: 139.90424623721262
Iteration: 6 || Loss: 139.70907426208933
Iteration: 7 || Loss: 138.72842475589317
Iteration: 8 || Loss: 137.6318638653704
Iteration: 9 || Loss: 137.50838599876906
Iteration: 10 || Loss: 137.15601271773403
Iteration: 11 || Loss: 136.83245378648598
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.79313
Epoch 71 loss:136.83245378648598
MSE loss S1.7472474654365033
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.79726680528133
MSE loss S0.43006496359099305
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:261.03418542247624
MSE loss S4.730870603244622
waveform batch: 2/2
Test loss - extrapolation:181.29749968146783
MSE loss S3.417562644274934
Epoch 71 mean train loss:5.910720732792091
Epoch 71 mean test loss - interpolation:5.966211134213555
Epoch 71 mean test loss - extrapolation:36.860973758662006
Start training epoch 72
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.79313
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.9043778051345726
Iteration: 2 || Loss: 7.903517010289312
Iteration: 3 || Loss: 7.902655522928107
Iteration: 4 || Loss: 7.901716695777704
Iteration: 5 || Loss: 7.900810493292007
Iteration: 6 || Loss: 7.900810493292007
saving ADAM checkpoint...
Sum of params:-98.79314
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.900810493292007
Iteration: 2 || Loss: 7.8824925537852035
Iteration: 3 || Loss: 7.865098537041611
Iteration: 4 || Loss: 7.7196432468732255
Iteration: 5 || Loss: 7.169989953652582
Iteration: 6 || Loss: 6.973263084191876
Iteration: 7 || Loss: 6.9145001359658185
Iteration: 8 || Loss: 6.853765444790466
Iteration: 9 || Loss: 6.818306659472658
Iteration: 10 || Loss: 6.757943021512853
Iteration: 11 || Loss: 6.7082481059146755
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.77908
Epoch 72 loss:6.7082481059146755
MSE loss S0.22127626016222696
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.77908
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.025741282713028
Iteration: 2 || Loss: 30.024293210762686
Iteration: 3 || Loss: 30.02280430787007
Iteration: 4 || Loss: 30.02127204727002
Iteration: 5 || Loss: 30.019782275948593
Iteration: 6 || Loss: 30.019782275948593
saving ADAM checkpoint...
Sum of params:-98.77902
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.019782275948593
Iteration: 2 || Loss: 29.922756897116646
Iteration: 3 || Loss: 29.849244422440727
Iteration: 4 || Loss: 29.57155530955924
Iteration: 5 || Loss: 28.388354545673323
Iteration: 6 || Loss: 28.253803448996607
Iteration: 7 || Loss: 28.17581694268737
Iteration: 8 || Loss: 27.956264248890808
Iteration: 9 || Loss: 27.890364545997524
Iteration: 10 || Loss: 27.841506924109055
Iteration: 11 || Loss: 27.813679621431177
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.236305
Epoch 72 loss:27.813679621431177
MSE loss S0.48460434807397706
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.236305
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 142.85573554520656
Iteration: 2 || Loss: 142.8510099562583
Iteration: 3 || Loss: 142.8462502674072
Iteration: 4 || Loss: 142.84145903814127
Iteration: 5 || Loss: 142.83672595449585
Iteration: 6 || Loss: 142.83672595449585
saving ADAM checkpoint...
Sum of params:-98.236404
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 142.83672595449585
Iteration: 2 || Loss: 142.03263437303698
Iteration: 3 || Loss: 141.28199972274882
Iteration: 4 || Loss: 139.6941693944545
Iteration: 5 || Loss: 138.64719968667328
Iteration: 6 || Loss: 138.35400066704318
Iteration: 7 || Loss: 137.5622444667449
Iteration: 8 || Loss: 136.21427526820403
Iteration: 9 || Loss: 136.08566575930723
Iteration: 10 || Loss: 135.7821744337189
Iteration: 11 || Loss: 135.55770876847194
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.84817
Epoch 72 loss:135.55770876847194
MSE loss S1.7352241402278537
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.48359818705818
MSE loss S0.4173493780682799
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:258.77589470831145
MSE loss S4.695533796240914
waveform batch: 2/2
Test loss - extrapolation:179.68237151564742
MSE loss S3.404403440830695
Epoch 72 mean train loss:5.8648150515799236
Epoch 72 mean test loss - interpolation:5.913933031176363
Epoch 72 mean test loss - extrapolation:36.53818885199657
Start training epoch 73
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.84817
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.744073385829476
Iteration: 2 || Loss: 7.743725624438883
Iteration: 3 || Loss: 7.743405595714197
Iteration: 4 || Loss: 7.7430813980126905
Iteration: 5 || Loss: 7.742783901818373
Iteration: 6 || Loss: 7.742783901818373
saving ADAM checkpoint...
Sum of params:-98.848175
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.742783901818373
Iteration: 2 || Loss: 7.737820636988657
Iteration: 3 || Loss: 7.72693973276802
Iteration: 4 || Loss: 7.11049024595127
Iteration: 5 || Loss: 7.0980491201571
Iteration: 6 || Loss: 6.92284754139761
Iteration: 7 || Loss: 6.878846601961324
Iteration: 8 || Loss: 6.81913584878218
Iteration: 9 || Loss: 6.749954411209267
Iteration: 10 || Loss: 6.678282123776095
Iteration: 11 || Loss: 6.6610326336495795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.811966
Epoch 73 loss:6.6610326336495795
MSE loss S0.20740684411849175
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.811966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.005466715301893
Iteration: 2 || Loss: 30.003681149516016
Iteration: 3 || Loss: 30.00200749629869
Iteration: 4 || Loss: 30.000289088496732
Iteration: 5 || Loss: 29.998505451036966
Iteration: 6 || Loss: 29.998505451036966
saving ADAM checkpoint...
Sum of params:-98.81202
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.998505451036966
Iteration: 2 || Loss: 29.859621516482484
Iteration: 3 || Loss: 29.771141149123398
Iteration: 4 || Loss: 29.49596879428553
Iteration: 5 || Loss: 28.282315136497637
Iteration: 6 || Loss: 28.14788819373758
Iteration: 7 || Loss: 28.083026107803427
Iteration: 8 || Loss: 27.852644402770572
Iteration: 9 || Loss: 27.80329324645016
Iteration: 10 || Loss: 27.755813557077104
Iteration: 11 || Loss: 27.7336048001732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.33792
Epoch 73 loss:27.7336048001732
MSE loss S0.48840653940438494
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.33792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 140.96748320029624
Iteration: 2 || Loss: 140.96302327060218
Iteration: 3 || Loss: 140.95851238371569
Iteration: 4 || Loss: 140.95408686761283
Iteration: 5 || Loss: 140.94959196212378
Iteration: 6 || Loss: 140.94959196212378
saving ADAM checkpoint...
Sum of params:-98.338
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 140.94959196212378
Iteration: 2 || Loss: 140.2429796345247
Iteration: 3 || Loss: 139.81215176415796
Iteration: 4 || Loss: 137.9804866785241
Iteration: 5 || Loss: 137.10490343186586
Iteration: 6 || Loss: 136.882698308561
Iteration: 7 || Loss: 135.89948399119012
Iteration: 8 || Loss: 134.94531394687877
Iteration: 9 || Loss: 134.8403269378744
Iteration: 10 || Loss: 134.48712885241656
Iteration: 11 || Loss: 134.1764295810509
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.95791
Epoch 73 loss:134.1764295810509
MSE loss S1.731587319612418
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.29223151455506
MSE loss S0.43434134628172866
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:256.2751485003231
MSE loss S4.654682541810104
waveform batch: 2/2
Test loss - extrapolation:177.52566280727683
MSE loss S3.3541616556516045
Epoch 73 mean train loss:5.812795414305989
Epoch 73 mean test loss - interpolation:5.882038585759177
Epoch 73 mean test loss - extrapolation:36.15006760896666
Start training epoch 74
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.95791
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.907790852074957
Iteration: 2 || Loss: 7.9066927334961665
Iteration: 3 || Loss: 7.9056133172516265
Iteration: 4 || Loss: 7.904474360511864
Iteration: 5 || Loss: 7.903421810853805
Iteration: 6 || Loss: 7.903421810853805
saving ADAM checkpoint...
Sum of params:-98.95792
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.903421810853805
Iteration: 2 || Loss: 7.876378839389944
Iteration: 3 || Loss: 7.8599822621663735
Iteration: 4 || Loss: 7.393595608803326
Iteration: 5 || Loss: 7.164237368344891
Iteration: 6 || Loss: 6.971376423908127
Iteration: 7 || Loss: 6.88720714653639
Iteration: 8 || Loss: 6.840745325158875
Iteration: 9 || Loss: 6.800382315449003
Iteration: 10 || Loss: 6.756109456462457
Iteration: 11 || Loss: 6.712576085580584
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.996765
Epoch 74 loss:6.712576085580584
MSE loss S0.22358197011032624
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.996765
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.975182207527116
Iteration: 2 || Loss: 29.973549705731276
Iteration: 3 || Loss: 29.971972519880524
Iteration: 4 || Loss: 29.97035621019479
Iteration: 5 || Loss: 29.968789433054834
Iteration: 6 || Loss: 29.968789433054834
saving ADAM checkpoint...
Sum of params:-98.9967
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.968789433054834
Iteration: 2 || Loss: 29.857278579548375
Iteration: 3 || Loss: 29.742914844557298
Iteration: 4 || Loss: 29.474969736816938
Iteration: 5 || Loss: 28.243901067932782
Iteration: 6 || Loss: 28.117676574966683
Iteration: 7 || Loss: 28.05095772427331
Iteration: 8 || Loss: 27.81335062208667
Iteration: 9 || Loss: 27.754502313086018
Iteration: 10 || Loss: 27.69185494106377
Iteration: 11 || Loss: 27.668931817401905
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.40699
Epoch 74 loss:27.668931817401905
MSE loss S0.481608677608009
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.40699
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 139.9165999333663
Iteration: 2 || Loss: 139.9119903681117
Iteration: 3 || Loss: 139.90742164624046
Iteration: 4 || Loss: 139.9028714284782
Iteration: 5 || Loss: 139.89832465861386
Iteration: 6 || Loss: 139.89832465861386
saving ADAM checkpoint...
Sum of params:-98.40709
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 139.89832465861386
Iteration: 2 || Loss: 139.14807145259036
Iteration: 3 || Loss: 138.12894130940336
Iteration: 4 || Loss: 136.93457253334037
Iteration: 5 || Loss: 135.90605151388615
Iteration: 6 || Loss: 135.66914026436717
Iteration: 7 || Loss: 134.85724460964528
Iteration: 8 || Loss: 133.555142978397
Iteration: 9 || Loss: 133.44385112405982
Iteration: 10 || Loss: 133.16328479583538
Iteration: 11 || Loss: 132.9200630009965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.01638
Epoch 74 loss:132.9200630009965
MSE loss S1.7513951693076601
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:34.82752265582366
MSE loss S0.43129757867138396
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:254.0458990592199
MSE loss S4.64619102011901
waveform batch: 2/2
Test loss - extrapolation:176.2733015843663
MSE loss S3.3691898407839163
Epoch 74 mean train loss:5.769019686344103
Epoch 74 mean test loss - interpolation:5.8045871093039425
Epoch 74 mean test loss - extrapolation:35.859933386965515
Start training epoch 75
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.01638
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.8203617676794
Iteration: 2 || Loss: 7.819472355334068
Iteration: 3 || Loss: 7.818671821905997
Iteration: 4 || Loss: 7.81781616013464
Iteration: 5 || Loss: 7.8169550469769025
Iteration: 6 || Loss: 7.8169550469769025
saving ADAM checkpoint...
Sum of params:-99.01638
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.8169550469769025
Iteration: 2 || Loss: 7.800074798589397
Iteration: 3 || Loss: 7.780690164542053
Iteration: 4 || Loss: 7.6850709885376105
Iteration: 5 || Loss: 7.0981727736810125
Iteration: 6 || Loss: 6.912237717060396
Iteration: 7 || Loss: 6.862795899958509
Iteration: 8 || Loss: 6.799404969548405
Iteration: 9 || Loss: 6.747909547441618
Iteration: 10 || Loss: 6.703893848929353
Iteration: 11 || Loss: 6.648852509767876
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.00129
Epoch 75 loss:6.648852509767876
MSE loss S0.21245316343124915
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.00129
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.798981162164925
Iteration: 2 || Loss: 29.797402512442876
Iteration: 3 || Loss: 29.795711717136143
Iteration: 4 || Loss: 29.79414372773092
Iteration: 5 || Loss: 29.792637474432986
Iteration: 6 || Loss: 29.792637474432986
saving ADAM checkpoint...
Sum of params:-99.00124
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.792637474432986
Iteration: 2 || Loss: 29.67949781015783
Iteration: 3 || Loss: 29.492846272794225
Iteration: 4 || Loss: 29.316578919815306
Iteration: 5 || Loss: 28.135593660560176
Iteration: 6 || Loss: 27.99668361900708
Iteration: 7 || Loss: 27.92924916818844
Iteration: 8 || Loss: 27.723556759845916
Iteration: 9 || Loss: 27.664375945839783
Iteration: 10 || Loss: 27.60973402628292
Iteration: 11 || Loss: 27.58698193313973
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.50885
Epoch 75 loss:27.58698193313973
MSE loss S0.48694399922815257
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.50885
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 137.9561053483962
Iteration: 2 || Loss: 137.9516504288643
Iteration: 3 || Loss: 137.9472693926115
Iteration: 4 || Loss: 137.94282101250553
Iteration: 5 || Loss: 137.93844041166813
Iteration: 6 || Loss: 137.93844041166813
saving ADAM checkpoint...
Sum of params:-98.508934
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 137.93844041166813
Iteration: 2 || Loss: 137.2593783297112
Iteration: 3 || Loss: 136.9214886861854
Iteration: 4 || Loss: 135.18564584796817
Iteration: 5 || Loss: 134.44863986593268
Iteration: 6 || Loss: 134.14273344588793
Iteration: 7 || Loss: 133.09699115085849
Iteration: 8 || Loss: 132.2879460688109
Iteration: 9 || Loss: 132.18851979564522
Iteration: 10 || Loss: 131.87924259103542
Iteration: 11 || Loss: 131.5952839049466
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.11921
Epoch 75 loss:131.5952839049466
MSE loss S1.7115175213986515
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:34.77157410348957
MSE loss S0.43586291632321333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:251.3900570669093
MSE loss S4.566900020595375
waveform batch: 2/2
Test loss - extrapolation:173.84767492551694
MSE loss S3.292366031902816
Epoch 75 mean train loss:5.718314425788076
Epoch 75 mean test loss - interpolation:5.795262350581595
Epoch 75 mean test loss - extrapolation:35.43647766603552
Start training epoch 76
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.11921
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.943248227867331
Iteration: 2 || Loss: 7.94186599854368
Iteration: 3 || Loss: 7.940473250055872
Iteration: 4 || Loss: 7.939068701091646
Iteration: 5 || Loss: 7.937720944162215
Iteration: 6 || Loss: 7.937720944162215
saving ADAM checkpoint...
Sum of params:-99.11919
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.937720944162215
Iteration: 2 || Loss: 7.898514705997625
Iteration: 3 || Loss: 7.8844570408873285
Iteration: 4 || Loss: 7.684073395245146
Iteration: 5 || Loss: 7.19436200159179
Iteration: 6 || Loss: 6.979649032239239
Iteration: 7 || Loss: 6.86687895080588
Iteration: 8 || Loss: 6.83029422525923
Iteration: 9 || Loss: 6.789658419610497
Iteration: 10 || Loss: 6.76005098256479
Iteration: 11 || Loss: 6.701804974350576
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.18631
Epoch 76 loss:6.701804974350576
MSE loss S0.2205254285685904
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.18631
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.975261449564805
Iteration: 2 || Loss: 29.973815822913433
Iteration: 3 || Loss: 29.972320498233046
Iteration: 4 || Loss: 29.97089495331006
Iteration: 5 || Loss: 29.969507076763797
Iteration: 6 || Loss: 29.969507076763797
saving ADAM checkpoint...
Sum of params:-99.186264
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.969507076763797
Iteration: 2 || Loss: 29.87843793943978
Iteration: 3 || Loss: 29.771831867821394
Iteration: 4 || Loss: 29.273197263442288
Iteration: 5 || Loss: 28.107654134828618
Iteration: 6 || Loss: 27.979012520435724
Iteration: 7 || Loss: 27.903012126413476
Iteration: 8 || Loss: 27.670035387058935
Iteration: 9 || Loss: 27.61826534936244
Iteration: 10 || Loss: 27.55088062717278
Iteration: 11 || Loss: 27.526128717607392
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.543816
Epoch 76 loss:27.526128717607392
MSE loss S0.4616913396861537
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.543816
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 137.19690970274624
Iteration: 2 || Loss: 137.19225736139353
Iteration: 3 || Loss: 137.18766500875697
Iteration: 4 || Loss: 137.1830433535788
Iteration: 5 || Loss: 137.17845550574492
Iteration: 6 || Loss: 137.17845550574492
saving ADAM checkpoint...
Sum of params:-98.54393
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 137.17845550574492
Iteration: 2 || Loss: 136.39576266612065
Iteration: 3 || Loss: 135.11001394317574
Iteration: 4 || Loss: 134.46894859608815
Iteration: 5 || Loss: 133.51550265220493
Iteration: 6 || Loss: 133.187483141734
Iteration: 7 || Loss: 131.3479941861999
Iteration: 8 || Loss: 131.03439535100878
Iteration: 9 || Loss: 130.74242091099717
Iteration: 10 || Loss: 130.61368190558943
Iteration: 11 || Loss: 129.85332602340708
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.204834
Epoch 76 loss:129.85332602340708
MSE loss S1.8429711314515327
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:34.42125280090964
MSE loss S0.528481938856104
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:249.1012872755954
MSE loss S4.7000690127856615
waveform batch: 2/2
Test loss - extrapolation:171.18197929011689
MSE loss S3.2914590388374148
Epoch 76 mean train loss:5.657974472943622
Epoch 76 mean test loss - interpolation:5.736875466818273
Epoch 76 mean test loss - extrapolation:35.023605547142694
Start training epoch 77
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.204834
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.282156343449179
Iteration: 2 || Loss: 8.280164115549805
Iteration: 3 || Loss: 8.278149417217504
Iteration: 4 || Loss: 8.276229387464165
Iteration: 5 || Loss: 8.274192805674474
Iteration: 6 || Loss: 8.274192805674474
saving ADAM checkpoint...
Sum of params:-99.20482
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.274192805674474
Iteration: 2 || Loss: 8.17387126088626
Iteration: 3 || Loss: 8.059023070866346
Iteration: 4 || Loss: 7.892674353857821
Iteration: 5 || Loss: 7.4101855219260875
Iteration: 6 || Loss: 6.966992559127274
Iteration: 7 || Loss: 6.836532220814142
Iteration: 8 || Loss: 6.814613017686835
Iteration: 9 || Loss: 6.788143864726118
Iteration: 10 || Loss: 6.755047175279449
Iteration: 11 || Loss: 6.74360975226525
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.33766
Epoch 77 loss:6.74360975226525
MSE loss S0.2348959158823205
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.33766
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.203688416801675
Iteration: 2 || Loss: 30.20205023065118
Iteration: 3 || Loss: 30.200545138276418
Iteration: 4 || Loss: 30.199006435020248
Iteration: 5 || Loss: 30.197413996485675
Iteration: 6 || Loss: 30.197413996485675
saving ADAM checkpoint...
Sum of params:-99.33761
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.197413996485675
Iteration: 2 || Loss: 30.091217434697818
Iteration: 3 || Loss: 30.01544889214731
Iteration: 4 || Loss: 29.555521544424014
Iteration: 5 || Loss: 28.214327810557677
Iteration: 6 || Loss: 28.06669451466684
Iteration: 7 || Loss: 27.965723924665628
Iteration: 8 || Loss: 27.675689572186013
Iteration: 9 || Loss: 27.613708357058822
Iteration: 10 || Loss: 27.54126733717438
Iteration: 11 || Loss: 27.50316980416325
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.690025
Epoch 77 loss:27.50316980416325
MSE loss S0.4629038630689458
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.690025
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 134.82994169308853
Iteration: 2 || Loss: 134.8259822947032
Iteration: 3 || Loss: 134.8219441706684
Iteration: 4 || Loss: 134.81795473899683
Iteration: 5 || Loss: 134.813950125962
Iteration: 6 || Loss: 134.813950125962
saving ADAM checkpoint...
Sum of params:-98.69014
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 134.813950125962
Iteration: 2 || Loss: 134.2477314080844
Iteration: 3 || Loss: 134.0162956074932
Iteration: 4 || Loss: 132.34271529073706
Iteration: 5 || Loss: 131.72227746118804
Iteration: 6 || Loss: 131.24541215088058
Iteration: 7 || Loss: 129.75747094819516
Iteration: 8 || Loss: 129.27216971764568
Iteration: 9 || Loss: 129.01766160584535
Iteration: 10 || Loss: 128.85655821641745
Iteration: 11 || Loss: 127.70376899594125
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.392715
Epoch 77 loss:127.70376899594125
MSE loss S1.6553410722170794
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:34.54144199820182
MSE loss S0.4783757074966271
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:243.50230056791605
MSE loss S4.47522906021433
waveform batch: 2/2
Test loss - extrapolation:164.7553668084393
MSE loss S3.0794968680043935
Epoch 77 mean train loss:5.584501674219647
Epoch 77 mean test loss - interpolation:5.756906999700303
Epoch 77 mean test loss - extrapolation:34.021472281362946
Start training epoch 78
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.392715
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.48859980460951
Iteration: 2 || Loss: 8.485857462000403
Iteration: 3 || Loss: 8.48321754278917
Iteration: 4 || Loss: 8.48048641683471
Iteration: 5 || Loss: 8.477875029948015
Iteration: 6 || Loss: 8.477875029948015
saving ADAM checkpoint...
Sum of params:-99.39271
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.477875029948015
Iteration: 2 || Loss: 8.29867855016173
Iteration: 3 || Loss: 8.240585563703922
Iteration: 4 || Loss: 8.010831118771133
Iteration: 5 || Loss: 7.667240607958147
Iteration: 6 || Loss: 7.029293356386228
Iteration: 7 || Loss: 6.8653483544744045
Iteration: 8 || Loss: 6.846238150575186
Iteration: 9 || Loss: 6.816369684517681
Iteration: 10 || Loss: 6.793438658337086
Iteration: 11 || Loss: 6.771304988394273
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.54359
Epoch 78 loss:6.771304988394273
MSE loss S0.23898073445934243
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.54359
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.720057275252334
Iteration: 2 || Loss: 30.718013779757836
Iteration: 3 || Loss: 30.71592680636203
Iteration: 4 || Loss: 30.71385630307511
Iteration: 5 || Loss: 30.711875328283522
Iteration: 6 || Loss: 30.711875328283522
saving ADAM checkpoint...
Sum of params:-99.54353
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.711875328283522
Iteration: 2 || Loss: 30.515069972237253
Iteration: 3 || Loss: 30.411892600381
Iteration: 4 || Loss: 29.72759841823235
Iteration: 5 || Loss: 28.367837117951602
Iteration: 6 || Loss: 28.243290446797662
Iteration: 7 || Loss: 28.09368217470109
Iteration: 8 || Loss: 27.84749172762547
Iteration: 9 || Loss: 27.662500676667648
Iteration: 10 || Loss: 27.588784685269538
Iteration: 11 || Loss: 27.538553145493378
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.948326
Epoch 78 loss:27.538553145493378
MSE loss S0.4660748513065451
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.948326
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 131.1628826462999
Iteration: 2 || Loss: 131.1591485487337
Iteration: 3 || Loss: 131.15540495398724
Iteration: 4 || Loss: 131.1516881031903
Iteration: 5 || Loss: 131.1480314063793
Iteration: 6 || Loss: 131.1480314063793
saving ADAM checkpoint...
Sum of params:-98.9484
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 131.1480314063793
Iteration: 2 || Loss: 130.64433305224063
Iteration: 3 || Loss: 129.66224959858712
Iteration: 4 || Loss: 129.48004334832743
Iteration: 5 || Loss: 129.0405376475604
Iteration: 6 || Loss: 128.52914342320128
Iteration: 7 || Loss: 127.08608535148967
Iteration: 8 || Loss: 126.9514122567451
Iteration: 9 || Loss: 126.70427356168511
Iteration: 10 || Loss: 126.48095689215306
Iteration: 11 || Loss: 125.61079827780061
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.51721
Epoch 78 loss:125.61079827780061
MSE loss S1.6324727037298823
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:34.190275133159425
MSE loss S0.4711164088190714
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:239.07461302175986
MSE loss S4.380145551469111
waveform batch: 2/2
Test loss - extrapolation:161.74960976287576
MSE loss S3.0287917887919997
Epoch 78 mean train loss:5.514505393506492
Epoch 78 mean test loss - interpolation:5.698379188859904
Epoch 78 mean test loss - extrapolation:33.4020185653863
Start training epoch 79
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.51721
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.118902433388042
Iteration: 2 || Loss: 8.116924608439305
Iteration: 3 || Loss: 8.114919284089984
Iteration: 4 || Loss: 8.113054720155624
Iteration: 5 || Loss: 8.111127096906035
Iteration: 6 || Loss: 8.111127096906035
saving ADAM checkpoint...
Sum of params:-99.51723
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.111127096906035
Iteration: 2 || Loss: 8.023905755009618
Iteration: 3 || Loss: 8.00479305348914
Iteration: 4 || Loss: 7.43461471889561
Iteration: 5 || Loss: 7.362210381579992
Iteration: 6 || Loss: 6.988843960943393
Iteration: 7 || Loss: 6.851010811022987
Iteration: 8 || Loss: 6.8188914156344245
Iteration: 9 || Loss: 6.802707688305292
Iteration: 10 || Loss: 6.764635533016627
Iteration: 11 || Loss: 6.746323090181564
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.67156
Epoch 79 loss:6.746323090181564
MSE loss S0.2330883703940956
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.67156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.640352564326708
Iteration: 2 || Loss: 30.638697885944378
Iteration: 3 || Loss: 30.637019447801688
Iteration: 4 || Loss: 30.635297000689235
Iteration: 5 || Loss: 30.633691452235638
Iteration: 6 || Loss: 30.633691452235638
saving ADAM checkpoint...
Sum of params:-99.67149
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.633691452235638
Iteration: 2 || Loss: 30.506481456497525
Iteration: 3 || Loss: 30.293008425953253
Iteration: 4 || Loss: 29.46363633798787
Iteration: 5 || Loss: 28.23320070196913
Iteration: 6 || Loss: 28.10414764163707
Iteration: 7 || Loss: 27.983260443450796
Iteration: 8 || Loss: 27.645597203955194
Iteration: 9 || Loss: 27.57459909359032
Iteration: 10 || Loss: 27.506909396249025
Iteration: 11 || Loss: 27.486127418614817
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.075325
Epoch 79 loss:27.486127418614817
MSE loss S0.4845435266478075
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.075325
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 129.03180124458885
Iteration: 2 || Loss: 129.02786286113903
Iteration: 3 || Loss: 129.02383651134545
Iteration: 4 || Loss: 129.0199244667258
Iteration: 5 || Loss: 129.016029287753
Iteration: 6 || Loss: 129.016029287753
saving ADAM checkpoint...
Sum of params:-99.07543
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 129.016029287753
Iteration: 2 || Loss: 128.45892288872435
Iteration: 3 || Loss: 128.09452138813126
Iteration: 4 || Loss: 127.33698131325306
Iteration: 5 || Loss: 126.91574557800554
Iteration: 6 || Loss: 126.44447414753125
Iteration: 7 || Loss: 125.55932321185274
Iteration: 8 || Loss: 124.96793514456654
Iteration: 9 || Loss: 124.7803933914237
Iteration: 10 || Loss: 124.61929158119145
Iteration: 11 || Loss: 124.00837569469365
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.671715
Epoch 79 loss:124.00837569469365
MSE loss S1.6025397709278644
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:34.099416255148924
MSE loss S0.4501162958291194
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:234.33943721253735
MSE loss S4.273819846734536
waveform batch: 2/2
Test loss - extrapolation:158.8891341083896
MSE loss S2.9974149832922867
Epoch 79 mean train loss:5.456580213913449
Epoch 79 mean test loss - interpolation:5.683236042524821
Epoch 79 mean test loss - extrapolation:32.769047610077244
Start training epoch 80
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.671715
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.915914222113111
Iteration: 2 || Loss: 7.914585529857039
Iteration: 3 || Loss: 7.913312327903041
Iteration: 4 || Loss: 7.9119908932027405
Iteration: 5 || Loss: 7.910716723366589
Iteration: 6 || Loss: 7.910716723366589
saving ADAM checkpoint...
Sum of params:-99.67171
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.910716723366589
Iteration: 2 || Loss: 7.868595947300631
Iteration: 3 || Loss: 7.845403151048753
Iteration: 4 || Loss: 7.5848692755388
Iteration: 5 || Loss: 7.188886834825687
Iteration: 6 || Loss: 6.94935575282089
Iteration: 7 || Loss: 6.866603169968461
Iteration: 8 || Loss: 6.833907810816753
Iteration: 9 || Loss: 6.796066423134875
Iteration: 10 || Loss: 6.762474711439341
Iteration: 11 || Loss: 6.7134888341597865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.76982
Epoch 80 loss:6.7134888341597865
MSE loss S0.23435473599206458
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.76982
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.321698461861676
Iteration: 2 || Loss: 30.320115434004656
Iteration: 3 || Loss: 30.31862264853111
Iteration: 4 || Loss: 30.31711419472289
Iteration: 5 || Loss: 30.31562925441061
Iteration: 6 || Loss: 30.31562925441061
saving ADAM checkpoint...
Sum of params:-99.76976
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.31562925441061
Iteration: 2 || Loss: 30.21448414075423
Iteration: 3 || Loss: 29.973799402647355
Iteration: 4 || Loss: 29.584173126077484
Iteration: 5 || Loss: 28.048118213965864
Iteration: 6 || Loss: 27.909173595256046
Iteration: 7 || Loss: 27.831399250101494
Iteration: 8 || Loss: 27.584061700457514
Iteration: 9 || Loss: 27.50477775432333
Iteration: 10 || Loss: 27.419170742330238
Iteration: 11 || Loss: 27.394747993297244
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.12765
Epoch 80 loss:27.394747993297244
MSE loss S0.46525484153832564
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.12765
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 127.53828256513265
Iteration: 2 || Loss: 127.53449752774773
Iteration: 3 || Loss: 127.53068207558589
Iteration: 4 || Loss: 127.52694826772567
Iteration: 5 || Loss: 127.52317255761074
Iteration: 6 || Loss: 127.52317255761074
saving ADAM checkpoint...
Sum of params:-99.12772
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 127.52317255761074
Iteration: 2 || Loss: 127.01494562945132
Iteration: 3 || Loss: 125.99171513687023
Iteration: 4 || Loss: 125.89230756790805
Iteration: 5 || Loss: 125.50067756141534
Iteration: 6 || Loss: 125.03283200293805
Iteration: 7 || Loss: 123.62875178524442
Iteration: 8 || Loss: 123.47039573550342
Iteration: 9 || Loss: 123.27302771580689
Iteration: 10 || Loss: 123.12601881159472
Iteration: 11 || Loss: 122.32403854376075
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.72695
Epoch 80 loss:122.32403854376075
MSE loss S1.5509373926708592
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:33.50198546501042
MSE loss S0.43428672569146143
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:232.8020735973076
MSE loss S4.217576362506096
waveform batch: 2/2
Test loss - extrapolation:157.4394084045371
MSE loss S2.9343683171824466
Epoch 80 mean train loss:5.394216392110957
Epoch 80 mean test loss - interpolation:5.5836642441684035
Epoch 80 mean test loss - extrapolation:32.52012350015372
Start training epoch 81
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.72695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.81465944900108
Iteration: 2 || Loss: 7.8129456958082155
Iteration: 3 || Loss: 7.811290273203086
Iteration: 4 || Loss: 7.809637844840241
Iteration: 5 || Loss: 7.807962238739417
Iteration: 6 || Loss: 7.807962238739417
saving ADAM checkpoint...
Sum of params:-99.72698
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.807962238739417
Iteration: 2 || Loss: 7.74394859094256
Iteration: 3 || Loss: 7.72107985955446
Iteration: 4 || Loss: 7.323391001178877
Iteration: 5 || Loss: 7.2044687045763345
Iteration: 6 || Loss: 6.888527175451066
Iteration: 7 || Loss: 6.796987371150662
Iteration: 8 || Loss: 6.770003683358936
Iteration: 9 || Loss: 6.750828534702191
Iteration: 10 || Loss: 6.7202082759518165
Iteration: 11 || Loss: 6.703171834332315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.85628
Epoch 81 loss:6.703171834332315
MSE loss S0.2385402720563079
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.85628
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.11851857820753
Iteration: 2 || Loss: 30.117050617662635
Iteration: 3 || Loss: 30.115597171681355
Iteration: 4 || Loss: 30.114272607011408
Iteration: 5 || Loss: 30.112785064469993
Iteration: 6 || Loss: 30.112785064469993
saving ADAM checkpoint...
Sum of params:-99.85621
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.112785064469993
Iteration: 2 || Loss: 30.025537205374725
Iteration: 3 || Loss: 29.764350201972352
Iteration: 4 || Loss: 29.521130118799544
Iteration: 5 || Loss: 27.96192560827665
Iteration: 6 || Loss: 27.81624484642711
Iteration: 7 || Loss: 27.74322157938968
Iteration: 8 || Loss: 27.476880476752108
Iteration: 9 || Loss: 27.405024426478988
Iteration: 10 || Loss: 27.342782120750538
Iteration: 11 || Loss: 27.31479562917999
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.26792
Epoch 81 loss:27.31479562917999
MSE loss S0.48751885275015533
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.26792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 125.64094854794998
Iteration: 2 || Loss: 125.63717403552101
Iteration: 3 || Loss: 125.63341142372764
Iteration: 4 || Loss: 125.6296930676988
Iteration: 5 || Loss: 125.62598020607453
Iteration: 6 || Loss: 125.62598020607453
saving ADAM checkpoint...
Sum of params:-99.26801
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 125.62598020607453
Iteration: 2 || Loss: 125.14279329466439
Iteration: 3 || Loss: 124.92784431013825
Iteration: 4 || Loss: 124.09387484645873
Iteration: 5 || Loss: 123.73317859889345
Iteration: 6 || Loss: 123.28633189067376
Iteration: 7 || Loss: 122.30045976531868
Iteration: 8 || Loss: 121.92371505082981
Iteration: 9 || Loss: 121.71137154825337
Iteration: 10 || Loss: 121.59462255364781
Iteration: 11 || Loss: 120.92023403304725
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.846985
Epoch 81 loss:120.92023403304725
MSE loss S1.603550493375587
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:33.34061203753628
MSE loss S0.45706408239602436
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:229.18955976554534
MSE loss S4.218877688275371
waveform batch: 2/2
Test loss - extrapolation:154.84059027756373
MSE loss S2.935931050497034
Epoch 81 mean train loss:5.34269660332964
Epoch 81 mean test loss - interpolation:5.556768672922714
Epoch 81 mean test loss - extrapolation:32.00251250359242
Start training epoch 82
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.846985
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.739402260182233
Iteration: 2 || Loss: 7.738323015248726
Iteration: 3 || Loss: 7.737171101732812
Iteration: 4 || Loss: 7.736080436300554
Iteration: 5 || Loss: 7.735042068421299
Iteration: 6 || Loss: 7.735042068421299
saving ADAM checkpoint...
Sum of params:-99.84699
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.735042068421299
Iteration: 2 || Loss: 7.704073682745447
Iteration: 3 || Loss: 7.683421341879985
Iteration: 4 || Loss: 7.308789278393617
Iteration: 5 || Loss: 7.077535872195585
Iteration: 6 || Loss: 6.841378008992763
Iteration: 7 || Loss: 6.798259882365615
Iteration: 8 || Loss: 6.766220028936676
Iteration: 9 || Loss: 6.7311032146316965
Iteration: 10 || Loss: 6.70805950972535
Iteration: 11 || Loss: 6.67407782168816
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.94663
Epoch 82 loss:6.67407782168816
MSE loss S0.23109394641011122
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.94663
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.890703737764106
Iteration: 2 || Loss: 29.889184009008925
Iteration: 3 || Loss: 29.887614293531435
Iteration: 4 || Loss: 29.886139238263645
Iteration: 5 || Loss: 29.884566964367835
Iteration: 6 || Loss: 29.884566964367835
saving ADAM checkpoint...
Sum of params:-99.94657
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.884566964367835
Iteration: 2 || Loss: 29.779187758697606
Iteration: 3 || Loss: 29.653089077634657
Iteration: 4 || Loss: 29.22644570067567
Iteration: 5 || Loss: 27.82985581356554
Iteration: 6 || Loss: 27.687098608697603
Iteration: 7 || Loss: 27.628198740766205
Iteration: 8 || Loss: 27.39435796280216
Iteration: 9 || Loss: 27.326271750242558
Iteration: 10 || Loss: 27.250518245757945
Iteration: 11 || Loss: 27.2200007741147
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.35312
Epoch 82 loss:27.2200007741147
MSE loss S0.4922082870850151
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.35312
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 124.12870826131588
Iteration: 2 || Loss: 124.12494634150926
Iteration: 3 || Loss: 124.12121881412067
Iteration: 4 || Loss: 124.11757365808579
Iteration: 5 || Loss: 124.11383702516751
Iteration: 6 || Loss: 124.11383702516751
saving ADAM checkpoint...
Sum of params:-99.35322
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 124.11383702516751
Iteration: 2 || Loss: 123.65368288868964
Iteration: 3 || Loss: 123.56618405989526
Iteration: 4 || Loss: 122.62127241396966
Iteration: 5 || Loss: 122.26552897843754
Iteration: 6 || Loss: 121.82827337330217
Iteration: 7 || Loss: 120.67061417202332
Iteration: 8 || Loss: 120.48830535189316
Iteration: 9 || Loss: 120.28598077790653
Iteration: 10 || Loss: 120.13298574937048
Iteration: 11 || Loss: 119.50107556057773
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.93225
Epoch 82 loss:119.50107556057773
MSE loss S1.5320879694151788
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:32.977174628609426
MSE loss S0.428549518542808
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:226.73655290928926
MSE loss S4.1202435508151405
waveform batch: 2/2
Test loss - extrapolation:152.82614698913804
MSE loss S2.873145222304495
Epoch 82 mean train loss:5.289488074357951
Epoch 82 mean test loss - interpolation:5.496195771434905
Epoch 82 mean test loss - extrapolation:31.63022499153561
Start training epoch 83
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.93225
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.648779275080575
Iteration: 2 || Loss: 7.6472352991130865
Iteration: 3 || Loss: 7.645694302370537
Iteration: 4 || Loss: 7.644128521433579
Iteration: 5 || Loss: 7.642660487117548
Iteration: 6 || Loss: 7.642660487117548
saving ADAM checkpoint...
Sum of params:-99.93226
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.642660487117548
Iteration: 2 || Loss: 7.58409413590885
Iteration: 3 || Loss: 7.572875893027515
Iteration: 4 || Loss: 7.454596419823739
Iteration: 5 || Loss: 7.133294589860021
Iteration: 6 || Loss: 6.825511316889798
Iteration: 7 || Loss: 6.770071791752064
Iteration: 8 || Loss: 6.742286119222328
Iteration: 9 || Loss: 6.72261461269039
Iteration: 10 || Loss: 6.690030336410983
Iteration: 11 || Loss: 6.6715558820624326
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.04739
Epoch 83 loss:6.6715558820624326
MSE loss S0.23408471853596755
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.04739
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.896328605781406
Iteration: 2 || Loss: 29.89486675905524
Iteration: 3 || Loss: 29.893469185132595
Iteration: 4 || Loss: 29.892040914585117
Iteration: 5 || Loss: 29.890618480094044
Iteration: 6 || Loss: 29.890618480094044
saving ADAM checkpoint...
Sum of params:-100.04732
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.890618480094044
Iteration: 2 || Loss: 29.80436583132723
Iteration: 3 || Loss: 29.35298510430928
Iteration: 4 || Loss: 29.093876680423804
Iteration: 5 || Loss: 27.847379015818166
Iteration: 6 || Loss: 27.64670729486176
Iteration: 7 || Loss: 27.547315221483316
Iteration: 8 || Loss: 27.323213218112333
Iteration: 9 || Loss: 27.245423181154905
Iteration: 10 || Loss: 27.179156065589627
Iteration: 11 || Loss: 27.153877982757045
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.48638
Epoch 83 loss:27.153877982757045
MSE loss S0.4785416184418916
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.48638
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 122.36443677617099
Iteration: 2 || Loss: 122.36086093809612
Iteration: 3 || Loss: 122.35732965507735
Iteration: 4 || Loss: 122.35376613784413
Iteration: 5 || Loss: 122.35025239928012
Iteration: 6 || Loss: 122.35025239928012
saving ADAM checkpoint...
Sum of params:-99.48648
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 122.35025239928012
Iteration: 2 || Loss: 121.91633144155485
Iteration: 3 || Loss: 121.51256317175371
Iteration: 4 || Loss: 121.07407018272859
Iteration: 5 || Loss: 120.74172215536923
Iteration: 6 || Loss: 120.3191857139509
Iteration: 7 || Loss: 119.48419632520014
Iteration: 8 || Loss: 119.0814692397646
Iteration: 9 || Loss: 118.88931198164144
Iteration: 10 || Loss: 118.77263626209916
Iteration: 11 || Loss: 118.170337144851
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.05319
Epoch 83 loss:118.170337144851
MSE loss S1.5751001526824306
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:32.84541468973314
MSE loss S0.45473861612917543
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:223.7039277514343
MSE loss S4.112325129088905
waveform batch: 2/2
Test loss - extrapolation:150.67583219242073
MSE loss S2.8731850919931703
Epoch 83 mean train loss:5.241233483092086
Epoch 83 mean test loss - interpolation:5.47423578162219
Epoch 83 mean test loss - extrapolation:31.19831332865458
Start training epoch 84
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.05319
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.801357065895223
Iteration: 2 || Loss: 7.799871979987342
Iteration: 3 || Loss: 7.798359462113115
Iteration: 4 || Loss: 7.796910168341402
Iteration: 5 || Loss: 7.795362202623597
Iteration: 6 || Loss: 7.795362202623597
saving ADAM checkpoint...
Sum of params:-100.053185
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.795362202623597
Iteration: 2 || Loss: 7.745653922004426
Iteration: 3 || Loss: 7.731530699721437
Iteration: 4 || Loss: 7.18936541640413
Iteration: 5 || Loss: 7.146644848045448
Iteration: 6 || Loss: 6.873021586812791
Iteration: 7 || Loss: 6.785654929696525
Iteration: 8 || Loss: 6.754482331381609
Iteration: 9 || Loss: 6.731422151799809
Iteration: 10 || Loss: 6.696421565548656
Iteration: 11 || Loss: 6.664802431548516
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.160255
Epoch 84 loss:6.664802431548516
MSE loss S0.23530092976901845
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.160255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.798945587124603
Iteration: 2 || Loss: 29.797558656761062
Iteration: 3 || Loss: 29.796286147291035
Iteration: 4 || Loss: 29.795026043461863
Iteration: 5 || Loss: 29.79375306174993
Iteration: 6 || Loss: 29.79375306174993
saving ADAM checkpoint...
Sum of params:-100.16019
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.79375306174993
Iteration: 2 || Loss: 29.724454858239476
Iteration: 3 || Loss: 29.34442721362971
Iteration: 4 || Loss: 29.18415751494347
Iteration: 5 || Loss: 27.6725056962787
Iteration: 6 || Loss: 27.516585919587776
Iteration: 7 || Loss: 27.440190324675523
Iteration: 8 || Loss: 27.231820857714972
Iteration: 9 || Loss: 27.154586708946635
Iteration: 10 || Loss: 27.103315888124058
Iteration: 11 || Loss: 27.075002434879682
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.590034
Epoch 84 loss:27.075002434879682
MSE loss S0.4880443528312435
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.590034
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 120.95666804874374
Iteration: 2 || Loss: 120.95316719260023
Iteration: 3 || Loss: 120.94973851689863
Iteration: 4 || Loss: 120.94628014617248
Iteration: 5 || Loss: 120.94285225747214
Iteration: 6 || Loss: 120.94285225747214
saving ADAM checkpoint...
Sum of params:-99.590126
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 120.94285225747214
Iteration: 2 || Loss: 120.54709400171332
Iteration: 3 || Loss: 120.45212835880051
Iteration: 4 || Loss: 119.71667021605363
Iteration: 5 || Loss: 119.3754313931243
Iteration: 6 || Loss: 118.94903212602058
Iteration: 7 || Loss: 117.91992747476205
Iteration: 8 || Loss: 117.74610002375228
Iteration: 9 || Loss: 117.55795427583723
Iteration: 10 || Loss: 117.32974287400815
Iteration: 11 || Loss: 116.84392765805903
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.1405
Epoch 84 loss:116.84392765805903
MSE loss S1.5257797492447498
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:32.565189902759
MSE loss S0.43803133911296405
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:221.25418701262566
MSE loss S4.037972431532134
waveform batch: 2/2
Test loss - extrapolation:148.5658783543282
MSE loss S2.8104834995890933
Epoch 84 mean train loss:5.192542500844388
Epoch 84 mean test loss - interpolation:5.427531650459834
Epoch 84 mean test loss - extrapolation:30.818338780579484
Start training epoch 85
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.1405
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.60032513664632
Iteration: 2 || Loss: 7.599054464147965
Iteration: 3 || Loss: 7.597787143532155
Iteration: 4 || Loss: 7.596549057413703
Iteration: 5 || Loss: 7.595250111336061
Iteration: 6 || Loss: 7.595250111336061
saving ADAM checkpoint...
Sum of params:-100.1405
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.595250111336061
Iteration: 2 || Loss: 7.558965697598807
Iteration: 3 || Loss: 7.547235633581823
Iteration: 4 || Loss: 7.183373046190073
Iteration: 5 || Loss: 7.0353261702744145
Iteration: 6 || Loss: 6.788550190320332
Iteration: 7 || Loss: 6.749245953321778
Iteration: 8 || Loss: 6.712629919082799
Iteration: 9 || Loss: 6.690641583940984
Iteration: 10 || Loss: 6.661107366108306
Iteration: 11 || Loss: 6.630794714203576
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.23871
Epoch 85 loss:6.630794714203576
MSE loss S0.23004885515899384
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.23871
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.515948535627025
Iteration: 2 || Loss: 29.514582765097565
Iteration: 3 || Loss: 29.513224618485072
Iteration: 4 || Loss: 29.511874153994167
Iteration: 5 || Loss: 29.510495918320327
Iteration: 6 || Loss: 29.510495918320327
saving ADAM checkpoint...
Sum of params:-100.23866
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.510495918320327
Iteration: 2 || Loss: 29.42961290350901
Iteration: 3 || Loss: 29.15858153067179
Iteration: 4 || Loss: 28.95444205991659
Iteration: 5 || Loss: 27.546822247097435
Iteration: 6 || Loss: 27.3978030769723
Iteration: 7 || Loss: 27.335782075276256
Iteration: 8 || Loss: 27.14414852060387
Iteration: 9 || Loss: 27.070778051529874
Iteration: 10 || Loss: 27.01375954688411
Iteration: 11 || Loss: 26.989685583954536
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.68546
Epoch 85 loss:26.989685583954536
MSE loss S0.48718555260773533
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.68546
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 119.513070374279
Iteration: 2 || Loss: 119.50962477099156
Iteration: 3 || Loss: 119.50620580275793
Iteration: 4 || Loss: 119.5026918570266
Iteration: 5 || Loss: 119.49917821307967
Iteration: 6 || Loss: 119.49917821307967
saving ADAM checkpoint...
Sum of params:-99.68556
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 119.49917821307967
Iteration: 2 || Loss: 119.10328290556207
Iteration: 3 || Loss: 119.01680086513792
Iteration: 4 || Loss: 118.29349141388228
Iteration: 5 || Loss: 117.9900707424014
Iteration: 6 || Loss: 117.60008356978663
Iteration: 7 || Loss: 116.60814651596354
Iteration: 8 || Loss: 116.42981840038401
Iteration: 9 || Loss: 116.24182573644332
Iteration: 10 || Loss: 116.02823945454607
Iteration: 11 || Loss: 115.59787822475839
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.23488
Epoch 85 loss:115.59787822475839
MSE loss S1.5077264080331667
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:32.332540732935364
MSE loss S0.4315963555116618
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:218.82116871701604
MSE loss S3.9853686806182
waveform batch: 2/2
Test loss - extrapolation:146.78502513424849
MSE loss S2.780945626923594
Epoch 85 mean train loss:5.145460638721259
Epoch 85 mean test loss - interpolation:5.388756788822561
Epoch 85 mean test loss - extrapolation:30.467182820938707
Start training epoch 86
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.23488
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.557459994918954
Iteration: 2 || Loss: 7.556223171653218
Iteration: 3 || Loss: 7.554963662290728
Iteration: 4 || Loss: 7.553806595054398
Iteration: 5 || Loss: 7.5526407052008695
Iteration: 6 || Loss: 7.5526407052008695
saving ADAM checkpoint...
Sum of params:-100.234886
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.5526407052008695
Iteration: 2 || Loss: 7.518826929028825
Iteration: 3 || Loss: 7.509861446394989
Iteration: 4 || Loss: 7.067030067468081
Iteration: 5 || Loss: 7.007253316561586
Iteration: 6 || Loss: 6.771003343325289
Iteration: 7 || Loss: 6.732931430463127
Iteration: 8 || Loss: 6.695695921166488
Iteration: 9 || Loss: 6.66997192974608
Iteration: 10 || Loss: 6.646114672253002
Iteration: 11 || Loss: 6.615305159222968
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.32597
Epoch 86 loss:6.615305159222968
MSE loss S0.2301707107653963
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.32597
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.33939246418577
Iteration: 2 || Loss: 29.337972223730905
Iteration: 3 || Loss: 29.336687136969367
Iteration: 4 || Loss: 29.335318743340032
Iteration: 5 || Loss: 29.33393991858244
Iteration: 6 || Loss: 29.33393991858244
saving ADAM checkpoint...
Sum of params:-100.32593
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.33393991858244
Iteration: 2 || Loss: 29.255301551891336
Iteration: 3 || Loss: 29.05938376899145
Iteration: 4 || Loss: 28.83282937284947
Iteration: 5 || Loss: 27.441132223520462
Iteration: 6 || Loss: 27.293182899691693
Iteration: 7 || Loss: 27.230736560316405
Iteration: 8 || Loss: 27.053037847958375
Iteration: 9 || Loss: 26.98268112979437
Iteration: 10 || Loss: 26.93069499503308
Iteration: 11 || Loss: 26.907099693305202
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.78923
Epoch 86 loss:26.907099693305202
MSE loss S0.48599058104640247
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.78923
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 118.1425257496856
Iteration: 2 || Loss: 118.13919866193197
Iteration: 3 || Loss: 118.13585307701794
Iteration: 4 || Loss: 118.13257250435633
Iteration: 5 || Loss: 118.12922287935892
Iteration: 6 || Loss: 118.12922287935892
saving ADAM checkpoint...
Sum of params:-99.789345
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 118.12922287935892
Iteration: 2 || Loss: 117.76443781414143
Iteration: 3 || Loss: 117.68640039743775
Iteration: 4 || Loss: 117.0246080100182
Iteration: 5 || Loss: 116.71808362143612
Iteration: 6 || Loss: 116.33582937552318
Iteration: 7 || Loss: 115.37704384486968
Iteration: 8 || Loss: 115.20042455309327
Iteration: 9 || Loss: 115.02732872311391
Iteration: 10 || Loss: 114.74709785068352
Iteration: 11 || Loss: 114.40428700072349
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.33364
Epoch 86 loss:114.40428700072349
MSE loss S1.4996230477026842
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:32.1381145006732
MSE loss S0.4323668234019674
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:216.370572904908
MSE loss S3.9417633470245423
waveform batch: 2/2
Test loss - extrapolation:144.88386717038068
MSE loss S2.751430890240269
Epoch 86 mean train loss:5.100920408732816
Epoch 86 mean test loss - interpolation:5.356352416778866
Epoch 86 mean test loss - extrapolation:30.104536672940725
Start training epoch 87
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.33364
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.529221620780928
Iteration: 2 || Loss: 7.5280201543469065
Iteration: 3 || Loss: 7.526877064509241
Iteration: 4 || Loss: 7.525742100896594
Iteration: 5 || Loss: 7.524553874866634
Iteration: 6 || Loss: 7.524553874866634
saving ADAM checkpoint...
Sum of params:-100.33365
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.524553874866634
Iteration: 2 || Loss: 7.4928993015619225
Iteration: 3 || Loss: 7.484466145140992
Iteration: 4 || Loss: 7.0033702449702595
Iteration: 5 || Loss: 6.952051227577726
Iteration: 6 || Loss: 6.756335994708417
Iteration: 7 || Loss: 6.721512321358294
Iteration: 8 || Loss: 6.684714176498766
Iteration: 9 || Loss: 6.659634401957556
Iteration: 10 || Loss: 6.633271898602442
Iteration: 11 || Loss: 6.6019486311600355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.42256
Epoch 87 loss:6.6019486311600355
MSE loss S0.22886757482514752
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.42256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.244510999586854
Iteration: 2 || Loss: 29.243207531613066
Iteration: 3 || Loss: 29.24179980224504
Iteration: 4 || Loss: 29.240463262185248
Iteration: 5 || Loss: 29.239152459572743
Iteration: 6 || Loss: 29.239152459572743
saving ADAM checkpoint...
Sum of params:-100.4225
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.239152459572743
Iteration: 2 || Loss: 29.16083284922869
Iteration: 3 || Loss: 28.948358436496438
Iteration: 4 || Loss: 28.731766061999277
Iteration: 5 || Loss: 27.352674010961806
Iteration: 6 || Loss: 27.203526999252468
Iteration: 7 || Loss: 27.14320180944102
Iteration: 8 || Loss: 26.974005822334842
Iteration: 9 || Loss: 26.906387957738456
Iteration: 10 || Loss: 26.853801970604575
Iteration: 11 || Loss: 26.830289994134496
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.89391
Epoch 87 loss:26.830289994134496
MSE loss S0.4842093082552496
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.89391
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 116.80964539619875
Iteration: 2 || Loss: 116.80647786360547
Iteration: 3 || Loss: 116.8033538819455
Iteration: 4 || Loss: 116.80015367368897
Iteration: 5 || Loss: 116.79695878875692
Iteration: 6 || Loss: 116.79695878875692
saving ADAM checkpoint...
Sum of params:-99.894035
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 116.79695878875692
Iteration: 2 || Loss: 116.47015719403252
Iteration: 3 || Loss: 116.40258137239442
Iteration: 4 || Loss: 115.78764217392231
Iteration: 5 || Loss: 115.4709662030167
Iteration: 6 || Loss: 115.09859894919818
Iteration: 7 || Loss: 114.16843673478792
Iteration: 8 || Loss: 113.9866901080904
Iteration: 9 || Loss: 113.83598899951107
Iteration: 10 || Loss: 113.4751017583119
Iteration: 11 || Loss: 113.21975494596158
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.445244
Epoch 87 loss:113.21975494596158
MSE loss S1.4882700027374711
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:31.987351174040068
MSE loss S0.43517157243076515
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:213.7682539663672
MSE loss S3.8922498232153524
waveform batch: 2/2
Test loss - extrapolation:142.72863229664193
MSE loss S2.7120813197861593
Epoch 87 mean train loss:5.056965295560556
Epoch 87 mean test loss - interpolation:5.331225195673345
Epoch 87 mean test loss - extrapolation:29.708073855250763
Start training epoch 88
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.445244
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.536755461950018
Iteration: 2 || Loss: 7.535516050341559
Iteration: 3 || Loss: 7.534270181406819
Iteration: 4 || Loss: 7.533070938221795
Iteration: 5 || Loss: 7.531876548078011
Iteration: 6 || Loss: 7.531876548078011
saving ADAM checkpoint...
Sum of params:-100.445244
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.531876548078011
Iteration: 2 || Loss: 7.497488449869016
Iteration: 3 || Loss: 7.487976082813343
Iteration: 4 || Loss: 7.028319443137201
Iteration: 5 || Loss: 6.993414210038937
Iteration: 6 || Loss: 6.754424762427266
Iteration: 7 || Loss: 6.71551173408116
Iteration: 8 || Loss: 6.676487323326657
Iteration: 9 || Loss: 6.652452546777566
Iteration: 10 || Loss: 6.624844570556034
Iteration: 11 || Loss: 6.59353983646844
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.53198
Epoch 88 loss:6.59353983646844
MSE loss S0.23026198756221905
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.53198
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.128615965313095
Iteration: 2 || Loss: 29.127257847972256
Iteration: 3 || Loss: 29.125935835198955
Iteration: 4 || Loss: 29.12469113858984
Iteration: 5 || Loss: 29.1233658948934
Iteration: 6 || Loss: 29.1233658948934
saving ADAM checkpoint...
Sum of params:-100.531944
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.1233658948934
Iteration: 2 || Loss: 29.05095263558284
Iteration: 3 || Loss: 28.80627646219162
Iteration: 4 || Loss: 28.654799285505355
Iteration: 5 || Loss: 27.28022503408198
Iteration: 6 || Loss: 27.130575098030647
Iteration: 7 || Loss: 27.066139228618848
Iteration: 8 || Loss: 26.902855651339763
Iteration: 9 || Loss: 26.833532090069934
Iteration: 10 || Loss: 26.784942720984695
Iteration: 11 || Loss: 26.75890000635007
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.00483
Epoch 88 loss:26.75890000635007
MSE loss S0.4839734377915878
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-100.00483
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 115.4683191732702
Iteration: 2 || Loss: 115.46534140899412
Iteration: 3 || Loss: 115.46234535699593
Iteration: 4 || Loss: 115.45936710451026
Iteration: 5 || Loss: 115.45634285017886
Iteration: 6 || Loss: 115.45634285017886
saving ADAM checkpoint...
Sum of params:-100.00492
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 115.45634285017886
Iteration: 2 || Loss: 115.16398724332952
Iteration: 3 || Loss: 115.10071400702846
Iteration: 4 || Loss: 114.54182442982511
Iteration: 5 || Loss: 114.210676013242
Iteration: 6 || Loss: 113.84037873975979
Iteration: 7 || Loss: 112.95435031009781
Iteration: 8 || Loss: 112.76249137946027
Iteration: 9 || Loss: 112.63269768631038
Iteration: 10 || Loss: 112.20668927331796
Iteration: 11 || Loss: 111.99718849962522
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.57009
Epoch 88 loss:111.99718849962522
MSE loss S1.4762981505706132
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:31.86796869804697
MSE loss S0.4396877527644621
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:210.96982833352945
MSE loss S3.839592488327402
waveform batch: 2/2
Test loss - extrapolation:140.3624654729999
MSE loss S2.6675320676945793
Epoch 88 mean train loss:5.012056149739439
Epoch 88 mean test loss - interpolation:5.311328116341161
Epoch 88 mean test loss - extrapolation:29.277691150544115
Start training epoch 89
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.57009
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.577674310843012
Iteration: 2 || Loss: 7.576384532839922
Iteration: 3 || Loss: 7.575065416469801
Iteration: 4 || Loss: 7.573801294310774
Iteration: 5 || Loss: 7.5725096373324785
Iteration: 6 || Loss: 7.5725096373324785
saving ADAM checkpoint...
Sum of params:-100.57011
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.5725096373324785
Iteration: 2 || Loss: 7.535713264608283
Iteration: 3 || Loss: 7.400599139156634
Iteration: 4 || Loss: 7.266285881890988
Iteration: 5 || Loss: 7.023045429529573
Iteration: 6 || Loss: 6.757790129358197
Iteration: 7 || Loss: 6.7158818168028365
Iteration: 8 || Loss: 6.674818674600381
Iteration: 9 || Loss: 6.650776455896463
Iteration: 10 || Loss: 6.6211408361689035
Iteration: 11 || Loss: 6.590578521686064
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.66272
Epoch 89 loss:6.590578521686064
MSE loss S0.22992819368363626
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.66272
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.18437613430835
Iteration: 2 || Loss: 29.182978553069447
Iteration: 3 || Loss: 29.181631895910765
Iteration: 4 || Loss: 29.180205312484965
Iteration: 5 || Loss: 29.178850480396612
Iteration: 6 || Loss: 29.178850480396612
saving ADAM checkpoint...
Sum of params:-100.66264
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.178850480396612
Iteration: 2 || Loss: 29.093101226887836
Iteration: 3 || Loss: 29.0238951512314
Iteration: 4 || Loss: 28.624621706767297
Iteration: 5 || Loss: 27.227602070479428
Iteration: 6 || Loss: 27.072755570684677
Iteration: 7 || Loss: 27.022024223210263
Iteration: 8 || Loss: 26.856976793149123
Iteration: 9 || Loss: 26.78864189575573
Iteration: 10 || Loss: 26.730703478286976
Iteration: 11 || Loss: 26.71109322354642
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.150734
Epoch 89 loss:26.71109322354642
MSE loss S0.4786289937715595
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-100.150734
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 113.9612249265036
Iteration: 2 || Loss: 113.95830547739409
Iteration: 3 || Loss: 113.9554182358096
Iteration: 4 || Loss: 113.95246687802685
Iteration: 5 || Loss: 113.94963799862491
Iteration: 6 || Loss: 113.94963799862491
saving ADAM checkpoint...
Sum of params:-100.15087
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 113.94963799862491
Iteration: 2 || Loss: 113.67730009709598
Iteration: 3 || Loss: 113.61588542867929
Iteration: 4 || Loss: 113.16675599491577
Iteration: 5 || Loss: 112.8628256879752
Iteration: 6 || Loss: 112.46583056062182
Iteration: 7 || Loss: 111.64139192912963
Iteration: 8 || Loss: 111.47459571981132
Iteration: 9 || Loss: 111.35639890284813
Iteration: 10 || Loss: 110.90355837438906
Iteration: 11 || Loss: 110.67688661931096
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.71135
Epoch 89 loss:110.67688661931096
MSE loss S1.4408037107410403
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:31.729634783458604
MSE loss S0.43639971305644576
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:208.21135877295075
MSE loss S3.76126297883095
waveform batch: 2/2
Test loss - extrapolation:138.08223538403678
MSE loss S2.611048537890066
Epoch 89 mean train loss:4.96477787463943
Epoch 89 mean test loss - interpolation:5.288272463909768
Epoch 89 mean test loss - extrapolation:28.857799513082295
Start training epoch 90
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.71135
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.7681288451661015
Iteration: 2 || Loss: 7.766574884151609
Iteration: 3 || Loss: 7.76497913893959
Iteration: 4 || Loss: 7.763411500190861
Iteration: 5 || Loss: 7.761867571536067
Iteration: 6 || Loss: 7.761867571536067
saving ADAM checkpoint...
Sum of params:-100.711365
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.761867571536067
Iteration: 2 || Loss: 7.707753499920175
Iteration: 3 || Loss: 7.627072122846322
Iteration: 4 || Loss: 7.562834884022775
Iteration: 5 || Loss: 7.15216399608729
Iteration: 6 || Loss: 6.824430395810921
Iteration: 7 || Loss: 6.730482802638076
Iteration: 8 || Loss: 6.709395876132579
Iteration: 9 || Loss: 6.666270638194539
Iteration: 10 || Loss: 6.631546848083943
Iteration: 11 || Loss: 6.616445556230732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.81949
Epoch 90 loss:6.616445556230732
MSE loss S0.23317721381639417
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-100.81949
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.054575870786355
Iteration: 2 || Loss: 29.05326314535483
Iteration: 3 || Loss: 29.051916610442124
Iteration: 4 || Loss: 29.050669027168315
Iteration: 5 || Loss: 29.049347442185933
Iteration: 6 || Loss: 29.049347442185933
saving ADAM checkpoint...
Sum of params:-100.81946
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.049347442185933
Iteration: 2 || Loss: 28.975073347161654
Iteration: 3 || Loss: 28.88052831191771
Iteration: 4 || Loss: 28.380312692676238
Iteration: 5 || Loss: 27.233048214393083
Iteration: 6 || Loss: 27.07444963311645
Iteration: 7 || Loss: 26.991103923838644
Iteration: 8 || Loss: 26.80932239924344
Iteration: 9 || Loss: 26.733068829230515
Iteration: 10 || Loss: 26.702258101586246
Iteration: 11 || Loss: 26.662773495585665
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.30189
Epoch 90 loss:26.662773495585665
MSE loss S0.48556916366521086
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-100.30189
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 112.35318850350802
Iteration: 2 || Loss: 112.35070421721235
Iteration: 3 || Loss: 112.34823081274988
Iteration: 4 || Loss: 112.34574911960063
Iteration: 5 || Loss: 112.34328863973928
Iteration: 6 || Loss: 112.34328863973928
saving ADAM checkpoint...
Sum of params:-100.301895
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 112.34328863973928
Iteration: 2 || Loss: 112.15363699081317
Iteration: 3 || Loss: 112.10762036145542
Iteration: 4 || Loss: 111.79881252933926
Iteration: 5 || Loss: 111.34209977246759
Iteration: 6 || Loss: 110.89026631159028
Iteration: 7 || Loss: 110.25124761148355
Iteration: 8 || Loss: 110.05705508328603
Iteration: 9 || Loss: 109.86644743803026
Iteration: 10 || Loss: 109.42133705423404
Iteration: 11 || Loss: 108.9916908709092
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.8753
Epoch 90 loss:108.9916908709092
MSE loss S1.5657815204416474
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:31.577904008666085
MSE loss S0.5036217947659567
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:206.1224652706614
MSE loss S3.867364323022638
waveform batch: 2/2
Test loss - extrapolation:137.16306431496977
MSE loss S2.6702423151726027
Epoch 90 mean train loss:4.905893445611228
Epoch 90 mean test loss - interpolation:5.262984001444347
Epoch 90 mean test loss - extrapolation:28.60712746546926
Start training epoch 91
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-100.8753
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.196126897444515
Iteration: 2 || Loss: 8.193864549180358
Iteration: 3 || Loss: 8.191537231686713
Iteration: 4 || Loss: 8.189210159927175
Iteration: 5 || Loss: 8.18688659001695
Iteration: 6 || Loss: 8.18688659001695
saving ADAM checkpoint...
Sum of params:-100.875305
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.18688659001695
Iteration: 2 || Loss: 8.051055887881795
Iteration: 3 || Loss: 7.984697465707591
Iteration: 4 || Loss: 7.734111939998541
Iteration: 5 || Loss: 7.548119549179631
Iteration: 6 || Loss: 6.894948691791656
Iteration: 7 || Loss: 6.765063764025811
Iteration: 8 || Loss: 6.740498703701176
Iteration: 9 || Loss: 6.6878496823702225
Iteration: 10 || Loss: 6.66411431576796
Iteration: 11 || Loss: 6.652308563409351
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.01602
Epoch 91 loss:6.652308563409351
MSE loss S0.241530514571444
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-101.01602
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.0150182283171
Iteration: 2 || Loss: 29.01390180960785
Iteration: 3 || Loss: 29.012750343618954
Iteration: 4 || Loss: 29.011749553631688
Iteration: 5 || Loss: 29.010603843753376
Iteration: 6 || Loss: 29.010603843753376
saving ADAM checkpoint...
Sum of params:-101.01596
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.010603843753376
Iteration: 2 || Loss: 28.958736786454164
Iteration: 3 || Loss: 28.804534820085372
Iteration: 4 || Loss: 28.532591969484734
Iteration: 5 || Loss: 27.272252304113522
Iteration: 6 || Loss: 27.112477278291795
Iteration: 7 || Loss: 26.972483988105758
Iteration: 8 || Loss: 26.7807935609612
Iteration: 9 || Loss: 26.714702244566176
Iteration: 10 || Loss: 26.681360782998294
Iteration: 11 || Loss: 26.58364573841015
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.426125
Epoch 91 loss:26.58364573841015
MSE loss S0.5069412479945168
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-100.426125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 110.50724225678931
Iteration: 2 || Loss: 110.50490653787419
Iteration: 3 || Loss: 110.50270772599866
Iteration: 4 || Loss: 110.5003840753876
Iteration: 5 || Loss: 110.49805823046955
Iteration: 6 || Loss: 110.49805823046955
saving ADAM checkpoint...
Sum of params:-100.42611
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 110.49805823046955
Iteration: 2 || Loss: 110.33811936022238
Iteration: 3 || Loss: 110.29446546543986
Iteration: 4 || Loss: 110.0017822875639
Iteration: 5 || Loss: 109.52637708997281
Iteration: 6 || Loss: 109.06893259867469
Iteration: 7 || Loss: 108.53155956871771
Iteration: 8 || Loss: 108.33091723625657
Iteration: 9 || Loss: 108.05911123285885
Iteration: 10 || Loss: 107.72715630660426
Iteration: 11 || Loss: 107.28577705060184
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.01879
Epoch 91 loss:107.28577705060184
MSE loss S1.5017493900573682
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:31.258322982511313
MSE loss S0.4756152497849266
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:202.92648184370407
MSE loss S3.765965384283688
waveform batch: 2/2
Test loss - extrapolation:135.01811144356108
MSE loss S2.6200449112845687
Epoch 91 mean train loss:4.845576943186943
Epoch 91 mean test loss - interpolation:5.209720497085219
Epoch 91 mean test loss - extrapolation:28.16204944060543
Start training epoch 92
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-101.01879
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.062178065099205
Iteration: 2 || Loss: 8.060268575172806
Iteration: 3 || Loss: 8.058380580027134
Iteration: 4 || Loss: 8.056539456394626
Iteration: 5 || Loss: 8.054719630449503
Iteration: 6 || Loss: 8.054719630449503
saving ADAM checkpoint...
Sum of params:-101.0188
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.054719630449503
Iteration: 2 || Loss: 7.963216682560346
Iteration: 3 || Loss: 7.908015291450155
Iteration: 4 || Loss: 7.681272292094574
Iteration: 5 || Loss: 7.420124662408883
Iteration: 6 || Loss: 6.908363749287726
Iteration: 7 || Loss: 6.759639815589133
Iteration: 8 || Loss: 6.73135430071863
Iteration: 9 || Loss: 6.6924421505602085
Iteration: 10 || Loss: 6.656997582145279
Iteration: 11 || Loss: 6.647430815523411
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.15539
Epoch 92 loss:6.647430815523411
MSE loss S0.24285253393152367
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-101.15539
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.762041607551716
Iteration: 2 || Loss: 28.761109817618497
Iteration: 3 || Loss: 28.76009095048214
Iteration: 4 || Loss: 28.759119977252805
Iteration: 5 || Loss: 28.758228739776676
Iteration: 6 || Loss: 28.758228739776676
saving ADAM checkpoint...
Sum of params:-101.15532
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.758228739776676
Iteration: 2 || Loss: 28.722575551334003
Iteration: 3 || Loss: 28.5377872848449
Iteration: 4 || Loss: 28.336910701199187
Iteration: 5 || Loss: 27.16160082875154
Iteration: 6 || Loss: 27.007349269547362
Iteration: 7 || Loss: 26.851814215793812
Iteration: 8 || Loss: 26.683497375527125
Iteration: 9 || Loss: 26.61920177798685
Iteration: 10 || Loss: 26.587728343168266
Iteration: 11 || Loss: 26.46662711901118
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.52197
Epoch 92 loss:26.46662711901118
MSE loss S0.4976634610064765
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-100.52197
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 108.86644888387991
Iteration: 2 || Loss: 108.8641063380898
Iteration: 3 || Loss: 108.86182685032668
Iteration: 4 || Loss: 108.85958184828624
Iteration: 5 || Loss: 108.85727878118274
Iteration: 6 || Loss: 108.85727878118274
saving ADAM checkpoint...
Sum of params:-100.52194
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 108.85727878118274
Iteration: 2 || Loss: 108.69956446794647
Iteration: 3 || Loss: 108.65696891038277
Iteration: 4 || Loss: 108.42130846167112
Iteration: 5 || Loss: 107.97683887348786
Iteration: 6 || Loss: 107.46166966989223
Iteration: 7 || Loss: 106.95685007159035
Iteration: 8 || Loss: 106.74996787268888
Iteration: 9 || Loss: 106.44413292801111
Iteration: 10 || Loss: 106.17047380489493
Iteration: 11 || Loss: 105.67194183423327
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.157295
Epoch 92 loss:105.67194183423327
MSE loss S1.45417240216068
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:30.99736760920411
MSE loss S0.464241568675663
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:199.86378753032412
MSE loss S3.693699613509416
waveform batch: 2/2
Test loss - extrapolation:132.47157326530433
MSE loss S2.561647375912416
Epoch 92 mean train loss:4.785724129957512
Epoch 92 mean test loss - interpolation:5.166227934867352
Epoch 92 mean test loss - extrapolation:27.694613399635703
Start training epoch 93
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-101.157295
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.099831965539238
Iteration: 2 || Loss: 8.09798953243719
Iteration: 3 || Loss: 8.096136830123724
Iteration: 4 || Loss: 8.094353487721898
Iteration: 5 || Loss: 8.092535136389477
Iteration: 6 || Loss: 8.092535136389477
saving ADAM checkpoint...
Sum of params:-101.1573
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.092535136389477
Iteration: 2 || Loss: 8.00478640386175
Iteration: 3 || Loss: 7.938286078967147
Iteration: 4 || Loss: 7.509303463824976
Iteration: 5 || Loss: 7.356013599965081
Iteration: 6 || Loss: 6.941861747616867
Iteration: 7 || Loss: 6.7433098091660515
Iteration: 8 || Loss: 6.711561747240029
Iteration: 9 || Loss: 6.6837924255005
Iteration: 10 || Loss: 6.6391323925186825
Iteration: 11 || Loss: 6.631559416666358
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.28559
Epoch 93 loss:6.631559416666358
MSE loss S0.24390973207449956
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-101.28559
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.479052005378538
Iteration: 2 || Loss: 28.478153713513976
Iteration: 3 || Loss: 28.477389859686003
Iteration: 4 || Loss: 28.476529070395006
Iteration: 5 || Loss: 28.475646327344187
Iteration: 6 || Loss: 28.475646327344187
saving ADAM checkpoint...
Sum of params:-101.28553
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.475646327344187
Iteration: 2 || Loss: 28.448314886390694
Iteration: 3 || Loss: 28.219458631453428
Iteration: 4 || Loss: 28.09432646948479
Iteration: 5 || Loss: 27.030155807789686
Iteration: 6 || Loss: 26.881765210269634
Iteration: 7 || Loss: 26.722902090343254
Iteration: 8 || Loss: 26.56647643029971
Iteration: 9 || Loss: 26.505685588137585
Iteration: 10 || Loss: 26.469311436546082
Iteration: 11 || Loss: 26.330083382551265
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.60315
Epoch 93 loss:26.330083382551265
MSE loss S0.48398639214830774
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-100.60315
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 107.14931370635313
Iteration: 2 || Loss: 107.14716666652751
Iteration: 3 || Loss: 107.1450613351254
Iteration: 4 || Loss: 107.14291771629864
Iteration: 5 || Loss: 107.1408803973759
Iteration: 6 || Loss: 107.1408803973759
saving ADAM checkpoint...
Sum of params:-100.60308
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 107.1408803973759
Iteration: 2 || Loss: 107.00807401667196
Iteration: 3 || Loss: 106.95758785705672
Iteration: 4 || Loss: 106.7862717469256
Iteration: 5 || Loss: 106.38554578057692
Iteration: 6 || Loss: 105.79135041137708
Iteration: 7 || Loss: 105.39738684667515
Iteration: 8 || Loss: 105.1825448665463
Iteration: 9 || Loss: 104.84677012980256
Iteration: 10 || Loss: 104.60919956628176
Iteration: 11 || Loss: 104.12635750040731
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.30289
Epoch 93 loss:104.12635750040731
MSE loss S1.4061219286697892
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:30.712375059762092
MSE loss S0.45117350573769777
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:196.69792798071379
MSE loss S3.604992707068014
waveform batch: 2/2
Test loss - extrapolation:129.82441000132707
MSE loss S2.4952259450462684
Epoch 93 mean train loss:4.727172424124998
Epoch 93 mean test loss - interpolation:5.118729176627015
Epoch 93 mean test loss - extrapolation:27.21019483183674
Start training epoch 94
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-101.30289
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.065287355381102
Iteration: 2 || Loss: 8.063248090108646
Iteration: 3 || Loss: 8.061231303200794
Iteration: 4 || Loss: 8.059193833437078
Iteration: 5 || Loss: 8.057217968052322
Iteration: 6 || Loss: 8.057217968052322
saving ADAM checkpoint...
Sum of params:-101.3029
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.057217968052322
Iteration: 2 || Loss: 7.9610765462584725
Iteration: 3 || Loss: 7.917509349352147
Iteration: 4 || Loss: 7.52914864025436
Iteration: 5 || Loss: 7.393470382265091
Iteration: 6 || Loss: 6.9157133014578545
Iteration: 7 || Loss: 6.726037611799571
Iteration: 8 || Loss: 6.6980835269049495
Iteration: 9 || Loss: 6.668063348119647
Iteration: 10 || Loss: 6.628198975384695
Iteration: 11 || Loss: 6.61882453080128
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.42753
Epoch 94 loss:6.61882453080128
MSE loss S0.24423298839557156
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-101.42753
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.242217943285215
Iteration: 2 || Loss: 28.241309583766586
Iteration: 3 || Loss: 28.240415873061565
Iteration: 4 || Loss: 28.23957742274565
Iteration: 5 || Loss: 28.238720571611005
Iteration: 6 || Loss: 28.238720571611005
saving ADAM checkpoint...
Sum of params:-101.427444
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.238720571611005
Iteration: 2 || Loss: 28.210003505653642
Iteration: 3 || Loss: 28.046306531199807
Iteration: 4 || Loss: 27.868513746066096
Iteration: 5 || Loss: 26.90470224833699
Iteration: 6 || Loss: 26.766180866229604
Iteration: 7 || Loss: 26.61604906788231
Iteration: 8 || Loss: 26.461667790497604
Iteration: 9 || Loss: 26.3989144196631
Iteration: 10 || Loss: 26.365245431286244
Iteration: 11 || Loss: 26.24043609932265
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-100.78531
Epoch 94 loss:26.24043609932265
MSE loss S0.48445401968471785
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-100.78531
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 105.28395814756631
Iteration: 2 || Loss: 105.28210884059311
Iteration: 3 || Loss: 105.28030384467948
Iteration: 4 || Loss: 105.27851459366526
Iteration: 5 || Loss: 105.2766591873322
Iteration: 6 || Loss: 105.2766591873322
saving ADAM checkpoint...
Sum of params:-100.78525
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 105.2766591873322
Iteration: 2 || Loss: 105.17926477637744
Iteration: 3 || Loss: 105.14014712669825
Iteration: 4 || Loss: 104.99994989767738
Iteration: 5 || Loss: 104.6043544584723
Iteration: 6 || Loss: 103.99805310371914
Iteration: 7 || Loss: 103.74548199051668
Iteration: 8 || Loss: 103.57716699990372
Iteration: 9 || Loss: 103.19146508660967
Iteration: 10 || Loss: 102.9148422858797
Iteration: 11 || Loss: 102.3571084002005
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.56056
Epoch 94 loss:102.3571084002005
MSE loss S1.2925666944640803
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:30.639416035088207
MSE loss S0.42843325667068144
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:191.9151330864097
MSE loss S3.437300954907628
waveform batch: 2/2
Test loss - extrapolation:125.20789374776095
MSE loss S2.348633973439687
Epoch 94 mean train loss:4.662633414838773
Epoch 94 mean test loss - interpolation:5.106569339181368
Epoch 94 mean test loss - extrapolation:26.426918902847557
Start training epoch 95
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-101.56056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.146677189466992
Iteration: 2 || Loss: 8.144563881701997
Iteration: 3 || Loss: 8.142420672017003
Iteration: 4 || Loss: 8.140287625719985
Iteration: 5 || Loss: 8.13819657615577
Iteration: 6 || Loss: 8.13819657615577
saving ADAM checkpoint...
Sum of params:-101.56059
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.13819657615577
Iteration: 2 || Loss: 8.041384059416812
Iteration: 3 || Loss: 8.015148484397788
Iteration: 4 || Loss: 7.820587653454147
Iteration: 5 || Loss: 7.4415198956630855
Iteration: 6 || Loss: 6.959086260563447
Iteration: 7 || Loss: 6.742983277031849
Iteration: 8 || Loss: 6.7096728358709825
Iteration: 9 || Loss: 6.682168541272133
Iteration: 10 || Loss: 6.636747831670641
Iteration: 11 || Loss: 6.62798398068592
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.7006
Epoch 95 loss:6.62798398068592
MSE loss S0.24629699560614599
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-101.7006
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.280014675238878
Iteration: 2 || Loss: 28.27909522183836
Iteration: 3 || Loss: 28.278159178384684
Iteration: 4 || Loss: 28.277222505633336
Iteration: 5 || Loss: 28.276292559534173
Iteration: 6 || Loss: 28.276292559534173
saving ADAM checkpoint...
Sum of params:-101.700554
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.276292559534173
Iteration: 2 || Loss: 28.242642023287413
Iteration: 3 || Loss: 27.976521467964186
Iteration: 4 || Loss: 27.87091215124818
Iteration: 5 || Loss: 26.903660042142302
Iteration: 6 || Loss: 26.76642279058381
Iteration: 7 || Loss: 26.637974930008067
Iteration: 8 || Loss: 26.43052779878683
Iteration: 9 || Loss: 26.34937700108827
Iteration: 10 || Loss: 26.317235391449987
Iteration: 11 || Loss: 26.205441398547404
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.03912
Epoch 95 loss:26.205441398547404
MSE loss S0.4788992648425836
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-101.03912
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 103.1504148225612
Iteration: 2 || Loss: 103.14887490238848
Iteration: 3 || Loss: 103.14731298174951
Iteration: 4 || Loss: 103.14582659381814
Iteration: 5 || Loss: 103.14427143149071
Iteration: 6 || Loss: 103.14427143149071
saving ADAM checkpoint...
Sum of params:-101.03907
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 103.14427143149071
Iteration: 2 || Loss: 103.07565385410881
Iteration: 3 || Loss: 103.0003002934173
Iteration: 4 || Loss: 102.93454315899761
Iteration: 5 || Loss: 102.47326414128739
Iteration: 6 || Loss: 101.95127012754664
Iteration: 7 || Loss: 101.73246182248961
Iteration: 8 || Loss: 101.43920878671167
Iteration: 9 || Loss: 101.20545377931857
Iteration: 10 || Loss: 100.55605361292965
Iteration: 11 || Loss: 100.28732302239477
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.80638
Epoch 95 loss:100.28732302239477
MSE loss S1.271280606134956
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:30.35961029818397
MSE loss S0.44221120555020965
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:187.1205290559592
MSE loss S3.357607561105546
waveform batch: 2/2
Test loss - extrapolation:120.77891176530527
MSE loss S2.253248647854735
Epoch 95 mean train loss:4.590370634538901
Epoch 95 mean test loss - interpolation:5.059935049697328
Epoch 95 mean test loss - extrapolation:25.658286735105374
Start training epoch 96
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-101.80638
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.1457440109501
Iteration: 2 || Loss: 8.144072571025841
Iteration: 3 || Loss: 8.142473736808531
Iteration: 4 || Loss: 8.140849623643067
Iteration: 5 || Loss: 8.139173869914563
Iteration: 6 || Loss: 8.139173869914563
saving ADAM checkpoint...
Sum of params:-101.80637
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.139173869914563
Iteration: 2 || Loss: 8.079453454902273
Iteration: 3 || Loss: 8.05186355541647
Iteration: 4 || Loss: 7.595741310363588
Iteration: 5 || Loss: 7.295914869320543
Iteration: 6 || Loss: 6.942514965553642
Iteration: 7 || Loss: 6.75320087212055
Iteration: 8 || Loss: 6.717026060146564
Iteration: 9 || Loss: 6.691570810224666
Iteration: 10 || Loss: 6.638634055980192
Iteration: 11 || Loss: 6.627934915722584
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.94442
Epoch 96 loss:6.627934915722584
MSE loss S0.2505047193689062
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-101.94442
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.276772819944505
Iteration: 2 || Loss: 28.276005697782743
Iteration: 3 || Loss: 28.27518346069971
Iteration: 4 || Loss: 28.274495389062338
Iteration: 5 || Loss: 28.27380965982529
Iteration: 6 || Loss: 28.27380965982529
saving ADAM checkpoint...
Sum of params:-101.94436
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.27380965982529
Iteration: 2 || Loss: 28.252684597339144
Iteration: 3 || Loss: 28.12151711075629
Iteration: 4 || Loss: 27.92091753856999
Iteration: 5 || Loss: 26.84546755941615
Iteration: 6 || Loss: 26.700822180793523
Iteration: 7 || Loss: 26.492624373129605
Iteration: 8 || Loss: 26.338124373606064
Iteration: 9 || Loss: 26.272179009307006
Iteration: 10 || Loss: 26.185460512170767
Iteration: 11 || Loss: 26.109720750358946
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.19206
Epoch 96 loss:26.109720750358946
MSE loss S0.47641286602898875
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-101.19206
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 100.95792849895902
Iteration: 2 || Loss: 100.95625719945008
Iteration: 3 || Loss: 100.95467707690621
Iteration: 4 || Loss: 100.9530557065548
Iteration: 5 || Loss: 100.95143555529013
Iteration: 6 || Loss: 100.95143555529013
saving ADAM checkpoint...
Sum of params:-101.19204
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 100.95143555529013
Iteration: 2 || Loss: 100.87601724294466
Iteration: 3 || Loss: 100.8511229425126
Iteration: 4 || Loss: 100.77092752645288
Iteration: 5 || Loss: 100.3391693974917
Iteration: 6 || Loss: 99.73703529343081
Iteration: 7 || Loss: 99.57887211210962
Iteration: 8 || Loss: 99.31768549001735
Iteration: 9 || Loss: 99.0920048267825
Iteration: 10 || Loss: 98.57126568637113
Iteration: 11 || Loss: 98.39654817463129
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.91507
Epoch 96 loss:98.39654817463129
MSE loss S1.2530416721181128
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:29.912398179080988
MSE loss S0.4255455360875314
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:183.61215236723777
MSE loss S3.3057978186312886
waveform batch: 2/2
Test loss - extrapolation:118.5543968463972
MSE loss S2.236909381414087
Epoch 96 mean train loss:4.521869097955614
Epoch 96 mean test loss - interpolation:4.985399696513498
Epoch 96 mean test loss - extrapolation:25.180545767802915
Start training epoch 97
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-101.91507
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.779255489989852
Iteration: 2 || Loss: 7.77846195811593
Iteration: 3 || Loss: 7.7776320769871345
Iteration: 4 || Loss: 7.7767992157882135
Iteration: 5 || Loss: 7.776016675396113
Iteration: 6 || Loss: 7.776016675396113
saving ADAM checkpoint...
Sum of params:-101.91505
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.776016675396113
Iteration: 2 || Loss: 7.760742265793278
Iteration: 3 || Loss: 7.739678497490655
Iteration: 4 || Loss: 7.65838963832757
Iteration: 5 || Loss: 6.992212890258854
Iteration: 6 || Loss: 6.733699584699796
Iteration: 7 || Loss: 6.685619717708612
Iteration: 8 || Loss: 6.62331603485843
Iteration: 9 || Loss: 6.608025670729242
Iteration: 10 || Loss: 6.550048526362847
Iteration: 11 || Loss: 6.516772214466956
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.02782
Epoch 97 loss:6.516772214466956
MSE loss S0.23774513371114175
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-102.02782
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.00522181644932
Iteration: 2 || Loss: 28.003645972734805
Iteration: 3 || Loss: 28.00199489899104
Iteration: 4 || Loss: 28.000414233088648
Iteration: 5 || Loss: 27.998773577513337
Iteration: 6 || Loss: 27.998773577513337
saving ADAM checkpoint...
Sum of params:-102.02784
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.998773577513337
Iteration: 2 || Loss: 27.87622196740763
Iteration: 3 || Loss: 27.83025198965303
Iteration: 4 || Loss: 27.590047431787383
Iteration: 5 || Loss: 26.563373788154955
Iteration: 6 || Loss: 26.444766353092948
Iteration: 7 || Loss: 26.407396475050216
Iteration: 8 || Loss: 26.228969495399337
Iteration: 9 || Loss: 26.18859104440472
Iteration: 10 || Loss: 26.147069623184077
Iteration: 11 || Loss: 26.124824947720235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.58792
Epoch 97 loss:26.124824947720235
MSE loss S0.4742983513263387
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-101.58792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 98.64589933250366
Iteration: 2 || Loss: 98.64447536820698
Iteration: 3 || Loss: 98.64314558277451
Iteration: 4 || Loss: 98.64186146977671
Iteration: 5 || Loss: 98.64051289432845
Iteration: 6 || Loss: 98.64051289432845
saving ADAM checkpoint...
Sum of params:-101.58802
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 98.64051289432845
Iteration: 2 || Loss: 98.58968130385554
Iteration: 3 || Loss: 98.51787728908829
Iteration: 4 || Loss: 98.47798143478923
Iteration: 5 || Loss: 98.16997416521824
Iteration: 6 || Loss: 97.8888734856173
Iteration: 7 || Loss: 97.75854018034042
Iteration: 8 || Loss: 97.6461320938747
Iteration: 9 || Loss: 97.42953112836727
Iteration: 10 || Loss: 96.17065273265686
Iteration: 11 || Loss: 95.55470334070657
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.32462
Epoch 97 loss:95.55470334070657
MSE loss S1.2949773799554203
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:30.408735904255778
MSE loss S0.5443103903651584
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:176.59069029057133
MSE loss S3.303285444286244
waveform batch: 2/2
Test loss - extrapolation:110.85645806695986
MSE loss S2.1052717162497485
Epoch 97 mean train loss:4.420562086306681
Epoch 97 mean test loss - interpolation:5.068122650709296
Epoch 97 mean test loss - extrapolation:23.953929029794267
Start training epoch 98
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-102.32462
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.588110549453239
Iteration: 2 || Loss: 9.584315596345915
Iteration: 3 || Loss: 9.580509891252738
Iteration: 4 || Loss: 9.576696031399607
Iteration: 5 || Loss: 9.572946236863938
Iteration: 6 || Loss: 9.572946236863938
saving ADAM checkpoint...
Sum of params:-102.32463
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.572946236863938
Iteration: 2 || Loss: 9.222713884164664
Iteration: 3 || Loss: 9.133975858614269
Iteration: 4 || Loss: 8.502469078970526
Iteration: 5 || Loss: 8.366990856691068
Iteration: 6 || Loss: 7.22305640246409
Iteration: 7 || Loss: 6.879681033177402
Iteration: 8 || Loss: 6.860569726558228
Iteration: 9 || Loss: 6.777617113332567
Iteration: 10 || Loss: 6.748095816984642
Iteration: 11 || Loss: 6.717484239821562
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.43345
Epoch 98 loss:6.717484239821562
MSE loss S0.2709662141328263
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-102.43345
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.291452056746852
Iteration: 2 || Loss: 28.29050690460551
Iteration: 3 || Loss: 28.289550569578395
Iteration: 4 || Loss: 28.288666517653244
Iteration: 5 || Loss: 28.287762267010336
Iteration: 6 || Loss: 28.287762267010336
saving ADAM checkpoint...
Sum of params:-102.43342
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.287762267010336
Iteration: 2 || Loss: 28.249708131947088
Iteration: 3 || Loss: 28.163974097396576
Iteration: 4 || Loss: 27.67613043971293
Iteration: 5 || Loss: 26.857244266158744
Iteration: 6 || Loss: 26.67160027461965
Iteration: 7 || Loss: 26.59830303622017
Iteration: 8 || Loss: 26.332530981187283
Iteration: 9 || Loss: 26.287305313219953
Iteration: 10 || Loss: 26.21754194981385
Iteration: 11 || Loss: 26.13609569812974
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.00066
Epoch 98 loss:26.13609569812974
MSE loss S0.4868384975680367
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-102.00066
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 95.33450314343227
Iteration: 2 || Loss: 95.33363718028878
Iteration: 3 || Loss: 95.3327894810874
Iteration: 4 || Loss: 95.33202988753267
Iteration: 5 || Loss: 95.33119056484426
Iteration: 6 || Loss: 95.33119056484426
saving ADAM checkpoint...
Sum of params:-102.000595
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 95.33119056484426
Iteration: 2 || Loss: 95.31197860097362
Iteration: 3 || Loss: 95.2587914018121
Iteration: 4 || Loss: 95.0508430001039
Iteration: 5 || Loss: 94.57478895703859
Iteration: 6 || Loss: 94.2380795774298
Iteration: 7 || Loss: 93.9864995064149
Iteration: 8 || Loss: 93.89025985002046
Iteration: 9 || Loss: 93.76092980254421
Iteration: 10 || Loss: 93.29292794179442
Iteration: 11 || Loss: 92.87717470302209
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.42939
Epoch 98 loss:92.87717470302209
MSE loss S1.195049139309756
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:29.139677531346408
MSE loss S0.41673242639592367
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:173.1395195987817
MSE loss S3.154814649809061
waveform batch: 2/2
Test loss - extrapolation:110.83069114798992
MSE loss S2.171085954470607
Epoch 98 mean train loss:4.335543263481841
Epoch 98 mean test loss - interpolation:4.856612921891068
Epoch 98 mean test loss - extrapolation:23.664184228897636
Start training epoch 99
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-102.42939
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.739543050287768
Iteration: 2 || Loss: 7.737694309067054
Iteration: 3 || Loss: 7.735914855640269
Iteration: 4 || Loss: 7.734117134476205
Iteration: 5 || Loss: 7.732244447102206
Iteration: 6 || Loss: 7.732244447102206
saving ADAM checkpoint...
Sum of params:-102.429306
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.732244447102206
Iteration: 2 || Loss: 7.580224260473242
Iteration: 3 || Loss: 7.551585960391047
Iteration: 4 || Loss: 7.406013894717946
Iteration: 5 || Loss: 7.039650905493805
Iteration: 6 || Loss: 6.824435978174566
Iteration: 7 || Loss: 6.772421039588244
Iteration: 8 || Loss: 6.748360201212206
Iteration: 9 || Loss: 6.667254093253582
Iteration: 10 || Loss: 6.655121839632804
Iteration: 11 || Loss: 6.599025714824698
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.473595
Epoch 99 loss:6.599025714824698
MSE loss S0.2611128835536064
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-102.473595
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.123636403982488
Iteration: 2 || Loss: 27.12342619614667
Iteration: 3 || Loss: 27.123198745695458
Iteration: 4 || Loss: 27.12298487394238
Iteration: 5 || Loss: 27.122802954463392
Iteration: 6 || Loss: 27.122802954463392
saving ADAM checkpoint...
Sum of params:-102.4735
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.122802954463392
Iteration: 2 || Loss: 27.10676292024864
Iteration: 3 || Loss: 27.07796079750471
Iteration: 4 || Loss: 26.966409481775777
Iteration: 5 || Loss: 26.283375569738393
Iteration: 6 || Loss: 26.175595839275243
Iteration: 7 || Loss: 26.093114099177857
Iteration: 8 || Loss: 25.98421753261112
Iteration: 9 || Loss: 25.934526003574067
Iteration: 10 || Loss: 25.82305469443926
Iteration: 11 || Loss: 25.760458121434397
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.82006
Epoch 99 loss:25.760458121434397
MSE loss S0.4711804271188007
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-101.82006
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 93.55442412888897
Iteration: 2 || Loss: 93.55388778981846
Iteration: 3 || Loss: 93.55337868224298
Iteration: 4 || Loss: 93.55282056498949
Iteration: 5 || Loss: 93.55235954798218
Iteration: 6 || Loss: 93.55235954798218
saving ADAM checkpoint...
Sum of params:-101.820015
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 93.55235954798218
Iteration: 2 || Loss: 93.54447925985998
Iteration: 3 || Loss: 93.51762262587675
Iteration: 4 || Loss: 93.3725787947962
Iteration: 5 || Loss: 93.001929480628
Iteration: 6 || Loss: 92.53510918464511
Iteration: 7 || Loss: 92.3019773223635
Iteration: 8 || Loss: 92.155692305387
Iteration: 9 || Loss: 91.96099188668255
Iteration: 10 || Loss: 91.59860163469021
Iteration: 11 || Loss: 91.37876235044193
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.40707
Epoch 99 loss:91.37876235044193
MSE loss S1.2010696348566077
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:28.54574191945789
MSE loss S0.4049973524500307
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:171.55834828504902
MSE loss S3.1359714096919564
waveform batch: 2/2
Test loss - extrapolation:109.5162261740982
MSE loss S2.1718565600852284
Epoch 99 mean train loss:4.266836075403483
Epoch 99 mean test loss - interpolation:4.757623653242981
Epoch 99 mean test loss - extrapolation:23.422881204928938
Start training epoch 100
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-102.40707
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.471693170905428
Iteration: 2 || Loss: 7.470611834385425
Iteration: 3 || Loss: 7.469564422012774
Iteration: 4 || Loss: 7.468508087145625
Iteration: 5 || Loss: 7.467498160973542
Iteration: 6 || Loss: 7.467498160973542
saving ADAM checkpoint...
Sum of params:-102.40705
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.467498160973542
Iteration: 2 || Loss: 7.428007728901823
Iteration: 3 || Loss: 7.291923016462384
Iteration: 4 || Loss: 7.283122588237983
Iteration: 5 || Loss: 6.818799185787953
Iteration: 6 || Loss: 6.663713586047937
Iteration: 7 || Loss: 6.629182808263966
Iteration: 8 || Loss: 6.590123370376653
Iteration: 9 || Loss: 6.5712318467990665
Iteration: 10 || Loss: 6.544484615957251
Iteration: 11 || Loss: 6.498777094430238
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.42259
Epoch 100 loss:6.498777094430238
MSE loss S0.2490197591661693
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-102.42259
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.467439050599094
Iteration: 2 || Loss: 26.467066350910702
Iteration: 3 || Loss: 26.46663941557492
Iteration: 4 || Loss: 26.466272747691054
Iteration: 5 || Loss: 26.46590304636229
Iteration: 6 || Loss: 26.46590304636229
saving ADAM checkpoint...
Sum of params:-102.4225
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.46590304636229
Iteration: 2 || Loss: 26.46263040081354
Iteration: 3 || Loss: 26.445664184315017
Iteration: 4 || Loss: 26.36106155728408
Iteration: 5 || Loss: 25.953504087673952
Iteration: 6 || Loss: 25.884508719313136
Iteration: 7 || Loss: 25.81883707045924
Iteration: 8 || Loss: 25.735584724828527
Iteration: 9 || Loss: 25.69647236601366
Iteration: 10 || Loss: 25.6043913342809
Iteration: 11 || Loss: 25.54863248011244
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-101.88995
Epoch 100 loss:25.54863248011244
MSE loss S0.46694773596665246
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-101.88995
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 92.31851868861848
Iteration: 2 || Loss: 92.31799004981502
Iteration: 3 || Loss: 92.31744147451934
Iteration: 4 || Loss: 92.31698046274924
Iteration: 5 || Loss: 92.31646607169783
Iteration: 6 || Loss: 92.31646607169783
saving ADAM checkpoint...
Sum of params:-101.88992
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 92.31646607169783
Iteration: 2 || Loss: 92.31002946062374
Iteration: 3 || Loss: 92.28308243335935
Iteration: 4 || Loss: 92.18387829647476
Iteration: 5 || Loss: 91.87539717965747
Iteration: 6 || Loss: 91.40261512703331
Iteration: 7 || Loss: 91.23409759902748
Iteration: 8 || Loss: 91.09084116059948
Iteration: 9 || Loss: 90.87134863006574
Iteration: 10 || Loss: 90.51073216192346
Iteration: 11 || Loss: 90.26742785272297
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.52925
Epoch 100 loss:90.26742785272297
MSE loss S1.1866662972180477
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:28.290567229651167
MSE loss S0.3948869338884818
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:169.2504039795935
MSE loss S3.084963667488484
waveform batch: 2/2
Test loss - extrapolation:107.3374224798076
MSE loss S2.1408892107580684
Epoch 100 mean train loss:4.217753014733298
Epoch 100 mean test loss - interpolation:4.715094538275195
Epoch 100 mean test loss - extrapolation:23.04898553828342
Start training epoch 101
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-102.52925
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.3326995780475075
Iteration: 2 || Loss: 7.3320819233167205
Iteration: 3 || Loss: 7.331519160667322
Iteration: 4 || Loss: 7.3309207426345955
Iteration: 5 || Loss: 7.3303940997260835
Iteration: 6 || Loss: 7.3303940997260835
saving ADAM checkpoint...
Sum of params:-102.52925
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.3303940997260835
Iteration: 2 || Loss: 7.323658035857924
Iteration: 3 || Loss: 7.225361799216003
Iteration: 4 || Loss: 7.2143877871837
Iteration: 5 || Loss: 6.755297629787949
Iteration: 6 || Loss: 6.61749272563679
Iteration: 7 || Loss: 6.582180781339491
Iteration: 8 || Loss: 6.542391007687075
Iteration: 9 || Loss: 6.518795445209975
Iteration: 10 || Loss: 6.502563225522174
Iteration: 11 || Loss: 6.429568369965856
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.54427
Epoch 101 loss:6.429568369965856
MSE loss S0.2314584412009329
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-102.54427
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.802812273887834
Iteration: 2 || Loss: 26.801711557475826
Iteration: 3 || Loss: 26.800674174528105
Iteration: 4 || Loss: 26.799583418987954
Iteration: 5 || Loss: 26.798569611082847
Iteration: 6 || Loss: 26.798569611082847
saving ADAM checkpoint...
Sum of params:-102.54421
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.798569611082847
Iteration: 2 || Loss: 26.75016350892626
Iteration: 3 || Loss: 26.70269909519395
Iteration: 4 || Loss: 26.519976267882576
Iteration: 5 || Loss: 25.87265259659745
Iteration: 6 || Loss: 25.780171881122335
Iteration: 7 || Loss: 25.739857447546456
Iteration: 8 || Loss: 25.624074650593993
Iteration: 9 || Loss: 25.58893346519282
Iteration: 10 || Loss: 25.56966576957577
Iteration: 11 || Loss: 25.53852147311292
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.34159
Epoch 101 loss:25.53852147311292
MSE loss S0.47382549267692414
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-102.34159
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 90.56582275426038
Iteration: 2 || Loss: 90.56505942691167
Iteration: 3 || Loss: 90.56431163271314
Iteration: 4 || Loss: 90.56351917757851
Iteration: 5 || Loss: 90.56275875760227
Iteration: 6 || Loss: 90.56275875760227
saving ADAM checkpoint...
Sum of params:-102.341515
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 90.56275875760227
Iteration: 2 || Loss: 90.5484976386978
Iteration: 3 || Loss: 90.5342274290244
Iteration: 4 || Loss: 90.41555422686736
Iteration: 5 || Loss: 90.21473267786021
Iteration: 6 || Loss: 90.0403074650527
Iteration: 7 || Loss: 89.8932836831849
Iteration: 8 || Loss: 89.81193116618094
Iteration: 9 || Loss: 89.7007910121731
Iteration: 10 || Loss: 89.15597929463156
Iteration: 11 || Loss: 88.2411552579599
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.967636
Epoch 101 loss:88.2411552579599
MSE loss S1.2774499419768328
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:28.744563603995307
MSE loss S0.5268237921807486
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:163.01367636538515
MSE loss S3.0703655210409484
waveform batch: 2/2
Test loss - extrapolation:102.2637447905849
MSE loss S2.085123807141785
Epoch 101 mean train loss:4.1451463827944375
Epoch 101 mean test loss - interpolation:4.790760600665885
Epoch 101 mean test loss - extrapolation:22.106451762997505
Start training epoch 102
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-102.967636
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.288089033371243
Iteration: 2 || Loss: 8.286644940540702
Iteration: 3 || Loss: 8.285228494632792
Iteration: 4 || Loss: 8.28378210446533
Iteration: 5 || Loss: 8.282340194833191
Iteration: 6 || Loss: 8.282340194833191
saving ADAM checkpoint...
Sum of params:-102.96765
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.282340194833191
Iteration: 2 || Loss: 8.196696879740525
Iteration: 3 || Loss: 8.173860193857488
Iteration: 4 || Loss: 7.966622037320971
Iteration: 5 || Loss: 7.254979534803842
Iteration: 6 || Loss: 6.929308605066348
Iteration: 7 || Loss: 6.836754057462125
Iteration: 8 || Loss: 6.801694908622852
Iteration: 9 || Loss: 6.739973424661299
Iteration: 10 || Loss: 6.661133519899477
Iteration: 11 || Loss: 6.6406807849356815
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.98835
Epoch 102 loss:6.6406807849356815
MSE loss S0.2650307222460301
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-102.98835
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.649895659793543
Iteration: 2 || Loss: 26.64948524018332
Iteration: 3 || Loss: 26.64889162960472
Iteration: 4 || Loss: 26.64844171130882
Iteration: 5 || Loss: 26.64796281246576
Iteration: 6 || Loss: 26.64796281246576
saving ADAM checkpoint...
Sum of params:-102.98827
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.64796281246576
Iteration: 2 || Loss: 26.64250307551863
Iteration: 3 || Loss: 26.622550405515806
Iteration: 4 || Loss: 26.521320466334302
Iteration: 5 || Loss: 25.99663434440621
Iteration: 6 || Loss: 25.888933216483746
Iteration: 7 || Loss: 25.771250902161217
Iteration: 8 || Loss: 25.66938485768241
Iteration: 9 || Loss: 25.625629443200918
Iteration: 10 || Loss: 25.546406222254358
Iteration: 11 || Loss: 25.479468933341202
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.65961
Epoch 102 loss:25.479468933341202
MSE loss S0.472158976502152
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-102.65961
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 88.63199243648273
Iteration: 2 || Loss: 88.63159691669698
Iteration: 3 || Loss: 88.63117900449268
Iteration: 4 || Loss: 88.63082170085374
Iteration: 5 || Loss: 88.63042308164142
Iteration: 6 || Loss: 88.63042308164142
saving ADAM checkpoint...
Sum of params:-102.659485
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 88.63042308164142
Iteration: 2 || Loss: 88.61134270441775
Iteration: 3 || Loss: 88.58479496846027
Iteration: 4 || Loss: 88.08264431923621
Iteration: 5 || Loss: 87.76334984088332
Iteration: 6 || Loss: 87.48156330552575
Iteration: 7 || Loss: 87.20856714685566
Iteration: 8 || Loss: 87.11959994789007
Iteration: 9 || Loss: 87.00976211650432
Iteration: 10 || Loss: 86.73900439695167
Iteration: 11 || Loss: 86.46515827349398
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.01777
Epoch 102 loss:86.46515827349398
MSE loss S1.1165036624475175
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:27.872666795073965
MSE loss S0.39772109676189116
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:160.57149483948632
MSE loss S2.9145008858095114
waveform batch: 2/2
Test loss - extrapolation:100.11040795414762
MSE loss S2.008596428752962
Epoch 102 mean train loss:4.089148551440375
Epoch 102 mean test loss - interpolation:4.645444465845661
Epoch 102 mean test loss - extrapolation:21.723491899469494
Start training epoch 103
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.01777
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.542455594258651
Iteration: 2 || Loss: 7.540055555322437
Iteration: 3 || Loss: 7.5376592960360576
Iteration: 4 || Loss: 7.5353263297446205
Iteration: 5 || Loss: 7.532993399288031
Iteration: 6 || Loss: 7.532993399288031
saving ADAM checkpoint...
Sum of params:-103.017815
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.532993399288031
Iteration: 2 || Loss: 7.298765720163466
Iteration: 3 || Loss: 7.188205373673033
Iteration: 4 || Loss: 7.15700629701012
Iteration: 5 || Loss: 6.763332277217163
Iteration: 6 || Loss: 6.661869499802469
Iteration: 7 || Loss: 6.629578033936656
Iteration: 8 || Loss: 6.60630391220937
Iteration: 9 || Loss: 6.5723197858383955
Iteration: 10 || Loss: 6.561055487213089
Iteration: 11 || Loss: 6.5226371588592755
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.02478
Epoch 103 loss:6.5226371588592755
MSE loss S0.2516763316800056
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.02478
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.15270491481371
Iteration: 2 || Loss: 26.152271698376776
Iteration: 3 || Loss: 26.151653634652195
Iteration: 4 || Loss: 26.151125155862353
Iteration: 5 || Loss: 26.150603303870962
Iteration: 6 || Loss: 26.150603303870962
saving ADAM checkpoint...
Sum of params:-103.0247
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.150603303870962
Iteration: 2 || Loss: 26.14383544795308
Iteration: 3 || Loss: 26.133068062329297
Iteration: 4 || Loss: 25.938006179470207
Iteration: 5 || Loss: 25.657894922453558
Iteration: 6 || Loss: 25.58244951909814
Iteration: 7 || Loss: 25.521559074377716
Iteration: 8 || Loss: 25.444642600012997
Iteration: 9 || Loss: 25.413980436633885
Iteration: 10 || Loss: 25.3578528409387
Iteration: 11 || Loss: 25.30919757466731
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.69341
Epoch 103 loss:25.30919757466731
MSE loss S0.46263079350971426
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-102.69341
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 87.47088556424805
Iteration: 2 || Loss: 87.47044125327228
Iteration: 3 || Loss: 87.47003149053847
Iteration: 4 || Loss: 87.46958731089825
Iteration: 5 || Loss: 87.46910239065676
Iteration: 6 || Loss: 87.46910239065676
saving ADAM checkpoint...
Sum of params:-102.6933
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 87.46910239065676
Iteration: 2 || Loss: 87.45834628896225
Iteration: 3 || Loss: 87.42280599097614
Iteration: 4 || Loss: 87.15909989408665
Iteration: 5 || Loss: 86.77977811844008
Iteration: 6 || Loss: 86.47808872925077
Iteration: 7 || Loss: 86.23913211544648
Iteration: 8 || Loss: 86.16351231433487
Iteration: 9 || Loss: 86.04506837652403
Iteration: 10 || Loss: 85.86494574072076
Iteration: 11 || Loss: 85.6335304627595
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.02828
Epoch 103 loss:85.6335304627595
MSE loss S1.1179040100527675
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:27.62395130209331
MSE loss S0.3827142685578083
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:159.65763061683683
MSE loss S2.9122611491244537
waveform batch: 2/2
Test loss - extrapolation:98.9586539243299
MSE loss S2.0159874503821973
Epoch 103 mean train loss:4.050529834354693
Epoch 103 mean test loss - interpolation:4.603991883682219
Epoch 103 mean test loss - extrapolation:21.551357045097223
Start training epoch 104
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.02828
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.402385046083141
Iteration: 2 || Loss: 7.399742204771206
Iteration: 3 || Loss: 7.397184189433928
Iteration: 4 || Loss: 7.394670109026347
Iteration: 5 || Loss: 7.3920562437685735
Iteration: 6 || Loss: 7.3920562437685735
saving ADAM checkpoint...
Sum of params:-103.0283
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.3920562437685735
Iteration: 2 || Loss: 7.105740530880825
Iteration: 3 || Loss: 7.06552408186841
Iteration: 4 || Loss: 7.03941148969714
Iteration: 5 || Loss: 6.706847662547724
Iteration: 6 || Loss: 6.595630703536332
Iteration: 7 || Loss: 6.573924724489206
Iteration: 8 || Loss: 6.545878066614998
Iteration: 9 || Loss: 6.527395062960448
Iteration: 10 || Loss: 6.510414626279207
Iteration: 11 || Loss: 6.483364081892266
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.03404
Epoch 104 loss:6.483364081892266
MSE loss S0.24674512579093133
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.03404
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.034802063548995
Iteration: 2 || Loss: 26.03429110897047
Iteration: 3 || Loss: 26.03368010534836
Iteration: 4 || Loss: 26.03313838986943
Iteration: 5 || Loss: 26.032596355520887
Iteration: 6 || Loss: 26.032596355520887
saving ADAM checkpoint...
Sum of params:-103.03394
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.032596355520887
Iteration: 2 || Loss: 26.02635070278334
Iteration: 3 || Loss: 26.01406457799412
Iteration: 4 || Loss: 25.89883489912378
Iteration: 5 || Loss: 25.51084835953584
Iteration: 6 || Loss: 25.429336626661332
Iteration: 7 || Loss: 25.37730193074069
Iteration: 8 || Loss: 25.30711244865282
Iteration: 9 || Loss: 25.280911691376524
Iteration: 10 || Loss: 25.239970694860748
Iteration: 11 || Loss: 25.200600745806447
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.78171
Epoch 104 loss:25.200600745806447
MSE loss S0.4556310025400928
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-102.78171
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 86.58534280466245
Iteration: 2 || Loss: 86.58489091642585
Iteration: 3 || Loss: 86.58445342456307
Iteration: 4 || Loss: 86.58393901943646
Iteration: 5 || Loss: 86.58349726977202
Iteration: 6 || Loss: 86.58349726977202
saving ADAM checkpoint...
Sum of params:-102.781586
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 86.58349726977202
Iteration: 2 || Loss: 86.573746584487
Iteration: 3 || Loss: 86.54213540215089
Iteration: 4 || Loss: 86.27346834726559
Iteration: 5 || Loss: 85.94793349656571
Iteration: 6 || Loss: 85.67362620348406
Iteration: 7 || Loss: 85.45220027593236
Iteration: 8 || Loss: 85.38645997275114
Iteration: 9 || Loss: 85.28112870238665
Iteration: 10 || Loss: 85.10103353610997
Iteration: 11 || Loss: 84.84078213385726
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.12669
Epoch 104 loss:84.84078213385726
MSE loss S1.1150961919764484
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:27.478309092512045
MSE loss S0.383698577861381
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:158.09066068690845
MSE loss S2.8939637774254914
waveform batch: 2/2
Test loss - extrapolation:97.39346008769715
MSE loss S1.9958858327633098
Epoch 104 mean train loss:4.018094722812275
Epoch 104 mean test loss - interpolation:4.579718182085341
Epoch 104 mean test loss - extrapolation:21.2903433978838
Start training epoch 105
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.12669
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.395079926710605
Iteration: 2 || Loss: 7.392369290334838
Iteration: 3 || Loss: 7.389805913419558
Iteration: 4 || Loss: 7.387122270479162
Iteration: 5 || Loss: 7.384511793927062
Iteration: 6 || Loss: 7.384511793927062
saving ADAM checkpoint...
Sum of params:-103.1267
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.384511793927062
Iteration: 2 || Loss: 7.086664187969478
Iteration: 3 || Loss: 7.033120619208638
Iteration: 4 || Loss: 7.0118907218781406
Iteration: 5 || Loss: 6.6853552043571245
Iteration: 6 || Loss: 6.582755163599169
Iteration: 7 || Loss: 6.558942255772684
Iteration: 8 || Loss: 6.529009314275802
Iteration: 9 || Loss: 6.51496500185946
Iteration: 10 || Loss: 6.495837145711503
Iteration: 11 || Loss: 6.469482450785719
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.13159
Epoch 105 loss:6.469482450785719
MSE loss S0.24575792148017628
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.13159
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.90949790546444
Iteration: 2 || Loss: 25.9088355588066
Iteration: 3 || Loss: 25.908193479258806
Iteration: 4 || Loss: 25.907565347801583
Iteration: 5 || Loss: 25.90693925032984
Iteration: 6 || Loss: 25.90693925032984
saving ADAM checkpoint...
Sum of params:-103.13152
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.90693925032984
Iteration: 2 || Loss: 25.89918558141157
Iteration: 3 || Loss: 25.887443454429356
Iteration: 4 || Loss: 25.7967865194436
Iteration: 5 || Loss: 25.42385881161823
Iteration: 6 || Loss: 25.349384050742675
Iteration: 7 || Loss: 25.29268084521796
Iteration: 8 || Loss: 25.223111229768353
Iteration: 9 || Loss: 25.200230516049803
Iteration: 10 || Loss: 25.161132722623396
Iteration: 11 || Loss: 25.12833603250226
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.91334
Epoch 105 loss:25.12833603250226
MSE loss S0.4529444681544843
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-102.91334
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 85.69643725900664
Iteration: 2 || Loss: 85.69586768194141
Iteration: 3 || Loss: 85.69530495553934
Iteration: 4 || Loss: 85.69467073155877
Iteration: 5 || Loss: 85.6941234764845
Iteration: 6 || Loss: 85.6941234764845
saving ADAM checkpoint...
Sum of params:-102.913315
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 85.6941234764845
Iteration: 2 || Loss: 85.68503413518225
Iteration: 3 || Loss: 85.65109477428277
Iteration: 4 || Loss: 85.40494384935339
Iteration: 5 || Loss: 85.07780070309282
Iteration: 6 || Loss: 84.82219763650244
Iteration: 7 || Loss: 84.60890910373142
Iteration: 8 || Loss: 84.55242604638588
Iteration: 9 || Loss: 84.44635628008342
Iteration: 10 || Loss: 84.28378609226078
Iteration: 11 || Loss: 84.08042122360786
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.19286
Epoch 105 loss:84.08042122360786
MSE loss S1.1096604099654255
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:27.308167269650085
MSE loss S0.3809951571148623
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:156.74395780605093
MSE loss S2.870635170874855
waveform batch: 2/2
Test loss - extrapolation:96.03847986425859
MSE loss S1.982636304464492
Epoch 105 mean train loss:3.988904817479167
Epoch 105 mean test loss - interpolation:4.551361211608348
Epoch 105 mean test loss - extrapolation:21.06520313919246
Start training epoch 106
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.19286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.315165139069162
Iteration: 2 || Loss: 7.3125112604773195
Iteration: 3 || Loss: 7.309914991269807
Iteration: 4 || Loss: 7.30730379591959
Iteration: 5 || Loss: 7.304732089592016
Iteration: 6 || Loss: 7.304732089592016
saving ADAM checkpoint...
Sum of params:-103.19287
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.304732089592016
Iteration: 2 || Loss: 7.009574649540097
Iteration: 3 || Loss: 6.988229274010976
Iteration: 4 || Loss: 6.967490305420905
Iteration: 5 || Loss: 6.666961741374408
Iteration: 6 || Loss: 6.5631013244860235
Iteration: 7 || Loss: 6.539711588786517
Iteration: 8 || Loss: 6.501804016339937
Iteration: 9 || Loss: 6.4925877181461775
Iteration: 10 || Loss: 6.4655836585185575
Iteration: 11 || Loss: 6.451170512099175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.19546
Epoch 106 loss:6.451170512099175
MSE loss S0.24386049805071708
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.19546
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.84568865180156
Iteration: 2 || Loss: 25.845022746963707
Iteration: 3 || Loss: 25.84434604135554
Iteration: 4 || Loss: 25.843727980902443
Iteration: 5 || Loss: 25.84311898065342
Iteration: 6 || Loss: 25.84311898065342
saving ADAM checkpoint...
Sum of params:-103.19538
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.84311898065342
Iteration: 2 || Loss: 25.835586714433244
Iteration: 3 || Loss: 25.821302665633286
Iteration: 4 || Loss: 25.735508102982617
Iteration: 5 || Loss: 25.329083969649947
Iteration: 6 || Loss: 25.2505921773267
Iteration: 7 || Loss: 25.201894149067915
Iteration: 8 || Loss: 25.137535598491045
Iteration: 9 || Loss: 25.119135812936044
Iteration: 10 || Loss: 25.08471981900837
Iteration: 11 || Loss: 25.05462604479839
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-102.99673
Epoch 106 loss:25.05462604479839
MSE loss S0.45056115574390937
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-102.99673
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 84.9504429760559
Iteration: 2 || Loss: 84.94982614619737
Iteration: 3 || Loss: 84.94918384129106
Iteration: 4 || Loss: 84.94855669412945
Iteration: 5 || Loss: 84.94794751980338
Iteration: 6 || Loss: 84.94794751980338
saving ADAM checkpoint...
Sum of params:-102.99671
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 84.94794751980338
Iteration: 2 || Loss: 84.93809617051869
Iteration: 3 || Loss: 84.90381614136227
Iteration: 4 || Loss: 84.62668517996129
Iteration: 5 || Loss: 84.32701478891238
Iteration: 6 || Loss: 84.09031491264291
Iteration: 7 || Loss: 83.87626964993315
Iteration: 8 || Loss: 83.8237218486863
Iteration: 9 || Loss: 83.7197287850616
Iteration: 10 || Loss: 83.52677591747745
Iteration: 11 || Loss: 83.34490378770712
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.2903
Epoch 106 loss:83.34490378770712
MSE loss S1.0895120084701544
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:27.180277803104605
MSE loss S0.3781239833199052
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:154.9087366046874
MSE loss S2.8287308826849085
waveform batch: 2/2
Test loss - extrapolation:94.16309342334266
MSE loss S1.9416722225059888
Epoch 106 mean train loss:3.9603689774001616
Epoch 106 mean test loss - interpolation:4.5300463005174345
Epoch 106 mean test loss - extrapolation:20.75598583566917
Start training epoch 107
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.2903
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.165351355112935
Iteration: 2 || Loss: 7.163006002286705
Iteration: 3 || Loss: 7.160738412382179
Iteration: 4 || Loss: 7.158462908855299
Iteration: 5 || Loss: 7.156122446683014
Iteration: 6 || Loss: 7.156122446683014
saving ADAM checkpoint...
Sum of params:-103.2903
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.156122446683014
Iteration: 2 || Loss: 6.932786707911646
Iteration: 3 || Loss: 6.894085562846637
Iteration: 4 || Loss: 6.862657765482423
Iteration: 5 || Loss: 6.624931272714587
Iteration: 6 || Loss: 6.530209780009894
Iteration: 7 || Loss: 6.516272394253138
Iteration: 8 || Loss: 6.489521178042546
Iteration: 9 || Loss: 6.470922933114405
Iteration: 10 || Loss: 6.460204009158558
Iteration: 11 || Loss: 6.436852135849131
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.29181
Epoch 107 loss:6.436852135849131
MSE loss S0.24529603507189293
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.29181
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.67830694435508
Iteration: 2 || Loss: 25.67771395000154
Iteration: 3 || Loss: 25.677051411508277
Iteration: 4 || Loss: 25.676424243063817
Iteration: 5 || Loss: 25.675818600684348
Iteration: 6 || Loss: 25.675818600684348
saving ADAM checkpoint...
Sum of params:-103.29172
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.675818600684348
Iteration: 2 || Loss: 25.667859069133122
Iteration: 3 || Loss: 25.655113135609888
Iteration: 4 || Loss: 25.59623837965781
Iteration: 5 || Loss: 25.241027441040686
Iteration: 6 || Loss: 25.180527844827736
Iteration: 7 || Loss: 25.1360359318335
Iteration: 8 || Loss: 25.07393663420557
Iteration: 9 || Loss: 25.054099074342783
Iteration: 10 || Loss: 25.023125980064002
Iteration: 11 || Loss: 24.99061053784029
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.096344
Epoch 107 loss:24.99061053784029
MSE loss S0.4503049504738818
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.096344
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 84.20727089840814
Iteration: 2 || Loss: 84.20660062064405
Iteration: 3 || Loss: 84.20596803268188
Iteration: 4 || Loss: 84.20527315589047
Iteration: 5 || Loss: 84.20467243725997
Iteration: 6 || Loss: 84.20467243725997
saving ADAM checkpoint...
Sum of params:-103.09634
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 84.20467243725997
Iteration: 2 || Loss: 84.19441693143824
Iteration: 3 || Loss: 84.15758194478883
Iteration: 4 || Loss: 83.8854569385914
Iteration: 5 || Loss: 83.57090734932088
Iteration: 6 || Loss: 83.35139178323335
Iteration: 7 || Loss: 83.14031967361866
Iteration: 8 || Loss: 83.09016701355513
Iteration: 9 || Loss: 82.99137322517933
Iteration: 10 || Loss: 82.80639168985732
Iteration: 11 || Loss: 82.61213892630357
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.38255
Epoch 107 loss:82.61213892630357
MSE loss S1.0575051384563368
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:27.110635597173825
MSE loss S0.3688606672769186
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:153.10195896216294
MSE loss S2.7743288320456188
waveform batch: 2/2
Test loss - extrapolation:92.15299698109338
MSE loss S1.8708900353305722
Epoch 107 mean train loss:3.932400055172172
Epoch 107 mean test loss - interpolation:4.518439266195638
Epoch 107 mean test loss - extrapolation:20.43791299527136
Start training epoch 108
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.38255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.208117256413974
Iteration: 2 || Loss: 7.205127316068582
Iteration: 3 || Loss: 7.20227720080734
Iteration: 4 || Loss: 7.199375528646922
Iteration: 5 || Loss: 7.196545946975287
Iteration: 6 || Loss: 7.196545946975287
saving ADAM checkpoint...
Sum of params:-103.382545
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.196545946975287
Iteration: 2 || Loss: 6.834620127573444
Iteration: 3 || Loss: 6.812142210662624
Iteration: 4 || Loss: 6.7878266652562225
Iteration: 5 || Loss: 6.581650665096618
Iteration: 6 || Loss: 6.528202439642183
Iteration: 7 || Loss: 6.495869939654265
Iteration: 8 || Loss: 6.4715262028763725
Iteration: 9 || Loss: 6.463365149235188
Iteration: 10 || Loss: 6.44375264148799
Iteration: 11 || Loss: 6.425420517236729
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.38538
Epoch 108 loss:6.425420517236729
MSE loss S0.2436951699149921
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.38538
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.638146300892338
Iteration: 2 || Loss: 25.637443033520668
Iteration: 3 || Loss: 25.636707318972572
Iteration: 4 || Loss: 25.636037548791045
Iteration: 5 || Loss: 25.63536540530737
Iteration: 6 || Loss: 25.63536540530737
saving ADAM checkpoint...
Sum of params:-103.385284
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.63536540530737
Iteration: 2 || Loss: 25.626609760609764
Iteration: 3 || Loss: 25.614092494014223
Iteration: 4 || Loss: 25.549508906147867
Iteration: 5 || Loss: 25.17710712888425
Iteration: 6 || Loss: 25.115575321850752
Iteration: 7 || Loss: 25.073314261179753
Iteration: 8 || Loss: 25.013405661371017
Iteration: 9 || Loss: 24.993814124435488
Iteration: 10 || Loss: 24.958674201460425
Iteration: 11 || Loss: 24.933710789318944
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.200386
Epoch 108 loss:24.933710789318944
MSE loss S0.4506189317962406
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.200386
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 83.48097920228288
Iteration: 2 || Loss: 83.48017853118412
Iteration: 3 || Loss: 83.47941442248023
Iteration: 4 || Loss: 83.47873545853442
Iteration: 5 || Loss: 83.4779848739228
Iteration: 6 || Loss: 83.4779848739228
saving ADAM checkpoint...
Sum of params:-103.20039
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 83.4779848739228
Iteration: 2 || Loss: 83.46169059477894
Iteration: 3 || Loss: 83.4238860035309
Iteration: 4 || Loss: 83.11973488701152
Iteration: 5 || Loss: 82.83173008353756
Iteration: 6 || Loss: 82.62726518292897
Iteration: 7 || Loss: 82.41676231471658
Iteration: 8 || Loss: 82.36979661714228
Iteration: 9 || Loss: 82.28497162170089
Iteration: 10 || Loss: 82.19578709098145
Iteration: 11 || Loss: 82.03142777565162
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.39311
Epoch 108 loss:82.03142777565162
MSE loss S1.0909623015721963
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.896000474350345
MSE loss S0.37896872881640786
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:152.6214025903883
MSE loss S2.8001633055287245
waveform batch: 2/2
Test loss - extrapolation:91.90551285897807
MSE loss S1.9127977929003297
Epoch 108 mean train loss:3.910019278696803
Epoch 108 mean test loss - interpolation:4.482666745725058
Epoch 108 mean test loss - extrapolation:20.377242954113864
Start training epoch 109
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.39311
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.219968365239285
Iteration: 2 || Loss: 7.217259780741126
Iteration: 3 || Loss: 7.2145796963945985
Iteration: 4 || Loss: 7.211854446644972
Iteration: 5 || Loss: 7.209183330465166
Iteration: 6 || Loss: 7.209183330465166
saving ADAM checkpoint...
Sum of params:-103.39311
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.209183330465166
Iteration: 2 || Loss: 6.896381079506687
Iteration: 3 || Loss: 6.888743170616695
Iteration: 4 || Loss: 6.855184664711856
Iteration: 5 || Loss: 6.591949346092534
Iteration: 6 || Loss: 6.51035198650032
Iteration: 7 || Loss: 6.48615603803452
Iteration: 8 || Loss: 6.462689495922317
Iteration: 9 || Loss: 6.451219610761367
Iteration: 10 || Loss: 6.433685078208042
Iteration: 11 || Loss: 6.4109051913142565
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.39591
Epoch 109 loss:6.4109051913142565
MSE loss S0.24474244981466972
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.39591
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.552954530411892
Iteration: 2 || Loss: 25.55222666287823
Iteration: 3 || Loss: 25.551508157760967
Iteration: 4 || Loss: 25.550764921094213
Iteration: 5 || Loss: 25.550150381957472
Iteration: 6 || Loss: 25.550150381957472
saving ADAM checkpoint...
Sum of params:-103.395805
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.550150381957472
Iteration: 2 || Loss: 25.540787854179207
Iteration: 3 || Loss: 25.52618444479694
Iteration: 4 || Loss: 25.470596415391228
Iteration: 5 || Loss: 25.090809079889535
Iteration: 6 || Loss: 25.030341096142628
Iteration: 7 || Loss: 24.991404769724724
Iteration: 8 || Loss: 24.939060397914034
Iteration: 9 || Loss: 24.922109633158513
Iteration: 10 || Loss: 24.89400866136936
Iteration: 11 || Loss: 24.872398573546114
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.24056
Epoch 109 loss:24.872398573546114
MSE loss S0.4472230842368602
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.24056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 82.91177845558772
Iteration: 2 || Loss: 82.91108265368487
Iteration: 3 || Loss: 82.91039048330587
Iteration: 4 || Loss: 82.90967193222876
Iteration: 5 || Loss: 82.90896435384582
Iteration: 6 || Loss: 82.90896435384582
saving ADAM checkpoint...
Sum of params:-103.240555
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 82.90896435384582
Iteration: 2 || Loss: 82.89867646334613
Iteration: 3 || Loss: 82.86128008924955
Iteration: 4 || Loss: 82.56579574880806
Iteration: 5 || Loss: 82.29720818998497
Iteration: 6 || Loss: 82.10697526848426
Iteration: 7 || Loss: 81.90547865211705
Iteration: 8 || Loss: 81.86143217681548
Iteration: 9 || Loss: 81.77975590227722
Iteration: 10 || Loss: 81.69163632907541
Iteration: 11 || Loss: 81.52894804032476
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.43877
Epoch 109 loss:81.52894804032476
MSE loss S1.0692743254995931
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.829060478326774
MSE loss S0.3732109457783317
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:151.6118986298852
MSE loss S2.7701548042244233
waveform batch: 2/2
Test loss - extrapolation:90.6120518438952
MSE loss S1.872500474083683
Epoch 109 mean train loss:3.8900776484546595
Epoch 109 mean test loss - interpolation:4.471510079721129
Epoch 109 mean test loss - extrapolation:20.185329206148367
Start training epoch 110
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.43877
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.2177166872959715
Iteration: 2 || Loss: 7.214827751827025
Iteration: 3 || Loss: 7.211918310878907
Iteration: 4 || Loss: 7.209002748603967
Iteration: 5 || Loss: 7.206087320123352
Iteration: 6 || Loss: 7.206087320123352
saving ADAM checkpoint...
Sum of params:-103.43877
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.206087320123352
Iteration: 2 || Loss: 6.845180877151853
Iteration: 3 || Loss: 6.8382297325033266
Iteration: 4 || Loss: 6.804240732116971
Iteration: 5 || Loss: 6.566805094267455
Iteration: 6 || Loss: 6.499472442874806
Iteration: 7 || Loss: 6.466887562566747
Iteration: 8 || Loss: 6.44445937270681
Iteration: 9 || Loss: 6.433476898431761
Iteration: 10 || Loss: 6.416196559818127
Iteration: 11 || Loss: 6.400112361523811
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.443375
Epoch 110 loss:6.400112361523811
MSE loss S0.24401998133149855
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.443375
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.4943909540504
Iteration: 2 || Loss: 25.493630958368758
Iteration: 3 || Loss: 25.49274188576957
Iteration: 4 || Loss: 25.492000339641773
Iteration: 5 || Loss: 25.49123005611228
Iteration: 6 || Loss: 25.49123005611228
saving ADAM checkpoint...
Sum of params:-103.443275
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.49123005611228
Iteration: 2 || Loss: 25.480190821319173
Iteration: 3 || Loss: 25.463970326929555
Iteration: 4 || Loss: 25.410741361023213
Iteration: 5 || Loss: 25.034842579464975
Iteration: 6 || Loss: 24.97318075288887
Iteration: 7 || Loss: 24.935063663569917
Iteration: 8 || Loss: 24.886898991623337
Iteration: 9 || Loss: 24.869953228543295
Iteration: 10 || Loss: 24.838738016625218
Iteration: 11 || Loss: 24.815903845277038
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.283104
Epoch 110 loss:24.815903845277038
MSE loss S0.44964212657635033
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.283104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 82.37009314711901
Iteration: 2 || Loss: 82.36946519705083
Iteration: 3 || Loss: 82.36881221703314
Iteration: 4 || Loss: 82.36816787002878
Iteration: 5 || Loss: 82.36750617529144
Iteration: 6 || Loss: 82.36750617529144
saving ADAM checkpoint...
Sum of params:-103.2831
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 82.36750617529144
Iteration: 2 || Loss: 82.35886919175546
Iteration: 3 || Loss: 82.3263712114721
Iteration: 4 || Loss: 82.06005765026896
Iteration: 5 || Loss: 81.8016298840082
Iteration: 6 || Loss: 81.6152715355944
Iteration: 7 || Loss: 81.41903330918151
Iteration: 8 || Loss: 81.37319163671728
Iteration: 9 || Loss: 81.2932596587914
Iteration: 10 || Loss: 81.14999467589087
Iteration: 11 || Loss: 81.021118204915
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.50867
Epoch 110 loss:81.021118204915
MSE loss S1.0555386319546824
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.751865967483702
MSE loss S0.36798926914120567
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:150.17367259261428
MSE loss S2.7402356109163355
waveform batch: 2/2
Test loss - extrapolation:89.12725842676107
MSE loss S1.836589603837294
Epoch 110 mean train loss:3.870246014197098
Epoch 110 mean test loss - interpolation:4.458644327913951
Epoch 110 mean test loss - extrapolation:19.94174425161461
Start training epoch 111
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.50867
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.13784760441171
Iteration: 2 || Loss: 7.134923976870378
Iteration: 3 || Loss: 7.132014161922365
Iteration: 4 || Loss: 7.129110413269899
Iteration: 5 || Loss: 7.126246891102396
Iteration: 6 || Loss: 7.126246891102396
saving ADAM checkpoint...
Sum of params:-103.50865
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.126246891102396
Iteration: 2 || Loss: 6.765363707108304
Iteration: 3 || Loss: 6.757757948121261
Iteration: 4 || Loss: 6.737411247265468
Iteration: 5 || Loss: 6.536535043334056
Iteration: 6 || Loss: 6.481993788020137
Iteration: 7 || Loss: 6.4519017696069945
Iteration: 8 || Loss: 6.435889752141726
Iteration: 9 || Loss: 6.426055128485601
Iteration: 10 || Loss: 6.4082669866354
Iteration: 11 || Loss: 6.387762154476103
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.51254
Epoch 111 loss:6.387762154476103
MSE loss S0.24289935300526433
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.51254
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.41235316526021
Iteration: 2 || Loss: 25.411670959025184
Iteration: 3 || Loss: 25.41091695129927
Iteration: 4 || Loss: 25.410226255565735
Iteration: 5 || Loss: 25.409614830365335
Iteration: 6 || Loss: 25.409614830365335
saving ADAM checkpoint...
Sum of params:-103.51246
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.409614830365335
Iteration: 2 || Loss: 25.40014392579623
Iteration: 3 || Loss: 25.385855210549828
Iteration: 4 || Loss: 25.334978785137213
Iteration: 5 || Loss: 24.97776596951411
Iteration: 6 || Loss: 24.92428860627986
Iteration: 7 || Loss: 24.88716253446149
Iteration: 8 || Loss: 24.836474748351787
Iteration: 9 || Loss: 24.820674166693745
Iteration: 10 || Loss: 24.78913197876885
Iteration: 11 || Loss: 24.769370696671047
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.359375
Epoch 111 loss:24.769370696671047
MSE loss S0.4479650295449833
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.359375
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 81.88292563684782
Iteration: 2 || Loss: 81.88211386791734
Iteration: 3 || Loss: 81.88145765512722
Iteration: 4 || Loss: 81.88075490553827
Iteration: 5 || Loss: 81.88004977989542
Iteration: 6 || Loss: 81.88004977989542
saving ADAM checkpoint...
Sum of params:-103.359375
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 81.88004977989542
Iteration: 2 || Loss: 81.86944768291347
Iteration: 3 || Loss: 81.83104604153236
Iteration: 4 || Loss: 81.53344628252692
Iteration: 5 || Loss: 81.27373483230504
Iteration: 6 || Loss: 81.09939862777749
Iteration: 7 || Loss: 80.90302050397132
Iteration: 8 || Loss: 80.85969106464765
Iteration: 9 || Loss: 80.78818944925946
Iteration: 10 || Loss: 80.71667396952381
Iteration: 11 || Loss: 80.57247123642433
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.535675
Epoch 111 loss:80.57247123642433
MSE loss S1.0604585473272754
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.619096689531958
MSE loss S0.3717424230083265
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:149.49580198846562
MSE loss S2.7300969520668152
waveform batch: 2/2
Test loss - extrapolation:88.45727135291152
MSE loss S1.8368409249891398
Epoch 111 mean train loss:3.852744968536948
Epoch 111 mean test loss - interpolation:4.436516114921993
Epoch 111 mean test loss - extrapolation:19.829422778448095
Start training epoch 112
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.535675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.0709246084702375
Iteration: 2 || Loss: 7.06834296448561
Iteration: 3 || Loss: 7.065704275772958
Iteration: 4 || Loss: 7.063102951353178
Iteration: 5 || Loss: 7.060535860468841
Iteration: 6 || Loss: 7.060535860468841
saving ADAM checkpoint...
Sum of params:-103.53565
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.060535860468841
Iteration: 2 || Loss: 6.7692453917766215
Iteration: 3 || Loss: 6.761026203080967
Iteration: 4 || Loss: 6.718967753917164
Iteration: 5 || Loss: 6.5302220237530495
Iteration: 6 || Loss: 6.4595214276977755
Iteration: 7 || Loss: 6.440176385442524
Iteration: 8 || Loss: 6.425533611596646
Iteration: 9 || Loss: 6.406349493721534
Iteration: 10 || Loss: 6.393072930190206
Iteration: 11 || Loss: 6.372774476300138
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.540344
Epoch 112 loss:6.372774476300138
MSE loss S0.24318894527146384
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.540344
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.31966288618281
Iteration: 2 || Loss: 25.31892906708566
Iteration: 3 || Loss: 25.31826213228759
Iteration: 4 || Loss: 25.3175747307571
Iteration: 5 || Loss: 25.316922909656128
Iteration: 6 || Loss: 25.316922909656128
saving ADAM checkpoint...
Sum of params:-103.54027
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.316922909656128
Iteration: 2 || Loss: 25.308494377524113
Iteration: 3 || Loss: 25.29212999226899
Iteration: 4 || Loss: 25.24535895222301
Iteration: 5 || Loss: 24.906075247060766
Iteration: 6 || Loss: 24.85919484892474
Iteration: 7 || Loss: 24.825871989940072
Iteration: 8 || Loss: 24.7767730739926
Iteration: 9 || Loss: 24.763476467451877
Iteration: 10 || Loss: 24.73732005703759
Iteration: 11 || Loss: 24.717162027332854
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.39523
Epoch 112 loss:24.717162027332854
MSE loss S0.447135778382607
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.39523
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 81.45105750103924
Iteration: 2 || Loss: 81.45033512664503
Iteration: 3 || Loss: 81.44969512551155
Iteration: 4 || Loss: 81.44898952794037
Iteration: 5 || Loss: 81.4483421583495
Iteration: 6 || Loss: 81.4483421583495
saving ADAM checkpoint...
Sum of params:-103.39529
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 81.4483421583495
Iteration: 2 || Loss: 81.43657013543229
Iteration: 3 || Loss: 81.39711304034572
Iteration: 4 || Loss: 81.09395145395845
Iteration: 5 || Loss: 80.84982394321091
Iteration: 6 || Loss: 80.68477196475052
Iteration: 7 || Loss: 80.49510812616172
Iteration: 8 || Loss: 80.451811533626
Iteration: 9 || Loss: 80.37958318059806
Iteration: 10 || Loss: 80.30642226319544
Iteration: 11 || Loss: 80.17050671611935
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.578064
Epoch 112 loss:80.17050671611935
MSE loss S1.057332675995811
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.532684920020966
MSE loss S0.3705919134628324
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:148.5917725400048
MSE loss S2.7155964588856767
waveform batch: 2/2
Test loss - extrapolation:87.53008384966543
MSE loss S1.822116463273644
Epoch 112 mean train loss:3.836567007577667
Epoch 112 mean test loss - interpolation:4.422114153336827
Epoch 112 mean test loss - extrapolation:19.676821365805854
Start training epoch 113
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.578064
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.025141536924348
Iteration: 2 || Loss: 7.022543303243769
Iteration: 3 || Loss: 7.0199810788650705
Iteration: 4 || Loss: 7.0174677375533445
Iteration: 5 || Loss: 7.0149529568805455
Iteration: 6 || Loss: 7.0149529568805455
saving ADAM checkpoint...
Sum of params:-103.57803
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.0149529568805455
Iteration: 2 || Loss: 6.740696528653937
Iteration: 3 || Loss: 6.7319852275992975
Iteration: 4 || Loss: 6.683878035598984
Iteration: 5 || Loss: 6.513695517798723
Iteration: 6 || Loss: 6.444444370791675
Iteration: 7 || Loss: 6.426357815436012
Iteration: 8 || Loss: 6.411840172902921
Iteration: 9 || Loss: 6.391560744900572
Iteration: 10 || Loss: 6.381411698166026
Iteration: 11 || Loss: 6.362191393984856
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.58321
Epoch 113 loss:6.362191393984856
MSE loss S0.24229828654799027
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.58321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.241750595070112
Iteration: 2 || Loss: 25.241035857635435
Iteration: 3 || Loss: 25.240364851624822
Iteration: 4 || Loss: 25.239681934094985
Iteration: 5 || Loss: 25.23899002329821
Iteration: 6 || Loss: 25.23899002329821
saving ADAM checkpoint...
Sum of params:-103.58314
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.23899002329821
Iteration: 2 || Loss: 25.230280048446048
Iteration: 3 || Loss: 25.214459774726773
Iteration: 4 || Loss: 25.171067895578766
Iteration: 5 || Loss: 24.853123462218566
Iteration: 6 || Loss: 24.81164305431897
Iteration: 7 || Loss: 24.778443881733356
Iteration: 8 || Loss: 24.73039702477787
Iteration: 9 || Loss: 24.717280046008714
Iteration: 10 || Loss: 24.690345908089597
Iteration: 11 || Loss: 24.663538813045655
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.4266
Epoch 113 loss:24.663538813045655
MSE loss S0.45073291823599865
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.4266
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 81.03173495948487
Iteration: 2 || Loss: 81.03115084127457
Iteration: 3 || Loss: 81.03063562605043
Iteration: 4 || Loss: 81.03005435230337
Iteration: 5 || Loss: 81.0295557780048
Iteration: 6 || Loss: 81.0295557780048
saving ADAM checkpoint...
Sum of params:-103.426765
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 81.0295557780048
Iteration: 2 || Loss: 81.018004013542
Iteration: 3 || Loss: 80.9823646349913
Iteration: 4 || Loss: 80.70831469773191
Iteration: 5 || Loss: 80.46820230353953
Iteration: 6 || Loss: 80.30338958161855
Iteration: 7 || Loss: 80.11772476380757
Iteration: 8 || Loss: 80.06979618050808
Iteration: 9 || Loss: 79.99266011388384
Iteration: 10 || Loss: 79.8649711641675
Iteration: 11 || Loss: 79.75781099831688
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.64802
Epoch 113 loss:79.75781099831688
MSE loss S1.0635485529454165
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.42276196794495
MSE loss S0.37138906145334105
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:147.23368354682384
MSE loss S2.705179109593905
waveform batch: 2/2
Test loss - extrapolation:86.33623511509028
MSE loss S1.8206785360651474
Epoch 113 mean train loss:3.8201221105292205
Epoch 113 mean test loss - interpolation:4.403793661324158
Epoch 113 mean test loss - extrapolation:19.464159888492844
Start training epoch 114
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.64802
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.842823281044615
Iteration: 2 || Loss: 6.841099945185655
Iteration: 3 || Loss: 6.839235676684998
Iteration: 4 || Loss: 6.837495960880368
Iteration: 5 || Loss: 6.835821030196833
Iteration: 6 || Loss: 6.835821030196833
saving ADAM checkpoint...
Sum of params:-103.64799
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.835821030196833
Iteration: 2 || Loss: 6.7073353439756245
Iteration: 3 || Loss: 6.69529191095878
Iteration: 4 || Loss: 6.676992228935367
Iteration: 5 || Loss: 6.495392635093668
Iteration: 6 || Loss: 6.415770884144878
Iteration: 7 || Loss: 6.404067546043535
Iteration: 8 || Loss: 6.3841420626071335
Iteration: 9 || Loss: 6.368239402046573
Iteration: 10 || Loss: 6.358310470089786
Iteration: 11 || Loss: 6.349474337346033
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.650314
Epoch 114 loss:6.349474337346033
MSE loss S0.23977256621879856
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.650314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.154413438528977
Iteration: 2 || Loss: 25.15377393609478
Iteration: 3 || Loss: 25.153116303396533
Iteration: 4 || Loss: 25.152473226493896
Iteration: 5 || Loss: 25.15187435950204
Iteration: 6 || Loss: 25.15187435950204
saving ADAM checkpoint...
Sum of params:-103.650215
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.15187435950204
Iteration: 2 || Loss: 25.144659961982754
Iteration: 3 || Loss: 25.130941273608286
Iteration: 4 || Loss: 25.092759860160736
Iteration: 5 || Loss: 24.810232142796863
Iteration: 6 || Loss: 24.77083610328261
Iteration: 7 || Loss: 24.737502430074002
Iteration: 8 || Loss: 24.685404464565714
Iteration: 9 || Loss: 24.673265261260156
Iteration: 10 || Loss: 24.649319599512253
Iteration: 11 || Loss: 24.626595316838547
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.51355
Epoch 114 loss:24.626595316838547
MSE loss S0.439896257273803
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.51355
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 80.59902240124426
Iteration: 2 || Loss: 80.59828311065054
Iteration: 3 || Loss: 80.59759005313288
Iteration: 4 || Loss: 80.59687800634663
Iteration: 5 || Loss: 80.59618475933632
Iteration: 6 || Loss: 80.59618475933632
saving ADAM checkpoint...
Sum of params:-103.513504
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 80.59618475933632
Iteration: 2 || Loss: 80.58385825953096
Iteration: 3 || Loss: 80.54209806969769
Iteration: 4 || Loss: 80.23438706902321
Iteration: 5 || Loss: 79.99427047238072
Iteration: 6 || Loss: 79.85149678949581
Iteration: 7 || Loss: 79.67010569152554
Iteration: 8 || Loss: 79.62995881837378
Iteration: 9 || Loss: 79.56608072362242
Iteration: 10 || Loss: 79.50863810435119
Iteration: 11 || Loss: 79.38055719337734
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.673904
Epoch 114 loss:79.38055719337734
MSE loss S1.0653870063747897
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.35844679479384
MSE loss S0.37022299712893203
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:146.61989689914412
MSE loss S2.703014000335142
waveform batch: 2/2
Test loss - extrapolation:85.62429584594533
MSE loss S1.8022607094000613
Epoch 114 mean train loss:3.8054009257779975
Epoch 114 mean test loss - interpolation:4.393074465798973
Epoch 114 mean test loss - extrapolation:19.35368272875745
Start training epoch 115
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.673904
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.956687952132028
Iteration: 2 || Loss: 6.954285608156074
Iteration: 3 || Loss: 6.951797861129177
Iteration: 4 || Loss: 6.9494435783282125
Iteration: 5 || Loss: 6.947018754837454
Iteration: 6 || Loss: 6.947018754837454
saving ADAM checkpoint...
Sum of params:-103.6739
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.947018754837454
Iteration: 2 || Loss: 6.699776421872122
Iteration: 3 || Loss: 6.687932969394753
Iteration: 4 || Loss: 6.5597700391618226
Iteration: 5 || Loss: 6.495389754981787
Iteration: 6 || Loss: 6.422491158633022
Iteration: 7 || Loss: 6.403115167429079
Iteration: 8 || Loss: 6.387485021817918
Iteration: 9 || Loss: 6.365282545765212
Iteration: 10 || Loss: 6.358406464124101
Iteration: 11 || Loss: 6.341654366783665
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.67906
Epoch 115 loss:6.341654366783665
MSE loss S0.2405383747893144
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.67906
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.12082494672218
Iteration: 2 || Loss: 25.12014046181517
Iteration: 3 || Loss: 25.119419176616862
Iteration: 4 || Loss: 25.118763623678305
Iteration: 5 || Loss: 25.11807449773233
Iteration: 6 || Loss: 25.11807449773233
saving ADAM checkpoint...
Sum of params:-103.678986
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.11807449773233
Iteration: 2 || Loss: 25.1093901175255
Iteration: 3 || Loss: 25.09339624378278
Iteration: 4 || Loss: 25.05244877338392
Iteration: 5 || Loss: 24.753880079979368
Iteration: 6 || Loss: 24.71724209554842
Iteration: 7 || Loss: 24.686304949483727
Iteration: 8 || Loss: 24.64047462801284
Iteration: 9 || Loss: 24.628562273954586
Iteration: 10 || Loss: 24.603817958726296
Iteration: 11 || Loss: 24.576819583257738
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.52897
Epoch 115 loss:24.576819583257738
MSE loss S0.44719918971625816
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.52897
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 80.22843677276312
Iteration: 2 || Loss: 80.22788526493501
Iteration: 3 || Loss: 80.22738986190635
Iteration: 4 || Loss: 80.22690391153395
Iteration: 5 || Loss: 80.22640760315784
Iteration: 6 || Loss: 80.22640760315784
saving ADAM checkpoint...
Sum of params:-103.5291
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 80.22640760315784
Iteration: 2 || Loss: 80.21458129762526
Iteration: 3 || Loss: 80.17969518376434
Iteration: 4 || Loss: 79.89425646806835
Iteration: 5 || Loss: 79.67702698837724
Iteration: 6 || Loss: 79.52876348057296
Iteration: 7 || Loss: 79.3487776748824
Iteration: 8 || Loss: 79.29997076885111
Iteration: 9 || Loss: 79.23104270615775
Iteration: 10 || Loss: 79.12834830994926
Iteration: 11 || Loss: 79.0214711927534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.73138
Epoch 115 loss:79.0214711927534
MSE loss S1.0637541909128176
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.25648572302242
MSE loss S0.3707426733564153
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:145.59803401406887
MSE loss S2.6837225447577753
waveform batch: 2/2
Test loss - extrapolation:84.69993720056637
MSE loss S1.7973973744833787
Epoch 115 mean train loss:3.791032591130855
Epoch 115 mean test loss - interpolation:4.37608095383707
Epoch 115 mean test loss - extrapolation:19.191497601219606
Start training epoch 116
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.73138
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.793404255550428
Iteration: 2 || Loss: 6.791686738959566
Iteration: 3 || Loss: 6.789990710124549
Iteration: 4 || Loss: 6.788259038001936
Iteration: 5 || Loss: 6.7865641620572354
Iteration: 6 || Loss: 6.7865641620572354
saving ADAM checkpoint...
Sum of params:-103.73137
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.7865641620572354
Iteration: 2 || Loss: 6.6673375126704295
Iteration: 3 || Loss: 6.654022388181216
Iteration: 4 || Loss: 6.63115435748684
Iteration: 5 || Loss: 6.470134672410058
Iteration: 6 || Loss: 6.397463341240633
Iteration: 7 || Loss: 6.379806229454197
Iteration: 8 || Loss: 6.368255784099533
Iteration: 9 || Loss: 6.353207550666561
Iteration: 10 || Loss: 6.3381918600254705
Iteration: 11 || Loss: 6.331812836795905
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.73588
Epoch 116 loss:6.331812836795905
MSE loss S0.23921230122247922
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.73588
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.064067051593206
Iteration: 2 || Loss: 25.063347220668636
Iteration: 3 || Loss: 25.06265449806437
Iteration: 4 || Loss: 25.06194166645179
Iteration: 5 || Loss: 25.061220048391373
Iteration: 6 || Loss: 25.061220048391373
saving ADAM checkpoint...
Sum of params:-103.7358
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.061220048391373
Iteration: 2 || Loss: 25.052388132922715
Iteration: 3 || Loss: 25.038324784522356
Iteration: 4 || Loss: 25.00120006766893
Iteration: 5 || Loss: 24.71641321581193
Iteration: 6 || Loss: 24.681980625978657
Iteration: 7 || Loss: 24.650391470037178
Iteration: 8 || Loss: 24.601255826225447
Iteration: 9 || Loss: 24.588694456866524
Iteration: 10 || Loss: 24.563638366370885
Iteration: 11 || Loss: 24.54626509020903
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.60824
Epoch 116 loss:24.54626509020903
MSE loss S0.438607309027753
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.60824
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 79.82777004731085
Iteration: 2 || Loss: 79.82712084685078
Iteration: 3 || Loss: 79.82646086873349
Iteration: 4 || Loss: 79.82582809905931
Iteration: 5 || Loss: 79.82528237653865
Iteration: 6 || Loss: 79.82528237653865
saving ADAM checkpoint...
Sum of params:-103.608215
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 79.82528237653865
Iteration: 2 || Loss: 79.81306914105429
Iteration: 3 || Loss: 79.77404089333847
Iteration: 4 || Loss: 79.47848623163883
Iteration: 5 || Loss: 79.26057731074799
Iteration: 6 || Loss: 79.12772287412082
Iteration: 7 || Loss: 78.9577043856149
Iteration: 8 || Loss: 78.91737595808868
Iteration: 9 || Loss: 78.8597294311172
Iteration: 10 || Loss: 78.78764267078026
Iteration: 11 || Loss: 78.68806210927524
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.76091
Epoch 116 loss:78.68806210927524
MSE loss S1.062052387171226
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.213721908467075
MSE loss S0.3682386835683038
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:144.90781809884234
MSE loss S2.6792474808792006
waveform batch: 2/2
Test loss - extrapolation:83.92468994681491
MSE loss S1.7736165679337152
Epoch 116 mean train loss:3.7781427598717303
Epoch 116 mean test loss - interpolation:4.3689536514111795
Epoch 116 mean test loss - extrapolation:19.069375670471437
Start training epoch 117
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.76091
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.905520127564788
Iteration: 2 || Loss: 6.903084963522388
Iteration: 3 || Loss: 6.900655663533192
Iteration: 4 || Loss: 6.898198604796141
Iteration: 5 || Loss: 6.895686858988275
Iteration: 6 || Loss: 6.895686858988275
saving ADAM checkpoint...
Sum of params:-103.7609
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.895686858988275
Iteration: 2 || Loss: 6.644142261514071
Iteration: 3 || Loss: 6.6302787892245565
Iteration: 4 || Loss: 6.607582024704033
Iteration: 5 || Loss: 6.4680602392775794
Iteration: 6 || Loss: 6.398374279715597
Iteration: 7 || Loss: 6.382376250443964
Iteration: 8 || Loss: 6.366640735967277
Iteration: 9 || Loss: 6.34746041686901
Iteration: 10 || Loss: 6.340053483071754
Iteration: 11 || Loss: 6.324080092552376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.766235
Epoch 117 loss:6.324080092552376
MSE loss S0.24002445040756573
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.766235
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.031988613482543
Iteration: 2 || Loss: 25.03122273760123
Iteration: 3 || Loss: 25.030554264399207
Iteration: 4 || Loss: 25.029883146388393
Iteration: 5 || Loss: 25.02917911815634
Iteration: 6 || Loss: 25.02917911815634
saving ADAM checkpoint...
Sum of params:-103.76613
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.02917911815634
Iteration: 2 || Loss: 25.020489813695033
Iteration: 3 || Loss: 25.00277257608662
Iteration: 4 || Loss: 24.961187980320922
Iteration: 5 || Loss: 24.666185885689668
Iteration: 6 || Loss: 24.634242654112892
Iteration: 7 || Loss: 24.605368159896155
Iteration: 8 || Loss: 24.561856482638454
Iteration: 9 || Loss: 24.55075491523867
Iteration: 10 || Loss: 24.528224434067397
Iteration: 11 || Loss: 24.51049649063443
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.6417
Epoch 117 loss:24.51049649063443
MSE loss S0.4380648719372428
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.6417
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 79.51093097595779
Iteration: 2 || Loss: 79.51027221313007
Iteration: 3 || Loss: 79.50965109254078
Iteration: 4 || Loss: 79.50908675473941
Iteration: 5 || Loss: 79.50847419916373
Iteration: 6 || Loss: 79.50847419916373
saving ADAM checkpoint...
Sum of params:-103.641754
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 79.50847419916373
Iteration: 2 || Loss: 79.49632215221511
Iteration: 3 || Loss: 79.45780519766969
Iteration: 4 || Loss: 79.13190929870979
Iteration: 5 || Loss: 78.94331479481868
Iteration: 6 || Loss: 78.81437119391408
Iteration: 7 || Loss: 78.64627387487742
Iteration: 8 || Loss: 78.60447077530993
Iteration: 9 || Loss: 78.54920560080707
Iteration: 10 || Loss: 78.47872965991503
Iteration: 11 || Loss: 78.38531619059823
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.79565
Epoch 117 loss:78.38531619059823
MSE loss S1.060769401734697
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.144095274018042
MSE loss S0.36789787439138116
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:144.20518094309173
MSE loss S2.668978767418667
waveform batch: 2/2
Test loss - extrapolation:83.20948081605131
MSE loss S1.7623194200296335
Epoch 117 mean train loss:3.7662031990960356
Epoch 117 mean test loss - interpolation:4.357349212336341
Epoch 117 mean test loss - extrapolation:18.95122181326192
Start training epoch 118
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.79565
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.86477300823043
Iteration: 2 || Loss: 6.862307698259167
Iteration: 3 || Loss: 6.859993931991097
Iteration: 4 || Loss: 6.857590896400134
Iteration: 5 || Loss: 6.85528592795362
Iteration: 6 || Loss: 6.85528592795362
saving ADAM checkpoint...
Sum of params:-103.79566
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.85528592795362
Iteration: 2 || Loss: 6.615699116767996
Iteration: 3 || Loss: 6.601046178084307
Iteration: 4 || Loss: 6.5765496715487926
Iteration: 5 || Loss: 6.4531824016889345
Iteration: 6 || Loss: 6.385782408689648
Iteration: 7 || Loss: 6.370889464321875
Iteration: 8 || Loss: 6.356413285876315
Iteration: 9 || Loss: 6.337340464820497
Iteration: 10 || Loss: 6.330978241278683
Iteration: 11 || Loss: 6.315627152262382
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.80103
Epoch 118 loss:6.315627152262382
MSE loss S0.2396229648870158
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.80103
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.986636225393234
Iteration: 2 || Loss: 24.985907434114317
Iteration: 3 || Loss: 24.985252234227513
Iteration: 4 || Loss: 24.984514033011457
Iteration: 5 || Loss: 24.98390398837634
Iteration: 6 || Loss: 24.98390398837634
saving ADAM checkpoint...
Sum of params:-103.80094
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.98390398837634
Iteration: 2 || Loss: 24.97495027791569
Iteration: 3 || Loss: 24.957175564912493
Iteration: 4 || Loss: 24.916187955663023
Iteration: 5 || Loss: 24.62711348155012
Iteration: 6 || Loss: 24.59697912226783
Iteration: 7 || Loss: 24.569224274821167
Iteration: 8 || Loss: 24.527272205377013
Iteration: 9 || Loss: 24.516416896176864
Iteration: 10 || Loss: 24.494773067429524
Iteration: 11 || Loss: 24.478519886145254
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.682556
Epoch 118 loss:24.478519886145254
MSE loss S0.4364402501936184
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.682556
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 79.20375776428962
Iteration: 2 || Loss: 79.20306334288192
Iteration: 3 || Loss: 79.2024402893607
Iteration: 4 || Loss: 79.20176877693326
Iteration: 5 || Loss: 79.20116279333926
Iteration: 6 || Loss: 79.20116279333926
saving ADAM checkpoint...
Sum of params:-103.6826
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 79.20116279333926
Iteration: 2 || Loss: 79.18897990792127
Iteration: 3 || Loss: 79.1496838675406
Iteration: 4 || Loss: 78.8318343135306
Iteration: 5 || Loss: 78.63684712257134
Iteration: 6 || Loss: 78.5129360240845
Iteration: 7 || Loss: 78.34763311405408
Iteration: 8 || Loss: 78.30837631764719
Iteration: 9 || Loss: 78.25563079075636
Iteration: 10 || Loss: 78.19607965123211
Iteration: 11 || Loss: 78.100645432967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.83003
Epoch 118 loss:78.100645432967
MSE loss S1.0615240469632252
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.078192126309958
MSE loss S0.3682738087424933
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:143.53048465916322
MSE loss S2.6610658453192264
waveform batch: 2/2
Test loss - extrapolation:82.54030055522237
MSE loss S1.754559319054673
Epoch 118 mean train loss:3.754992843840505
Epoch 118 mean test loss - interpolation:4.346365354384993
Epoch 118 mean test loss - extrapolation:18.8392321011988
Start training epoch 119
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.83003
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.829823656952337
Iteration: 2 || Loss: 6.827539487870613
Iteration: 3 || Loss: 6.825260846415611
Iteration: 4 || Loss: 6.823014401605516
Iteration: 5 || Loss: 6.820812591210828
Iteration: 6 || Loss: 6.820812591210828
saving ADAM checkpoint...
Sum of params:-103.83003
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.820812591210828
Iteration: 2 || Loss: 6.605126605818298
Iteration: 3 || Loss: 6.589039944904142
Iteration: 4 || Loss: 6.571882534677095
Iteration: 5 || Loss: 6.444895070700576
Iteration: 6 || Loss: 6.376308532239161
Iteration: 7 || Loss: 6.360363146361897
Iteration: 8 || Loss: 6.347689160939429
Iteration: 9 || Loss: 6.3277565373445785
Iteration: 10 || Loss: 6.32269832244172
Iteration: 11 || Loss: 6.307997828376406
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.83563
Epoch 119 loss:6.307997828376406
MSE loss S0.23920576855054249
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.83563
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.94888586767071
Iteration: 2 || Loss: 24.9482089655069
Iteration: 3 || Loss: 24.947518031754303
Iteration: 4 || Loss: 24.946816911787543
Iteration: 5 || Loss: 24.946159909280375
Iteration: 6 || Loss: 24.946159909280375
saving ADAM checkpoint...
Sum of params:-103.83556
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.946159909280375
Iteration: 2 || Loss: 24.937394609995046
Iteration: 3 || Loss: 24.918792322674282
Iteration: 4 || Loss: 24.87735130300862
Iteration: 5 || Loss: 24.59182486546612
Iteration: 6 || Loss: 24.562995966546875
Iteration: 7 || Loss: 24.53617407485723
Iteration: 8 || Loss: 24.49499877621097
Iteration: 9 || Loss: 24.48461224890311
Iteration: 10 || Loss: 24.46385671484174
Iteration: 11 || Loss: 24.448325232778338
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.72135
Epoch 119 loss:24.448325232778338
MSE loss S0.4353385578058445
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.72135
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 78.91449690359526
Iteration: 2 || Loss: 78.91382065642122
Iteration: 3 || Loss: 78.91319563838701
Iteration: 4 || Loss: 78.91252974050506
Iteration: 5 || Loss: 78.91184407553898
Iteration: 6 || Loss: 78.91184407553898
saving ADAM checkpoint...
Sum of params:-103.721405
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 78.91184407553898
Iteration: 2 || Loss: 78.89955918748032
Iteration: 3 || Loss: 78.85974782608186
Iteration: 4 || Loss: 78.52884031494118
Iteration: 5 || Loss: 78.34737296380385
Iteration: 6 || Loss: 78.22827078407363
Iteration: 7 || Loss: 78.06653016063018
Iteration: 8 || Loss: 78.02819627961672
Iteration: 9 || Loss: 77.97702525882548
Iteration: 10 || Loss: 77.92345950025457
Iteration: 11 || Loss: 77.82800046490206
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.865395
Epoch 119 loss:77.82800046490206
MSE loss S1.0604892103857537
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.019084886945205
MSE loss S0.36827553172396316
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:142.86602237416565
MSE loss S2.6514599978270805
waveform batch: 2/2
Test loss - extrapolation:81.88032289065595
MSE loss S1.74459433594929
Epoch 119 mean train loss:3.74428701813989
Epoch 119 mean test loss - interpolation:4.336514147824201
Epoch 119 mean test loss - extrapolation:18.7288621054018
Start training epoch 120
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.865395
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.7996600111915
Iteration: 2 || Loss: 6.797450369394866
Iteration: 3 || Loss: 6.795268620095006
Iteration: 4 || Loss: 6.793078577386209
Iteration: 5 || Loss: 6.7909495216317906
Iteration: 6 || Loss: 6.7909495216317906
saving ADAM checkpoint...
Sum of params:-103.86541
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.7909495216317906
Iteration: 2 || Loss: 6.59098568338928
Iteration: 3 || Loss: 6.574088572534298
Iteration: 4 || Loss: 6.558271758004333
Iteration: 5 || Loss: 6.434606852497377
Iteration: 6 || Loss: 6.367264836725208
Iteration: 7 || Loss: 6.3506785598605
Iteration: 8 || Loss: 6.339424994039171
Iteration: 9 || Loss: 6.319502528655098
Iteration: 10 || Loss: 6.315031887012773
Iteration: 11 || Loss: 6.300977381646996
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.8713
Epoch 120 loss:6.300977381646996
MSE loss S0.23877980170583504
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.8713
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.913383215266276
Iteration: 2 || Loss: 24.912648307833745
Iteration: 3 || Loss: 24.911935979057176
Iteration: 4 || Loss: 24.911233211425714
Iteration: 5 || Loss: 24.9105269660966
Iteration: 6 || Loss: 24.9105269660966
saving ADAM checkpoint...
Sum of params:-103.87122
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.9105269660966
Iteration: 2 || Loss: 24.9016736024053
Iteration: 3 || Loss: 24.882642004188938
Iteration: 4 || Loss: 24.841134534491786
Iteration: 5 || Loss: 24.55973309609961
Iteration: 6 || Loss: 24.53194921990299
Iteration: 7 || Loss: 24.50576066733619
Iteration: 8 || Loss: 24.4656073187592
Iteration: 9 || Loss: 24.45539689315479
Iteration: 10 || Loss: 24.435123001727302
Iteration: 11 || Loss: 24.420225143198863
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.76056
Epoch 120 loss:24.420225143198863
MSE loss S0.4344361726445426
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.76056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 78.6395648662448
Iteration: 2 || Loss: 78.63890210461754
Iteration: 3 || Loss: 78.63822718337725
Iteration: 4 || Loss: 78.63756942991841
Iteration: 5 || Loss: 78.6368996958625
Iteration: 6 || Loss: 78.6368996958625
saving ADAM checkpoint...
Sum of params:-103.76061
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 78.6368996958625
Iteration: 2 || Loss: 78.62450802674392
Iteration: 3 || Loss: 78.58427489521787
Iteration: 4 || Loss: 78.25009667685106
Iteration: 5 || Loss: 78.07092066726858
Iteration: 6 || Loss: 77.95558771917656
Iteration: 7 || Loss: 77.79641428738991
Iteration: 8 || Loss: 77.75915505648018
Iteration: 9 || Loss: 77.7096658624452
Iteration: 10 || Loss: 77.66077608245924
Iteration: 11 || Loss: 77.56579234394296
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.90098
Epoch 120 loss:77.56579234394296
MSE loss S1.0589322200259081
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.963428850426588
MSE loss S0.3681063396113866
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:142.21564232763097
MSE loss S2.6415595192356123
waveform batch: 2/2
Test loss - extrapolation:81.23511317470194
MSE loss S1.7344293387888983
Epoch 120 mean train loss:3.7340343058203036
Epoch 120 mean test loss - interpolation:4.327238141737765
Epoch 120 mean test loss - extrapolation:18.620896291861076
Start training epoch 121
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.90098
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.772640332421975
Iteration: 2 || Loss: 6.770497377631397
Iteration: 3 || Loss: 6.768322908850493
Iteration: 4 || Loss: 6.766221112474273
Iteration: 5 || Loss: 6.764183284331086
Iteration: 6 || Loss: 6.764183284331086
saving ADAM checkpoint...
Sum of params:-103.900986
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.764183284331086
Iteration: 2 || Loss: 6.579128493162797
Iteration: 3 || Loss: 6.561589774639327
Iteration: 4 || Loss: 6.547284010119272
Iteration: 5 || Loss: 6.425089826134809
Iteration: 6 || Loss: 6.359146555689131
Iteration: 7 || Loss: 6.341644595273917
Iteration: 8 || Loss: 6.33165870502243
Iteration: 9 || Loss: 6.312066405810905
Iteration: 10 || Loss: 6.307783142967418
Iteration: 11 || Loss: 6.294492530025604
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.90727
Epoch 121 loss:6.294492530025604
MSE loss S0.23833560271618265
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.90727
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.879527686379905
Iteration: 2 || Loss: 24.87883883122216
Iteration: 3 || Loss: 24.878096343629114
Iteration: 4 || Loss: 24.877385472639
Iteration: 5 || Loss: 24.87672481355793
Iteration: 6 || Loss: 24.87672481355793
saving ADAM checkpoint...
Sum of params:-103.9072
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.87672481355793
Iteration: 2 || Loss: 24.867636767212556
Iteration: 3 || Loss: 24.8484966942274
Iteration: 4 || Loss: 24.80707331648444
Iteration: 5 || Loss: 24.52961859534349
Iteration: 6 || Loss: 24.502795955934022
Iteration: 7 || Loss: 24.477175192887746
Iteration: 8 || Loss: 24.43788998257545
Iteration: 9 || Loss: 24.427928403198937
Iteration: 10 || Loss: 24.407998703999635
Iteration: 11 || Loss: 24.393561833360447
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.79926
Epoch 121 loss:24.393561833360447
MSE loss S0.43373142429501177
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.79926
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 78.37628614604297
Iteration: 2 || Loss: 78.37557014423277
Iteration: 3 || Loss: 78.3749062689718
Iteration: 4 || Loss: 78.37425585019872
Iteration: 5 || Loss: 78.37364612211479
Iteration: 6 || Loss: 78.37364612211479
saving ADAM checkpoint...
Sum of params:-103.7993
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 78.37364612211479
Iteration: 2 || Loss: 78.36112748050671
Iteration: 3 || Loss: 78.32051169275843
Iteration: 4 || Loss: 77.97787190786157
Iteration: 5 || Loss: 77.80561340505956
Iteration: 6 || Loss: 77.69358109868045
Iteration: 7 || Loss: 77.53705867225
Iteration: 8 || Loss: 77.50050828432663
Iteration: 9 || Loss: 77.45251874574704
Iteration: 10 || Loss: 77.40703224549
Iteration: 11 || Loss: 77.31355792552746
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.93634
Epoch 121 loss:77.31355792552746
MSE loss S1.0574395682576356
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.91043117246699
MSE loss S0.36800111136911673
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:141.5793876497953
MSE loss S2.631824568200936
waveform batch: 2/2
Test loss - extrapolation:80.60922044189516
MSE loss S1.7243084963163922
Epoch 121 mean train loss:3.7241935272039144
Epoch 121 mean test loss - interpolation:4.318405195411165
Epoch 121 mean test loss - extrapolation:18.515717340974206
Start training epoch 122
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.93634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.747465025413519
Iteration: 2 || Loss: 6.745388550577041
Iteration: 3 || Loss: 6.743369916913421
Iteration: 4 || Loss: 6.741303383466117
Iteration: 5 || Loss: 6.7392953846588535
Iteration: 6 || Loss: 6.7392953846588535
saving ADAM checkpoint...
Sum of params:-103.93635
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.7392953846588535
Iteration: 2 || Loss: 6.5666635540206535
Iteration: 3 || Loss: 6.54849302048394
Iteration: 4 || Loss: 6.535014525078497
Iteration: 5 || Loss: 6.415502709453603
Iteration: 6 || Loss: 6.351388677942831
Iteration: 7 || Loss: 6.333136026606997
Iteration: 8 || Loss: 6.324317861443414
Iteration: 9 || Loss: 6.305223121650014
Iteration: 10 || Loss: 6.300938535194643
Iteration: 11 || Loss: 6.288456288281895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.94295
Epoch 122 loss:6.288456288281895
MSE loss S0.23798901510455645
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.94295
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.84744457419146
Iteration: 2 || Loss: 24.84665368234673
Iteration: 3 || Loss: 24.845931288019276
Iteration: 4 || Loss: 24.84517771306536
Iteration: 5 || Loss: 24.844532930415482
Iteration: 6 || Loss: 24.844532930415482
saving ADAM checkpoint...
Sum of params:-103.94288
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.844532930415482
Iteration: 2 || Loss: 24.835510896516205
Iteration: 3 || Loss: 24.815973097318825
Iteration: 4 || Loss: 24.774605541103377
Iteration: 5 || Loss: 24.500945861743233
Iteration: 6 || Loss: 24.474936642616548
Iteration: 7 || Loss: 24.449819157300624
Iteration: 8 || Loss: 24.41156702198461
Iteration: 9 || Loss: 24.4017754908783
Iteration: 10 || Loss: 24.382106733551286
Iteration: 11 || Loss: 24.36808201217611
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.83734
Epoch 122 loss:24.36808201217611
MSE loss S0.43316520625385246
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.83734
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 78.12363106773196
Iteration: 2 || Loss: 78.12291986731145
Iteration: 3 || Loss: 78.12220816409412
Iteration: 4 || Loss: 78.12159834638236
Iteration: 5 || Loss: 78.1209144710518
Iteration: 6 || Loss: 78.1209144710518
saving ADAM checkpoint...
Sum of params:-103.83739
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 78.1209144710518
Iteration: 2 || Loss: 78.10838110608543
Iteration: 3 || Loss: 78.06736140109646
Iteration: 4 || Loss: 77.7185522228606
Iteration: 5 || Loss: 77.55101718737909
Iteration: 6 || Loss: 77.44207090022043
Iteration: 7 || Loss: 77.28789528899415
Iteration: 8 || Loss: 77.25206274807489
Iteration: 9 || Loss: 77.2054023389642
Iteration: 10 || Loss: 77.16252872781456
Iteration: 11 || Loss: 77.07090548299054
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.97151
Epoch 122 loss:77.07090548299054
MSE loss S1.055852053512706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.859643899019318
MSE loss S0.3678445400265181
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:140.96035321114653
MSE loss S2.622212766460988
waveform batch: 2/2
Test loss - extrapolation:80.00301378989478
MSE loss S1.714370847793288
Epoch 122 mean train loss:3.7147394408085708
Epoch 122 mean test loss - interpolation:4.309940649836553
Epoch 122 mean test loss - extrapolation:18.413613916753444
Start training epoch 123
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-103.97151
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.724502818158906
Iteration: 2 || Loss: 6.72251643473728
Iteration: 3 || Loss: 6.720496359419842
Iteration: 4 || Loss: 6.7185173898036865
Iteration: 5 || Loss: 6.71657503862141
Iteration: 6 || Loss: 6.71657503862141
saving ADAM checkpoint...
Sum of params:-103.97152
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.71657503862141
Iteration: 2 || Loss: 6.555372672549589
Iteration: 3 || Loss: 6.5365196732646
Iteration: 4 || Loss: 6.523645047240619
Iteration: 5 || Loss: 6.406576583998768
Iteration: 6 || Loss: 6.344024342766275
Iteration: 7 || Loss: 6.3253731747853434
Iteration: 8 || Loss: 6.3173617420499095
Iteration: 9 || Loss: 6.298861278778462
Iteration: 10 || Loss: 6.2941398451656045
Iteration: 11 || Loss: 6.282721133465065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.97842
Epoch 123 loss:6.282721133465065
MSE loss S0.23772992395389023
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-103.97842
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.81643954742162
Iteration: 2 || Loss: 24.81570221251494
Iteration: 3 || Loss: 24.815021109687176
Iteration: 4 || Loss: 24.81429066897327
Iteration: 5 || Loss: 24.81360617324539
Iteration: 6 || Loss: 24.81360617324539
saving ADAM checkpoint...
Sum of params:-103.97834
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.81360617324539
Iteration: 2 || Loss: 24.804429188594977
Iteration: 3 || Loss: 24.78454340772469
Iteration: 4 || Loss: 24.74322462660521
Iteration: 5 || Loss: 24.473701814279902
Iteration: 6 || Loss: 24.448229534058065
Iteration: 7 || Loss: 24.423580524104775
Iteration: 8 || Loss: 24.38658408453306
Iteration: 9 || Loss: 24.376913358912148
Iteration: 10 || Loss: 24.357199632777018
Iteration: 11 || Loss: 24.34340245065889
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.87431
Epoch 123 loss:24.34340245065889
MSE loss S0.43270490119876337
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.87431
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 77.88193666256059
Iteration: 2 || Loss: 77.88118767677977
Iteration: 3 || Loss: 77.8805445064284
Iteration: 4 || Loss: 77.87983401257199
Iteration: 5 || Loss: 77.87915779846767
Iteration: 6 || Loss: 77.87915779846767
saving ADAM checkpoint...
Sum of params:-103.8744
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 77.87915779846767
Iteration: 2 || Loss: 77.8665179975606
Iteration: 3 || Loss: 77.82535957009156
Iteration: 4 || Loss: 77.47172391841278
Iteration: 5 || Loss: 77.30754546513424
Iteration: 6 || Loss: 77.2010523539882
Iteration: 7 || Loss: 77.04902433849233
Iteration: 8 || Loss: 77.01345321194252
Iteration: 9 || Loss: 76.96808766958566
Iteration: 10 || Loss: 76.92656526345925
Iteration: 11 || Loss: 76.83819148431783
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.0059
Epoch 123 loss:76.83819148431783
MSE loss S1.0546383374031594
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.810300404092544
MSE loss S0.3676600950279706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:140.35929836765084
MSE loss S2.6130731612764424
waveform batch: 2/2
Test loss - extrapolation:79.41955128566526
MSE loss S1.7049118068196216
Epoch 123 mean train loss:3.70566603684282
Epoch 123 mean test loss - interpolation:4.301716734015424
Epoch 123 mean test loss - extrapolation:18.31490413777634
Start training epoch 124
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.0059
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.703004663662934
Iteration: 2 || Loss: 6.701051580098029
Iteration: 3 || Loss: 6.699106324584293
Iteration: 4 || Loss: 6.697223832722053
Iteration: 5 || Loss: 6.695311638996427
Iteration: 6 || Loss: 6.695311638996427
saving ADAM checkpoint...
Sum of params:-104.00589
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.695311638996427
Iteration: 2 || Loss: 6.543029946069588
Iteration: 3 || Loss: 6.5236384380891606
Iteration: 4 || Loss: 6.51133080154391
Iteration: 5 || Loss: 6.3975533530992355
Iteration: 6 || Loss: 6.3366241579354465
Iteration: 7 || Loss: 6.317808347892436
Iteration: 8 || Loss: 6.310557028647372
Iteration: 9 || Loss: 6.2927569063799975
Iteration: 10 || Loss: 6.287488036289813
Iteration: 11 || Loss: 6.277165048857967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.01302
Epoch 124 loss:6.277165048857967
MSE loss S0.23741609056589374
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.01302
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.787195748107827
Iteration: 2 || Loss: 24.786451218386915
Iteration: 3 || Loss: 24.785695371151544
Iteration: 4 || Loss: 24.78501203600784
Iteration: 5 || Loss: 24.784321628131263
Iteration: 6 || Loss: 24.784321628131263
saving ADAM checkpoint...
Sum of params:-104.012955
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.784321628131263
Iteration: 2 || Loss: 24.774995872636072
Iteration: 3 || Loss: 24.75480487936627
Iteration: 4 || Loss: 24.713486528831332
Iteration: 5 || Loss: 24.447330588491834
Iteration: 6 || Loss: 24.422126184084156
Iteration: 7 || Loss: 24.397718166526484
Iteration: 8 || Loss: 24.36227011663642
Iteration: 9 || Loss: 24.3526692114359
Iteration: 10 || Loss: 24.33293136290283
Iteration: 11 || Loss: 24.319279769399802
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.91023
Epoch 124 loss:24.319279769399802
MSE loss S0.4324140139368166
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.91023
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 77.65055086221071
Iteration: 2 || Loss: 77.64986737906922
Iteration: 3 || Loss: 77.64922647556874
Iteration: 4 || Loss: 77.64857092095221
Iteration: 5 || Loss: 77.64789857255167
Iteration: 6 || Loss: 77.64789857255167
saving ADAM checkpoint...
Sum of params:-103.91032
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 77.64789857255167
Iteration: 2 || Loss: 77.63518370477279
Iteration: 3 || Loss: 77.59415045117703
Iteration: 4 || Loss: 77.2331549022818
Iteration: 5 || Loss: 77.07508514704013
Iteration: 6 || Loss: 76.97078649530424
Iteration: 7 || Loss: 76.82073522071296
Iteration: 8 || Loss: 76.78535592133285
Iteration: 9 || Loss: 76.7410282019119
Iteration: 10 || Loss: 76.70038018695463
Iteration: 11 || Loss: 76.61520492227856
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.0397
Epoch 124 loss:76.61520492227856
MSE loss S1.0533551069756104
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.762308365341635
MSE loss S0.3673862121708512
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:139.77908428655215
MSE loss S2.60412851464494
waveform batch: 2/2
Test loss - extrapolation:78.857724496575
MSE loss S1.695713062070407
Epoch 124 mean train loss:3.696953439328839
Epoch 124 mean test loss - interpolation:4.2937180608902725
Epoch 124 mean test loss - extrapolation:18.219734065260596
Start training epoch 125
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.0397
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.682234273176006
Iteration: 2 || Loss: 6.68039852444166
Iteration: 3 || Loss: 6.678507791041845
Iteration: 4 || Loss: 6.676693597085658
Iteration: 5 || Loss: 6.674784922634405
Iteration: 6 || Loss: 6.674784922634405
saving ADAM checkpoint...
Sum of params:-104.03972
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.674784922634405
Iteration: 2 || Loss: 6.531107938184881
Iteration: 3 || Loss: 6.510969809276475
Iteration: 4 || Loss: 6.499007176260709
Iteration: 5 || Loss: 6.388584984581654
Iteration: 6 || Loss: 6.329528785041003
Iteration: 7 || Loss: 6.310695560359111
Iteration: 8 || Loss: 6.303863868122688
Iteration: 9 || Loss: 6.286984962174068
Iteration: 10 || Loss: 6.280862802247931
Iteration: 11 || Loss: 6.271660095180923
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.04708
Epoch 125 loss:6.271660095180923
MSE loss S0.23708391790687916
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.04708
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.758413927315054
Iteration: 2 || Loss: 24.75763977133912
Iteration: 3 || Loss: 24.756876843720143
Iteration: 4 || Loss: 24.756152505843396
Iteration: 5 || Loss: 24.755509985051717
Iteration: 6 || Loss: 24.755509985051717
saving ADAM checkpoint...
Sum of params:-104.04705
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.755509985051717
Iteration: 2 || Loss: 24.746029319496728
Iteration: 3 || Loss: 24.72564619730621
Iteration: 4 || Loss: 24.684448145433873
Iteration: 5 || Loss: 24.421820775833467
Iteration: 6 || Loss: 24.396699685009505
Iteration: 7 || Loss: 24.372160031010356
Iteration: 8 || Loss: 24.338589262532924
Iteration: 9 || Loss: 24.329186235134628
Iteration: 10 || Loss: 24.309376912299758
Iteration: 11 || Loss: 24.29567825178698
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.94524
Epoch 125 loss:24.29567825178698
MSE loss S0.43206641014723335
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.94524
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 77.43007164090541
Iteration: 2 || Loss: 77.42938709078068
Iteration: 3 || Loss: 77.42872048289685
Iteration: 4 || Loss: 77.42804574469817
Iteration: 5 || Loss: 77.42736952820914
Iteration: 6 || Loss: 77.42736952820914
saving ADAM checkpoint...
Sum of params:-103.945335
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 77.42736952820914
Iteration: 2 || Loss: 77.41467209024773
Iteration: 3 || Loss: 77.373715824421
Iteration: 4 || Loss: 77.008247161787
Iteration: 5 || Loss: 76.85296527476514
Iteration: 6 || Loss: 76.7506541848722
Iteration: 7 || Loss: 76.60238237164832
Iteration: 8 || Loss: 76.56710008580754
Iteration: 9 || Loss: 76.52348701448507
Iteration: 10 || Loss: 76.48325698478894
Iteration: 11 || Loss: 76.4011334199032
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.0734
Epoch 125 loss:76.4011334199032
MSE loss S1.051953033974902
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.716310865802072
MSE loss S0.3670468237724018
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:139.21638310695582
MSE loss S2.5952093863309007
waveform batch: 2/2
Test loss - extrapolation:78.3134686633637
MSE loss S1.6866457503896448
Epoch 125 mean train loss:3.6885679919610728
Epoch 125 mean test loss - interpolation:4.286051810967012
Epoch 125 mean test loss - extrapolation:18.127487647526625
Start training epoch 126
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.0734
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.662598746309364
Iteration: 2 || Loss: 6.6607396024181265
Iteration: 3 || Loss: 6.658945135647944
Iteration: 4 || Loss: 6.657115551028681
Iteration: 5 || Loss: 6.655327893484053
Iteration: 6 || Loss: 6.655327893484053
saving ADAM checkpoint...
Sum of params:-104.07345
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.655327893484053
Iteration: 2 || Loss: 6.51935763215209
Iteration: 3 || Loss: 6.4987403744730905
Iteration: 4 || Loss: 6.487050902977539
Iteration: 5 || Loss: 6.379754169378251
Iteration: 6 || Loss: 6.322603738280359
Iteration: 7 || Loss: 6.303997550175209
Iteration: 8 || Loss: 6.297374352931968
Iteration: 9 || Loss: 6.281309766128489
Iteration: 10 || Loss: 6.27453064760674
Iteration: 11 || Loss: 6.266377067117325
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.080986
Epoch 126 loss:6.266377067117325
MSE loss S0.2367411598739358
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.080986
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.73015728406325
Iteration: 2 || Loss: 24.729339996551364
Iteration: 3 || Loss: 24.728702129318236
Iteration: 4 || Loss: 24.72792811067901
Iteration: 5 || Loss: 24.727173398347915
Iteration: 6 || Loss: 24.727173398347915
saving ADAM checkpoint...
Sum of params:-104.08099
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.727173398347915
Iteration: 2 || Loss: 24.717670991803313
Iteration: 3 || Loss: 24.69705667673038
Iteration: 4 || Loss: 24.656036123800693
Iteration: 5 || Loss: 24.39716092773706
Iteration: 6 || Loss: 24.371977702298036
Iteration: 7 || Loss: 24.346778710165857
Iteration: 8 || Loss: 24.315384759713726
Iteration: 9 || Loss: 24.306190938910902
Iteration: 10 || Loss: 24.286332321198536
Iteration: 11 || Loss: 24.272604558695864
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-103.98003
Epoch 126 loss:24.272604558695864
MSE loss S0.43192880496390273
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-103.98003
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 77.21725680102942
Iteration: 2 || Loss: 77.2165973485654
Iteration: 3 || Loss: 77.21591095166923
Iteration: 4 || Loss: 77.21529031238587
Iteration: 5 || Loss: 77.21459393577693
Iteration: 6 || Loss: 77.21459393577693
saving ADAM checkpoint...
Sum of params:-103.980156
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 77.21459393577693
Iteration: 2 || Loss: 77.20180852094386
Iteration: 3 || Loss: 77.16095236635908
Iteration: 4 || Loss: 76.78947103558471
Iteration: 5 || Loss: 76.63925878398636
Iteration: 6 || Loss: 76.53874383492719
Iteration: 7 || Loss: 76.39280203844518
Iteration: 8 || Loss: 76.35739489342261
Iteration: 9 || Loss: 76.31405525087712
Iteration: 10 || Loss: 76.27385328113816
Iteration: 11 || Loss: 76.19441671733864
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.107574
Epoch 126 loss:76.19441671733864
MSE loss S1.0502935463870715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.672040921868827
MSE loss S0.36660265386501356
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:138.6683977087473
MSE loss S2.586360194652897
waveform batch: 2/2
Test loss - extrapolation:77.78533523518774
MSE loss S1.6777059042437674
Epoch 126 mean train loss:3.6804620118328217
Epoch 126 mean test loss - interpolation:4.278673486978138
Epoch 126 mean test loss - extrapolation:18.037811078661253
Start training epoch 127
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.107574
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.6450825149526525
Iteration: 2 || Loss: 6.643308140452186
Iteration: 3 || Loss: 6.641525568860491
Iteration: 4 || Loss: 6.639754081904142
Iteration: 5 || Loss: 6.638050291557728
Iteration: 6 || Loss: 6.638050291557728
saving ADAM checkpoint...
Sum of params:-104.10759
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.638050291557728
Iteration: 2 || Loss: 6.509384202199239
Iteration: 3 || Loss: 6.488271796323722
Iteration: 4 || Loss: 6.476589701735795
Iteration: 5 || Loss: 6.3714239354666855
Iteration: 6 || Loss: 6.316106931527859
Iteration: 7 || Loss: 6.29766640204331
Iteration: 8 || Loss: 6.291261687713321
Iteration: 9 || Loss: 6.276002734490871
Iteration: 10 || Loss: 6.268317821294479
Iteration: 11 || Loss: 6.261364513694041
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.11538
Epoch 127 loss:6.261364513694041
MSE loss S0.23639663725323468
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.11538
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.702579454162525
Iteration: 2 || Loss: 24.70182513137841
Iteration: 3 || Loss: 24.701076751221528
Iteration: 4 || Loss: 24.700341022089066
Iteration: 5 || Loss: 24.69962988397036
Iteration: 6 || Loss: 24.69962988397036
saving ADAM checkpoint...
Sum of params:-104.11537
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.69962988397036
Iteration: 2 || Loss: 24.68996583996647
Iteration: 3 || Loss: 24.66938393770615
Iteration: 4 || Loss: 24.628850072293815
Iteration: 5 || Loss: 24.373627380576774
Iteration: 6 || Loss: 24.34823174719998
Iteration: 7 || Loss: 24.32135570754004
Iteration: 8 || Loss: 24.29279149024583
Iteration: 9 || Loss: 24.283906582346432
Iteration: 10 || Loss: 24.26406042697727
Iteration: 11 || Loss: 24.250383995462045
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.015625
Epoch 127 loss:24.250383995462045
MSE loss S0.43188718176210694
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.015625
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 77.00608427595897
Iteration: 2 || Loss: 77.00538677691368
Iteration: 3 || Loss: 77.00473086013592
Iteration: 4 || Loss: 77.00414673637499
Iteration: 5 || Loss: 77.00351683104239
Iteration: 6 || Loss: 77.00351683104239
saving ADAM checkpoint...
Sum of params:-104.015724
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 77.00351683104239
Iteration: 2 || Loss: 76.99061021741001
Iteration: 3 || Loss: 76.95000219367856
Iteration: 4 || Loss: 76.574024097923
Iteration: 5 || Loss: 76.4302463406906
Iteration: 6 || Loss: 76.33184711066544
Iteration: 7 || Loss: 76.18937551923037
Iteration: 8 || Loss: 76.15401318308284
Iteration: 9 || Loss: 76.11059437423074
Iteration: 10 || Loss: 76.0696352166679
Iteration: 11 || Loss: 75.99280292271918
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.14305
Epoch 127 loss:75.99280292271918
MSE loss S1.0486176091289416
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.630761066728944
MSE loss S0.36621681998479433
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:138.12247500648994
MSE loss S2.57745409579989
waveform batch: 2/2
Test loss - extrapolation:77.26644531068374
MSE loss S1.6687177030744618
Epoch 127 mean train loss:3.6725707390301814
Epoch 127 mean test loss - interpolation:4.271793511121491
Epoch 127 mean test loss - extrapolation:17.949076693097805
Start training epoch 128
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.14305
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.630699433705438
Iteration: 2 || Loss: 6.628935276492811
Iteration: 3 || Loss: 6.62719236699008
Iteration: 4 || Loss: 6.625486109055377
Iteration: 5 || Loss: 6.623807135484781
Iteration: 6 || Loss: 6.623807135484781
saving ADAM checkpoint...
Sum of params:-104.14307
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.623807135484781
Iteration: 2 || Loss: 6.500755333165836
Iteration: 3 || Loss: 6.479452276222217
Iteration: 4 || Loss: 6.467669087835564
Iteration: 5 || Loss: 6.363887498112614
Iteration: 6 || Loss: 6.310492695284408
Iteration: 7 || Loss: 6.29203951912992
Iteration: 8 || Loss: 6.285650788183911
Iteration: 9 || Loss: 6.271249337998239
Iteration: 10 || Loss: 6.262961739559393
Iteration: 11 || Loss: 6.256750592443854
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.15095
Epoch 128 loss:6.256750592443854
MSE loss S0.23619235451851878
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.15095
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.678473123797648
Iteration: 2 || Loss: 24.67767533107896
Iteration: 3 || Loss: 24.676944470474734
Iteration: 4 || Loss: 24.67619764892592
Iteration: 5 || Loss: 24.67546649626374
Iteration: 6 || Loss: 24.67546649626374
saving ADAM checkpoint...
Sum of params:-104.15097
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.67546649626374
Iteration: 2 || Loss: 24.6655544904552
Iteration: 3 || Loss: 24.644488192733295
Iteration: 4 || Loss: 24.60393292432475
Iteration: 5 || Loss: 24.352173606336617
Iteration: 6 || Loss: 24.326340323269275
Iteration: 7 || Loss: 24.29593994595798
Iteration: 8 || Loss: 24.270832876445215
Iteration: 9 || Loss: 24.262784081122714
Iteration: 10 || Loss: 24.242825911299896
Iteration: 11 || Loss: 24.228593027563555
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.05193
Epoch 128 loss:24.228593027563555
MSE loss S0.4327032325436685
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.05193
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 76.78537750701875
Iteration: 2 || Loss: 76.78482156101728
Iteration: 3 || Loss: 76.78420197840691
Iteration: 4 || Loss: 76.78368646693377
Iteration: 5 || Loss: 76.78316370890143
Iteration: 6 || Loss: 76.78316370890143
saving ADAM checkpoint...
Sum of params:-104.05204
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 76.78316370890143
Iteration: 2 || Loss: 76.77021785856127
Iteration: 3 || Loss: 76.73095443867012
Iteration: 4 || Loss: 76.35888178480616
Iteration: 5 || Loss: 76.22179967049189
Iteration: 6 || Loss: 76.1268515659739
Iteration: 7 || Loss: 75.99059351664276
Iteration: 8 || Loss: 75.9546795067741
Iteration: 9 || Loss: 75.90829853481699
Iteration: 10 || Loss: 75.85433399422395
Iteration: 11 || Loss: 75.7866363227127
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.186226
Epoch 128 loss:75.7866363227127
MSE loss S1.0423721416066216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.601724179409857
MSE loss S0.3645211354082126
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:137.52743925803148
MSE loss S2.5646506947165864
waveform batch: 2/2
Test loss - extrapolation:76.69714929725124
MSE loss S1.6550578791677715
Epoch 128 mean train loss:3.66455103250759
Epoch 128 mean test loss - interpolation:4.266954029901643
Epoch 128 mean test loss - extrapolation:17.85204904627356
Start training epoch 129
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.186226
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.633059352600008
Iteration: 2 || Loss: 6.6312584173811455
Iteration: 3 || Loss: 6.629476013590983
Iteration: 4 || Loss: 6.627735232911027
Iteration: 5 || Loss: 6.625982923116199
Iteration: 6 || Loss: 6.625982923116199
saving ADAM checkpoint...
Sum of params:-104.18624
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.625982923116199
Iteration: 2 || Loss: 6.496485010005532
Iteration: 3 || Loss: 6.47661943101401
Iteration: 4 || Loss: 6.463432833865874
Iteration: 5 || Loss: 6.355744739916613
Iteration: 6 || Loss: 6.305816335993466
Iteration: 7 || Loss: 6.288481217026407
Iteration: 8 || Loss: 6.281871870170773
Iteration: 9 || Loss: 6.26841573999769
Iteration: 10 || Loss: 6.259903182794008
Iteration: 11 || Loss: 6.254107781360559
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.19445
Epoch 129 loss:6.254107781360559
MSE loss S0.2361283560778597
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.19445
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.658662985268567
Iteration: 2 || Loss: 24.657830514199972
Iteration: 3 || Loss: 24.657023415414816
Iteration: 4 || Loss: 24.656325436251894
Iteration: 5 || Loss: 24.655569392322036
Iteration: 6 || Loss: 24.655569392322036
saving ADAM checkpoint...
Sum of params:-104.194496
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.655569392322036
Iteration: 2 || Loss: 24.64559461943601
Iteration: 3 || Loss: 24.62449850838833
Iteration: 4 || Loss: 24.584266974572493
Iteration: 5 || Loss: 24.33661003673809
Iteration: 6 || Loss: 24.30961688920133
Iteration: 7 || Loss: 24.272742804365706
Iteration: 8 || Loss: 24.24914701150585
Iteration: 9 || Loss: 24.241013651466876
Iteration: 10 || Loss: 24.222977339559332
Iteration: 11 || Loss: 24.207480316137264
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.091515
Epoch 129 loss:24.207480316137264
MSE loss S0.4334950942772876
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.091515
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 76.5983401986306
Iteration: 2 || Loss: 76.59779832119784
Iteration: 3 || Loss: 76.59723185405478
Iteration: 4 || Loss: 76.59667678630159
Iteration: 5 || Loss: 76.59614508322922
Iteration: 6 || Loss: 76.59614508322922
saving ADAM checkpoint...
Sum of params:-104.09164
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 76.59614508322922
Iteration: 2 || Loss: 76.58292057304082
Iteration: 3 || Loss: 76.54285216295848
Iteration: 4 || Loss: 76.16261461735331
Iteration: 5 || Loss: 76.02129341104535
Iteration: 6 || Loss: 75.92610318674119
Iteration: 7 || Loss: 75.7849285299243
Iteration: 8 || Loss: 75.74839433738642
Iteration: 9 || Loss: 75.70272591942528
Iteration: 10 || Loss: 75.65218766525851
Iteration: 11 || Loss: 75.58665092507279
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.221725
Epoch 129 loss:75.58665092507279
MSE loss S1.0414404728197342
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.55401481771797
MSE loss S0.36475477248792254
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:136.99327425593472
MSE loss S2.555100634194048
waveform batch: 2/2
Test loss - extrapolation:76.18727175719734
MSE loss S1.6470069820859385
Epoch 129 mean train loss:3.6568358283645037
Epoch 129 mean test loss - interpolation:4.259002469619662
Epoch 129 mean test loss - extrapolation:17.765045501094338
Start training epoch 130
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.221725
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.602962953286945
Iteration: 2 || Loss: 6.601284514397732
Iteration: 3 || Loss: 6.599620816866529
Iteration: 4 || Loss: 6.597969628007515
Iteration: 5 || Loss: 6.596354433804269
Iteration: 6 || Loss: 6.596354433804269
saving ADAM checkpoint...
Sum of params:-104.22171
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.596354433804269
Iteration: 2 || Loss: 6.485820557935604
Iteration: 3 || Loss: 6.4651888304277385
Iteration: 4 || Loss: 6.453020642695835
Iteration: 5 || Loss: 6.347291642344151
Iteration: 6 || Loss: 6.298537200354875
Iteration: 7 || Loss: 6.282097706745089
Iteration: 8 || Loss: 6.275069554738833
Iteration: 9 || Loss: 6.263328003881194
Iteration: 10 || Loss: 6.253400537093439
Iteration: 11 || Loss: 6.248578157888202
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.229805
Epoch 130 loss:6.248578157888202
MSE loss S0.23570054166846338
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.229805
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.625250810031435
Iteration: 2 || Loss: 24.62448082390931
Iteration: 3 || Loss: 24.623659897542446
Iteration: 4 || Loss: 24.622953830489504
Iteration: 5 || Loss: 24.62218618474033
Iteration: 6 || Loss: 24.62218618474033
saving ADAM checkpoint...
Sum of params:-104.22983
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.62218618474033
Iteration: 2 || Loss: 24.612081089744507
Iteration: 3 || Loss: 24.591153109873105
Iteration: 4 || Loss: 24.553273989674437
Iteration: 5 || Loss: 24.311768953216554
Iteration: 6 || Loss: 24.285679969675495
Iteration: 7 || Loss: 24.25035410539414
Iteration: 8 || Loss: 24.227901133758394
Iteration: 9 || Loss: 24.221248676098206
Iteration: 10 || Loss: 24.201618116777624
Iteration: 11 || Loss: 24.187701554785207
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.13055
Epoch 130 loss:24.187701554785207
MSE loss S0.4322115513941342
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.13055
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 76.40975460544672
Iteration: 2 || Loss: 76.40910426980749
Iteration: 3 || Loss: 76.4084744129814
Iteration: 4 || Loss: 76.40786777030942
Iteration: 5 || Loss: 76.40729544238877
Iteration: 6 || Loss: 76.40729544238877
saving ADAM checkpoint...
Sum of params:-104.13066
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 76.40729544238877
Iteration: 2 || Loss: 76.39404441687078
Iteration: 3 || Loss: 76.35271676025192
Iteration: 4 || Loss: 75.96114645790782
Iteration: 5 || Loss: 75.82587574004069
Iteration: 6 || Loss: 75.73091696547479
Iteration: 7 || Loss: 75.58894239102094
Iteration: 8 || Loss: 75.55382686476722
Iteration: 9 || Loss: 75.5122588182573
Iteration: 10 || Loss: 75.47479147577235
Iteration: 11 || Loss: 75.40429951399882
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.25156
Epoch 130 loss:75.40429951399882
MSE loss S1.0444403826410216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.503812705808294
MSE loss S0.3657095922805188
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:136.5055974165911
MSE loss S2.5508674084961944
waveform batch: 2/2
Test loss - extrapolation:75.73310726122908
MSE loss S1.6422831107417017
Epoch 130 mean train loss:3.6496751457473184
Epoch 130 mean test loss - interpolation:4.250635450968049
Epoch 130 mean test loss - extrapolation:17.68655872315168
Start training epoch 131
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.25156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.57846896014787
Iteration: 2 || Loss: 6.576923867606475
Iteration: 3 || Loss: 6.575392738437065
Iteration: 4 || Loss: 6.573870636759601
Iteration: 5 || Loss: 6.572358913916766
Iteration: 6 || Loss: 6.572358913916766
saving ADAM checkpoint...
Sum of params:-104.25147
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.572358913916766
Iteration: 2 || Loss: 6.474646963582916
Iteration: 3 || Loss: 6.452172382655993
Iteration: 4 || Loss: 6.4404783261468435
Iteration: 5 || Loss: 6.341362998644721
Iteration: 6 || Loss: 6.292937354791109
Iteration: 7 || Loss: 6.275495646422404
Iteration: 8 || Loss: 6.268113694733863
Iteration: 9 || Loss: 6.257344368138142
Iteration: 10 || Loss: 6.247325058177215
Iteration: 11 || Loss: 6.240643285335507
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.25874
Epoch 131 loss:6.240643285335507
MSE loss S0.23470186286307426
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.25874
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.585437870065004
Iteration: 2 || Loss: 24.584690840367422
Iteration: 3 || Loss: 24.58385885434979
Iteration: 4 || Loss: 24.58304640783837
Iteration: 5 || Loss: 24.582318890585555
Iteration: 6 || Loss: 24.582318890585555
saving ADAM checkpoint...
Sum of params:-104.25871
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.582318890585555
Iteration: 2 || Loss: 24.571967951220095
Iteration: 3 || Loss: 24.54968864442481
Iteration: 4 || Loss: 24.514513105722546
Iteration: 5 || Loss: 24.283858931688606
Iteration: 6 || Loss: 24.259606928904063
Iteration: 7 || Loss: 24.227502185914616
Iteration: 8 || Loss: 24.204822920826754
Iteration: 9 || Loss: 24.198174486840035
Iteration: 10 || Loss: 24.180161653940573
Iteration: 11 || Loss: 24.16779182172862
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.168365
Epoch 131 loss:24.16779182172862
MSE loss S0.4318134843702106
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.168365
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 76.13947164423264
Iteration: 2 || Loss: 76.1389350254957
Iteration: 3 || Loss: 76.13838440972535
Iteration: 4 || Loss: 76.1378295387743
Iteration: 5 || Loss: 76.13733825868985
Iteration: 6 || Loss: 76.13733825868985
saving ADAM checkpoint...
Sum of params:-104.16849
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 76.13733825868985
Iteration: 2 || Loss: 76.12493510053915
Iteration: 3 || Loss: 76.08733589979519
Iteration: 4 || Loss: 75.73779465048514
Iteration: 5 || Loss: 75.61577162327148
Iteration: 6 || Loss: 75.53045149717708
Iteration: 7 || Loss: 75.40710302065023
Iteration: 8 || Loss: 75.37351195952601
Iteration: 9 || Loss: 75.32906712802505
Iteration: 10 || Loss: 75.27716506899846
Iteration: 11 || Loss: 75.21406014058519
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.29933
Epoch 131 loss:75.21406014058519
MSE loss S1.0385447790991733
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.48656997936391
MSE loss S0.3636817268382917
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:135.90140239694946
MSE loss S2.539864221322215
waveform batch: 2/2
Test loss - extrapolation:75.19335452801958
MSE loss S1.629175565827619
Epoch 131 mean train loss:3.6421550085396315
Epoch 131 mean test loss - interpolation:4.247761663227318
Epoch 131 mean test loss - extrapolation:17.59122974374742
Start training epoch 132
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.29933
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.60343798032618
Iteration: 2 || Loss: 6.60171281438554
Iteration: 3 || Loss: 6.599991193306409
Iteration: 4 || Loss: 6.598197301950204
Iteration: 5 || Loss: 6.596518819348498
Iteration: 6 || Loss: 6.596518819348498
saving ADAM checkpoint...
Sum of params:-104.299355
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.596518819348498
Iteration: 2 || Loss: 6.476338601505393
Iteration: 3 || Loss: 6.45493291374593
Iteration: 4 || Loss: 6.4393548190956205
Iteration: 5 || Loss: 6.337988729600885
Iteration: 6 || Loss: 6.290656494246152
Iteration: 7 || Loss: 6.2741467614952615
Iteration: 8 || Loss: 6.267515730741874
Iteration: 9 || Loss: 6.255781278342454
Iteration: 10 || Loss: 6.246711414309793
Iteration: 11 || Loss: 6.241844344718037
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.30777
Epoch 132 loss:6.241844344718037
MSE loss S0.23542263487921716
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.30777
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.594699231445887
Iteration: 2 || Loss: 24.593820605300827
Iteration: 3 || Loss: 24.593114151626747
Iteration: 4 || Loss: 24.592305321208155
Iteration: 5 || Loss: 24.591562373428083
Iteration: 6 || Loss: 24.591562373428083
saving ADAM checkpoint...
Sum of params:-104.3078
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.591562373428083
Iteration: 2 || Loss: 24.58099537100327
Iteration: 3 || Loss: 24.558467001968374
Iteration: 4 || Loss: 24.5179853348833
Iteration: 5 || Loss: 24.279728881995606
Iteration: 6 || Loss: 24.252895890640364
Iteration: 7 || Loss: 24.223613149557046
Iteration: 8 || Loss: 24.195716816251334
Iteration: 9 || Loss: 24.18724565697554
Iteration: 10 || Loss: 24.165954839705517
Iteration: 11 || Loss: 24.15248395710048
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.20762
Epoch 132 loss:24.15248395710048
MSE loss S0.43244780210637146
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.20762
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 76.06869476653888
Iteration: 2 || Loss: 76.06799915478503
Iteration: 3 || Loss: 76.06733865572431
Iteration: 4 || Loss: 76.06659401006634
Iteration: 5 || Loss: 76.06596193059806
Iteration: 6 || Loss: 76.06596193059806
saving ADAM checkpoint...
Sum of params:-104.20776
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 76.06596193059806
Iteration: 2 || Loss: 76.05279272781932
Iteration: 3 || Loss: 76.00965636147416
Iteration: 4 || Loss: 75.59533413850932
Iteration: 5 || Loss: 75.4621111346138
Iteration: 6 || Loss: 75.3651059815205
Iteration: 7 || Loss: 75.21361979918619
Iteration: 8 || Loss: 75.18036383802755
Iteration: 9 || Loss: 75.13887536085039
Iteration: 10 || Loss: 75.10937615534716
Iteration: 11 || Loss: 75.03512215196665
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.322975
Epoch 132 loss:75.03512215196665
MSE loss S1.0357390895951235
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.42957192325328
MSE loss S0.36616741167306943
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:135.5337868543348
MSE loss S2.5258666853351555
waveform batch: 2/2
Test loss - extrapolation:74.75109365290318
MSE loss S1.6200958198372564
Epoch 132 mean train loss:3.6354982915098333
Epoch 132 mean test loss - interpolation:4.2382619872088805
Epoch 132 mean test loss - extrapolation:17.523740042269832
Start training epoch 133
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.322975
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.537680620009215
Iteration: 2 || Loss: 6.53638115546485
Iteration: 3 || Loss: 6.535144946843666
Iteration: 4 || Loss: 6.5338786488458265
Iteration: 5 || Loss: 6.532583824545905
Iteration: 6 || Loss: 6.532583824545905
saving ADAM checkpoint...
Sum of params:-104.32292
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.532583824545905
Iteration: 2 || Loss: 6.469061623797801
Iteration: 3 || Loss: 6.448193067291786
Iteration: 4 || Loss: 6.436260139851817
Iteration: 5 || Loss: 6.328087607086338
Iteration: 6 || Loss: 6.278059775586816
Iteration: 7 || Loss: 6.264727741721889
Iteration: 8 || Loss: 6.2532210152916745
Iteration: 9 || Loss: 6.247658290149896
Iteration: 10 || Loss: 6.238525571000772
Iteration: 11 || Loss: 6.20876185168291
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.326485
Epoch 133 loss:6.20876185168291
MSE loss S0.22471535990117922
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.326485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.497179795910938
Iteration: 2 || Loss: 24.49693602009226
Iteration: 3 || Loss: 24.496783606447753
Iteration: 4 || Loss: 24.496554412943205
Iteration: 5 || Loss: 24.49636107331861
Iteration: 6 || Loss: 24.49636107331861
saving ADAM checkpoint...
Sum of params:-104.326546
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.49636107331861
Iteration: 2 || Loss: 24.46649805261556
Iteration: 3 || Loss: 24.451751819942956
Iteration: 4 || Loss: 24.419015183652846
Iteration: 5 || Loss: 24.265960202501585
Iteration: 6 || Loss: 24.235849936982262
Iteration: 7 || Loss: 24.211693931050057
Iteration: 8 || Loss: 24.168540619174593
Iteration: 9 || Loss: 24.15844122333495
Iteration: 10 || Loss: 24.139869055786395
Iteration: 11 || Loss: 24.12880228375009
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.24681
Epoch 133 loss:24.12880228375009
MSE loss S0.437583265259438
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.24681
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 75.73951005676832
Iteration: 2 || Loss: 75.73897014052027
Iteration: 3 || Loss: 75.73841506351431
Iteration: 4 || Loss: 75.73787523549338
Iteration: 5 || Loss: 75.73736723842484
Iteration: 6 || Loss: 75.73736723842484
saving ADAM checkpoint...
Sum of params:-104.24696
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 75.73736723842484
Iteration: 2 || Loss: 75.72525148991464
Iteration: 3 || Loss: 75.68847660277612
Iteration: 4 || Loss: 75.34417291898791
Iteration: 5 || Loss: 75.24648312080166
Iteration: 6 || Loss: 75.16135163573553
Iteration: 7 || Loss: 75.04881688500355
Iteration: 8 || Loss: 75.0173971697388
Iteration: 9 || Loss: 74.96893315039173
Iteration: 10 || Loss: 74.92053992080258
Iteration: 11 || Loss: 74.84945164518915
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.38335
Epoch 133 loss:74.84945164518915
MSE loss S1.0269657022248964
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.425789150690775
MSE loss S0.36225265819597263
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:134.8817615314884
MSE loss S2.5079651296194507
waveform batch: 2/2
Test loss - extrapolation:74.2658055385094
MSE loss S1.6084643196116652
Epoch 133 mean train loss:3.6271384751938673
Epoch 133 mean test loss - interpolation:4.2376315251151295
Epoch 133 mean test loss - extrapolation:17.428963922499815
Start training epoch 134
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.38335
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.589854928283902
Iteration: 2 || Loss: 6.588258913723472
Iteration: 3 || Loss: 6.586682676930225
Iteration: 4 || Loss: 6.585091268776074
Iteration: 5 || Loss: 6.583505489077553
Iteration: 6 || Loss: 6.583505489077553
saving ADAM checkpoint...
Sum of params:-104.38336
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.583505489077553
Iteration: 2 || Loss: 6.48385815231763
Iteration: 3 || Loss: 6.46191656349101
Iteration: 4 || Loss: 6.439473288866293
Iteration: 5 || Loss: 6.334274870141726
Iteration: 6 || Loss: 6.283533418911509
Iteration: 7 || Loss: 6.258815254776696
Iteration: 8 || Loss: 6.250911514556273
Iteration: 9 || Loss: 6.242500198625342
Iteration: 10 || Loss: 6.232099488222141
Iteration: 11 || Loss: 6.225491125502531
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.39113
Epoch 134 loss:6.225491125502531
MSE loss S0.23244394070914362
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.39113
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.537247050729057
Iteration: 2 || Loss: 24.53641155211559
Iteration: 3 || Loss: 24.535623779593
Iteration: 4 || Loss: 24.53478247781117
Iteration: 5 || Loss: 24.533941256169342
Iteration: 6 || Loss: 24.533941256169342
saving ADAM checkpoint...
Sum of params:-104.391136
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.533941256169342
Iteration: 2 || Loss: 24.522327036156426
Iteration: 3 || Loss: 24.486493532089742
Iteration: 4 || Loss: 24.457589243088847
Iteration: 5 || Loss: 24.249580924997385
Iteration: 6 || Loss: 24.222515129479294
Iteration: 7 || Loss: 24.1908316662796
Iteration: 8 || Loss: 24.16167150029314
Iteration: 9 || Loss: 24.153075548795965
Iteration: 10 || Loss: 24.1291433035318
Iteration: 11 || Loss: 24.116636588471373
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.294205
Epoch 134 loss:24.116636588471373
MSE loss S0.4304163915772533
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.294205
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 75.69832003004537
Iteration: 2 || Loss: 75.6975935858663
Iteration: 3 || Loss: 75.69683826896924
Iteration: 4 || Loss: 75.6961254680512
Iteration: 5 || Loss: 75.69537765143659
Iteration: 6 || Loss: 75.69537765143659
saving ADAM checkpoint...
Sum of params:-104.294334
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 75.69537765143659
Iteration: 2 || Loss: 75.68279000607434
Iteration: 3 || Loss: 75.64044666069024
Iteration: 4 || Loss: 75.19923409550097
Iteration: 5 || Loss: 75.08706069721592
Iteration: 6 || Loss: 74.99225551984662
Iteration: 7 || Loss: 74.84448732366636
Iteration: 8 || Loss: 74.81199432561385
Iteration: 9 || Loss: 74.77151712463525
Iteration: 10 || Loss: 74.74505749174246
Iteration: 11 || Loss: 74.67626403456961
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.40043
Epoch 134 loss:74.67626403456961
MSE loss S1.0342332054735957
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.35427265524214
MSE loss S0.36527179814407285
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:134.48229648777917
MSE loss S2.510097806146266
waveform batch: 2/2
Test loss - extrapolation:73.79003696571102
MSE loss S1.6037685778056923
Epoch 134 mean train loss:3.621323853398052
Epoch 134 mean test loss - interpolation:4.225712109207024
Epoch 134 mean test loss - extrapolation:17.35602778779085
Start training epoch 135
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.40043
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.518282829938488
Iteration: 2 || Loss: 6.517033096441832
Iteration: 3 || Loss: 6.515725264471746
Iteration: 4 || Loss: 6.514581686114346
Iteration: 5 || Loss: 6.513300033149495
Iteration: 6 || Loss: 6.513300033149495
saving ADAM checkpoint...
Sum of params:-104.40036
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.513300033149495
Iteration: 2 || Loss: 6.4517998765301945
Iteration: 3 || Loss: 6.430049672146012
Iteration: 4 || Loss: 6.416827754301854
Iteration: 5 || Loss: 6.312546961332885
Iteration: 6 || Loss: 6.264012205554265
Iteration: 7 || Loss: 6.251232878491151
Iteration: 8 || Loss: 6.240263418471885
Iteration: 9 || Loss: 6.234837545387403
Iteration: 10 || Loss: 6.226436892287828
Iteration: 11 || Loss: 6.196751887649581
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.40535
Epoch 135 loss:6.196751887649581
MSE loss S0.2227582159300906
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.40535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.47034372650346
Iteration: 2 || Loss: 24.47013763104339
Iteration: 3 || Loss: 24.469872601152627
Iteration: 4 || Loss: 24.469728642618435
Iteration: 5 || Loss: 24.469471668526538
Iteration: 6 || Loss: 24.469471668526538
saving ADAM checkpoint...
Sum of params:-104.40543
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.469471668526538
Iteration: 2 || Loss: 24.433611642355032
Iteration: 3 || Loss: 24.42324554383298
Iteration: 4 || Loss: 24.381928394089712
Iteration: 5 || Loss: 24.231151589304073
Iteration: 6 || Loss: 24.200833561508258
Iteration: 7 || Loss: 24.175221158285403
Iteration: 8 || Loss: 24.130376504853512
Iteration: 9 || Loss: 24.122250484123818
Iteration: 10 || Loss: 24.10202083331283
Iteration: 11 || Loss: 24.091746822518857
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.32587
Epoch 135 loss:24.091746822518857
MSE loss S0.4337462860442353
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.32587
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 75.43631136174957
Iteration: 2 || Loss: 75.4355866882018
Iteration: 3 || Loss: 75.43503766461158
Iteration: 4 || Loss: 75.43435882307051
Iteration: 5 || Loss: 75.43374904700933
Iteration: 6 || Loss: 75.43374904700933
saving ADAM checkpoint...
Sum of params:-104.326035
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 75.43374904700933
Iteration: 2 || Loss: 75.4204357866333
Iteration: 3 || Loss: 75.37855576777066
Iteration: 4 || Loss: 75.01835004250046
Iteration: 5 || Loss: 74.89339564968594
Iteration: 6 || Loss: 74.80940697069855
Iteration: 7 || Loss: 74.69680485275936
Iteration: 8 || Loss: 74.6651363744939
Iteration: 9 || Loss: 74.62558507776903
Iteration: 10 || Loss: 74.59218528189244
Iteration: 11 || Loss: 74.51605044309909
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.444725
Epoch 135 loss:74.51605044309909
MSE loss S1.0285360868536937
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.333021494795894
MSE loss S0.3629005302788612
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:133.99497983099712
MSE loss S2.494704488077402
waveform batch: 2/2
Test loss - extrapolation:73.39220948804093
MSE loss S1.5949671118687414
Epoch 135 mean train loss:3.6139499708023286
Epoch 135 mean test loss - interpolation:4.222170249132649
Epoch 135 mean test loss - extrapolation:17.282265776586502
Start training epoch 136
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.444725
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.531783446455319
Iteration: 2 || Loss: 6.530445414951156
Iteration: 3 || Loss: 6.529175381711224
Iteration: 4 || Loss: 6.527854693368113
Iteration: 5 || Loss: 6.526542353272982
Iteration: 6 || Loss: 6.526542353272982
saving ADAM checkpoint...
Sum of params:-104.44476
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.526542353272982
Iteration: 2 || Loss: 6.4586870975725255
Iteration: 3 || Loss: 6.435563041640279
Iteration: 4 || Loss: 6.4145922495926415
Iteration: 5 || Loss: 6.317824112355135
Iteration: 6 || Loss: 6.265189417910934
Iteration: 7 || Loss: 6.242180447922
Iteration: 8 || Loss: 6.231493912165839
Iteration: 9 || Loss: 6.226362032522047
Iteration: 10 || Loss: 6.217569740751615
Iteration: 11 || Loss: 6.193162339883285
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.45029
Epoch 136 loss:6.193162339883285
MSE loss S0.22557390763169669
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.45029
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.453316247317844
Iteration: 2 || Loss: 24.452998424681024
Iteration: 3 || Loss: 24.45268973142488
Iteration: 4 || Loss: 24.452391230626322
Iteration: 5 || Loss: 24.45206772704301
Iteration: 6 || Loss: 24.45206772704301
saving ADAM checkpoint...
Sum of params:-104.45048
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.45206772704301
Iteration: 2 || Loss: 24.44550569200916
Iteration: 3 || Loss: 24.387578977174574
Iteration: 4 || Loss: 24.34376145974403
Iteration: 5 || Loss: 24.203214751375643
Iteration: 6 || Loss: 24.182336827737
Iteration: 7 || Loss: 24.154840910617057
Iteration: 8 || Loss: 24.118961412185808
Iteration: 9 || Loss: 24.109226198018437
Iteration: 10 || Loss: 24.079878790729325
Iteration: 11 || Loss: 24.071277408938688
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.36683
Epoch 136 loss:24.071277408938688
MSE loss S0.43187982660824964
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.36683
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 75.24361477607167
Iteration: 2 || Loss: 75.243107505731
Iteration: 3 || Loss: 75.24255221926012
Iteration: 4 || Loss: 75.24213545669434
Iteration: 5 || Loss: 75.24166673359298
Iteration: 6 || Loss: 75.24166673359298
saving ADAM checkpoint...
Sum of params:-104.36698
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 75.24166673359298
Iteration: 2 || Loss: 75.22840906944596
Iteration: 3 || Loss: 75.18956758370531
Iteration: 4 || Loss: 74.82912213140992
Iteration: 5 || Loss: 74.71463581935903
Iteration: 6 || Loss: 74.63515248468197
Iteration: 7 || Loss: 74.53224039713292
Iteration: 8 || Loss: 74.49800045460823
Iteration: 9 || Loss: 74.45000779013449
Iteration: 10 || Loss: 74.38906998886984
Iteration: 11 || Loss: 74.3387384245577
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.49575
Epoch 136 loss:74.3387384245577
MSE loss S1.0208041991006198
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.312859687711533
MSE loss S0.35782074453460433
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:133.27857916487463
MSE loss S2.4857371406962714
waveform batch: 2/2
Test loss - extrapolation:72.76214737168134
MSE loss S1.5811060072151402
Epoch 136 mean train loss:3.607006143909644
Epoch 136 mean test loss - interpolation:4.2188099479519225
Epoch 136 mean test loss - extrapolation:17.170060544712996
Start training epoch 137
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.49575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.561003570433762
Iteration: 2 || Loss: 6.559266349459848
Iteration: 3 || Loss: 6.557659671496232
Iteration: 4 || Loss: 6.556011111453794
Iteration: 5 || Loss: 6.554339411362028
Iteration: 6 || Loss: 6.554339411362028
saving ADAM checkpoint...
Sum of params:-104.49577
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.554339411362028
Iteration: 2 || Loss: 6.446760296996919
Iteration: 3 || Loss: 6.424502407842552
Iteration: 4 || Loss: 6.3996622504871965
Iteration: 5 || Loss: 6.303539855516466
Iteration: 6 || Loss: 6.259722516153812
Iteration: 7 || Loss: 6.237961573174148
Iteration: 8 || Loss: 6.2313366952649645
Iteration: 9 || Loss: 6.223835754265938
Iteration: 10 || Loss: 6.213864719826036
Iteration: 11 || Loss: 6.209380108345115
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.50448
Epoch 137 loss:6.209380108345115
MSE loss S0.23039989001047212
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.50448
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.485359263442795
Iteration: 2 || Loss: 24.4844419519129
Iteration: 3 || Loss: 24.48362661422764
Iteration: 4 || Loss: 24.48279850670647
Iteration: 5 || Loss: 24.4820067363288
Iteration: 6 || Loss: 24.4820067363288
saving ADAM checkpoint...
Sum of params:-104.50451
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.4820067363288
Iteration: 2 || Loss: 24.470108571148405
Iteration: 3 || Loss: 24.439798444086776
Iteration: 4 || Loss: 24.403800883314155
Iteration: 5 || Loss: 24.195555414527732
Iteration: 6 || Loss: 24.165825977256034
Iteration: 7 || Loss: 24.13699147991695
Iteration: 8 || Loss: 24.107298248003758
Iteration: 9 || Loss: 24.097702203701193
Iteration: 10 || Loss: 24.074087909280948
Iteration: 11 || Loss: 24.06322295351194
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.41484
Epoch 137 loss:24.06322295351194
MSE loss S0.42813838483141786
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.41484
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 75.14769025631789
Iteration: 2 || Loss: 75.14704984888134
Iteration: 3 || Loss: 75.14639125759624
Iteration: 4 || Loss: 75.14576153747052
Iteration: 5 || Loss: 75.14508500458884
Iteration: 6 || Loss: 75.14508500458884
saving ADAM checkpoint...
Sum of params:-104.415
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 75.14508500458884
Iteration: 2 || Loss: 75.13216449866994
Iteration: 3 || Loss: 75.09062485677289
Iteration: 4 || Loss: 74.66059687180189
Iteration: 5 || Loss: 74.55724071689312
Iteration: 6 || Loss: 74.46919696754146
Iteration: 7 || Loss: 74.33519396602622
Iteration: 8 || Loss: 74.30581593567877
Iteration: 9 || Loss: 74.27025268275327
Iteration: 10 || Loss: 74.24307190065092
Iteration: 11 || Loss: 74.17935899228594
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.51334
Epoch 137 loss:74.17935899228594
MSE loss S1.0303041136006024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.251944608948282
MSE loss S0.36493252287671
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:133.05359195894323
MSE loss S2.4849183634222705
waveform batch: 2/2
Test loss - extrapolation:72.45420221739658
MSE loss S1.578996313005566
Epoch 137 mean train loss:3.601791794970448
Epoch 137 mean test loss - interpolation:4.208657434824714
Epoch 137 mean test loss - extrapolation:17.125649514694985
Start training epoch 138
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.51334
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.494324800695321
Iteration: 2 || Loss: 6.493199344947347
Iteration: 3 || Loss: 6.492067354816477
Iteration: 4 || Loss: 6.491027227929673
Iteration: 5 || Loss: 6.4899410689871475
Iteration: 6 || Loss: 6.4899410689871475
saving ADAM checkpoint...
Sum of params:-104.51328
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4899410689871475
Iteration: 2 || Loss: 6.440934057253863
Iteration: 3 || Loss: 6.418320098749771
Iteration: 4 || Loss: 6.400467885306645
Iteration: 5 || Loss: 6.303634470162606
Iteration: 6 || Loss: 6.257180394662077
Iteration: 7 || Loss: 6.235930227332633
Iteration: 8 || Loss: 6.224684407412985
Iteration: 9 || Loss: 6.218596196528926
Iteration: 10 || Loss: 6.211519529927258
Iteration: 11 || Loss: 6.18279896384053
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.52143
Epoch 138 loss:6.18279896384053
MSE loss S0.22197643217564872
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.52143
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.438238847847156
Iteration: 2 || Loss: 24.438023644424852
Iteration: 3 || Loss: 24.437761867198486
Iteration: 4 || Loss: 24.43749248907534
Iteration: 5 || Loss: 24.43726244006325
Iteration: 6 || Loss: 24.43726244006325
saving ADAM checkpoint...
Sum of params:-104.52146
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.43726244006325
Iteration: 2 || Loss: 24.40239180534516
Iteration: 3 || Loss: 24.38508205218745
Iteration: 4 || Loss: 24.32409701270198
Iteration: 5 || Loss: 24.18081488050595
Iteration: 6 || Loss: 24.149459282681043
Iteration: 7 || Loss: 24.126092851768423
Iteration: 8 || Loss: 24.094922298603873
Iteration: 9 || Loss: 24.075395903709598
Iteration: 10 || Loss: 24.05925876642665
Iteration: 11 || Loss: 24.041412456871463
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.443535
Epoch 138 loss:24.041412456871463
MSE loss S0.4274725196656681
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.443535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.93288091503427
Iteration: 2 || Loss: 74.93223930439126
Iteration: 3 || Loss: 74.93160814535291
Iteration: 4 || Loss: 74.93107908155983
Iteration: 5 || Loss: 74.9305416733957
Iteration: 6 || Loss: 74.9305416733957
saving ADAM checkpoint...
Sum of params:-104.4437
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.9305416733957
Iteration: 2 || Loss: 74.91852661773585
Iteration: 3 || Loss: 74.88025973861448
Iteration: 4 || Loss: 74.52508574891732
Iteration: 5 || Loss: 74.3864643746243
Iteration: 6 || Loss: 74.30714021410748
Iteration: 7 || Loss: 74.19973139509943
Iteration: 8 || Loss: 74.16992934403488
Iteration: 9 || Loss: 74.1306783509143
Iteration: 10 || Loss: 74.09924535779506
Iteration: 11 || Loss: 74.02941451408388
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.557014
Epoch 138 loss:74.02941451408388
MSE loss S1.0256782842401935
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.233511914133604
MSE loss S0.36152479245770774
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:132.55429360455702
MSE loss S2.4736834266007692
waveform batch: 2/2
Test loss - extrapolation:72.05584077531486
MSE loss S1.5721901148002282
Epoch 138 mean train loss:3.594952618441237
Epoch 138 mean test loss - interpolation:4.205585319022267
Epoch 138 mean test loss - extrapolation:17.05084453165599
Start training epoch 139
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.557014
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.510542600512806
Iteration: 2 || Loss: 6.509217903093996
Iteration: 3 || Loss: 6.507971347553059
Iteration: 4 || Loss: 6.506677736350244
Iteration: 5 || Loss: 6.50539788590437
Iteration: 6 || Loss: 6.50539788590437
saving ADAM checkpoint...
Sum of params:-104.55703
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.50539788590437
Iteration: 2 || Loss: 6.442047460954859
Iteration: 3 || Loss: 6.417925019739593
Iteration: 4 || Loss: 6.393835333100371
Iteration: 5 || Loss: 6.300522217082916
Iteration: 6 || Loss: 6.250478641234675
Iteration: 7 || Loss: 6.227267909566539
Iteration: 8 || Loss: 6.216846430053409
Iteration: 9 || Loss: 6.212200763422775
Iteration: 10 || Loss: 6.204032249797421
Iteration: 11 || Loss: 6.179256692935073
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.564285
Epoch 139 loss:6.179256692935073
MSE loss S0.2227665601297993
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.564285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.41658178373741
Iteration: 2 || Loss: 24.41624282073325
Iteration: 3 || Loss: 24.415940987610355
Iteration: 4 || Loss: 24.41566305478091
Iteration: 5 || Loss: 24.41524537838419
Iteration: 6 || Loss: 24.41524537838419
saving ADAM checkpoint...
Sum of params:-104.564476
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.41524537838419
Iteration: 2 || Loss: 24.40766928257033
Iteration: 3 || Loss: 24.3521031321267
Iteration: 4 || Loss: 24.299294328405395
Iteration: 5 || Loss: 24.158684088521483
Iteration: 6 || Loss: 24.135721722929432
Iteration: 7 || Loss: 24.106721314208663
Iteration: 8 || Loss: 24.075007990880007
Iteration: 9 || Loss: 24.06305317829211
Iteration: 10 || Loss: 24.029880532890783
Iteration: 11 || Loss: 24.023358544220365
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.48407
Epoch 139 loss:24.023358544220365
MSE loss S0.4297285268777663
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.48407
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.7396150868759
Iteration: 2 || Loss: 74.73910085995395
Iteration: 3 || Loss: 74.7386018848677
Iteration: 4 || Loss: 74.73815865719895
Iteration: 5 || Loss: 74.73763601226172
Iteration: 6 || Loss: 74.73763601226172
saving ADAM checkpoint...
Sum of params:-104.4842
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.73763601226172
Iteration: 2 || Loss: 74.7247748651241
Iteration: 3 || Loss: 74.68647884237795
Iteration: 4 || Loss: 74.32903040830934
Iteration: 5 || Loss: 74.2186285557266
Iteration: 6 || Loss: 74.1426092547179
Iteration: 7 || Loss: 74.04616307919918
Iteration: 8 || Loss: 74.01290832608206
Iteration: 9 || Loss: 73.96943169832196
Iteration: 10 || Loss: 73.912114542713
Iteration: 11 || Loss: 73.86869054930705
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.60105
Epoch 139 loss:73.86869054930705
MSE loss S1.0191953533518883
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.2102004074362
MSE loss S0.35709362479086804
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:131.93465983782548
MSE loss S2.4661692534474544
waveform batch: 2/2
Test loss - extrapolation:71.50244145011297
MSE loss S1.560329513120501
Epoch 139 mean train loss:3.5886657167745684
Epoch 139 mean test loss - interpolation:4.2017000679060335
Epoch 139 mean test loss - extrapolation:16.95309177399487
Start training epoch 140
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.60105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.535822006157593
Iteration: 2 || Loss: 6.534158938406776
Iteration: 3 || Loss: 6.532606410295395
Iteration: 4 || Loss: 6.531039449288299
Iteration: 5 || Loss: 6.529420614951739
Iteration: 6 || Loss: 6.529420614951739
saving ADAM checkpoint...
Sum of params:-104.60109
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.529420614951739
Iteration: 2 || Loss: 6.43077392657408
Iteration: 3 || Loss: 6.406165214957486
Iteration: 4 || Loss: 6.377166629620485
Iteration: 5 || Loss: 6.288750864587893
Iteration: 6 || Loss: 6.244646749182989
Iteration: 7 || Loss: 6.222630696610521
Iteration: 8 || Loss: 6.215729826631182
Iteration: 9 || Loss: 6.20937228006547
Iteration: 10 || Loss: 6.200109196011074
Iteration: 11 || Loss: 6.19366637120486
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.609535
Epoch 140 loss:6.19366637120486
MSE loss S0.22881493387567742
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.609535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.429384083239427
Iteration: 2 || Loss: 24.4285920323833
Iteration: 3 || Loss: 24.427719450314374
Iteration: 4 || Loss: 24.42688996866427
Iteration: 5 || Loss: 24.426113312754023
Iteration: 6 || Loss: 24.426113312754023
saving ADAM checkpoint...
Sum of params:-104.609604
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.426113312754023
Iteration: 2 || Loss: 24.413021904425257
Iteration: 3 || Loss: 24.365079790707107
Iteration: 4 || Loss: 24.336964223383298
Iteration: 5 || Loss: 24.144806262250455
Iteration: 6 || Loss: 24.11418149870431
Iteration: 7 || Loss: 24.08393808828504
Iteration: 8 || Loss: 24.055790013410103
Iteration: 9 || Loss: 24.04861282339153
Iteration: 10 || Loss: 24.029055846777894
Iteration: 11 || Loss: 24.017048767411943
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.5288
Epoch 140 loss:24.017048767411943
MSE loss S0.42613620988075485
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.5288
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.6262539380465
Iteration: 2 || Loss: 74.62551173945354
Iteration: 3 || Loss: 74.62485858637069
Iteration: 4 || Loss: 74.62421489798122
Iteration: 5 || Loss: 74.62356894217167
Iteration: 6 || Loss: 74.62356894217167
saving ADAM checkpoint...
Sum of params:-104.528946
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.62356894217167
Iteration: 2 || Loss: 74.61152447267244
Iteration: 3 || Loss: 74.57189332157803
Iteration: 4 || Loss: 74.16465991678976
Iteration: 5 || Loss: 74.07037396710649
Iteration: 6 || Loss: 73.98682307522598
Iteration: 7 || Loss: 73.86891923102903
Iteration: 8 || Loss: 73.83961782126315
Iteration: 9 || Loss: 73.80811774844598
Iteration: 10 || Loss: 73.78308740432936
Iteration: 11 || Loss: 73.72264496991056
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.61971
Epoch 140 loss:73.72264496991056
MSE loss S1.0299894558761689
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.155422527601637
MSE loss S0.36381635364412984
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:131.70566846616092
MSE loss S2.468338945597094
waveform batch: 2/2
Test loss - extrapolation:71.20348992546296
MSE loss S1.5591356422784646
Epoch 140 mean train loss:3.5839089692595643
Epoch 140 mean test loss - interpolation:4.1925704212669395
Epoch 140 mean test loss - extrapolation:16.90909653263532
Start training epoch 141
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.61971
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.473482840623553
Iteration: 2 || Loss: 6.472349882850919
Iteration: 3 || Loss: 6.4712800491724245
Iteration: 4 || Loss: 6.470200136721623
Iteration: 5 || Loss: 6.469100762851163
Iteration: 6 || Loss: 6.469100762851163
saving ADAM checkpoint...
Sum of params:-104.61967
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.469100762851163
Iteration: 2 || Loss: 6.422300043934615
Iteration: 3 || Loss: 6.398104916447142
Iteration: 4 || Loss: 6.376753395919042
Iteration: 5 || Loss: 6.284253255359695
Iteration: 6 || Loss: 6.2396621898322895
Iteration: 7 || Loss: 6.221807342813284
Iteration: 8 || Loss: 6.210928597617218
Iteration: 9 || Loss: 6.204173646761295
Iteration: 10 || Loss: 6.198556572518296
Iteration: 11 || Loss: 6.171273639153587
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.62913
Epoch 141 loss:6.171273639153587
MSE loss S0.22020636968163554
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.62913
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.39765754344658
Iteration: 2 || Loss: 24.397390785837295
Iteration: 3 || Loss: 24.397148224494778
Iteration: 4 || Loss: 24.396940996215353
Iteration: 5 || Loss: 24.39665082975351
Iteration: 6 || Loss: 24.39665082975351
saving ADAM checkpoint...
Sum of params:-104.62924
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.39665082975351
Iteration: 2 || Loss: 24.381345804964603
Iteration: 3 || Loss: 24.34124399874624
Iteration: 4 || Loss: 24.278185891179348
Iteration: 5 || Loss: 24.133437067516724
Iteration: 6 || Loss: 24.10414763172941
Iteration: 7 || Loss: 24.07645381847976
Iteration: 8 || Loss: 24.0451239738416
Iteration: 9 || Loss: 24.03223041691111
Iteration: 10 || Loss: 24.007756738987595
Iteration: 11 || Loss: 23.993772810836592
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.55195
Epoch 141 loss:23.993772810836592
MSE loss S0.42723942350035016
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.55195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.43651566702856
Iteration: 2 || Loss: 74.43597349893882
Iteration: 3 || Loss: 74.4355050519344
Iteration: 4 || Loss: 74.43492206167606
Iteration: 5 || Loss: 74.43441330890992
Iteration: 6 || Loss: 74.43441330890992
saving ADAM checkpoint...
Sum of params:-104.552124
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.43441330890992
Iteration: 2 || Loss: 74.4224778484717
Iteration: 3 || Loss: 74.38528498976608
Iteration: 4 || Loss: 74.05620691480546
Iteration: 5 || Loss: 73.91880079342859
Iteration: 6 || Loss: 73.84421737770556
Iteration: 7 || Loss: 73.74846309089446
Iteration: 8 || Loss: 73.71726633540132
Iteration: 9 || Loss: 73.67556867324754
Iteration: 10 || Loss: 73.63010029783602
Iteration: 11 || Loss: 73.58473451405855
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.66201
Epoch 141 loss:73.58473451405855
MSE loss S1.0199404779877628
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.14513823792265
MSE loss S0.3583002413578749
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:131.21352154961994
MSE loss S2.4538916682365066
waveform batch: 2/2
Test loss - extrapolation:70.80695511765511
MSE loss S1.5495089696346855
Epoch 141 mean train loss:3.577578653932715
Epoch 141 mean test loss - interpolation:4.190856372987109
Epoch 141 mean test loss - extrapolation:16.83503972227292
Start training epoch 142
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.66201
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.513463245816111
Iteration: 2 || Loss: 6.512037489617268
Iteration: 3 || Loss: 6.510575163488295
Iteration: 4 || Loss: 6.509109609707816
Iteration: 5 || Loss: 6.507741522997379
Iteration: 6 || Loss: 6.507741522997379
saving ADAM checkpoint...
Sum of params:-104.66207
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.507741522997379
Iteration: 2 || Loss: 6.428747040085612
Iteration: 3 || Loss: 6.403007914205457
Iteration: 4 || Loss: 6.3739241061581415
Iteration: 5 || Loss: 6.286641491907431
Iteration: 6 || Loss: 6.2366655455979
Iteration: 7 || Loss: 6.213629813526516
Iteration: 8 || Loss: 6.205254300545025
Iteration: 9 || Loss: 6.200386079363436
Iteration: 10 || Loss: 6.1920498163630935
Iteration: 11 || Loss: 6.173793377564895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.66989
Epoch 142 loss:6.173793377564895
MSE loss S0.22434830898179664
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.66989
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.364658107265285
Iteration: 2 || Loss: 24.363982480305886
Iteration: 3 || Loss: 24.363415293375855
Iteration: 4 || Loss: 24.36279758300809
Iteration: 5 || Loss: 24.36226398067848
Iteration: 6 || Loss: 24.36226398067848
saving ADAM checkpoint...
Sum of params:-104.67006
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.36226398067848
Iteration: 2 || Loss: 24.352641719526233
Iteration: 3 || Loss: 24.277918992143828
Iteration: 4 || Loss: 24.250413008124482
Iteration: 5 || Loss: 24.103604114617234
Iteration: 6 || Loss: 24.08588869679219
Iteration: 7 || Loss: 24.059292283495385
Iteration: 8 || Loss: 24.029019258587688
Iteration: 9 || Loss: 24.020036729473514
Iteration: 10 || Loss: 24.00047384938097
Iteration: 11 || Loss: 23.983333545604175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.59252
Epoch 142 loss:23.983333545604175
MSE loss S0.4275305471432036
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.59252
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.31415338218787
Iteration: 2 || Loss: 74.31347398237152
Iteration: 3 || Loss: 74.31284909267075
Iteration: 4 || Loss: 74.31220411031371
Iteration: 5 || Loss: 74.31157550867576
Iteration: 6 || Loss: 74.31157550867576
saving ADAM checkpoint...
Sum of params:-104.592674
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.31157550867576
Iteration: 2 || Loss: 74.29882068688734
Iteration: 3 || Loss: 74.25815397287765
Iteration: 4 || Loss: 73.88568684305385
Iteration: 5 || Loss: 73.78196342972763
Iteration: 6 || Loss: 73.70389519298412
Iteration: 7 || Loss: 73.59971284975894
Iteration: 8 || Loss: 73.5683101295511
Iteration: 9 || Loss: 73.53666186946421
Iteration: 10 || Loss: 73.51082754191812
Iteration: 11 || Loss: 73.45257107817359
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.68431
Epoch 142 loss:73.45257107817359
MSE loss S1.0251435535672413
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.097506379374416
MSE loss S0.36179977102083033
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:130.926469129385
MSE loss S2.4515253215360335
waveform batch: 2/2
Test loss - extrapolation:70.47733891040241
MSE loss S1.5450687372617775
Epoch 142 mean train loss:3.5727482069428507
Epoch 142 mean test loss - interpolation:4.182917729895736
Epoch 142 mean test loss - extrapolation:16.783650669982283
Start training epoch 143
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.68431
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.464686348295783
Iteration: 2 || Loss: 6.463552717884305
Iteration: 3 || Loss: 6.462505879275056
Iteration: 4 || Loss: 6.461362877774885
Iteration: 5 || Loss: 6.460247549562336
Iteration: 6 || Loss: 6.460247549562336
saving ADAM checkpoint...
Sum of params:-104.68424
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.460247549562336
Iteration: 2 || Loss: 6.412459801450009
Iteration: 3 || Loss: 6.387574256774018
Iteration: 4 || Loss: 6.363052513693898
Iteration: 5 || Loss: 6.2796768391566005
Iteration: 6 || Loss: 6.230578644842716
Iteration: 7 || Loss: 6.209286794234541
Iteration: 8 || Loss: 6.199042081934172
Iteration: 9 || Loss: 6.194204625424728
Iteration: 10 || Loss: 6.187519005475458
Iteration: 11 || Loss: 6.162691958398968
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.694855
Epoch 143 loss:6.162691958398968
MSE loss S0.21950889146246305
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.694855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.378780843501282
Iteration: 2 || Loss: 24.37845676567177
Iteration: 3 || Loss: 24.378133673261676
Iteration: 4 || Loss: 24.377871839078917
Iteration: 5 || Loss: 24.377529675696263
Iteration: 6 || Loss: 24.377529675696263
saving ADAM checkpoint...
Sum of params:-104.69503
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.377529675696263
Iteration: 2 || Loss: 24.36606474058621
Iteration: 3 || Loss: 24.315405137127875
Iteration: 4 || Loss: 24.248846387534662
Iteration: 5 || Loss: 24.10098836296248
Iteration: 6 || Loss: 24.07424983260148
Iteration: 7 || Loss: 24.046239401243163
Iteration: 8 || Loss: 24.01870230509307
Iteration: 9 || Loss: 24.003676920248296
Iteration: 10 || Loss: 23.972356193725975
Iteration: 11 || Loss: 23.964944557158702
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.62049
Epoch 143 loss:23.964944557158702
MSE loss S0.42672769898124474
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.62049
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.1498825005791
Iteration: 2 || Loss: 74.14944117629763
Iteration: 3 || Loss: 74.14885807422596
Iteration: 4 || Loss: 74.14841991747318
Iteration: 5 || Loss: 74.14788202807058
Iteration: 6 || Loss: 74.14788202807058
saving ADAM checkpoint...
Sum of params:-104.620636
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.14788202807058
Iteration: 2 || Loss: 74.1357428560701
Iteration: 3 || Loss: 74.09855514540357
Iteration: 4 || Loss: 73.76928156675795
Iteration: 5 || Loss: 73.64494424892656
Iteration: 6 || Loss: 73.57124375010488
Iteration: 7 || Loss: 73.47864668042155
Iteration: 8 || Loss: 73.44767476929704
Iteration: 9 || Loss: 73.40919171511959
Iteration: 10 || Loss: 73.36713406083368
Iteration: 11 || Loss: 73.32345224152539
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.72336
Epoch 143 loss:73.32345224152539
MSE loss S1.0186743970843013
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.084224771226427
MSE loss S0.3575543917523494
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:130.48061119655267
MSE loss S2.4431186422006306
waveform batch: 2/2
Test loss - extrapolation:70.08889587983431
MSE loss S1.5372687294472032
Epoch 143 mean train loss:3.5672789226580366
Epoch 143 mean test loss - interpolation:4.180704128537738
Epoch 143 mean test loss - extrapolation:16.714125589698913
Start training epoch 144
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.72336
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.500499277913702
Iteration: 2 || Loss: 6.499107995521111
Iteration: 3 || Loss: 6.497695782405476
Iteration: 4 || Loss: 6.49630043452531
Iteration: 5 || Loss: 6.494913542609202
Iteration: 6 || Loss: 6.494913542609202
saving ADAM checkpoint...
Sum of params:-104.72337
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.494913542609202
Iteration: 2 || Loss: 6.421056347684467
Iteration: 3 || Loss: 6.393635744841696
Iteration: 4 || Loss: 6.361010654782007
Iteration: 5 || Loss: 6.277262743029947
Iteration: 6 || Loss: 6.226234615760312
Iteration: 7 || Loss: 6.203923805161316
Iteration: 8 || Loss: 6.195438316978575
Iteration: 9 || Loss: 6.190860798593846
Iteration: 10 || Loss: 6.183059540741661
Iteration: 11 || Loss: 6.163386733210582
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.732376
Epoch 144 loss:6.163386733210582
MSE loss S0.22248640404175507
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.732376
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.35095761531415
Iteration: 2 || Loss: 24.35033551269982
Iteration: 3 || Loss: 24.34967869312631
Iteration: 4 || Loss: 24.349146161701693
Iteration: 5 || Loss: 24.348620846703106
Iteration: 6 || Loss: 24.348620846703106
saving ADAM checkpoint...
Sum of params:-104.73255
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.348620846703106
Iteration: 2 || Loss: 24.3387120363966
Iteration: 3 || Loss: 24.264492020365925
Iteration: 4 || Loss: 24.224200288984402
Iteration: 5 || Loss: 24.078469502288552
Iteration: 6 || Loss: 24.05861885508796
Iteration: 7 || Loss: 24.034228316723883
Iteration: 8 || Loss: 24.005437054214205
Iteration: 9 || Loss: 23.992141539336842
Iteration: 10 || Loss: 23.97664628181409
Iteration: 11 || Loss: 23.955161587825856
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.65825
Epoch 144 loss:23.955161587825856
MSE loss S0.42715286906329997
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.65825
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.04077031764449
Iteration: 2 || Loss: 74.04018547936175
Iteration: 3 || Loss: 74.03952747934719
Iteration: 4 || Loss: 74.03892719475367
Iteration: 5 || Loss: 74.03833214348478
Iteration: 6 || Loss: 74.03833214348478
saving ADAM checkpoint...
Sum of params:-104.658394
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.03833214348478
Iteration: 2 || Loss: 74.02525424805128
Iteration: 3 || Loss: 73.98345540990061
Iteration: 4 || Loss: 73.63874364310134
Iteration: 5 || Loss: 73.51790176464645
Iteration: 6 || Loss: 73.44063813977918
Iteration: 7 || Loss: 73.34211315955842
Iteration: 8 || Loss: 73.30988137090164
Iteration: 9 || Loss: 73.28089607435498
Iteration: 10 || Loss: 73.25500737409084
Iteration: 11 || Loss: 73.20259251805852
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.74433
Epoch 144 loss:73.20259251805852
MSE loss S1.0227317018202378
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.041182721583414
MSE loss S0.3607444159100686
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:130.21900313087113
MSE loss S2.439705616541171
waveform batch: 2/2
Test loss - extrapolation:69.7950304935841
MSE loss S1.5324514910050064
Epoch 144 mean train loss:3.562797959968792
Epoch 144 mean test loss - interpolation:4.173530453597236
Epoch 144 mean test loss - extrapolation:16.66783613537127
Start training epoch 145
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.74433
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.464676518068458
Iteration: 2 || Loss: 6.463501201208562
Iteration: 3 || Loss: 6.462386875300426
Iteration: 4 || Loss: 6.461301202402076
Iteration: 5 || Loss: 6.460185539067773
Iteration: 6 || Loss: 6.460185539067773
saving ADAM checkpoint...
Sum of params:-104.744354
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.460185539067773
Iteration: 2 || Loss: 6.410540566702358
Iteration: 3 || Loss: 6.383738978320098
Iteration: 4 || Loss: 6.354133179980087
Iteration: 5 || Loss: 6.272379985959507
Iteration: 6 || Loss: 6.218162555965039
Iteration: 7 || Loss: 6.199106674905809
Iteration: 8 || Loss: 6.189209610209885
Iteration: 9 || Loss: 6.184894487155905
Iteration: 10 || Loss: 6.178031340350411
Iteration: 11 || Loss: 6.1548514132074965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.75585
Epoch 145 loss:6.1548514132074965
MSE loss S0.2187770369168791
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.75585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.362386078304247
Iteration: 2 || Loss: 24.362045083479995
Iteration: 3 || Loss: 24.36157705911359
Iteration: 4 || Loss: 24.36132547406777
Iteration: 5 || Loss: 24.36090729497638
Iteration: 6 || Loss: 24.36090729497638
saving ADAM checkpoint...
Sum of params:-104.75605
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.36090729497638
Iteration: 2 || Loss: 24.351992899317263
Iteration: 3 || Loss: 24.290866767389375
Iteration: 4 || Loss: 24.224138016609704
Iteration: 5 || Loss: 24.072035216270052
Iteration: 6 || Loss: 24.045686258148553
Iteration: 7 || Loss: 24.02066734622746
Iteration: 8 || Loss: 23.99446654857133
Iteration: 9 || Loss: 23.976601746792777
Iteration: 10 || Loss: 23.951095586058088
Iteration: 11 || Loss: 23.93855649603725
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.683846
Epoch 145 loss:23.93855649603725
MSE loss S0.426955932633695
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.683846
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.88385826930329
Iteration: 2 || Loss: 73.88325841274391
Iteration: 3 || Loss: 73.88278597491636
Iteration: 4 || Loss: 73.88224500755555
Iteration: 5 || Loss: 73.88171707929939
Iteration: 6 || Loss: 73.88171707929939
saving ADAM checkpoint...
Sum of params:-104.68399
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.88171707929939
Iteration: 2 || Loss: 73.8697855273057
Iteration: 3 || Loss: 73.83249267316835
Iteration: 4 || Loss: 73.51648105478047
Iteration: 5 || Loss: 73.39537651998847
Iteration: 6 || Loss: 73.32151232775517
Iteration: 7 || Loss: 73.23117452331269
Iteration: 8 || Loss: 73.19961192031346
Iteration: 9 || Loss: 73.16526093861819
Iteration: 10 || Loss: 73.13048771050714
Iteration: 11 || Loss: 73.08646862242458
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.77832
Epoch 145 loss:73.08646862242458
MSE loss S1.017859940600035
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:25.025160223891344
MSE loss S0.35731602259917317
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:129.84007748180235
MSE loss S2.433232620125086
waveform batch: 2/2
Test loss - extrapolation:69.44855940665768
MSE loss S1.5257974243540877
Epoch 145 mean train loss:3.5579267769541145
Epoch 145 mean test loss - interpolation:4.170860037315224
Epoch 145 mean test loss - extrapolation:16.607386407371667
Start training epoch 146
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.77832
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.490105754892277
Iteration: 2 || Loss: 6.488755422967529
Iteration: 3 || Loss: 6.487428130297945
Iteration: 4 || Loss: 6.486128942787712
Iteration: 5 || Loss: 6.48475247747084
Iteration: 6 || Loss: 6.48475247747084
saving ADAM checkpoint...
Sum of params:-104.77832
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.48475247747084
Iteration: 2 || Loss: 6.415856643608713
Iteration: 3 || Loss: 6.3868480124639495
Iteration: 4 || Loss: 6.351182451773271
Iteration: 5 || Loss: 6.268423539233335
Iteration: 6 || Loss: 6.213469557751711
Iteration: 7 || Loss: 6.194075338112617
Iteration: 8 || Loss: 6.185502655241927
Iteration: 9 || Loss: 6.181393088238433
Iteration: 10 || Loss: 6.173954075947398
Iteration: 11 || Loss: 6.153797120379456
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.78879
Epoch 146 loss:6.153797120379456
MSE loss S0.22049884009437265
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.78879
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.33941197405457
Iteration: 2 || Loss: 24.338810688597626
Iteration: 3 || Loss: 24.338255599330544
Iteration: 4 || Loss: 24.337717255986075
Iteration: 5 || Loss: 24.337133922949132
Iteration: 6 || Loss: 24.337133922949132
saving ADAM checkpoint...
Sum of params:-104.78898
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.337133922949132
Iteration: 2 || Loss: 24.327306725020673
Iteration: 3 || Loss: 24.252788651937777
Iteration: 4 || Loss: 24.20187092310151
Iteration: 5 || Loss: 24.053012986589994
Iteration: 6 || Loss: 24.031515459085025
Iteration: 7 || Loss: 24.00941811079078
Iteration: 8 || Loss: 23.980489041691236
Iteration: 9 || Loss: 23.964980730679414
Iteration: 10 || Loss: 23.95016015623491
Iteration: 11 || Loss: 23.928785697493886
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.71829
Epoch 146 loss:23.928785697493886
MSE loss S0.42694766339973
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.71829
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.78356332806491
Iteration: 2 || Loss: 73.78291070455688
Iteration: 3 || Loss: 73.78230133371635
Iteration: 4 || Loss: 73.78170439450324
Iteration: 5 || Loss: 73.78115821006666
Iteration: 6 || Loss: 73.78115821006666
saving ADAM checkpoint...
Sum of params:-104.71842
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.78115821006666
Iteration: 2 || Loss: 73.76799011050603
Iteration: 3 || Loss: 73.72587746540184
Iteration: 4 || Loss: 73.40526790534568
Iteration: 5 || Loss: 73.27831296583331
Iteration: 6 || Loss: 73.20168866006301
Iteration: 7 || Loss: 73.10969890667809
Iteration: 8 || Loss: 73.07682243961692
Iteration: 9 || Loss: 73.04983631610641
Iteration: 10 || Loss: 73.02406461078294
Iteration: 11 || Loss: 72.97491315999518
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.80165
Epoch 146 loss:72.97491315999518
MSE loss S1.0208531457669885
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.989589539452933
MSE loss S0.35931826235450515
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:129.56760377151446
MSE loss S2.4303976968517595
waveform batch: 2/2
Test loss - extrapolation:69.16007621343452
MSE loss S1.5214009172419718
Epoch 146 mean train loss:3.5537067578575354
Epoch 146 mean test loss - interpolation:4.164931589908822
Epoch 146 mean test loss - extrapolation:16.560639998745746
Start training epoch 147
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.80165
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.465870140708969
Iteration: 2 || Loss: 6.464643335250877
Iteration: 3 || Loss: 6.4635632576937745
Iteration: 4 || Loss: 6.462415263614501
Iteration: 5 || Loss: 6.461145464025454
Iteration: 6 || Loss: 6.461145464025454
saving ADAM checkpoint...
Sum of params:-104.80166
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.461145464025454
Iteration: 2 || Loss: 6.408782558759276
Iteration: 3 || Loss: 6.37975438949959
Iteration: 4 || Loss: 6.3449099166852765
Iteration: 5 || Loss: 6.263199233532047
Iteration: 6 || Loss: 6.206489211503018
Iteration: 7 || Loss: 6.189827873590486
Iteration: 8 || Loss: 6.180174571446249
Iteration: 9 || Loss: 6.176398562347959
Iteration: 10 || Loss: 6.169295942251378
Iteration: 11 || Loss: 6.1477101260216624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.81363
Epoch 147 loss:6.1477101260216624
MSE loss S0.21805654366688784
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.81363
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.343730156754766
Iteration: 2 || Loss: 24.343258149064074
Iteration: 3 || Loss: 24.342799297001037
Iteration: 4 || Loss: 24.34243175213791
Iteration: 5 || Loss: 24.341957048634256
Iteration: 6 || Loss: 24.341957048634256
saving ADAM checkpoint...
Sum of params:-104.813805
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.341957048634256
Iteration: 2 || Loss: 24.33328900465762
Iteration: 3 || Loss: 24.264302382096748
Iteration: 4 || Loss: 24.199027113007627
Iteration: 5 || Loss: 24.0449610191067
Iteration: 6 || Loss: 24.019564602905422
Iteration: 7 || Loss: 23.997346835432772
Iteration: 8 || Loss: 23.96969820278891
Iteration: 9 || Loss: 23.95168882740245
Iteration: 10 || Loss: 23.934388518925633
Iteration: 11 || Loss: 23.915343200246376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.74405
Epoch 147 loss:23.915343200246376
MSE loss S0.4279240558270445
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.74405
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.66263606069212
Iteration: 2 || Loss: 73.66206791560543
Iteration: 3 || Loss: 73.66155667350311
Iteration: 4 || Loss: 73.6609488742972
Iteration: 5 || Loss: 73.66041615703625
Iteration: 6 || Loss: 73.66041615703625
saving ADAM checkpoint...
Sum of params:-104.74417
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.66041615703625
Iteration: 2 || Loss: 73.64801876231716
Iteration: 3 || Loss: 73.60898913629815
Iteration: 4 || Loss: 73.30029628661113
Iteration: 5 || Loss: 73.17157453554069
Iteration: 6 || Loss: 73.09460336321492
Iteration: 7 || Loss: 73.00507628055666
Iteration: 8 || Loss: 72.97187119921487
Iteration: 9 || Loss: 72.94226916660622
Iteration: 10 || Loss: 72.91239916162424
Iteration: 11 || Loss: 72.86887069909125
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.8304
Epoch 147 loss:72.86887069909125
MSE loss S1.0178177262751822
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.969489684544886
MSE loss S0.35706442376338776
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:129.24927294093757
MSE loss S2.425447714604851
waveform batch: 2/2
Test loss - extrapolation:68.85363479318114
MSE loss S1.515802858007557
Epoch 147 mean train loss:3.5493766905296305
Epoch 147 mean test loss - interpolation:4.161581614090815
Epoch 147 mean test loss - extrapolation:16.508575644509893
Start training epoch 148
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.8304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.480580263386179
Iteration: 2 || Loss: 6.4792841104617525
Iteration: 3 || Loss: 6.47801307451069
Iteration: 4 || Loss: 6.476703721691451
Iteration: 5 || Loss: 6.475382294471206
Iteration: 6 || Loss: 6.475382294471206
saving ADAM checkpoint...
Sum of params:-104.83042
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.475382294471206
Iteration: 2 || Loss: 6.411633304335481
Iteration: 3 || Loss: 6.380377140671127
Iteration: 4 || Loss: 6.341238452893076
Iteration: 5 || Loss: 6.258508195715432
Iteration: 6 || Loss: 6.2011955126389475
Iteration: 7 || Loss: 6.185076746643206
Iteration: 8 || Loss: 6.176325107215391
Iteration: 9 || Loss: 6.17262090686329
Iteration: 10 || Loss: 6.1654407552412875
Iteration: 11 || Loss: 6.145772262818482
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.842064
Epoch 148 loss:6.145772262818482
MSE loss S0.219263869775359
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.842064
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.339080253175567
Iteration: 2 || Loss: 24.338616057877207
Iteration: 3 || Loss: 24.338031754957875
Iteration: 4 || Loss: 24.337501005691927
Iteration: 5 || Loss: 24.33691711624115
Iteration: 6 || Loss: 24.33691711624115
saving ADAM checkpoint...
Sum of params:-104.842255
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.33691711624115
Iteration: 2 || Loss: 24.326732410884755
Iteration: 3 || Loss: 24.245388723134486
Iteration: 4 || Loss: 24.188084065268388
Iteration: 5 || Loss: 24.029779813799703
Iteration: 6 || Loss: 24.00678777349763
Iteration: 7 || Loss: 23.98754778105792
Iteration: 8 || Loss: 23.957252010526396
Iteration: 9 || Loss: 23.940222351684934
Iteration: 10 || Loss: 23.92876240589119
Iteration: 11 || Loss: 23.907107264525656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.77691
Epoch 148 loss:23.907107264525656
MSE loss S0.426919150881951
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.77691
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.5326752155307
Iteration: 2 || Loss: 73.53205722343952
Iteration: 3 || Loss: 73.53144799811486
Iteration: 4 || Loss: 73.53083046697714
Iteration: 5 || Loss: 73.53020192405889
Iteration: 6 || Loss: 73.53020192405889
saving ADAM checkpoint...
Sum of params:-104.777016
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.53020192405889
Iteration: 2 || Loss: 73.51704828340341
Iteration: 3 || Loss: 73.47401106181523
Iteration: 4 || Loss: 73.18058310238463
Iteration: 5 || Loss: 73.05450045488054
Iteration: 6 || Loss: 72.97862041361259
Iteration: 7 || Loss: 72.89260940196638
Iteration: 8 || Loss: 72.86044048806893
Iteration: 9 || Loss: 72.83701001142144
Iteration: 10 || Loss: 72.81202523585335
Iteration: 11 || Loss: 72.76371931339621
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.85678
Epoch 148 loss:72.76371931339621
MSE loss S1.0197457244310764
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.942874195979456
MSE loss S0.35845194540249947
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:128.96676269810058
MSE loss S2.4225903788207708
waveform batch: 2/2
Test loss - extrapolation:68.56215717746038
MSE loss S1.5108209552347527
Epoch 148 mean train loss:3.545399960025529
Epoch 148 mean test loss - interpolation:4.157145699329909
Epoch 148 mean test loss - extrapolation:16.460743322963413
Start training epoch 149
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.85678
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.46820804412596
Iteration: 2 || Loss: 6.467006915574959
Iteration: 3 || Loss: 6.465833014824361
Iteration: 4 || Loss: 6.464604225863602
Iteration: 5 || Loss: 6.463477644335306
Iteration: 6 || Loss: 6.463477644335306
saving ADAM checkpoint...
Sum of params:-104.85679
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.463477644335306
Iteration: 2 || Loss: 6.411133651066391
Iteration: 3 || Loss: 6.378654146165241
Iteration: 4 || Loss: 6.337477246291362
Iteration: 5 || Loss: 6.254861663155652
Iteration: 6 || Loss: 6.196225112683756
Iteration: 7 || Loss: 6.181899325145327
Iteration: 8 || Loss: 6.172146859859133
Iteration: 9 || Loss: 6.168654282379981
Iteration: 10 || Loss: 6.161020257336002
Iteration: 11 || Loss: 6.141081890690967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.86908
Epoch 149 loss:6.141081890690967
MSE loss S0.2171779729253236
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.86908
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.33392742766982
Iteration: 2 || Loss: 24.333425096844874
Iteration: 3 || Loss: 24.332959367413643
Iteration: 4 || Loss: 24.332515253367035
Iteration: 5 || Loss: 24.331950533624173
Iteration: 6 || Loss: 24.331950533624173
saving ADAM checkpoint...
Sum of params:-104.86928
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.331950533624173
Iteration: 2 || Loss: 24.32276512536791
Iteration: 3 || Loss: 24.24417492595537
Iteration: 4 || Loss: 24.18095886652046
Iteration: 5 || Loss: 24.022977398441746
Iteration: 6 || Loss: 23.997484721942396
Iteration: 7 || Loss: 23.977866745085546
Iteration: 8 || Loss: 23.94799239262208
Iteration: 9 || Loss: 23.93004061289516
Iteration: 10 || Loss: 23.91781627884806
Iteration: 11 || Loss: 23.89566403610861
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.80307
Epoch 149 loss:23.89566403610861
MSE loss S0.42734112194779594
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.80307
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.44285153575323
Iteration: 2 || Loss: 73.4422262357343
Iteration: 3 || Loss: 73.44167751577587
Iteration: 4 || Loss: 73.44101878685234
Iteration: 5 || Loss: 73.44041039322532
Iteration: 6 || Loss: 73.44041039322532
saving ADAM checkpoint...
Sum of params:-104.803185
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.44041039322532
Iteration: 2 || Loss: 73.4274439693153
Iteration: 3 || Loss: 73.3856500989613
Iteration: 4 || Loss: 73.09489921953286
Iteration: 5 || Loss: 72.95660725839394
Iteration: 6 || Loss: 72.87865084944433
Iteration: 7 || Loss: 72.79175019073436
Iteration: 8 || Loss: 72.75855196181344
Iteration: 9 || Loss: 72.73399389823669
Iteration: 10 || Loss: 72.70842074369092
Iteration: 11 || Loss: 72.66247807393236
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.88339
Epoch 149 loss:72.66247807393236
MSE loss S1.0181422449674813
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.920891821473543
MSE loss S0.35720106979022637
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:128.67087431302446
MSE loss S2.418276548585928
waveform batch: 2/2
Test loss - extrapolation:68.27202459819732
MSE loss S1.5060786759065776
Epoch 149 mean train loss:3.541352551749377
Epoch 149 mean test loss - interpolation:4.153481970245591
Epoch 149 mean test loss - extrapolation:16.411908242601815
Start training epoch 150
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.88339
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.47046947233064
Iteration: 2 || Loss: 6.469248190799764
Iteration: 3 || Loss: 6.468043879941723
Iteration: 4 || Loss: 6.466876194824898
Iteration: 5 || Loss: 6.465661396973916
Iteration: 6 || Loss: 6.465661396973916
saving ADAM checkpoint...
Sum of params:-104.88341
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.465661396973916
Iteration: 2 || Loss: 6.410971004165946
Iteration: 3 || Loss: 6.377339493914022
Iteration: 4 || Loss: 6.334274860491055
Iteration: 5 || Loss: 6.250445753415128
Iteration: 6 || Loss: 6.191847042607745
Iteration: 7 || Loss: 6.177349874620797
Iteration: 8 || Loss: 6.168082485178114
Iteration: 9 || Loss: 6.164659913499447
Iteration: 10 || Loss: 6.1574779437875184
Iteration: 11 || Loss: 6.137965611215263
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.8961
Epoch 150 loss:6.137965611215263
MSE loss S0.2168398042991867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.8961
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.330383249064933
Iteration: 2 || Loss: 24.329858523050994
Iteration: 3 || Loss: 24.329345575431894
Iteration: 4 || Loss: 24.328786240175585
Iteration: 5 || Loss: 24.328284359142923
Iteration: 6 || Loss: 24.328284359142923
saving ADAM checkpoint...
Sum of params:-104.89627
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.328284359142923
Iteration: 2 || Loss: 24.318552491136234
Iteration: 3 || Loss: 24.237158614285516
Iteration: 4 || Loss: 24.17505758153079
Iteration: 5 || Loss: 24.0126416725508
Iteration: 6 || Loss: 23.987192036644903
Iteration: 7 || Loss: 23.968234050271043
Iteration: 8 || Loss: 23.93746832783489
Iteration: 9 || Loss: 23.919646014480254
Iteration: 10 || Loss: 23.908742606744266
Iteration: 11 || Loss: 23.886867612476106
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.832695
Epoch 150 loss:23.886867612476106
MSE loss S0.426539687253254
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.832695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.31823538908195
Iteration: 2 || Loss: 73.31762446711635
Iteration: 3 || Loss: 73.317019645435
Iteration: 4 || Loss: 73.31636661334707
Iteration: 5 || Loss: 73.31576040445263
Iteration: 6 || Loss: 73.31576040445263
saving ADAM checkpoint...
Sum of params:-104.832825
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.31576040445263
Iteration: 2 || Loss: 73.30319342705339
Iteration: 3 || Loss: 73.26163959421385
Iteration: 4 || Loss: 72.98236066425315
Iteration: 5 || Loss: 72.8489101887469
Iteration: 6 || Loss: 72.77176201744307
Iteration: 7 || Loss: 72.6880980036741
Iteration: 8 || Loss: 72.65567460534743
Iteration: 9 || Loss: 72.63328629780062
Iteration: 10 || Loss: 72.60875982316595
Iteration: 11 || Loss: 72.56210901619501
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.91083
Epoch 150 loss:72.56210901619501
MSE loss S1.0179294554451461
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.89950828399228
MSE loss S0.35717583239852163
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:128.38318054639709
MSE loss S2.4147700184402265
waveform batch: 2/2
Test loss - extrapolation:67.98452698709022
MSE loss S1.500785950017944
Epoch 150 mean train loss:3.537480766892634
Epoch 150 mean test loss - interpolation:4.149918047332046
Epoch 150 mean test loss - extrapolation:16.36397562779061
Start training epoch 151
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.91083
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.470124936236968
Iteration: 2 || Loss: 6.46889702924547
Iteration: 3 || Loss: 6.4677769086177515
Iteration: 4 || Loss: 6.466559127045997
Iteration: 5 || Loss: 6.465385538175676
Iteration: 6 || Loss: 6.465385538175676
saving ADAM checkpoint...
Sum of params:-104.91081
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.465385538175676
Iteration: 2 || Loss: 6.412370419849605
Iteration: 3 || Loss: 6.37693390621297
Iteration: 4 || Loss: 6.330941029772834
Iteration: 5 || Loss: 6.246723887670701
Iteration: 6 || Loss: 6.187827221569107
Iteration: 7 || Loss: 6.1742236332689275
Iteration: 8 || Loss: 6.164801954751348
Iteration: 9 || Loss: 6.1614719766340516
Iteration: 10 || Loss: 6.153826084630431
Iteration: 11 || Loss: 6.135055877833911
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.92364
Epoch 151 loss:6.135055877833911
MSE loss S0.2164067265626587
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.92364
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.32559683961925
Iteration: 2 || Loss: 24.32509425051809
Iteration: 3 || Loss: 24.32454053441835
Iteration: 4 || Loss: 24.32403069545841
Iteration: 5 || Loss: 24.323424575131444
Iteration: 6 || Loss: 24.323424575131444
saving ADAM checkpoint...
Sum of params:-104.92382
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.323424575131444
Iteration: 2 || Loss: 24.313399689860027
Iteration: 3 || Loss: 24.22941041765043
Iteration: 4 || Loss: 24.165614697262598
Iteration: 5 || Loss: 24.00351797631658
Iteration: 6 || Loss: 23.977780034494174
Iteration: 7 || Loss: 23.959566079204507
Iteration: 8 || Loss: 23.92791653272635
Iteration: 9 || Loss: 23.910313003312147
Iteration: 10 || Loss: 23.900182884529872
Iteration: 11 || Loss: 23.878559302966586
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.86186
Epoch 151 loss:23.878559302966586
MSE loss S0.42620629511555774
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.86186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.21027846530052
Iteration: 2 || Loss: 73.20960368895275
Iteration: 3 || Loss: 73.20894798775699
Iteration: 4 || Loss: 73.20827326559217
Iteration: 5 || Loss: 73.2077188886232
Iteration: 6 || Loss: 73.2077188886232
saving ADAM checkpoint...
Sum of params:-104.862
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.2077188886232
Iteration: 2 || Loss: 73.19507685332243
Iteration: 3 || Loss: 73.15223302592634
Iteration: 4 || Loss: 72.87374272306721
Iteration: 5 || Loss: 72.74604783678838
Iteration: 6 || Loss: 72.66908236527591
Iteration: 7 || Loss: 72.58585904234405
Iteration: 8 || Loss: 72.55427247177346
Iteration: 9 || Loss: 72.53407422214782
Iteration: 10 || Loss: 72.51060139757479
Iteration: 11 || Loss: 72.46433421002624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.93669
Epoch 151 loss:72.46433421002624
MSE loss S1.0180255383768493
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.877332503612095
MSE loss S0.3571112455617204
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:128.1001604810356
MSE loss S2.411543936136371
waveform batch: 2/2
Test loss - extrapolation:67.70273742634883
MSE loss S1.4959886801048585
Epoch 151 mean train loss:3.533722392787129
Epoch 151 mean test loss - interpolation:4.146222083935349
Epoch 151 mean test loss - extrapolation:16.316908158948703
Start training epoch 152
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.93669
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.46794326873409
Iteration: 2 || Loss: 6.466876035487127
Iteration: 3 || Loss: 6.465650004385279
Iteration: 4 || Loss: 6.464480980235975
Iteration: 5 || Loss: 6.463340076780412
Iteration: 6 || Loss: 6.463340076780412
saving ADAM checkpoint...
Sum of params:-104.936676
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.463340076780412
Iteration: 2 || Loss: 6.412447160174073
Iteration: 3 || Loss: 6.375432095280441
Iteration: 4 || Loss: 6.32741652665497
Iteration: 5 || Loss: 6.2429354926598934
Iteration: 6 || Loss: 6.183238166196206
Iteration: 7 || Loss: 6.170760629771275
Iteration: 8 || Loss: 6.161281362649045
Iteration: 9 || Loss: 6.158050277903458
Iteration: 10 || Loss: 6.150071946245379
Iteration: 11 || Loss: 6.131907640582429
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.94973
Epoch 152 loss:6.131907640582429
MSE loss S0.2156710754515504
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.94973
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.32329615526987
Iteration: 2 || Loss: 24.322779907168325
Iteration: 3 || Loss: 24.32230034476643
Iteration: 4 || Loss: 24.321687428072284
Iteration: 5 || Loss: 24.321204102687446
Iteration: 6 || Loss: 24.321204102687446
saving ADAM checkpoint...
Sum of params:-104.94993
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.321204102687446
Iteration: 2 || Loss: 24.311094894349022
Iteration: 3 || Loss: 24.225001049979607
Iteration: 4 || Loss: 24.161539330685326
Iteration: 5 || Loss: 23.995217040114593
Iteration: 6 || Loss: 23.969132972128637
Iteration: 7 || Loss: 23.951031879446827
Iteration: 8 || Loss: 23.91910154772625
Iteration: 9 || Loss: 23.901296840504973
Iteration: 10 || Loss: 23.89161089823685
Iteration: 11 || Loss: 23.869839094373713
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.88912
Epoch 152 loss:23.869839094373713
MSE loss S0.42584957475264457
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.88912
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.11728596735495
Iteration: 2 || Loss: 73.11656815261408
Iteration: 3 || Loss: 73.115916108569
Iteration: 4 || Loss: 73.11523611241043
Iteration: 5 || Loss: 73.11459320315535
Iteration: 6 || Loss: 73.11459320315535
saving ADAM checkpoint...
Sum of params:-104.88924
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.11459320315535
Iteration: 2 || Loss: 73.10211309989515
Iteration: 3 || Loss: 73.05917046201127
Iteration: 4 || Loss: 72.77352135461526
Iteration: 5 || Loss: 72.64958646079664
Iteration: 6 || Loss: 72.57195201324029
Iteration: 7 || Loss: 72.48873285292161
Iteration: 8 || Loss: 72.45719772969966
Iteration: 9 || Loss: 72.4379228736155
Iteration: 10 || Loss: 72.41496096346874
Iteration: 11 || Loss: 72.36901750486335
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.96256
Epoch 152 loss:72.36901750486335
MSE loss S1.017360588543005
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.857286998965517
MSE loss S0.3567598626856868
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:127.82504115824398
MSE loss S2.407836537339028
waveform batch: 2/2
Test loss - extrapolation:67.42530100832086
MSE loss S1.4910569443326023
Epoch 152 mean train loss:3.530026353097224
Epoch 152 mean test loss - interpolation:4.142881166494253
Epoch 152 mean test loss - extrapolation:16.270861847213737
Start training epoch 153
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.96256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.466437140739325
Iteration: 2 || Loss: 6.465267193354913
Iteration: 3 || Loss: 6.464061913215801
Iteration: 4 || Loss: 6.46287943285296
Iteration: 5 || Loss: 6.461773918908533
Iteration: 6 || Loss: 6.461773918908533
saving ADAM checkpoint...
Sum of params:-104.96259
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.461773918908533
Iteration: 2 || Loss: 6.412515258414485
Iteration: 3 || Loss: 6.374268171138771
Iteration: 4 || Loss: 6.324533585944278
Iteration: 5 || Loss: 6.239418699013233
Iteration: 6 || Loss: 6.17985105081386
Iteration: 7 || Loss: 6.167485410238578
Iteration: 8 || Loss: 6.1580137824530565
Iteration: 9 || Loss: 6.154641120927732
Iteration: 10 || Loss: 6.146883982840733
Iteration: 11 || Loss: 6.1290909482827125
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.97592
Epoch 153 loss:6.1290909482827125
MSE loss S0.2150694041150222
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-104.97592
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.315192033422203
Iteration: 2 || Loss: 24.31464649191953
Iteration: 3 || Loss: 24.314103117714787
Iteration: 4 || Loss: 24.313657007002806
Iteration: 5 || Loss: 24.313087342105877
Iteration: 6 || Loss: 24.313087342105877
saving ADAM checkpoint...
Sum of params:-104.97611
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.313087342105877
Iteration: 2 || Loss: 24.30273168335731
Iteration: 3 || Loss: 24.216176486179663
Iteration: 4 || Loss: 24.152823712182677
Iteration: 5 || Loss: 23.987052719858603
Iteration: 6 || Loss: 23.960423973466064
Iteration: 7 || Loss: 23.94239502673613
Iteration: 8 || Loss: 23.90989226711311
Iteration: 9 || Loss: 23.892572604125487
Iteration: 10 || Loss: 23.883248121726425
Iteration: 11 || Loss: 23.86186104199835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.91659
Epoch 153 loss:23.86186104199835
MSE loss S0.4249983860304206
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.91659
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.01130600141269
Iteration: 2 || Loss: 73.01062499490637
Iteration: 3 || Loss: 73.00993595355169
Iteration: 4 || Loss: 73.00922140697217
Iteration: 5 || Loss: 73.00864577536122
Iteration: 6 || Loss: 73.00864577536122
saving ADAM checkpoint...
Sum of params:-104.91669
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.00864577536122
Iteration: 2 || Loss: 72.99664392872315
Iteration: 3 || Loss: 72.9543715331082
Iteration: 4 || Loss: 72.67596104217256
Iteration: 5 || Loss: 72.5524023969461
Iteration: 6 || Loss: 72.47480976319248
Iteration: 7 || Loss: 72.39238391407581
Iteration: 8 || Loss: 72.36145256181256
Iteration: 9 || Loss: 72.34312377485331
Iteration: 10 || Loss: 72.32095896543716
Iteration: 11 || Loss: 72.27528747722256
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.98837
Epoch 153 loss:72.27528747722256
MSE loss S1.0168412839466474
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.837656276160654
MSE loss S0.35651216470072206
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:127.54890962199057
MSE loss S2.404347890036112
waveform batch: 2/2
Test loss - extrapolation:67.15011080654288
MSE loss S1.4861284499278073
Epoch 153 mean train loss:3.526422050603573
Epoch 153 mean test loss - interpolation:4.139609379360109
Epoch 153 mean test loss - extrapolation:16.224918369044456
Start training epoch 154
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-104.98837
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.465747049487665
Iteration: 2 || Loss: 6.464535245692333
Iteration: 3 || Loss: 6.463445206822699
Iteration: 4 || Loss: 6.462265794312318
Iteration: 5 || Loss: 6.461134102143566
Iteration: 6 || Loss: 6.461134102143566
saving ADAM checkpoint...
Sum of params:-104.98839
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.461134102143566
Iteration: 2 || Loss: 6.412821441898431
Iteration: 3 || Loss: 6.373184558142007
Iteration: 4 || Loss: 6.321681684972209
Iteration: 5 || Loss: 6.235958760052522
Iteration: 6 || Loss: 6.176541325164648
Iteration: 7 || Loss: 6.164417373322816
Iteration: 8 || Loss: 6.155071814290855
Iteration: 9 || Loss: 6.151788384574907
Iteration: 10 || Loss: 6.143833471545872
Iteration: 11 || Loss: 6.126650287156703
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.00196
Epoch 154 loss:6.126650287156703
MSE loss S0.21488063377357824
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.00196
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.31106998137108
Iteration: 2 || Loss: 24.31047922831864
Iteration: 3 || Loss: 24.3099039476262
Iteration: 4 || Loss: 24.30940185057485
Iteration: 5 || Loss: 24.30879853319554
Iteration: 6 || Loss: 24.30879853319554
saving ADAM checkpoint...
Sum of params:-105.00215
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.30879853319554
Iteration: 2 || Loss: 24.298021205598932
Iteration: 3 || Loss: 24.208439506437426
Iteration: 4 || Loss: 24.143709602817463
Iteration: 5 || Loss: 23.978345561174773
Iteration: 6 || Loss: 23.951838953391793
Iteration: 7 || Loss: 23.934293854814694
Iteration: 8 || Loss: 23.901245298384644
Iteration: 9 || Loss: 23.884113661511595
Iteration: 10 || Loss: 23.875036664458204
Iteration: 11 || Loss: 23.854627813904326
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.944214
Epoch 154 loss:23.854627813904326
MSE loss S0.42490768107436655
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.944214
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.9091649849523
Iteration: 2 || Loss: 72.90839784554079
Iteration: 3 || Loss: 72.90768366270639
Iteration: 4 || Loss: 72.90699730580926
Iteration: 5 || Loss: 72.90632189901557
Iteration: 6 || Loss: 72.90632189901557
saving ADAM checkpoint...
Sum of params:-104.94431
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.90632189901557
Iteration: 2 || Loss: 72.89471742833875
Iteration: 3 || Loss: 72.85205644734056
Iteration: 4 || Loss: 72.56336555156692
Iteration: 5 || Loss: 72.45653765200807
Iteration: 6 || Loss: 72.37910691751304
Iteration: 7 || Loss: 72.2974964935465
Iteration: 8 || Loss: 72.26709317882906
Iteration: 9 || Loss: 72.24933932326275
Iteration: 10 || Loss: 72.22862837615448
Iteration: 11 || Loss: 72.1834517121979
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.01393
Epoch 154 loss:72.1834517121979
MSE loss S1.0158926693314956
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.819274959871922
MSE loss S0.35617306099261625
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:127.28068177892517
MSE loss S2.4008392461309
waveform batch: 2/2
Test loss - extrapolation:66.88061913511135
MSE loss S1.4810775404597263
Epoch 154 mean train loss:3.522921717698584
Epoch 154 mean test loss - interpolation:4.13654582664532
Epoch 154 mean test loss - extrapolation:16.180108409503045
Start training epoch 155
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.01393
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.469636094079665
Iteration: 2 || Loss: 6.468495715049712
Iteration: 3 || Loss: 6.467324020815949
Iteration: 4 || Loss: 6.4661717530390845
Iteration: 5 || Loss: 6.465002122466557
Iteration: 6 || Loss: 6.465002122466557
saving ADAM checkpoint...
Sum of params:-105.01393
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.465002122466557
Iteration: 2 || Loss: 6.416213639777134
Iteration: 3 || Loss: 6.374235841224601
Iteration: 4 || Loss: 6.319799159324061
Iteration: 5 || Loss: 6.232831795953281
Iteration: 6 || Loss: 6.17330273429936
Iteration: 7 || Loss: 6.161621251109911
Iteration: 8 || Loss: 6.152384491632561
Iteration: 9 || Loss: 6.149136951177908
Iteration: 10 || Loss: 6.140965553262553
Iteration: 11 || Loss: 6.124200820544791
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.02762
Epoch 155 loss:6.124200820544791
MSE loss S0.21444977479061936
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.02762
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.310469743708065
Iteration: 2 || Loss: 24.3099191883535
Iteration: 3 || Loss: 24.309341463792975
Iteration: 4 || Loss: 24.308787073803014
Iteration: 5 || Loss: 24.30818375727526
Iteration: 6 || Loss: 24.30818375727526
saving ADAM checkpoint...
Sum of params:-105.0278
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.30818375727526
Iteration: 2 || Loss: 24.29726608168869
Iteration: 3 || Loss: 24.20504967310015
Iteration: 4 || Loss: 24.13962783742966
Iteration: 5 || Loss: 23.97091621032534
Iteration: 6 || Loss: 23.94454758762143
Iteration: 7 || Loss: 23.926998108880383
Iteration: 8 || Loss: 23.89362859337897
Iteration: 9 || Loss: 23.87640608079254
Iteration: 10 || Loss: 23.867552551467917
Iteration: 11 || Loss: 23.848307931439482
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-104.971436
Epoch 155 loss:23.848307931439482
MSE loss S0.4241897289086028
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-104.971436
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.80367682916376
Iteration: 2 || Loss: 72.80298407695616
Iteration: 3 || Loss: 72.80220815573597
Iteration: 4 || Loss: 72.80145614254698
Iteration: 5 || Loss: 72.80081963097386
Iteration: 6 || Loss: 72.80081963097386
saving ADAM checkpoint...
Sum of params:-104.97151
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.80081963097386
Iteration: 2 || Loss: 72.78317068309308
Iteration: 3 || Loss: 72.7421248981766
Iteration: 4 || Loss: 72.46663578784916
Iteration: 5 || Loss: 72.36272663509914
Iteration: 6 || Loss: 72.28492797267789
Iteration: 7 || Loss: 72.20445287964546
Iteration: 8 || Loss: 72.17465064037968
Iteration: 9 || Loss: 72.15595987532363
Iteration: 10 || Loss: 72.13724801932887
Iteration: 11 || Loss: 72.09189473808864
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.04321
Epoch 155 loss:72.09189473808864
MSE loss S1.0142339185209828
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.80619057685174
MSE loss S0.3553723963341931
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:127.04421701796866
MSE loss S2.397610432359268
waveform batch: 2/2
Test loss - extrapolation:66.68418032282959
MSE loss S1.4801069371555218
Epoch 155 mean train loss:3.5194621893128595
Epoch 155 mean test loss - interpolation:4.1343650961419565
Epoch 155 mean test loss - extrapolation:16.14403311173319
Start training epoch 156
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.04321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.505095559255134
Iteration: 2 || Loss: 6.50372441874405
Iteration: 3 || Loss: 6.502384430652143
Iteration: 4 || Loss: 6.501096811938625
Iteration: 5 || Loss: 6.49983143020485
Iteration: 6 || Loss: 6.49983143020485
saving ADAM checkpoint...
Sum of params:-105.04325
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.49983143020485
Iteration: 2 || Loss: 6.43541065486131
Iteration: 3 || Loss: 6.388198829364752
Iteration: 4 || Loss: 6.327478459900882
Iteration: 5 || Loss: 6.230984972048892
Iteration: 6 || Loss: 6.173313878290463
Iteration: 7 || Loss: 6.160306558952247
Iteration: 8 || Loss: 6.151978703250601
Iteration: 9 || Loss: 6.148884175247307
Iteration: 10 || Loss: 6.140655641428777
Iteration: 11 || Loss: 6.123693686712867
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.05585
Epoch 156 loss:6.123693686712867
MSE loss S0.21564649649927656
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.05585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.328387799123586
Iteration: 2 || Loss: 24.327748295950048
Iteration: 3 || Loss: 24.327051683783726
Iteration: 4 || Loss: 24.326373358140835
Iteration: 5 || Loss: 24.32574807278239
Iteration: 6 || Loss: 24.32574807278239
saving ADAM checkpoint...
Sum of params:-105.056015
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.32574807278239
Iteration: 2 || Loss: 24.312201490174118
Iteration: 3 || Loss: 24.203300671006424
Iteration: 4 || Loss: 24.143537618752333
Iteration: 5 || Loss: 23.967662636003368
Iteration: 6 || Loss: 23.9403682034905
Iteration: 7 || Loss: 23.92141318777962
Iteration: 8 || Loss: 23.88653801476337
Iteration: 9 || Loss: 23.870040352255014
Iteration: 10 || Loss: 23.86243051841417
Iteration: 11 || Loss: 23.846735839817462
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.00321
Epoch 156 loss:23.846735839817462
MSE loss S0.42461779397693333
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.00321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.65682630803305
Iteration: 2 || Loss: 72.65610294576857
Iteration: 3 || Loss: 72.6553386433394
Iteration: 4 || Loss: 72.65458976547741
Iteration: 5 || Loss: 72.65388492255806
Iteration: 6 || Loss: 72.65388492255806
saving ADAM checkpoint...
Sum of params:-105.00333
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.65388492255806
Iteration: 2 || Loss: 72.63848896759528
Iteration: 3 || Loss: 72.60001266516943
Iteration: 4 || Loss: 72.31472234412416
Iteration: 5 || Loss: 72.25944699235905
Iteration: 6 || Loss: 72.18242021824285
Iteration: 7 || Loss: 72.10380979567054
Iteration: 8 || Loss: 72.07661319663335
Iteration: 9 || Loss: 72.05988843100218
Iteration: 10 || Loss: 72.04198971334908
Iteration: 11 || Loss: 71.9953868130723
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.07094
Epoch 156 loss:71.9953868130723
MSE loss S1.0136691979964696
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.792189003516942
MSE loss S0.35596542095467537
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:126.7623234289241
MSE loss S2.394354439827004
waveform batch: 2/2
Test loss - extrapolation:66.4043390882701
MSE loss S1.4738078567516681
Epoch 156 mean train loss:3.516062632400091
Epoch 156 mean test loss - interpolation:4.132031500586157
Epoch 156 mean test loss - extrapolation:16.09722187643285
Start training epoch 157
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.07094
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.51979122837457
Iteration: 2 || Loss: 6.5184371655548725
Iteration: 3 || Loss: 6.517043771507373
Iteration: 4 || Loss: 6.5157232732167385
Iteration: 5 || Loss: 6.514355711614499
Iteration: 6 || Loss: 6.514355711614499
saving ADAM checkpoint...
Sum of params:-105.070946
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.514355711614499
Iteration: 2 || Loss: 6.4475490346899536
Iteration: 3 || Loss: 6.394418914226737
Iteration: 4 || Loss: 6.327652815386204
Iteration: 5 || Loss: 6.229973438779867
Iteration: 6 || Loss: 6.172525794566996
Iteration: 7 || Loss: 6.16011163189726
Iteration: 8 || Loss: 6.151867246497931
Iteration: 9 || Loss: 6.148517083244281
Iteration: 10 || Loss: 6.139148719181223
Iteration: 11 || Loss: 6.12341306029758
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.08268
Epoch 157 loss:6.12341306029758
MSE loss S0.21612043180626994
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.08268
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.32185562483211
Iteration: 2 || Loss: 24.32110790482932
Iteration: 3 || Loss: 24.32044839932957
Iteration: 4 || Loss: 24.319710740739435
Iteration: 5 || Loss: 24.31903762572778
Iteration: 6 || Loss: 24.31903762572778
saving ADAM checkpoint...
Sum of params:-105.08285
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.31903762572778
Iteration: 2 || Loss: 24.304643971478676
Iteration: 3 || Loss: 24.175291030467882
Iteration: 4 || Loss: 24.132495202776887
Iteration: 5 || Loss: 23.960592259341134
Iteration: 6 || Loss: 23.93330103706864
Iteration: 7 || Loss: 23.913309940768308
Iteration: 8 || Loss: 23.87820965823686
Iteration: 9 || Loss: 23.863660633250973
Iteration: 10 || Loss: 23.856607566434974
Iteration: 11 || Loss: 23.8422304825756
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.03151
Epoch 157 loss:23.8422304825756
MSE loss S0.4240134285913509
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.03151
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.54022266134884
Iteration: 2 || Loss: 72.53946835075594
Iteration: 3 || Loss: 72.53873374336735
Iteration: 4 || Loss: 72.53809035952418
Iteration: 5 || Loss: 72.53731763151632
Iteration: 6 || Loss: 72.53731763151632
saving ADAM checkpoint...
Sum of params:-105.03161
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.53731763151632
Iteration: 2 || Loss: 72.52354082724196
Iteration: 3 || Loss: 72.48812352081913
Iteration: 4 || Loss: 72.21005672484834
Iteration: 5 || Loss: 72.15958369419783
Iteration: 6 || Loss: 72.08249957742895
Iteration: 7 || Loss: 72.0036142893833
Iteration: 8 || Loss: 71.9779636949581
Iteration: 9 || Loss: 71.96245382575468
Iteration: 10 || Loss: 71.94460875092929
Iteration: 11 || Loss: 71.89764717557789
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.0979
Epoch 157 loss:71.89764717557789
MSE loss S1.0119769678959947
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.777582651661703
MSE loss S0.3557033505711193
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:126.47690702724131
MSE loss S2.390082568407091
waveform batch: 2/2
Test loss - extrapolation:66.11670056838278
MSE loss S1.4681855998357451
Epoch 157 mean train loss:3.512527266153485
Epoch 157 mean test loss - interpolation:4.129597108610284
Epoch 157 mean test loss - extrapolation:16.049467299635342
Start training epoch 158
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.0979
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.524451129200801
Iteration: 2 || Loss: 6.523128132973432
Iteration: 3 || Loss: 6.521687435321001
Iteration: 4 || Loss: 6.520327496262203
Iteration: 5 || Loss: 6.519039704564024
Iteration: 6 || Loss: 6.519039704564024
saving ADAM checkpoint...
Sum of params:-105.09789
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.519039704564024
Iteration: 2 || Loss: 6.4513304679327454
Iteration: 3 || Loss: 6.395173793353877
Iteration: 4 || Loss: 6.325947680962578
Iteration: 5 || Loss: 6.228913933011557
Iteration: 6 || Loss: 6.172428532697024
Iteration: 7 || Loss: 6.158924309564322
Iteration: 8 || Loss: 6.150737044783213
Iteration: 9 || Loss: 6.147439893041421
Iteration: 10 || Loss: 6.138659145335369
Iteration: 11 || Loss: 6.122027706465934
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.10973
Epoch 158 loss:6.122027706465934
MSE loss S0.21563055964936872
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.10973
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.311448816150826
Iteration: 2 || Loss: 24.310789894603435
Iteration: 3 || Loss: 24.3100594788804
Iteration: 4 || Loss: 24.309365223379384
Iteration: 5 || Loss: 24.308639144196558
Iteration: 6 || Loss: 24.308639144196558
saving ADAM checkpoint...
Sum of params:-105.1099
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.308639144196558
Iteration: 2 || Loss: 24.29401672537037
Iteration: 3 || Loss: 24.17090427502359
Iteration: 4 || Loss: 24.125887058185256
Iteration: 5 || Loss: 23.956707750208086
Iteration: 6 || Loss: 23.92864857566909
Iteration: 7 || Loss: 23.907863872514287
Iteration: 8 || Loss: 23.871951337352204
Iteration: 9 || Loss: 23.85831178718813
Iteration: 10 || Loss: 23.850824072761117
Iteration: 11 || Loss: 23.835468380068768
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.05777
Epoch 158 loss:23.835468380068768
MSE loss S0.4238040436962707
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.05777
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.44580982616677
Iteration: 2 || Loss: 72.44510557355316
Iteration: 3 || Loss: 72.44441162063156
Iteration: 4 || Loss: 72.44366500967209
Iteration: 5 || Loss: 72.44297164510894
Iteration: 6 || Loss: 72.44297164510894
saving ADAM checkpoint...
Sum of params:-105.05787
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.44297164510894
Iteration: 2 || Loss: 72.42896034254717
Iteration: 3 || Loss: 72.39333080149925
Iteration: 4 || Loss: 72.12083738668144
Iteration: 5 || Loss: 72.06249888251213
Iteration: 6 || Loss: 71.98502481326263
Iteration: 7 || Loss: 71.90603987010752
Iteration: 8 || Loss: 71.88054855593175
Iteration: 9 || Loss: 71.86533971715279
Iteration: 10 || Loss: 71.84827992735408
Iteration: 11 || Loss: 71.80270280930206
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.12318
Epoch 158 loss:71.80270280930206
MSE loss S1.0109461141831835
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.759611912517272
MSE loss S0.3554014019325693
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:126.20056605103242
MSE loss S2.385865028408052
waveform batch: 2/2
Test loss - extrapolation:65.83705231416053
MSE loss S1.463215326555903
Epoch 158 mean train loss:3.508972375718509
Epoch 158 mean test loss - interpolation:4.126601985419545
Epoch 158 mean test loss - extrapolation:16.00313486376608
Start training epoch 159
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.12318
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.514205652423252
Iteration: 2 || Loss: 6.512762922799908
Iteration: 3 || Loss: 6.511447226178842
Iteration: 4 || Loss: 6.5101112524497955
Iteration: 5 || Loss: 6.508754901992393
Iteration: 6 || Loss: 6.508754901992393
saving ADAM checkpoint...
Sum of params:-105.12318
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.508754901992393
Iteration: 2 || Loss: 6.443609537629482
Iteration: 3 || Loss: 6.389373134691326
Iteration: 4 || Loss: 6.321872478320867
Iteration: 5 || Loss: 6.226241804949849
Iteration: 6 || Loss: 6.170181289895205
Iteration: 7 || Loss: 6.156404489457293
Iteration: 8 || Loss: 6.1481769257727485
Iteration: 9 || Loss: 6.145096500709533
Iteration: 10 || Loss: 6.1371437719865565
Iteration: 11 || Loss: 6.120320649052338
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.135605
Epoch 159 loss:6.120320649052338
MSE loss S0.21552945460310455
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.135605
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.31702397671319
Iteration: 2 || Loss: 24.316371694051426
Iteration: 3 || Loss: 24.315778175830534
Iteration: 4 || Loss: 24.315049534946738
Iteration: 5 || Loss: 24.314465236566193
Iteration: 6 || Loss: 24.314465236566193
saving ADAM checkpoint...
Sum of params:-105.13579
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.314465236566193
Iteration: 2 || Loss: 24.300897113449828
Iteration: 3 || Loss: 24.17579310332842
Iteration: 4 || Loss: 24.12444834994742
Iteration: 5 || Loss: 23.948942634828878
Iteration: 6 || Loss: 23.923153577625293
Iteration: 7 || Loss: 23.90266964989041
Iteration: 8 || Loss: 23.866148064680903
Iteration: 9 || Loss: 23.85233275126102
Iteration: 10 || Loss: 23.84456851103894
Iteration: 11 || Loss: 23.829770791663208
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.0844
Epoch 159 loss:23.829770791663208
MSE loss S0.4241703136095232
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.0844
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.36179436350858
Iteration: 2 || Loss: 72.36102072052802
Iteration: 3 || Loss: 72.36029763872128
Iteration: 4 || Loss: 72.35950118178735
Iteration: 5 || Loss: 72.35873910680763
Iteration: 6 || Loss: 72.35873910680763
saving ADAM checkpoint...
Sum of params:-105.08449
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.35873910680763
Iteration: 2 || Loss: 72.34442504671863
Iteration: 3 || Loss: 72.30667044607901
Iteration: 4 || Loss: 72.0160143557762
Iteration: 5 || Loss: 71.9692091880499
Iteration: 6 || Loss: 71.89170975614942
Iteration: 7 || Loss: 71.8124228990217
Iteration: 8 || Loss: 71.78683119479118
Iteration: 9 || Loss: 71.77121054449448
Iteration: 10 || Loss: 71.7555797672992
Iteration: 11 || Loss: 71.71091309755374
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.14769
Epoch 159 loss:71.71091309755374
MSE loss S1.010413034558415
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.74183871140472
MSE loss S0.35556719491353916
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:125.93549875371531
MSE loss S2.3819977321654915
waveform batch: 2/2
Test loss - extrapolation:65.56644033480403
MSE loss S1.4582256579462642
Epoch 159 mean train loss:3.5055518806299752
Epoch 159 mean test loss - interpolation:4.12363978523412
Epoch 159 mean test loss - extrapolation:15.95849492404328
Start training epoch 160
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.14769
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.506812697386376
Iteration: 2 || Loss: 6.505515742986154
Iteration: 3 || Loss: 6.504214216455784
Iteration: 4 || Loss: 6.502934363320944
Iteration: 5 || Loss: 6.501677548447364
Iteration: 6 || Loss: 6.501677548447364
saving ADAM checkpoint...
Sum of params:-105.1477
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.501677548447364
Iteration: 2 || Loss: 6.441061383579592
Iteration: 3 || Loss: 6.3866840739491035
Iteration: 4 || Loss: 6.319057199677246
Iteration: 5 || Loss: 6.224323742582648
Iteration: 6 || Loss: 6.168362832523668
Iteration: 7 || Loss: 6.154630919858388
Iteration: 8 || Loss: 6.146189414728373
Iteration: 9 || Loss: 6.143308171928175
Iteration: 10 || Loss: 6.1355842577815665
Iteration: 11 || Loss: 6.118451717260282
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.16061
Epoch 160 loss:6.118451717260282
MSE loss S0.2148772170613155
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.16061
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.306190910660945
Iteration: 2 || Loss: 24.305599989720104
Iteration: 3 || Loss: 24.304940446467462
Iteration: 4 || Loss: 24.304322144479308
Iteration: 5 || Loss: 24.30361755784955
Iteration: 6 || Loss: 24.30361755784955
saving ADAM checkpoint...
Sum of params:-105.160805
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.30361755784955
Iteration: 2 || Loss: 24.2905908617513
Iteration: 3 || Loss: 24.171956461753098
Iteration: 4 || Loss: 24.11682784622543
Iteration: 5 || Loss: 23.943048239622453
Iteration: 6 || Loss: 23.917251540522827
Iteration: 7 || Loss: 23.896963585527523
Iteration: 8 || Loss: 23.85994502673929
Iteration: 9 || Loss: 23.846281484938345
Iteration: 10 || Loss: 23.838209816058797
Iteration: 11 || Loss: 23.82314492584959
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.10938
Epoch 160 loss:23.82314492584959
MSE loss S0.42305383351130865
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.10938
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.26714776677423
Iteration: 2 || Loss: 72.26635573376481
Iteration: 3 || Loss: 72.26559052904449
Iteration: 4 || Loss: 72.26485415570147
Iteration: 5 || Loss: 72.26406571078599
Iteration: 6 || Loss: 72.26406571078599
saving ADAM checkpoint...
Sum of params:-105.10945
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.26406571078599
Iteration: 2 || Loss: 72.251115838249
Iteration: 3 || Loss: 72.21549438925348
Iteration: 4 || Loss: 71.93860442902547
Iteration: 5 || Loss: 71.87903700609438
Iteration: 6 || Loss: 71.80083113141232
Iteration: 7 || Loss: 71.72185393705549
Iteration: 8 || Loss: 71.69646148633642
Iteration: 9 || Loss: 71.68168518347636
Iteration: 10 || Loss: 71.66601958538098
Iteration: 11 || Loss: 71.6217096960359
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.17249
Epoch 160 loss:71.6217096960359
MSE loss S1.0090005891763567
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.725822740579503
MSE loss S0.3551906725846063
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:125.67456393939247
MSE loss S2.377798636700985
waveform batch: 2/2
Test loss - extrapolation:65.3007644520709
MSE loss S1.4532817088745236
Epoch 160 mean train loss:3.502182977211923
Epoch 160 mean test loss - interpolation:4.12097045676325
Epoch 160 mean test loss - extrapolation:15.914610699288614
Start training epoch 161
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.17249
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.504731614479844
Iteration: 2 || Loss: 6.503428384726239
Iteration: 3 || Loss: 6.5021066450651155
Iteration: 4 || Loss: 6.500779605622217
Iteration: 5 || Loss: 6.4995645570259075
Iteration: 6 || Loss: 6.4995645570259075
saving ADAM checkpoint...
Sum of params:-105.17249
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4995645570259075
Iteration: 2 || Loss: 6.439355682501631
Iteration: 3 || Loss: 6.384675504940374
Iteration: 4 || Loss: 6.316721681570202
Iteration: 5 || Loss: 6.222370831578951
Iteration: 6 || Loss: 6.166620066290317
Iteration: 7 || Loss: 6.152683910448437
Iteration: 8 || Loss: 6.144358246599831
Iteration: 9 || Loss: 6.1414869368091844
Iteration: 10 || Loss: 6.13420147191186
Iteration: 11 || Loss: 6.116747057025379
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.18582
Epoch 161 loss:6.116747057025379
MSE loss S0.21443079606472198
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.18582
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.305255562686828
Iteration: 2 || Loss: 24.30458975398138
Iteration: 3 || Loss: 24.3039873974233
Iteration: 4 || Loss: 24.303305322387494
Iteration: 5 || Loss: 24.302625168971055
Iteration: 6 || Loss: 24.302625168971055
saving ADAM checkpoint...
Sum of params:-105.18602
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.302625168971055
Iteration: 2 || Loss: 24.289834495769156
Iteration: 3 || Loss: 24.175730315175876
Iteration: 4 || Loss: 24.116130315871594
Iteration: 5 || Loss: 23.937103625138946
Iteration: 6 || Loss: 23.91157677579725
Iteration: 7 || Loss: 23.890919276147013
Iteration: 8 || Loss: 23.85338903609811
Iteration: 9 || Loss: 23.84003029778091
Iteration: 10 || Loss: 23.83171564685751
Iteration: 11 || Loss: 23.816170922803998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.13434
Epoch 161 loss:23.816170922803998
MSE loss S0.422903285308455
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.13434
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.18279153192913
Iteration: 2 || Loss: 72.18207872526267
Iteration: 3 || Loss: 72.18131116467619
Iteration: 4 || Loss: 72.18049865156152
Iteration: 5 || Loss: 72.17978510982655
Iteration: 6 || Loss: 72.17978510982655
saving ADAM checkpoint...
Sum of params:-105.13445
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.17978510982655
Iteration: 2 || Loss: 72.16667155418364
Iteration: 3 || Loss: 72.13064404608343
Iteration: 4 || Loss: 71.85437310465677
Iteration: 5 || Loss: 71.79120346703635
Iteration: 6 || Loss: 71.71276404105615
Iteration: 7 || Loss: 71.63376069559698
Iteration: 8 || Loss: 71.60822600711771
Iteration: 9 || Loss: 71.59349568767526
Iteration: 10 || Loss: 71.57834846426137
Iteration: 11 || Loss: 71.5354152333558
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.196495
Epoch 161 loss:71.5354152333558
MSE loss S1.008215071171086
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.708296926011617
MSE loss S0.35496682264630947
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:125.42499154140077
MSE loss S2.374101249801699
waveform batch: 2/2
Test loss - extrapolation:65.04549268301575
MSE loss S1.448847159325846
Epoch 161 mean train loss:3.498908041833972
Epoch 161 mean test loss - interpolation:4.1180494876686025
Epoch 161 mean test loss - extrapolation:15.87254035203471
Start training epoch 162
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.196495
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.498779984778471
Iteration: 2 || Loss: 6.4974451725442055
Iteration: 3 || Loss: 6.4961429175594905
Iteration: 4 || Loss: 6.494945808711108
Iteration: 5 || Loss: 6.493711764676057
Iteration: 6 || Loss: 6.493711764676057
saving ADAM checkpoint...
Sum of params:-105.1965
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.493711764676057
Iteration: 2 || Loss: 6.436122879304046
Iteration: 3 || Loss: 6.381706186254968
Iteration: 4 || Loss: 6.314131958429765
Iteration: 5 || Loss: 6.219906248874144
Iteration: 6 || Loss: 6.164203360363789
Iteration: 7 || Loss: 6.1504031654738585
Iteration: 8 || Loss: 6.142353881574193
Iteration: 9 || Loss: 6.139475759693754
Iteration: 10 || Loss: 6.132515202979585
Iteration: 11 || Loss: 6.115343097021932
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.21021
Epoch 162 loss:6.115343097021932
MSE loss S0.21454529703913744
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.21021
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.29816449926231
Iteration: 2 || Loss: 24.297573626188857
Iteration: 3 || Loss: 24.296958375891638
Iteration: 4 || Loss: 24.296251508948846
Iteration: 5 || Loss: 24.295630434638877
Iteration: 6 || Loss: 24.295630434638877
saving ADAM checkpoint...
Sum of params:-105.2104
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.295630434638877
Iteration: 2 || Loss: 24.282105522332735
Iteration: 3 || Loss: 24.168303743971343
Iteration: 4 || Loss: 24.10752340871564
Iteration: 5 || Loss: 23.93115652722057
Iteration: 6 || Loss: 23.904406318350087
Iteration: 7 || Loss: 23.883657103264046
Iteration: 8 || Loss: 23.845879171601467
Iteration: 9 || Loss: 23.832967645687617
Iteration: 10 || Loss: 23.824569489057406
Iteration: 11 || Loss: 23.809183046348974
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.159164
Epoch 162 loss:23.809183046348974
MSE loss S0.4228538154281658
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.159164
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.09453505434
Iteration: 2 || Loss: 72.09376980093262
Iteration: 3 || Loss: 72.09302377376953
Iteration: 4 || Loss: 72.09222139984514
Iteration: 5 || Loss: 72.0914470567852
Iteration: 6 || Loss: 72.0914470567852
saving ADAM checkpoint...
Sum of params:-105.159256
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.0914470567852
Iteration: 2 || Loss: 72.07811163064028
Iteration: 3 || Loss: 72.0416545753145
Iteration: 4 || Loss: 71.76569663204248
Iteration: 5 || Loss: 71.70492053966773
Iteration: 6 || Loss: 71.6266116357327
Iteration: 7 || Loss: 71.54812343894456
Iteration: 8 || Loss: 71.52292970707677
Iteration: 9 || Loss: 71.50863913525997
Iteration: 10 || Loss: 71.49379045106633
Iteration: 11 || Loss: 71.4517543564109
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.22049
Epoch 162 loss:71.4517543564109
MSE loss S1.007659538583632
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.69158555099405
MSE loss S0.3548820093047168
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:125.17821733749778
MSE loss S2.370618718486802
waveform batch: 2/2
Test loss - extrapolation:64.79624795138754
MSE loss S1.44453845460783
Epoch 162 mean train loss:3.495733810337304
Epoch 162 mean test loss - interpolation:4.115264258499008
Epoch 162 mean test loss - extrapolation:15.831205440740442
Start training epoch 163
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.22049
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.494200970609938
Iteration: 2 || Loss: 6.492957620942651
Iteration: 3 || Loss: 6.491694103141279
Iteration: 4 || Loss: 6.490499422750896
Iteration: 5 || Loss: 6.489236135255583
Iteration: 6 || Loss: 6.489236135255583
saving ADAM checkpoint...
Sum of params:-105.22048
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.489236135255583
Iteration: 2 || Loss: 6.4331073339141005
Iteration: 3 || Loss: 6.379224186701835
Iteration: 4 || Loss: 6.312161515251023
Iteration: 5 || Loss: 6.217670390853359
Iteration: 6 || Loss: 6.161964349847303
Iteration: 7 || Loss: 6.148473346671416
Iteration: 8 || Loss: 6.140447446997565
Iteration: 9 || Loss: 6.137582838062617
Iteration: 10 || Loss: 6.13085131020744
Iteration: 11 || Loss: 6.113899706666002
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.23442
Epoch 163 loss:6.113899706666002
MSE loss S0.21432061376159
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.23442
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.291937473106852
Iteration: 2 || Loss: 24.29120866657464
Iteration: 3 || Loss: 24.29059212427831
Iteration: 4 || Loss: 24.28988547460805
Iteration: 5 || Loss: 24.289373000666615
Iteration: 6 || Loss: 24.289373000666615
saving ADAM checkpoint...
Sum of params:-105.23459
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.289373000666615
Iteration: 2 || Loss: 24.27620974792452
Iteration: 3 || Loss: 24.159217500978908
Iteration: 4 || Loss: 24.100538927050035
Iteration: 5 || Loss: 23.923327303414613
Iteration: 6 || Loss: 23.89720398138496
Iteration: 7 || Loss: 23.876396717955224
Iteration: 8 || Loss: 23.838255693158416
Iteration: 9 || Loss: 23.826012200601276
Iteration: 10 || Loss: 23.817579045771208
Iteration: 11 || Loss: 23.802311526296833
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.18367
Epoch 163 loss:23.802311526296833
MSE loss S0.42225748884611075
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.18367
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.00464235395086
Iteration: 2 || Loss: 72.00388114047756
Iteration: 3 || Loss: 72.00308484292172
Iteration: 4 || Loss: 72.00229049961298
Iteration: 5 || Loss: 72.00161231309707
Iteration: 6 || Loss: 72.00161231309707
saving ADAM checkpoint...
Sum of params:-105.183784
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.00161231309707
Iteration: 2 || Loss: 71.98897198426238
Iteration: 3 || Loss: 71.95374985530731
Iteration: 4 || Loss: 71.68450364026296
Iteration: 5 || Loss: 71.62103432626489
Iteration: 6 || Loss: 71.54302661917394
Iteration: 7 || Loss: 71.46497347275914
Iteration: 8 || Loss: 71.4400991537649
Iteration: 9 || Loss: 71.42625451511869
Iteration: 10 || Loss: 71.41148175371858
Iteration: 11 || Loss: 71.36994194368893
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.244675
Epoch 163 loss:71.36994194368893
MSE loss S1.00659846825477
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.675735777451692
MSE loss S0.35458265265382544
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:124.93737514627591
MSE loss S2.366881178972086
waveform batch: 2/2
Test loss - extrapolation:64.55095395269701
MSE loss S1.440170460195828
Epoch 163 mean train loss:3.4926259716086814
Epoch 163 mean test loss - interpolation:4.112622629575282
Epoch 163 mean test loss - extrapolation:15.790694091581079
Start training epoch 164
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.244675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.492198156927998
Iteration: 2 || Loss: 6.490994199265786
Iteration: 3 || Loss: 6.489641976125175
Iteration: 4 || Loss: 6.488516607339602
Iteration: 5 || Loss: 6.487271991050656
Iteration: 6 || Loss: 6.487271991050656
saving ADAM checkpoint...
Sum of params:-105.244644
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.487271991050656
Iteration: 2 || Loss: 6.432083301045464
Iteration: 3 || Loss: 6.3779458441180115
Iteration: 4 || Loss: 6.3106497541496305
Iteration: 5 || Loss: 6.215941613462928
Iteration: 6 || Loss: 6.160427076023728
Iteration: 7 || Loss: 6.146772528900259
Iteration: 8 || Loss: 6.138895551400468
Iteration: 9 || Loss: 6.136034535255823
Iteration: 10 || Loss: 6.129533108342633
Iteration: 11 || Loss: 6.1124060413030685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.25886
Epoch 164 loss:6.1124060413030685
MSE loss S0.213894941819808
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.25886
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.283605634133973
Iteration: 2 || Loss: 24.282935581661956
Iteration: 3 || Loss: 24.282262957583875
Iteration: 4 || Loss: 24.281678639107483
Iteration: 5 || Loss: 24.28095265916416
Iteration: 6 || Loss: 24.28095265916416
saving ADAM checkpoint...
Sum of params:-105.259026
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.28095265916416
Iteration: 2 || Loss: 24.268005119812
Iteration: 3 || Loss: 24.154157330060528
Iteration: 4 || Loss: 24.094017863691157
Iteration: 5 || Loss: 23.916611078667454
Iteration: 6 || Loss: 23.890508971895372
Iteration: 7 || Loss: 23.869444264629113
Iteration: 8 || Loss: 23.831062065801415
Iteration: 9 || Loss: 23.81927908327107
Iteration: 10 || Loss: 23.810412944025934
Iteration: 11 || Loss: 23.7943055440759
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.20745
Epoch 164 loss:23.7943055440759
MSE loss S0.4219457808753515
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.20745
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.93204611356853
Iteration: 2 || Loss: 71.93126796145599
Iteration: 3 || Loss: 71.93051698940626
Iteration: 4 || Loss: 71.92969053237829
Iteration: 5 || Loss: 71.9289571593897
Iteration: 6 || Loss: 71.9289571593897
saving ADAM checkpoint...
Sum of params:-105.207535
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.9289571593897
Iteration: 2 || Loss: 71.91566862813494
Iteration: 3 || Loss: 71.87893155577672
Iteration: 4 || Loss: 71.61546484907096
Iteration: 5 || Loss: 71.5421621357508
Iteration: 6 || Loss: 71.46368081822162
Iteration: 7 || Loss: 71.38411200010495
Iteration: 8 || Loss: 71.35951271521084
Iteration: 9 || Loss: 71.34539789622781
Iteration: 10 || Loss: 71.33110564605813
Iteration: 11 || Loss: 71.2921745191062
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.26709
Epoch 164 loss:71.2921745191062
MSE loss S1.0066479614900241
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.656663629699384
MSE loss S0.35449259881609113
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:124.71026506889962
MSE loss S2.3640584993721987
waveform batch: 2/2
Test loss - extrapolation:64.31996514524768
MSE loss S1.4366034693852976
Epoch 164 mean train loss:3.4896167622236267
Epoch 164 mean test loss - interpolation:4.10944393828323
Epoch 164 mean test loss - extrapolation:15.752519184512275
Start training epoch 165
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.26709
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4814583454992505
Iteration: 2 || Loss: 6.480218755565021
Iteration: 3 || Loss: 6.4789659671086
Iteration: 4 || Loss: 6.477788909179262
Iteration: 5 || Loss: 6.476642641886488
Iteration: 6 || Loss: 6.476642641886488
saving ADAM checkpoint...
Sum of params:-105.26705
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.476642641886488
Iteration: 2 || Loss: 6.425133533963672
Iteration: 3 || Loss: 6.37283592977564
Iteration: 4 || Loss: 6.3074429790552236
Iteration: 5 || Loss: 6.212683214418186
Iteration: 6 || Loss: 6.1567483108341
Iteration: 7 || Loss: 6.144013832823801
Iteration: 8 || Loss: 6.136277658387201
Iteration: 9 || Loss: 6.133269571728079
Iteration: 10 || Loss: 6.127412864590104
Iteration: 11 || Loss: 6.1104225258181515
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.2818
Epoch 165 loss:6.1104225258181515
MSE loss S0.2135648944187497
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.2818
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.278060287789067
Iteration: 2 || Loss: 24.277361979308946
Iteration: 3 || Loss: 24.276816405646244
Iteration: 4 || Loss: 24.27613193966446
Iteration: 5 || Loss: 24.27555229377379
Iteration: 6 || Loss: 24.27555229377379
saving ADAM checkpoint...
Sum of params:-105.282
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.27555229377379
Iteration: 2 || Loss: 24.26288054126794
Iteration: 3 || Loss: 24.1490638947086
Iteration: 4 || Loss: 24.08833737012086
Iteration: 5 || Loss: 23.909301000017255
Iteration: 6 || Loss: 23.883080326702203
Iteration: 7 || Loss: 23.862486002094528
Iteration: 8 || Loss: 23.824012420372405
Iteration: 9 || Loss: 23.812064589846408
Iteration: 10 || Loss: 23.803047312010783
Iteration: 11 || Loss: 23.78709775893818
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.231
Epoch 165 loss:23.78709775893818
MSE loss S0.42112689441199
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.231
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.8525174380835
Iteration: 2 || Loss: 71.85171907162034
Iteration: 3 || Loss: 71.85094346870454
Iteration: 4 || Loss: 71.85019421728659
Iteration: 5 || Loss: 71.84943881155743
Iteration: 6 || Loss: 71.84943881155743
saving ADAM checkpoint...
Sum of params:-105.231125
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.84943881155743
Iteration: 2 || Loss: 71.83719726599683
Iteration: 3 || Loss: 71.8018059494515
Iteration: 4 || Loss: 71.54289534566
Iteration: 5 || Loss: 71.46527542907684
Iteration: 6 || Loss: 71.38694078844183
Iteration: 7 || Loss: 71.30852136040207
Iteration: 8 || Loss: 71.28377476987698
Iteration: 9 || Loss: 71.27001523145496
Iteration: 10 || Loss: 71.2558117928282
Iteration: 11 || Loss: 71.21736175355346
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.29043
Epoch 165 loss:71.21736175355346
MSE loss S1.0057548300416668
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.641099153785134
MSE loss S0.35420691924359465
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:124.49084953315968
MSE loss S2.3607627566578318
waveform batch: 2/2
Test loss - extrapolation:64.09478783421076
MSE loss S1.4326114532716319
Epoch 165 mean train loss:3.486720070286544
Epoch 165 mean test loss - interpolation:4.106849858964189
Epoch 165 mean test loss - extrapolation:15.715469780614205
Start training epoch 166
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.29043
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.481561310885274
Iteration: 2 || Loss: 6.480354659671072
Iteration: 3 || Loss: 6.479179672149068
Iteration: 4 || Loss: 6.477972266983109
Iteration: 5 || Loss: 6.476835716633409
Iteration: 6 || Loss: 6.476835716633409
saving ADAM checkpoint...
Sum of params:-105.290375
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.476835716633409
Iteration: 2 || Loss: 6.426478353659306
Iteration: 3 || Loss: 6.373193876123608
Iteration: 4 || Loss: 6.306738380335722
Iteration: 5 || Loss: 6.2108509643234715
Iteration: 6 || Loss: 6.154716899239111
Iteration: 7 || Loss: 6.142196258698899
Iteration: 8 || Loss: 6.134657355205465
Iteration: 9 || Loss: 6.131764505042082
Iteration: 10 || Loss: 6.125766600704682
Iteration: 11 || Loss: 6.109104308098719
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.30532
Epoch 166 loss:6.109104308098719
MSE loss S0.21351396656305402
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.30532
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.27349148576101
Iteration: 2 || Loss: 24.272747560279583
Iteration: 3 || Loss: 24.272178053437617
Iteration: 4 || Loss: 24.27151912986167
Iteration: 5 || Loss: 24.270863712825722
Iteration: 6 || Loss: 24.270863712825722
saving ADAM checkpoint...
Sum of params:-105.3055
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.270863712825722
Iteration: 2 || Loss: 24.258078863195202
Iteration: 3 || Loss: 24.14282243033227
Iteration: 4 || Loss: 24.082673071976885
Iteration: 5 || Loss: 23.902175366567615
Iteration: 6 || Loss: 23.875704586257925
Iteration: 7 || Loss: 23.854960253923807
Iteration: 8 || Loss: 23.816219287202873
Iteration: 9 || Loss: 23.804855739912625
Iteration: 10 || Loss: 23.79614712217164
Iteration: 11 || Loss: 23.780773725794457
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.25544
Epoch 166 loss:23.780773725794457
MSE loss S0.4208684317925514
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.25544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.75865780616061
Iteration: 2 || Loss: 71.75789087964783
Iteration: 3 || Loss: 71.75713502142702
Iteration: 4 || Loss: 71.75632755200506
Iteration: 5 || Loss: 71.75566551692329
Iteration: 6 || Loss: 71.75566551692329
saving ADAM checkpoint...
Sum of params:-105.25552
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.75566551692329
Iteration: 2 || Loss: 71.74417447840636
Iteration: 3 || Loss: 71.71093509139828
Iteration: 4 || Loss: 71.45915567704382
Iteration: 5 || Loss: 71.38689751867747
Iteration: 6 || Loss: 71.3095489569077
Iteration: 7 || Loss: 71.23329221250256
Iteration: 8 || Loss: 71.20916171256225
Iteration: 9 || Loss: 71.19648461793834
Iteration: 10 || Loss: 71.1816181960448
Iteration: 11 || Loss: 71.14227939252437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.31535
Epoch 166 loss:71.14227939252437
MSE loss S1.0043975825266
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.628243455000923
MSE loss S0.3538434799821069
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:124.26392375652736
MSE loss S2.3572074427202794
waveform batch: 2/2
Test loss - extrapolation:63.866838090545166
MSE loss S1.4284304986403569
Epoch 166 mean train loss:3.4838674974626738
Epoch 166 mean test loss - interpolation:4.104707242500154
Epoch 166 mean test loss - extrapolation:15.677563487256045
Start training epoch 167
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.31535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.486449623968079
Iteration: 2 || Loss: 6.4852937623054165
Iteration: 3 || Loss: 6.484106427034621
Iteration: 4 || Loss: 6.482854790455568
Iteration: 5 || Loss: 6.481636373392179
Iteration: 6 || Loss: 6.481636373392179
saving ADAM checkpoint...
Sum of params:-105.31526
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.481636373392179
Iteration: 2 || Loss: 6.429694215316794
Iteration: 3 || Loss: 6.374770506619916
Iteration: 4 || Loss: 6.306974557966145
Iteration: 5 || Loss: 6.209994826635481
Iteration: 6 || Loss: 6.1542202850944205
Iteration: 7 || Loss: 6.141199087939813
Iteration: 8 || Loss: 6.133706015630366
Iteration: 9 || Loss: 6.130938070000043
Iteration: 10 || Loss: 6.124802288543042
Iteration: 11 || Loss: 6.108230594741146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.330215
Epoch 167 loss:6.108230594741146
MSE loss S0.2137065428451659
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.330215
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.270129948376713
Iteration: 2 || Loss: 24.269461020610166
Iteration: 3 || Loss: 24.268788538929734
Iteration: 4 || Loss: 24.26809280984282
Iteration: 5 || Loss: 24.267582705747138
Iteration: 6 || Loss: 24.267582705747138
saving ADAM checkpoint...
Sum of params:-105.330414
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.267582705747138
Iteration: 2 || Loss: 24.254155956110516
Iteration: 3 || Loss: 24.138135427829923
Iteration: 4 || Loss: 24.075968298465263
Iteration: 5 || Loss: 23.89598592835487
Iteration: 6 || Loss: 23.868997275781666
Iteration: 7 || Loss: 23.847842191884315
Iteration: 8 || Loss: 23.80878699673212
Iteration: 9 || Loss: 23.79804851326076
Iteration: 10 || Loss: 23.789837924224916
Iteration: 11 || Loss: 23.775040261756352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.28112
Epoch 167 loss:23.775040261756352
MSE loss S0.42086890064904664
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.28112
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.66345583039421
Iteration: 2 || Loss: 71.66269498850947
Iteration: 3 || Loss: 71.66193390219085
Iteration: 4 || Loss: 71.66125435520343
Iteration: 5 || Loss: 71.66055027941157
Iteration: 6 || Loss: 71.66055027941157
saving ADAM checkpoint...
Sum of params:-105.28119
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.66055027941157
Iteration: 2 || Loss: 71.6495040281835
Iteration: 3 || Loss: 71.61818312276264
Iteration: 4 || Loss: 71.36698075038828
Iteration: 5 || Loss: 71.30710311025847
Iteration: 6 || Loss: 71.23069837154465
Iteration: 7 || Loss: 71.15674572740365
Iteration: 8 || Loss: 71.13321741120798
Iteration: 9 || Loss: 71.12105446019567
Iteration: 10 || Loss: 71.10589975734744
Iteration: 11 || Loss: 71.06625460960807
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.340965
Epoch 167 loss:71.06625460960807
MSE loss S1.0033757628859057
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.615214021054218
MSE loss S0.35372290621707553
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:124.03170984669057
MSE loss S2.353772239884149
waveform batch: 2/2
Test loss - extrapolation:63.63407683493477
MSE loss S1.4242062489334302
Epoch 167 mean train loss:3.481018119520882
Epoch 167 mean test loss - interpolation:4.102535670175703
Epoch 167 mean test loss - extrapolation:15.638815556802113
Start training epoch 168
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.340965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.49023933617101
Iteration: 2 || Loss: 6.4890723158649335
Iteration: 3 || Loss: 6.4878338237256274
Iteration: 4 || Loss: 6.48667947393429
Iteration: 5 || Loss: 6.485427470693581
Iteration: 6 || Loss: 6.485427470693581
saving ADAM checkpoint...
Sum of params:-105.340866
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.485427470693581
Iteration: 2 || Loss: 6.433425529695799
Iteration: 3 || Loss: 6.376451573732211
Iteration: 4 || Loss: 6.307002205545745
Iteration: 5 || Loss: 6.209535114890313
Iteration: 6 || Loss: 6.154060189604786
Iteration: 7 || Loss: 6.140585138559536
Iteration: 8 || Loss: 6.133031553053251
Iteration: 9 || Loss: 6.130348341464337
Iteration: 10 || Loss: 6.124096791017605
Iteration: 11 || Loss: 6.107449767284236
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.3556
Epoch 168 loss:6.107449767284236
MSE loss S0.21366422037958677
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.3556
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.265272489172286
Iteration: 2 || Loss: 24.26457049942487
Iteration: 3 || Loss: 24.26388424960639
Iteration: 4 || Loss: 24.263293534885594
Iteration: 5 || Loss: 24.26261172854182
Iteration: 6 || Loss: 24.26261172854182
saving ADAM checkpoint...
Sum of params:-105.35578
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.26261172854182
Iteration: 2 || Loss: 24.249314312594862
Iteration: 3 || Loss: 24.129858039785102
Iteration: 4 || Loss: 24.071923098444028
Iteration: 5 || Loss: 23.88982335147406
Iteration: 6 || Loss: 23.862949490466395
Iteration: 7 || Loss: 23.841363129169295
Iteration: 8 || Loss: 23.801163775297706
Iteration: 9 || Loss: 23.791834189615336
Iteration: 10 || Loss: 23.783256777585148
Iteration: 11 || Loss: 23.767454104831124
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.3057
Epoch 168 loss:23.767454104831124
MSE loss S0.4208518061941202
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.3057
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.59143635378524
Iteration: 2 || Loss: 71.59065702289524
Iteration: 3 || Loss: 71.5899846785818
Iteration: 4 || Loss: 71.58923695680923
Iteration: 5 || Loss: 71.58854245302531
Iteration: 6 || Loss: 71.58854245302531
saving ADAM checkpoint...
Sum of params:-105.30581
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.58854245302531
Iteration: 2 || Loss: 71.57695484768546
Iteration: 3 || Loss: 71.54491468831975
Iteration: 4 || Loss: 71.30214278040913
Iteration: 5 || Loss: 71.23213004053405
Iteration: 6 || Loss: 71.15579327979502
Iteration: 7 || Loss: 71.08095764856797
Iteration: 8 || Loss: 71.05732519036493
Iteration: 9 || Loss: 71.04505847891585
Iteration: 10 || Loss: 71.03021851968677
Iteration: 11 || Loss: 70.9924305810253
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.364815
Epoch 168 loss:70.9924305810253
MSE loss S1.002915277130299
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.5985137860767
MSE loss S0.3534120889445729
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:123.80891538093123
MSE loss S2.3506126455835634
waveform batch: 2/2
Test loss - extrapolation:63.41124859711145
MSE loss S1.4207109993756029
Epoch 168 mean train loss:3.4781839466600233
Epoch 168 mean test loss - interpolation:4.0997522976794505
Epoch 168 mean test loss - extrapolation:15.601680331503557
Start training epoch 169
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.364815
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.480497165809558
Iteration: 2 || Loss: 6.479364111473066
Iteration: 3 || Loss: 6.478149407185052
Iteration: 4 || Loss: 6.476975203731794
Iteration: 5 || Loss: 6.475856121222809
Iteration: 6 || Loss: 6.475856121222809
saving ADAM checkpoint...
Sum of params:-105.36473
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.475856121222809
Iteration: 2 || Loss: 6.42567869317285
Iteration: 3 || Loss: 6.371126843460182
Iteration: 4 || Loss: 6.30401112570061
Iteration: 5 || Loss: 6.207096348690772
Iteration: 6 || Loss: 6.151867919262194
Iteration: 7 || Loss: 6.138390798263811
Iteration: 8 || Loss: 6.1309924901523765
Iteration: 9 || Loss: 6.12809352613776
Iteration: 10 || Loss: 6.12250929221428
Iteration: 11 || Loss: 6.1059066238866215
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.380035
Epoch 169 loss:6.1059066238866215
MSE loss S0.2133919417601601
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.380035
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.262777552473104
Iteration: 2 || Loss: 24.26215671318797
Iteration: 3 || Loss: 24.261474371965754
Iteration: 4 || Loss: 24.260895715173863
Iteration: 5 || Loss: 24.260308785401325
Iteration: 6 || Loss: 24.260308785401325
saving ADAM checkpoint...
Sum of params:-105.38022
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.260308785401325
Iteration: 2 || Loss: 24.247571643914807
Iteration: 3 || Loss: 24.12948147299411
Iteration: 4 || Loss: 24.06750374464571
Iteration: 5 || Loss: 23.882624893567918
Iteration: 6 || Loss: 23.856237904344454
Iteration: 7 || Loss: 23.835181457198022
Iteration: 8 || Loss: 23.79519916856437
Iteration: 9 || Loss: 23.78536846759448
Iteration: 10 || Loss: 23.776726662052933
Iteration: 11 || Loss: 23.76155717146661
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.33092
Epoch 169 loss:23.76155717146661
MSE loss S0.42043177095844864
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.33092
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.51686980835711
Iteration: 2 || Loss: 71.51613168518541
Iteration: 3 || Loss: 71.51543867376611
Iteration: 4 || Loss: 71.51466183549793
Iteration: 5 || Loss: 71.51393541579722
Iteration: 6 || Loss: 71.51393541579722
saving ADAM checkpoint...
Sum of params:-105.33101
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.51393541579722
Iteration: 2 || Loss: 71.50310774221207
Iteration: 3 || Loss: 71.47174219484121
Iteration: 4 || Loss: 71.22556904529405
Iteration: 5 || Loss: 71.15894974788833
Iteration: 6 || Loss: 71.08241488028943
Iteration: 7 || Loss: 71.00842930274007
Iteration: 8 || Loss: 70.98485195917132
Iteration: 9 || Loss: 70.97301070974488
Iteration: 10 || Loss: 70.95845361032727
Iteration: 11 || Loss: 70.92106520461948
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.38907
Epoch 169 loss:70.92106520461948
MSE loss S1.0022731092165138
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.58401450251388
MSE loss S0.3534074206663702
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:123.59113846706424
MSE loss S2.347301511165928
waveform batch: 2/2
Test loss - extrapolation:63.193270043149084
MSE loss S1.4166555753250258
Epoch 169 mean train loss:3.4754665172404384
Epoch 169 mean test loss - interpolation:4.0973357504189805
Epoch 169 mean test loss - extrapolation:15.56536737585111
Start training epoch 170
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.38907
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.480512991416961
Iteration: 2 || Loss: 6.479303520902758
Iteration: 3 || Loss: 6.478185285436233
Iteration: 4 || Loss: 6.476940822023202
Iteration: 5 || Loss: 6.475778427815578
Iteration: 6 || Loss: 6.475778427815578
saving ADAM checkpoint...
Sum of params:-105.38896
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.475778427815578
Iteration: 2 || Loss: 6.426799370219615
Iteration: 3 || Loss: 6.371519913925567
Iteration: 4 || Loss: 6.303654751156753
Iteration: 5 || Loss: 6.205638927592048
Iteration: 6 || Loss: 6.150167179490861
Iteration: 7 || Loss: 6.1371735256512165
Iteration: 8 || Loss: 6.129927437829486
Iteration: 9 || Loss: 6.127121291551379
Iteration: 10 || Loss: 6.121288360702824
Iteration: 11 || Loss: 6.104772772450854
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.404175
Epoch 170 loss:6.104772772450854
MSE loss S0.2132489196092802
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.404175
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.25625324672491
Iteration: 2 || Loss: 24.2556152911368
Iteration: 3 || Loss: 24.254911719003434
Iteration: 4 || Loss: 24.254239778934163
Iteration: 5 || Loss: 24.253684730581046
Iteration: 6 || Loss: 24.253684730581046
saving ADAM checkpoint...
Sum of params:-105.40437
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.253684730581046
Iteration: 2 || Loss: 24.24109050673463
Iteration: 3 || Loss: 24.117187713016015
Iteration: 4 || Loss: 24.062368136102734
Iteration: 5 || Loss: 23.875802804599182
Iteration: 6 || Loss: 23.849509831743248
Iteration: 7 || Loss: 23.828466485783522
Iteration: 8 || Loss: 23.787471578738224
Iteration: 9 || Loss: 23.778691164427144
Iteration: 10 || Loss: 23.769583286909022
Iteration: 11 || Loss: 23.75348652310629
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.35443
Epoch 170 loss:23.75348652310629
MSE loss S0.4199564693114308
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.35443
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.45477385387461
Iteration: 2 || Loss: 71.45399716613714
Iteration: 3 || Loss: 71.45321335412696
Iteration: 4 || Loss: 71.4525128548247
Iteration: 5 || Loss: 71.45180622134122
Iteration: 6 || Loss: 71.45180622134122
saving ADAM checkpoint...
Sum of params:-105.354515
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.45180622134122
Iteration: 2 || Loss: 71.4408045167518
Iteration: 3 || Loss: 71.40891182475015
Iteration: 4 || Loss: 71.16999459746228
Iteration: 5 || Loss: 71.09010329434386
Iteration: 6 || Loss: 71.01365583248105
Iteration: 7 || Loss: 70.93863543846685
Iteration: 8 || Loss: 70.91465031787719
Iteration: 9 || Loss: 70.9028182950348
Iteration: 10 || Loss: 70.88840319972871
Iteration: 11 || Loss: 70.8528135005565
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.41199
Epoch 170 loss:70.8528135005565
MSE loss S1.0018045303567618
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.567091999710353
MSE loss S0.35301606419226583
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:123.38540684189415
MSE loss S2.3445132934020982
waveform batch: 2/2
Test loss - extrapolation:62.985970881994504
MSE loss S1.4134831500487324
Epoch 170 mean train loss:3.472795613659091
Epoch 170 mean test loss - interpolation:4.094515333285059
Epoch 170 mean test loss - extrapolation:15.530948143657389
Start training epoch 171
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.41199
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.472321343792462
Iteration: 2 || Loss: 6.471214241905397
Iteration: 3 || Loss: 6.470041800352848
Iteration: 4 || Loss: 6.468896914598431
Iteration: 5 || Loss: 6.46772711970663
Iteration: 6 || Loss: 6.46772711970663
saving ADAM checkpoint...
Sum of params:-105.4119
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.46772711970663
Iteration: 2 || Loss: 6.42119929459741
Iteration: 3 || Loss: 6.367340965568351
Iteration: 4 || Loss: 6.301042870447584
Iteration: 5 || Loss: 6.203195072891301
Iteration: 6 || Loss: 6.147678915036857
Iteration: 7 || Loss: 6.134769340441421
Iteration: 8 || Loss: 6.127702213393311
Iteration: 9 || Loss: 6.124813373729501
Iteration: 10 || Loss: 6.119491579043248
Iteration: 11 || Loss: 6.102822505243668
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.427795
Epoch 171 loss:6.102822505243668
MSE loss S0.2126356036609109
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.427795
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.245635652941736
Iteration: 2 || Loss: 24.244931753766263
Iteration: 3 || Loss: 24.244328837754896
Iteration: 4 || Loss: 24.24374754303063
Iteration: 5 || Loss: 24.24314239790129
Iteration: 6 || Loss: 24.24314239790129
saving ADAM checkpoint...
Sum of params:-105.42797
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.24314239790129
Iteration: 2 || Loss: 24.23058331197101
Iteration: 3 || Loss: 24.1167018580353
Iteration: 4 || Loss: 24.055216411166672
Iteration: 5 || Loss: 23.870047843098632
Iteration: 6 || Loss: 23.842786901172012
Iteration: 7 || Loss: 23.82206474476915
Iteration: 8 || Loss: 23.78197333464657
Iteration: 9 || Loss: 23.772200882975646
Iteration: 10 || Loss: 23.762573143676196
Iteration: 11 || Loss: 23.74611426462553
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.377975
Epoch 171 loss:23.74611426462553
MSE loss S0.4198148513300458
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.377975
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.39998378070734
Iteration: 2 || Loss: 71.39919819047502
Iteration: 3 || Loss: 71.39843032427224
Iteration: 4 || Loss: 71.39765994730304
Iteration: 5 || Loss: 71.39690449534399
Iteration: 6 || Loss: 71.39690449534399
saving ADAM checkpoint...
Sum of params:-105.37808
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.39690449534399
Iteration: 2 || Loss: 71.38554750556274
Iteration: 3 || Loss: 71.35253639617784
Iteration: 4 || Loss: 71.10982490883734
Iteration: 5 || Loss: 71.0263863094887
Iteration: 6 || Loss: 70.94856006906926
Iteration: 7 || Loss: 70.87255908743441
Iteration: 8 || Loss: 70.84837840622349
Iteration: 9 || Loss: 70.83670555685029
Iteration: 10 || Loss: 70.82258197957867
Iteration: 11 || Loss: 70.78917501302536
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.43382
Epoch 171 loss:70.78917501302536
MSE loss S1.0019682781718302
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.549993828639703
MSE loss S0.3529598335060386
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:123.191870928177
MSE loss S2.342256082230876
waveform batch: 2/2
Test loss - extrapolation:62.793437154843595
MSE loss S1.4103164320021548
Epoch 171 mean train loss:3.4702797166515365
Epoch 171 mean test loss - interpolation:4.091665638106617
Epoch 171 mean test loss - extrapolation:15.49877567358505
Start training epoch 172
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.43382
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.46771569770822
Iteration: 2 || Loss: 6.466514415345068
Iteration: 3 || Loss: 6.465401712161525
Iteration: 4 || Loss: 6.464271935153806
Iteration: 5 || Loss: 6.463169592441472
Iteration: 6 || Loss: 6.463169592441472
saving ADAM checkpoint...
Sum of params:-105.433754
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.463169592441472
Iteration: 2 || Loss: 6.418319949470792
Iteration: 3 || Loss: 6.365435344597785
Iteration: 4 || Loss: 6.299532955440271
Iteration: 5 || Loss: 6.200060746065933
Iteration: 6 || Loss: 6.143625660677441
Iteration: 7 || Loss: 6.13234711496049
Iteration: 8 || Loss: 6.12546033134333
Iteration: 9 || Loss: 6.1225790059171405
Iteration: 10 || Loss: 6.117322732355252
Iteration: 11 || Loss: 6.1009369597279175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.45003
Epoch 172 loss:6.1009369597279175
MSE loss S0.21225153396860608
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.45003
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.24030806048527
Iteration: 2 || Loss: 24.239715061702846
Iteration: 3 || Loss: 24.23908732098523
Iteration: 4 || Loss: 24.238472551361834
Iteration: 5 || Loss: 24.237855204211726
Iteration: 6 || Loss: 24.237855204211726
saving ADAM checkpoint...
Sum of params:-105.450226
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.237855204211726
Iteration: 2 || Loss: 24.225501684034207
Iteration: 3 || Loss: 24.111368032996328
Iteration: 4 || Loss: 24.050029995091766
Iteration: 5 || Loss: 23.862658279500153
Iteration: 6 || Loss: 23.83538297099467
Iteration: 7 || Loss: 23.815099180461168
Iteration: 8 || Loss: 23.77533764745583
Iteration: 9 || Loss: 23.765150835998714
Iteration: 10 || Loss: 23.75592500496217
Iteration: 11 || Loss: 23.740461170742194
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.40196
Epoch 172 loss:23.740461170742194
MSE loss S0.41902756063384733
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.40196
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.30838862477519
Iteration: 2 || Loss: 71.30764119789345
Iteration: 3 || Loss: 71.30697017910543
Iteration: 4 || Loss: 71.30624045749266
Iteration: 5 || Loss: 71.30553392929366
Iteration: 6 || Loss: 71.30553392929366
saving ADAM checkpoint...
Sum of params:-105.40204
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.30553392929366
Iteration: 2 || Loss: 71.29540304743107
Iteration: 3 || Loss: 71.26519488482408
Iteration: 4 || Loss: 71.03450397308487
Iteration: 5 || Loss: 70.95749265433246
Iteration: 6 || Loss: 70.88125936603639
Iteration: 7 || Loss: 70.8090119874037
Iteration: 8 || Loss: 70.78576390780724
Iteration: 9 || Loss: 70.77445982893124
Iteration: 10 || Loss: 70.76001534821194
Iteration: 11 || Loss: 70.72514016541665
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.45887
Epoch 172 loss:70.72514016541665
MSE loss S1.0008100351788274
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.539160410819225
MSE loss S0.35272299017522335
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:122.99299104115198
MSE loss S2.3390205251879763
waveform batch: 2/2
Test loss - extrapolation:62.59241707741218
MSE loss S1.4065378970077993
Epoch 172 mean train loss:3.4678116653754056
Epoch 172 mean test loss - interpolation:4.0898600684698705
Epoch 172 mean test loss - extrapolation:15.465450676547013
Start training epoch 173
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.45887
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.476079447379548
Iteration: 2 || Loss: 6.474939680482496
Iteration: 3 || Loss: 6.473839654875321
Iteration: 4 || Loss: 6.47273542481178
Iteration: 5 || Loss: 6.4715707727138545
Iteration: 6 || Loss: 6.4715707727138545
saving ADAM checkpoint...
Sum of params:-105.458786
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4715707727138545
Iteration: 2 || Loss: 6.426606397725103
Iteration: 3 || Loss: 6.3702751854189055
Iteration: 4 || Loss: 6.301384045006978
Iteration: 5 || Loss: 6.199755246713439
Iteration: 6 || Loss: 6.143515906698464
Iteration: 7 || Loss: 6.131769600630097
Iteration: 8 || Loss: 6.124787340685232
Iteration: 9 || Loss: 6.122148422416769
Iteration: 10 || Loss: 6.116469637914319
Iteration: 11 || Loss: 6.1003943043539
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.47485
Epoch 173 loss:6.1003943043539
MSE loss S0.21286333473656072
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.47485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.246845359429877
Iteration: 2 || Loss: 24.24613305320485
Iteration: 3 || Loss: 24.24561955190997
Iteration: 4 || Loss: 24.24494307204285
Iteration: 5 || Loss: 24.244260430279116
Iteration: 6 || Loss: 24.244260430279116
saving ADAM checkpoint...
Sum of params:-105.47504
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.244260430279116
Iteration: 2 || Loss: 24.231555594727126
Iteration: 3 || Loss: 24.110306124928538
Iteration: 4 || Loss: 24.048181280394875
Iteration: 5 || Loss: 23.85583578191794
Iteration: 6 || Loss: 23.828758905538923
Iteration: 7 || Loss: 23.808330382035212
Iteration: 8 || Loss: 23.76749165601349
Iteration: 9 || Loss: 23.75850880072135
Iteration: 10 || Loss: 23.749592191534635
Iteration: 11 || Loss: 23.73438481683283
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.42678
Epoch 173 loss:23.73438481683283
MSE loss S0.4194049122935596
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.42678
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.2395140251153
Iteration: 2 || Loss: 71.23876316552271
Iteration: 3 || Loss: 71.23802548676366
Iteration: 4 || Loss: 71.23726273180999
Iteration: 5 || Loss: 71.2366649816192
Iteration: 6 || Loss: 71.2366649816192
saving ADAM checkpoint...
Sum of params:-105.42688
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.2366649816192
Iteration: 2 || Loss: 71.22629994363875
Iteration: 3 || Loss: 71.19604967825813
Iteration: 4 || Loss: 70.96084016419232
Iteration: 5 || Loss: 70.89158870517511
Iteration: 6 || Loss: 70.815705124555
Iteration: 7 || Loss: 70.74382668073156
Iteration: 8 || Loss: 70.72050294692568
Iteration: 9 || Loss: 70.70930903934364
Iteration: 10 || Loss: 70.69497456128475
Iteration: 11 || Loss: 70.66112441757676
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.482956
Epoch 173 loss:70.66112441757676
MSE loss S1.0004642188188122
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.525190873107427
MSE loss S0.3526162485801434
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:122.79379261734563
MSE loss S2.3363505754846283
waveform batch: 2/2
Test loss - extrapolation:62.39528613095208
MSE loss S1.4032005560797076
Epoch 173 mean train loss:3.4653759840952927
Epoch 173 mean test loss - interpolation:4.087531812184571
Epoch 173 mean test loss - extrapolation:15.432423229024808
Start training epoch 174
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.482956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.473618913125871
Iteration: 2 || Loss: 6.472511494083187
Iteration: 3 || Loss: 6.471344023465887
Iteration: 4 || Loss: 6.470260551403685
Iteration: 5 || Loss: 6.469163471886187
Iteration: 6 || Loss: 6.469163471886187
saving ADAM checkpoint...
Sum of params:-105.482864
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.469163471886187
Iteration: 2 || Loss: 6.425365459898964
Iteration: 3 || Loss: 6.368990558518551
Iteration: 4 || Loss: 6.300121133900241
Iteration: 5 || Loss: 6.198076846233523
Iteration: 6 || Loss: 6.141891859365181
Iteration: 7 || Loss: 6.130237536511102
Iteration: 8 || Loss: 6.123365878267354
Iteration: 9 || Loss: 6.120704258685922
Iteration: 10 || Loss: 6.1151883052032225
Iteration: 11 || Loss: 6.098950905828512
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.499
Epoch 174 loss:6.098950905828512
MSE loss S0.21231117179004866
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.499
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.23797052044699
Iteration: 2 || Loss: 24.237422490666816
Iteration: 3 || Loss: 24.236711328573215
Iteration: 4 || Loss: 24.236136313505202
Iteration: 5 || Loss: 24.235589990001742
Iteration: 6 || Loss: 24.235589990001742
saving ADAM checkpoint...
Sum of params:-105.499176
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.235589990001742
Iteration: 2 || Loss: 24.223364780333004
Iteration: 3 || Loss: 24.10080401628676
Iteration: 4 || Loss: 24.04239247520251
Iteration: 5 || Loss: 23.849499125865623
Iteration: 6 || Loss: 23.822510944248688
Iteration: 7 || Loss: 23.802313144472787
Iteration: 8 || Loss: 23.76128743637513
Iteration: 9 || Loss: 23.752525324846843
Iteration: 10 || Loss: 23.743088483441408
Iteration: 11 || Loss: 23.72735711984318
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.4506
Epoch 174 loss:23.72735711984318
MSE loss S0.41855852702624774
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.4506
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.18453499931736
Iteration: 2 || Loss: 71.18377331747222
Iteration: 3 || Loss: 71.18303710834822
Iteration: 4 || Loss: 71.1823189324247
Iteration: 5 || Loss: 71.18156496328432
Iteration: 6 || Loss: 71.18156496328432
saving ADAM checkpoint...
Sum of params:-105.4507
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.18156496328432
Iteration: 2 || Loss: 71.17204569131262
Iteration: 3 || Loss: 71.1419607775224
Iteration: 4 || Loss: 70.90816664576437
Iteration: 5 || Loss: 70.83025604062243
Iteration: 6 || Loss: 70.75405476471633
Iteration: 7 || Loss: 70.68098812799496
Iteration: 8 || Loss: 70.65748146012815
Iteration: 9 || Loss: 70.64661028524498
Iteration: 10 || Loss: 70.63254276782753
Iteration: 11 || Loss: 70.59997098586075
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.50568
Epoch 174 loss:70.59997098586075
MSE loss S0.999903503313611
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.509718390185327
MSE loss S0.3522440084844316
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:122.60159994709018
MSE loss S2.333602762018923
waveform batch: 2/2
Test loss - extrapolation:62.206415265208285
MSE loss S1.3999484247136698
Epoch 174 mean train loss:3.462975138328705
Epoch 174 mean test loss - interpolation:4.084953065030888
Epoch 174 mean test loss - extrapolation:15.400667934358205
Start training epoch 175
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.50568
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.468572642730209
Iteration: 2 || Loss: 6.467429309128204
Iteration: 3 || Loss: 6.466287877231731
Iteration: 4 || Loss: 6.465247661389544
Iteration: 5 || Loss: 6.464177493886274
Iteration: 6 || Loss: 6.464177493886274
saving ADAM checkpoint...
Sum of params:-105.5056
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.464177493886274
Iteration: 2 || Loss: 6.4215440532127
Iteration: 3 || Loss: 6.366152775922011
Iteration: 4 || Loss: 6.298114390267987
Iteration: 5 || Loss: 6.195619649663302
Iteration: 6 || Loss: 6.139187201776544
Iteration: 7 || Loss: 6.128087700193587
Iteration: 8 || Loss: 6.121419000734132
Iteration: 9 || Loss: 6.118679797655953
Iteration: 10 || Loss: 6.113380022954661
Iteration: 11 || Loss: 6.097348588619645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.522156
Epoch 175 loss:6.097348588619645
MSE loss S0.2121114504372006
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.522156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.235447569653903
Iteration: 2 || Loss: 24.234816811363686
Iteration: 3 || Loss: 24.234168159890647
Iteration: 4 || Loss: 24.233589163393056
Iteration: 5 || Loss: 24.23305613382268
Iteration: 6 || Loss: 24.23305613382268
saving ADAM checkpoint...
Sum of params:-105.52235
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.23305613382268
Iteration: 2 || Loss: 24.220868160615733
Iteration: 3 || Loss: 24.100554193070302
Iteration: 4 || Loss: 24.038861360637547
Iteration: 5 || Loss: 23.843167472602193
Iteration: 6 || Loss: 23.815849847279175
Iteration: 7 || Loss: 23.796104163670694
Iteration: 8 || Loss: 23.75549812060459
Iteration: 9 || Loss: 23.746155643706846
Iteration: 10 || Loss: 23.736752751787954
Iteration: 11 || Loss: 23.721370947690136
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.474594
Epoch 175 loss:23.721370947690136
MSE loss S0.4183163369623642
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.474594
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.11952205431899
Iteration: 2 || Loss: 71.11879622887126
Iteration: 3 || Loss: 71.11802841313799
Iteration: 4 || Loss: 71.11733356009812
Iteration: 5 || Loss: 71.11658094337858
Iteration: 6 || Loss: 71.11658094337858
saving ADAM checkpoint...
Sum of params:-105.47469
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.11658094337858
Iteration: 2 || Loss: 71.10723222182145
Iteration: 3 || Loss: 71.07712822078976
Iteration: 4 || Loss: 70.8426718848794
Iteration: 5 || Loss: 70.76859612272636
Iteration: 6 || Loss: 70.69270250556504
Iteration: 7 || Loss: 70.62082599588608
Iteration: 8 || Loss: 70.59755582421364
Iteration: 9 || Loss: 70.58688527006649
Iteration: 10 || Loss: 70.57299539691283
Iteration: 11 || Loss: 70.5407230496998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.52919
Epoch 175 loss:70.5407230496998
MSE loss S0.9995763239469717
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.496739256772535
MSE loss S0.3521834134412161
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:122.41263874814786
MSE loss S2.3308766016162625
waveform batch: 2/2
Test loss - extrapolation:62.02150752735236
MSE loss S1.3966728145308156
Epoch 175 mean train loss:3.4606704340003307
Epoch 175 mean test loss - interpolation:4.082789876128756
Epoch 175 mean test loss - extrapolation:15.369512189625018
Start training epoch 176
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.52919
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4682671850509985
Iteration: 2 || Loss: 6.46707303981724
Iteration: 3 || Loss: 6.466036001291807
Iteration: 4 || Loss: 6.464948487721138
Iteration: 5 || Loss: 6.463853114295708
Iteration: 6 || Loss: 6.463853114295708
saving ADAM checkpoint...
Sum of params:-105.5291
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.463853114295708
Iteration: 2 || Loss: 6.422173748727204
Iteration: 3 || Loss: 6.366353878439247
Iteration: 4 || Loss: 6.297919048652579
Iteration: 5 || Loss: 6.19397119041044
Iteration: 6 || Loss: 6.137367868604633
Iteration: 7 || Loss: 6.126643535558253
Iteration: 8 || Loss: 6.12000814291464
Iteration: 9 || Loss: 6.117405488249995
Iteration: 10 || Loss: 6.111934624849528
Iteration: 11 || Loss: 6.095956719119667
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.54562
Epoch 176 loss:6.095956719119667
MSE loss S0.2119141102480482
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.54562
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.22558957860286
Iteration: 2 || Loss: 24.224947713537123
Iteration: 3 || Loss: 24.224304665842457
Iteration: 4 || Loss: 24.22372959327878
Iteration: 5 || Loss: 24.223138226402423
Iteration: 6 || Loss: 24.223138226402423
saving ADAM checkpoint...
Sum of params:-105.54584
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.223138226402423
Iteration: 2 || Loss: 24.21077218560797
Iteration: 3 || Loss: 24.089934134594433
Iteration: 4 || Loss: 24.030527848627315
Iteration: 5 || Loss: 23.83738745669411
Iteration: 6 || Loss: 23.809514464082806
Iteration: 7 || Loss: 23.789602009241705
Iteration: 8 || Loss: 23.748841843982923
Iteration: 9 || Loss: 23.739884030644625
Iteration: 10 || Loss: 23.730331613851973
Iteration: 11 || Loss: 23.71492857479699
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.49841
Epoch 176 loss:23.71492857479699
MSE loss S0.4177856027823356
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.49841
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.05079399142016
Iteration: 2 || Loss: 71.05004666850277
Iteration: 3 || Loss: 71.04929625400965
Iteration: 4 || Loss: 71.04855425889203
Iteration: 5 || Loss: 71.04790692300257
Iteration: 6 || Loss: 71.04790692300257
saving ADAM checkpoint...
Sum of params:-105.49853
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.04790692300257
Iteration: 2 || Loss: 71.0387907202478
Iteration: 3 || Loss: 71.0100262305081
Iteration: 4 || Loss: 70.78386133185906
Iteration: 5 || Loss: 70.70738511733036
Iteration: 6 || Loss: 70.63202272246947
Iteration: 7 || Loss: 70.56179749860785
Iteration: 8 || Loss: 70.53867614526253
Iteration: 9 || Loss: 70.52787328195245
Iteration: 10 || Loss: 70.51395931892839
Iteration: 11 || Loss: 70.48197634354277
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.5531
Epoch 176 loss:70.48197634354277
MSE loss S0.9990892722594238
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.484185217058375
MSE loss S0.35200238989355803
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:122.22231177866323
MSE loss S2.3281306290241712
waveform batch: 2/2
Test loss - extrapolation:61.83598727849545
MSE loss S1.3935550553524205
Epoch 176 mean train loss:3.458374539222739
Epoch 176 mean test loss - interpolation:4.0806975361763955
Epoch 176 mean test loss - extrapolation:15.338191588096556
Start training epoch 177
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.5531
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.467268347435679
Iteration: 2 || Loss: 6.466150343951584
Iteration: 3 || Loss: 6.465079753446172
Iteration: 4 || Loss: 6.464004154293707
Iteration: 5 || Loss: 6.462972008156917
Iteration: 6 || Loss: 6.462972008156917
saving ADAM checkpoint...
Sum of params:-105.553024
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.462972008156917
Iteration: 2 || Loss: 6.422141838537174
Iteration: 3 || Loss: 6.366085125321586
Iteration: 4 || Loss: 6.297776536001993
Iteration: 5 || Loss: 6.192685344221345
Iteration: 6 || Loss: 6.136111922177767
Iteration: 7 || Loss: 6.125314358205525
Iteration: 8 || Loss: 6.118715171753742
Iteration: 9 || Loss: 6.116145770622049
Iteration: 10 || Loss: 6.1106504759914655
Iteration: 11 || Loss: 6.0947522818832205
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.56954
Epoch 177 loss:6.0947522818832205
MSE loss S0.2116748932990637
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.56954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.22244747162446
Iteration: 2 || Loss: 24.221837121380204
Iteration: 3 || Loss: 24.221161831018005
Iteration: 4 || Loss: 24.220629762040826
Iteration: 5 || Loss: 24.22001475514731
Iteration: 6 || Loss: 24.22001475514731
saving ADAM checkpoint...
Sum of params:-105.56971
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.22001475514731
Iteration: 2 || Loss: 24.20812685349511
Iteration: 3 || Loss: 24.082366260818873
Iteration: 4 || Loss: 24.026784784006008
Iteration: 5 || Loss: 23.83056480900033
Iteration: 6 || Loss: 23.803226777724046
Iteration: 7 || Loss: 23.78362233834563
Iteration: 8 || Loss: 23.742250156999717
Iteration: 9 || Loss: 23.73384229118209
Iteration: 10 || Loss: 23.723887460796544
Iteration: 11 || Loss: 23.70794078274854
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.52175
Epoch 177 loss:23.70794078274854
MSE loss S0.4178510530535364
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.52175
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.00360013564732
Iteration: 2 || Loss: 71.00282940062218
Iteration: 3 || Loss: 71.00207250923523
Iteration: 4 || Loss: 71.00135430135033
Iteration: 5 || Loss: 71.00062773590423
Iteration: 6 || Loss: 71.00062773590423
saving ADAM checkpoint...
Sum of params:-105.52187
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.00062773590423
Iteration: 2 || Loss: 70.99181602761509
Iteration: 3 || Loss: 70.96251321701558
Iteration: 4 || Loss: 70.7324270447343
Iteration: 5 || Loss: 70.65307733087666
Iteration: 6 || Loss: 70.57652264981022
Iteration: 7 || Loss: 70.50410534249703
Iteration: 8 || Loss: 70.48069534545128
Iteration: 9 || Loss: 70.47021115409525
Iteration: 10 || Loss: 70.4564524681836
Iteration: 11 || Loss: 70.42619142026017
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.575195
Epoch 177 loss:70.42619142026017
MSE loss S0.9990407451710427
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.468494576498575
MSE loss S0.3516921502611169
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:122.04658815341224
MSE loss S2.3261108797884402
waveform batch: 2/2
Test loss - extrapolation:61.665139207048455
MSE loss S1.3909075118393766
Epoch 177 mean train loss:3.4561684305135145
Epoch 177 mean test loss - interpolation:4.078082429416429
Epoch 177 mean test loss - extrapolation:15.309310613371727
Start training epoch 178
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.575195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.461173483168988
Iteration: 2 || Loss: 6.460072712346175
Iteration: 3 || Loss: 6.458993791747437
Iteration: 4 || Loss: 6.457890193523701
Iteration: 5 || Loss: 6.456875741549322
Iteration: 6 || Loss: 6.456875741549322
saving ADAM checkpoint...
Sum of params:-105.575096
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.456875741549322
Iteration: 2 || Loss: 6.417290650787779
Iteration: 3 || Loss: 6.362633081218114
Iteration: 4 || Loss: 6.295312456523612
Iteration: 5 || Loss: 6.189909545405884
Iteration: 6 || Loss: 6.133249531248953
Iteration: 7 || Loss: 6.123001846333438
Iteration: 8 || Loss: 6.11657211188528
Iteration: 9 || Loss: 6.113889245806224
Iteration: 10 || Loss: 6.108776205265568
Iteration: 11 || Loss: 6.092926242697175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.59198
Epoch 178 loss:6.092926242697175
MSE loss S0.21131393250984837
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.59198
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.215628783925943
Iteration: 2 || Loss: 24.21507409076412
Iteration: 3 || Loss: 24.21443698873112
Iteration: 4 || Loss: 24.213864134706796
Iteration: 5 || Loss: 24.21328140988241
Iteration: 6 || Loss: 24.21328140988241
saving ADAM checkpoint...
Sum of params:-105.59216
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.21328140988241
Iteration: 2 || Loss: 24.201574188341038
Iteration: 3 || Loss: 24.076883862841196
Iteration: 4 || Loss: 24.02030915439819
Iteration: 5 || Loss: 23.824149586918384
Iteration: 6 || Loss: 23.79643612095012
Iteration: 7 || Loss: 23.777308080660834
Iteration: 8 || Loss: 23.73673141744811
Iteration: 9 || Loss: 23.727520002880013
Iteration: 10 || Loss: 23.71777435012951
Iteration: 11 || Loss: 23.702687051249015
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.54582
Epoch 178 loss:23.702687051249015
MSE loss S0.41720559640109245
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.54582
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.92655542660029
Iteration: 2 || Loss: 70.92579037682913
Iteration: 3 || Loss: 70.92505641382063
Iteration: 4 || Loss: 70.92432590959025
Iteration: 5 || Loss: 70.9236487960078
Iteration: 6 || Loss: 70.9236487960078
saving ADAM checkpoint...
Sum of params:-105.54594
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.9236487960078
Iteration: 2 || Loss: 70.9152794177158
Iteration: 3 || Loss: 70.88763086006671
Iteration: 4 || Loss: 70.66752154701965
Iteration: 5 || Loss: 70.5922104378061
Iteration: 6 || Loss: 70.51699971822171
Iteration: 7 || Loss: 70.44861811295014
Iteration: 8 || Loss: 70.42586166349585
Iteration: 9 || Loss: 70.41524480386178
Iteration: 10 || Loss: 70.40160120109904
Iteration: 11 || Loss: 70.37029007692769
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.599915
Epoch 178 loss:70.37029007692769
MSE loss S0.9983394212311671
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.458502572588614
MSE loss S0.3516355095584692
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:121.86257077875854
MSE loss S2.3232238959423057
waveform batch: 2/2
Test loss - extrapolation:61.48597706359002
MSE loss S1.387589894002589
Epoch 178 mean train loss:3.453996667961168
Epoch 178 mean test loss - interpolation:4.076417095431435
Epoch 178 mean test loss - extrapolation:15.279045653529046
Start training epoch 179
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.599915
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.467280339106843
Iteration: 2 || Loss: 6.466213538385432
Iteration: 3 || Loss: 6.465158435092118
Iteration: 4 || Loss: 6.464082457063211
Iteration: 5 || Loss: 6.463040924199439
Iteration: 6 || Loss: 6.463040924199439
saving ADAM checkpoint...
Sum of params:-105.59986
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.463040924199439
Iteration: 2 || Loss: 6.423924334465408
Iteration: 3 || Loss: 6.36647368381701
Iteration: 4 || Loss: 6.2971997618931255
Iteration: 5 || Loss: 6.18949513818062
Iteration: 6 || Loss: 6.132812370077302
Iteration: 7 || Loss: 6.122357223509343
Iteration: 8 || Loss: 6.1158834869474505
Iteration: 9 || Loss: 6.1133982632522255
Iteration: 10 || Loss: 6.107844448058015
Iteration: 11 || Loss: 6.0923962099725335
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.616455
Epoch 179 loss:6.0923962099725335
MSE loss S0.21177274404623625
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.616455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.219018472414014
Iteration: 2 || Loss: 24.218397079768557
Iteration: 3 || Loss: 24.217839163394277
Iteration: 4 || Loss: 24.217298714575804
Iteration: 5 || Loss: 24.216616208434147
Iteration: 6 || Loss: 24.216616208434147
saving ADAM checkpoint...
Sum of params:-105.61663
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.216616208434147
Iteration: 2 || Loss: 24.204623475951
Iteration: 3 || Loss: 24.0715281568552
Iteration: 4 || Loss: 24.017448905247274
Iteration: 5 || Loss: 23.81762499424755
Iteration: 6 || Loss: 23.790253859061774
Iteration: 7 || Loss: 23.77104536499324
Iteration: 8 || Loss: 23.729353340345238
Iteration: 9 || Loss: 23.721412336287717
Iteration: 10 || Loss: 23.71194264845121
Iteration: 11 || Loss: 23.697040133732532
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.57049
Epoch 179 loss:23.697040133732532
MSE loss S0.4173620677324821
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.57049
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.86031532101018
Iteration: 2 || Loss: 70.85956156433903
Iteration: 3 || Loss: 70.85895034248776
Iteration: 4 || Loss: 70.85822951155885
Iteration: 5 || Loss: 70.85747525771684
Iteration: 6 || Loss: 70.85747525771684
saving ADAM checkpoint...
Sum of params:-105.57059
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.85747525771684
Iteration: 2 || Loss: 70.84900069172609
Iteration: 3 || Loss: 70.82201464859811
Iteration: 4 || Loss: 70.60345745344813
Iteration: 5 || Loss: 70.53336077653151
Iteration: 6 || Loss: 70.45901092180972
Iteration: 7 || Loss: 70.39157846608408
Iteration: 8 || Loss: 70.36887772213261
Iteration: 9 || Loss: 70.35807830609588
Iteration: 10 || Loss: 70.34456907893109
Iteration: 11 || Loss: 70.31381020300906
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.62433
Epoch 179 loss:70.31381020300906
MSE loss S0.9981202003370089
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.446265407490944
MSE loss S0.35160088884283425
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:121.67556199928703
MSE loss S2.320649398127892
waveform batch: 2/2
Test loss - extrapolation:61.30724800234688
MSE loss S1.3846154178785497
Epoch 179 mean train loss:3.4518360878177283
Epoch 179 mean test loss - interpolation:4.074377567915158
Epoch 179 mean test loss - extrapolation:15.24856750013616
Start training epoch 180
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.62433
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.465085856467134
Iteration: 2 || Loss: 6.46398552091924
Iteration: 3 || Loss: 6.46293042209949
Iteration: 4 || Loss: 6.461919085906713
Iteration: 5 || Loss: 6.46084002152288
Iteration: 6 || Loss: 6.46084002152288
saving ADAM checkpoint...
Sum of params:-105.62423
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.46084002152288
Iteration: 2 || Loss: 6.423227633179344
Iteration: 3 || Loss: 6.365516285162963
Iteration: 4 || Loss: 6.296514977564125
Iteration: 5 || Loss: 6.188216045804011
Iteration: 6 || Loss: 6.131793499988347
Iteration: 7 || Loss: 6.121251726940202
Iteration: 8 || Loss: 6.114743960002787
Iteration: 9 || Loss: 6.112313158075861
Iteration: 10 || Loss: 6.106765031122158
Iteration: 11 || Loss: 6.091018312468798
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.64085
Epoch 180 loss:6.091018312468798
MSE loss S0.21092496385696652
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.64085
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.20154558606834
Iteration: 2 || Loss: 24.200978224207486
Iteration: 3 || Loss: 24.200362223928664
Iteration: 4 || Loss: 24.19978716041871
Iteration: 5 || Loss: 24.199218819506875
Iteration: 6 || Loss: 24.199218819506875
saving ADAM checkpoint...
Sum of params:-105.64105
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.199218819506875
Iteration: 2 || Loss: 24.18779583948216
Iteration: 3 || Loss: 24.05844442496452
Iteration: 4 || Loss: 24.00704597124345
Iteration: 5 || Loss: 23.81137592509562
Iteration: 6 || Loss: 23.784257331309036
Iteration: 7 || Loss: 23.76500920370867
Iteration: 8 || Loss: 23.723858455843047
Iteration: 9 || Loss: 23.715985455712858
Iteration: 10 || Loss: 23.70608303913839
Iteration: 11 || Loss: 23.69073403403249
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.59497
Epoch 180 loss:23.69073403403249
MSE loss S0.4167360755205797
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.59497
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.80438043440293
Iteration: 2 || Loss: 70.80370327043617
Iteration: 3 || Loss: 70.80304669027642
Iteration: 4 || Loss: 70.80235814197998
Iteration: 5 || Loss: 70.80163635995662
Iteration: 6 || Loss: 70.80163635995662
saving ADAM checkpoint...
Sum of params:-105.59507
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.80163635995662
Iteration: 2 || Loss: 70.79317771824365
Iteration: 3 || Loss: 70.76653748773715
Iteration: 4 || Loss: 70.55145395432005
Iteration: 5 || Loss: 70.47629029832022
Iteration: 6 || Loss: 70.40229809057521
Iteration: 7 || Loss: 70.3358020013665
Iteration: 8 || Loss: 70.31300171215764
Iteration: 9 || Loss: 70.30217662993061
Iteration: 10 || Loss: 70.28890505241668
Iteration: 11 || Loss: 70.25855213684521
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.64836
Epoch 180 loss:70.25855213684521
MSE loss S0.9977747050191967
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.4336134279936
MSE loss S0.35141754597673436
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:121.48807871051608
MSE loss S2.3180139687502797
waveform batch: 2/2
Test loss - extrapolation:61.13174020482498
MSE loss S1.381710287762959
Epoch 180 mean train loss:3.4496656718395347
Epoch 180 mean test loss - interpolation:4.0722689046655995
Epoch 180 mean test loss - extrapolation:15.218318242945088
Start training epoch 181
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.64836
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4620676722599315
Iteration: 2 || Loss: 6.4609980698893725
Iteration: 3 || Loss: 6.459960353750788
Iteration: 4 || Loss: 6.4588969404076
Iteration: 5 || Loss: 6.457941292216679
Iteration: 6 || Loss: 6.457941292216679
saving ADAM checkpoint...
Sum of params:-105.64828
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.457941292216679
Iteration: 2 || Loss: 6.421214312912755
Iteration: 3 || Loss: 6.363966904989181
Iteration: 4 || Loss: 6.295713243495884
Iteration: 5 || Loss: 6.186811906478191
Iteration: 6 || Loss: 6.130438373169164
Iteration: 7 || Loss: 6.119822087316966
Iteration: 8 || Loss: 6.113337117908878
Iteration: 9 || Loss: 6.110882467547178
Iteration: 10 || Loss: 6.105497022735323
Iteration: 11 || Loss: 6.08983122027582
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.665306
Epoch 181 loss:6.08983122027582
MSE loss S0.21122770003187327
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.665306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.208513056017292
Iteration: 2 || Loss: 24.20791065513088
Iteration: 3 || Loss: 24.207374479119355
Iteration: 4 || Loss: 24.206851717437157
Iteration: 5 || Loss: 24.20613666206002
Iteration: 6 || Loss: 24.20613666206002
saving ADAM checkpoint...
Sum of params:-105.665504
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.20613666206002
Iteration: 2 || Loss: 24.19430376133181
Iteration: 3 || Loss: 24.065569541976842
Iteration: 4 || Loss: 24.008189992770617
Iteration: 5 || Loss: 23.807449224951164
Iteration: 6 || Loss: 23.779276332325534
Iteration: 7 || Loss: 23.760286387306483
Iteration: 8 || Loss: 23.71947770797752
Iteration: 9 || Loss: 23.7106311870914
Iteration: 10 || Loss: 23.7010450173561
Iteration: 11 || Loss: 23.686476088107543
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.62034
Epoch 181 loss:23.686476088107543
MSE loss S0.41642303591972796
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.62034
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.73789049862256
Iteration: 2 || Loss: 70.73717923163214
Iteration: 3 || Loss: 70.7364889120909
Iteration: 4 || Loss: 70.73581781829368
Iteration: 5 || Loss: 70.73512524156003
Iteration: 6 || Loss: 70.73512524156003
saving ADAM checkpoint...
Sum of params:-105.62042
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.73512524156003
Iteration: 2 || Loss: 70.72728775174987
Iteration: 3 || Loss: 70.70162425382425
Iteration: 4 || Loss: 70.48547026115642
Iteration: 5 || Loss: 70.41933684540966
Iteration: 6 || Loss: 70.34530423694808
Iteration: 7 || Loss: 70.27991787693374
Iteration: 8 || Loss: 70.2578443161117
Iteration: 9 || Loss: 70.24730140268814
Iteration: 10 || Loss: 70.23410995859807
Iteration: 11 || Loss: 70.20394904666868
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.67298
Epoch 181 loss:70.20394904666868
MSE loss S0.997309993184039
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.422326827836812
MSE loss S0.351418798452368
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:121.30387200556322
MSE loss S2.3154045544862907
waveform batch: 2/2
Test loss - extrapolation:60.957786092133254
MSE loss S1.378278571209787
Epoch 181 mean train loss:3.4475950467259326
Epoch 181 mean test loss - interpolation:4.070387804639469
Epoch 181 mean test loss - extrapolation:15.188471508141374
Start training epoch 182
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.67298
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.465997477469652
Iteration: 2 || Loss: 6.465058076184137
Iteration: 3 || Loss: 6.463949521492712
Iteration: 4 || Loss: 6.462933447759604
Iteration: 5 || Loss: 6.46193729016542
Iteration: 6 || Loss: 6.46193729016542
saving ADAM checkpoint...
Sum of params:-105.6729
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.46193729016542
Iteration: 2 || Loss: 6.42553520072227
Iteration: 3 || Loss: 6.366162127134654
Iteration: 4 || Loss: 6.296093032442615
Iteration: 5 || Loss: 6.185491048394376
Iteration: 6 || Loss: 6.128943355507768
Iteration: 7 || Loss: 6.118920603442935
Iteration: 8 || Loss: 6.112647077815321
Iteration: 9 || Loss: 6.110218899155749
Iteration: 10 || Loss: 6.104443196736386
Iteration: 11 || Loss: 6.088930906800801
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.68951
Epoch 182 loss:6.088930906800801
MSE loss S0.21095297942133132
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.68951
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.19268452401982
Iteration: 2 || Loss: 24.19205732119893
Iteration: 3 || Loss: 24.191493344776845
Iteration: 4 || Loss: 24.190844738315967
Iteration: 5 || Loss: 24.19031626260995
Iteration: 6 || Loss: 24.19031626260995
saving ADAM checkpoint...
Sum of params:-105.6897
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.19031626260995
Iteration: 2 || Loss: 24.17869981802098
Iteration: 3 || Loss: 24.044768253734034
Iteration: 4 || Loss: 23.994744534189998
Iteration: 5 || Loss: 23.799823156630964
Iteration: 6 || Loss: 23.772688805670942
Iteration: 7 || Loss: 23.75355706693551
Iteration: 8 || Loss: 23.712254079002207
Iteration: 9 || Loss: 23.704765378600666
Iteration: 10 || Loss: 23.69451742951406
Iteration: 11 || Loss: 23.679369971968015
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.64409
Epoch 182 loss:23.679369971968015
MSE loss S0.41632358917408196
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.64409
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.69298404421177
Iteration: 2 || Loss: 70.69226627254119
Iteration: 3 || Loss: 70.69158369253465
Iteration: 4 || Loss: 70.69086052619525
Iteration: 5 || Loss: 70.69016321459729
Iteration: 6 || Loss: 70.69016321459729
saving ADAM checkpoint...
Sum of params:-105.64421
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.69016321459729
Iteration: 2 || Loss: 70.68208503275574
Iteration: 3 || Loss: 70.65580753220028
Iteration: 4 || Loss: 70.44022628589403
Iteration: 5 || Loss: 70.36634991329993
Iteration: 6 || Loss: 70.29234546920239
Iteration: 7 || Loss: 70.22656192437249
Iteration: 8 || Loss: 70.20387293805886
Iteration: 9 || Loss: 70.19323335504674
Iteration: 10 || Loss: 70.18023890530176
Iteration: 11 || Loss: 70.15120680882293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.69613
Epoch 182 loss:70.15120680882293
MSE loss S0.9969548445960237
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.40910392020988
MSE loss S0.3511042408791979
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:121.12451805504887
MSE loss S2.3128147819582106
waveform batch: 2/2
Test loss - extrapolation:60.79201919703617
MSE loss S1.375716393631412
Epoch 182 mean train loss:3.4455002650893705
Epoch 182 mean test loss - interpolation:4.068183986701647
Epoch 182 mean test loss - extrapolation:15.159711437673755
Start training epoch 183
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.69613
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.459414148483025
Iteration: 2 || Loss: 6.458333639261566
Iteration: 3 || Loss: 6.457295420520979
Iteration: 4 || Loss: 6.456256642653201
Iteration: 5 || Loss: 6.455285917157147
Iteration: 6 || Loss: 6.455285917157147
saving ADAM checkpoint...
Sum of params:-105.69603
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.455285917157147
Iteration: 2 || Loss: 6.419896491618434
Iteration: 3 || Loss: 6.3626147841668
Iteration: 4 || Loss: 6.2943918356281525
Iteration: 5 || Loss: 6.1837511101093305
Iteration: 6 || Loss: 6.127177896356651
Iteration: 7 || Loss: 6.117073061164309
Iteration: 8 || Loss: 6.110811367053504
Iteration: 9 || Loss: 6.108432495115969
Iteration: 10 || Loss: 6.103048836579904
Iteration: 11 || Loss: 6.087472239048672
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.71313
Epoch 183 loss:6.087472239048672
MSE loss S0.21097599307684353
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.71313
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.19552130984035
Iteration: 2 || Loss: 24.195034524145264
Iteration: 3 || Loss: 24.194330619530543
Iteration: 4 || Loss: 24.193768092234073
Iteration: 5 || Loss: 24.193179265727878
Iteration: 6 || Loss: 24.193179265727878
saving ADAM checkpoint...
Sum of params:-105.71331
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.193179265727878
Iteration: 2 || Loss: 24.181298114899207
Iteration: 3 || Loss: 24.0483755099829
Iteration: 4 || Loss: 23.995039816680237
Iteration: 5 || Loss: 23.79602377289222
Iteration: 6 || Loss: 23.767662162203095
Iteration: 7 || Loss: 23.748877058477024
Iteration: 8 || Loss: 23.70824044806887
Iteration: 9 || Loss: 23.699446413169216
Iteration: 10 || Loss: 23.689293894273437
Iteration: 11 || Loss: 23.67462729389795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.66838
Epoch 183 loss:23.67462729389795
MSE loss S0.4159948958537363
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.66838
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.63650536398241
Iteration: 2 || Loss: 70.63579122392595
Iteration: 3 || Loss: 70.63507244301442
Iteration: 4 || Loss: 70.63440253640601
Iteration: 5 || Loss: 70.63366959871783
Iteration: 6 || Loss: 70.63366959871783
saving ADAM checkpoint...
Sum of params:-105.668495
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.63366959871783
Iteration: 2 || Loss: 70.62632968490263
Iteration: 3 || Loss: 70.60071745567406
Iteration: 4 || Loss: 70.38234506663153
Iteration: 5 || Loss: 70.31454906329022
Iteration: 6 || Loss: 70.23990661837064
Iteration: 7 || Loss: 70.17412929677015
Iteration: 8 || Loss: 70.15196134253318
Iteration: 9 || Loss: 70.14182440020042
Iteration: 10 || Loss: 70.12890846897008
Iteration: 11 || Loss: 70.10049077501706
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.719536
Epoch 183 loss:70.10049077501706
MSE loss S0.9966841879936719
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.39688603751663
MSE loss S0.35099814886061514
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:120.95150195235863
MSE loss S2.3105876806182093
waveform batch: 2/2
Test loss - extrapolation:60.630818346002464
MSE loss S1.372684450907719
Epoch 183 mean train loss:3.443537596826334
Epoch 183 mean test loss - interpolation:4.066147672919438
Epoch 183 mean test loss - extrapolation:15.131860024863423
Start training epoch 184
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.719536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.461506452136749
Iteration: 2 || Loss: 6.460469404608682
Iteration: 3 || Loss: 6.459416410155851
Iteration: 4 || Loss: 6.458386207657524
Iteration: 5 || Loss: 6.457361215376234
Iteration: 6 || Loss: 6.457361215376234
saving ADAM checkpoint...
Sum of params:-105.71944
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.457361215376234
Iteration: 2 || Loss: 6.422294033814036
Iteration: 3 || Loss: 6.363699251983123
Iteration: 4 || Loss: 6.294147387255604
Iteration: 5 || Loss: 6.181886766409335
Iteration: 6 || Loss: 6.125059435861872
Iteration: 7 || Loss: 6.115673150207147
Iteration: 8 || Loss: 6.109630208646858
Iteration: 9 || Loss: 6.10721412723957
Iteration: 10 || Loss: 6.101662655275004
Iteration: 11 || Loss: 6.086033151432843
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.73633
Epoch 184 loss:6.086033151432843
MSE loss S0.21009983672599786
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.73633
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.18480611896165
Iteration: 2 || Loss: 24.184234995618187
Iteration: 3 || Loss: 24.183624722474892
Iteration: 4 || Loss: 24.183115098555515
Iteration: 5 || Loss: 24.18255692960722
Iteration: 6 || Loss: 24.18255692960722
saving ADAM checkpoint...
Sum of params:-105.73653
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.18255692960722
Iteration: 2 || Loss: 24.171416096787045
Iteration: 3 || Loss: 24.040808447454218
Iteration: 4 || Loss: 23.990810326708473
Iteration: 5 || Loss: 23.78926000321867
Iteration: 6 || Loss: 23.761690506001546
Iteration: 7 || Loss: 23.743157918161586
Iteration: 8 || Loss: 23.701794688533937
Iteration: 9 || Loss: 23.694101906149033
Iteration: 10 || Loss: 23.68326023271063
Iteration: 11 || Loss: 23.667691372957165
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.69118
Epoch 184 loss:23.667691372957165
MSE loss S0.4150273233109142
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.69118
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.59099233584072
Iteration: 2 || Loss: 70.5903089121789
Iteration: 3 || Loss: 70.58953646376716
Iteration: 4 || Loss: 70.58887409968217
Iteration: 5 || Loss: 70.58818460146222
Iteration: 6 || Loss: 70.58818460146222
saving ADAM checkpoint...
Sum of params:-105.69127
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.58818460146222
Iteration: 2 || Loss: 70.58131415966255
Iteration: 3 || Loss: 70.55607713768562
Iteration: 4 || Loss: 70.3478297460981
Iteration: 5 || Loss: 70.26525718918889
Iteration: 6 || Loss: 70.19106822407613
Iteration: 7 || Loss: 70.12503785241219
Iteration: 8 || Loss: 70.10239431644828
Iteration: 9 || Loss: 70.09213185872733
Iteration: 10 || Loss: 70.07928166006944
Iteration: 11 || Loss: 70.05126893914492
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.74238
Epoch 184 loss:70.05126893914492
MSE loss S0.9960600266119167
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.38508693511663
MSE loss S0.35060063869950153
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:120.7811521728391
MSE loss S2.307763899677955
waveform batch: 2/2
Test loss - extrapolation:60.47315674018631
MSE loss S1.3701725247973315
Epoch 184 mean train loss:3.4415514987425837
Epoch 184 mean test loss - interpolation:4.064181155852771
Epoch 184 mean test loss - extrapolation:15.104525742752116
Start training epoch 185
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.74238
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.456128760631083
Iteration: 2 || Loss: 6.455130975771706
Iteration: 3 || Loss: 6.454124606113602
Iteration: 4 || Loss: 6.453043739534148
Iteration: 5 || Loss: 6.452072004653344
Iteration: 6 || Loss: 6.452072004653344
saving ADAM checkpoint...
Sum of params:-105.74228
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.452072004653344
Iteration: 2 || Loss: 6.417669519100337
Iteration: 3 || Loss: 6.36099113548934
Iteration: 4 || Loss: 6.2934073599675076
Iteration: 5 || Loss: 6.180523931723394
Iteration: 6 || Loss: 6.123810647803436
Iteration: 7 || Loss: 6.114080919267254
Iteration: 8 || Loss: 6.107929580531639
Iteration: 9 || Loss: 6.105550907784822
Iteration: 10 || Loss: 6.100188022413352
Iteration: 11 || Loss: 6.084571333803657
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.75954
Epoch 185 loss:6.084571333803657
MSE loss S0.20996888163840896
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.75954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.182416943272845
Iteration: 2 || Loss: 24.181959381097226
Iteration: 3 || Loss: 24.181243380787794
Iteration: 4 || Loss: 24.180839585359927
Iteration: 5 || Loss: 24.180244738460015
Iteration: 6 || Loss: 24.180244738460015
saving ADAM checkpoint...
Sum of params:-105.75972
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.180244738460015
Iteration: 2 || Loss: 24.16913150742982
Iteration: 3 || Loss: 24.036621696329817
Iteration: 4 || Loss: 23.98652736026181
Iteration: 5 || Loss: 23.784327439108907
Iteration: 6 || Loss: 23.756123488627743
Iteration: 7 || Loss: 23.737864910047648
Iteration: 8 || Loss: 23.697128477551807
Iteration: 9 || Loss: 23.688625089667244
Iteration: 10 || Loss: 23.67773329410954
Iteration: 11 || Loss: 23.662667024719926
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.71521
Epoch 185 loss:23.662667024719926
MSE loss S0.4150924411275508
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.71521
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.53845630835124
Iteration: 2 || Loss: 70.53771354839073
Iteration: 3 || Loss: 70.5369997758798
Iteration: 4 || Loss: 70.53627724177095
Iteration: 5 || Loss: 70.53559362667515
Iteration: 6 || Loss: 70.53559362667515
saving ADAM checkpoint...
Sum of params:-105.71533
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.53559362667515
Iteration: 2 || Loss: 70.52871310059385
Iteration: 3 || Loss: 70.50348358980366
Iteration: 4 || Loss: 70.29078109905542
Iteration: 5 || Loss: 70.21505675167013
Iteration: 6 || Loss: 70.14087407011768
Iteration: 7 || Loss: 70.07612982328104
Iteration: 8 || Loss: 70.05360215414044
Iteration: 9 || Loss: 70.04351759865712
Iteration: 10 || Loss: 70.03080915276949
Iteration: 11 || Loss: 70.0029758185931
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.76589
Epoch 185 loss:70.0029758185931
MSE loss S0.9959104634715206
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.374109388560395
MSE loss S0.35056956779404314
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:120.61124838465041
MSE loss S2.305448140123949
waveform batch: 2/2
Test loss - extrapolation:60.317651038371224
MSE loss S1.3673890068498709
Epoch 185 mean train loss:3.4396625578316096
Epoch 185 mean test loss - interpolation:4.062351564760066
Epoch 185 mean test loss - extrapolation:15.077408285251801
Start training epoch 186
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.76589
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.457352255875284
Iteration: 2 || Loss: 6.456319705125168
Iteration: 3 || Loss: 6.455306829311466
Iteration: 4 || Loss: 6.454381462044434
Iteration: 5 || Loss: 6.453384971441183
Iteration: 6 || Loss: 6.453384971441183
saving ADAM checkpoint...
Sum of params:-105.76579
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.453384971441183
Iteration: 2 || Loss: 6.4193813193053835
Iteration: 3 || Loss: 6.361916184773573
Iteration: 4 || Loss: 6.293836771506637
Iteration: 5 || Loss: 6.179023360548755
Iteration: 6 || Loss: 6.122131524793438
Iteration: 7 || Loss: 6.1128192194533915
Iteration: 8 || Loss: 6.106734114852949
Iteration: 9 || Loss: 6.104337594637697
Iteration: 10 || Loss: 6.098856172808377
Iteration: 11 || Loss: 6.083219819608154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.78304
Epoch 186 loss:6.083219819608154
MSE loss S0.20963186277758916
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.78304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.178480401424416
Iteration: 2 || Loss: 24.1778921012695
Iteration: 3 || Loss: 24.17734108307542
Iteration: 4 || Loss: 24.176761844065165
Iteration: 5 || Loss: 24.17632040944339
Iteration: 6 || Loss: 24.17632040944339
saving ADAM checkpoint...
Sum of params:-105.78321
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.17632040944339
Iteration: 2 || Loss: 24.1651734861043
Iteration: 3 || Loss: 24.034952670525207
Iteration: 4 || Loss: 23.984085400659577
Iteration: 5 || Loss: 23.779945953788047
Iteration: 6 || Loss: 23.751128762192668
Iteration: 7 || Loss: 23.732949189947625
Iteration: 8 || Loss: 23.692137995391576
Iteration: 9 || Loss: 23.683517571794784
Iteration: 10 || Loss: 23.67236355549484
Iteration: 11 || Loss: 23.657316510739225
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.73899
Epoch 186 loss:23.657316510739225
MSE loss S0.41495196920088295
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.73899
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.48407120637448
Iteration: 2 || Loss: 70.48337111602709
Iteration: 3 || Loss: 70.48264929519826
Iteration: 4 || Loss: 70.48201640534194
Iteration: 5 || Loss: 70.4813081353381
Iteration: 6 || Loss: 70.4813081353381
saving ADAM checkpoint...
Sum of params:-105.73909
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.4813081353381
Iteration: 2 || Loss: 70.4742215443023
Iteration: 3 || Loss: 70.44944546662357
Iteration: 4 || Loss: 70.24334430718112
Iteration: 5 || Loss: 70.166017306944
Iteration: 6 || Loss: 70.09175971209342
Iteration: 7 || Loss: 70.0279294240557
Iteration: 8 || Loss: 70.00554402632164
Iteration: 9 || Loss: 69.99503850837301
Iteration: 10 || Loss: 69.98252675790924
Iteration: 11 || Loss: 69.95520492394186
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.78942
Epoch 186 loss:69.95520492394186
MSE loss S0.9958619361740865
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.362746389052322
MSE loss S0.35046690408668385
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:120.4449384079076
MSE loss S2.3033216239140026
waveform batch: 2/2
Test loss - extrapolation:60.16516305194015
MSE loss S1.3649174605784227
Epoch 186 mean train loss:3.437784181182388
Epoch 186 mean test loss - interpolation:4.060457731508721
Epoch 186 mean test loss - extrapolation:15.050841788320646
Start training epoch 187
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.78942
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.457288323806021
Iteration: 2 || Loss: 6.456320173596363
Iteration: 3 || Loss: 6.455296865906673
Iteration: 4 || Loss: 6.454361887062241
Iteration: 5 || Loss: 6.453369779285834
Iteration: 6 || Loss: 6.453369779285834
saving ADAM checkpoint...
Sum of params:-105.78934
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.453369779285834
Iteration: 2 || Loss: 6.420436985480265
Iteration: 3 || Loss: 6.362036592754839
Iteration: 4 || Loss: 6.293595765997302
Iteration: 5 || Loss: 6.17755072797291
Iteration: 6 || Loss: 6.120711693006376
Iteration: 7 || Loss: 6.111424279255921
Iteration: 8 || Loss: 6.1054663106658245
Iteration: 9 || Loss: 6.103106542848869
Iteration: 10 || Loss: 6.097547410144327
Iteration: 11 || Loss: 6.082139088358107
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.80644
Epoch 187 loss:6.082139088358107
MSE loss S0.209754801834956
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.80644
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.17478659526831
Iteration: 2 || Loss: 24.174189047429508
Iteration: 3 || Loss: 24.173642710471814
Iteration: 4 || Loss: 24.173044545916035
Iteration: 5 || Loss: 24.17259217091527
Iteration: 6 || Loss: 24.17259217091527
saving ADAM checkpoint...
Sum of params:-105.80661
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.17259217091527
Iteration: 2 || Loss: 24.161426639574895
Iteration: 3 || Loss: 24.021176655228736
Iteration: 4 || Loss: 23.975599165882596
Iteration: 5 || Loss: 23.77339232311596
Iteration: 6 || Loss: 23.74502154628504
Iteration: 7 || Loss: 23.727084675590984
Iteration: 8 || Loss: 23.68652634622888
Iteration: 9 || Loss: 23.677923554644508
Iteration: 10 || Loss: 23.66669672787891
Iteration: 11 || Loss: 23.651969834891112
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.762726
Epoch 187 loss:23.651969834891112
MSE loss S0.4143818736720918
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.762726
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.4349115342407
Iteration: 2 || Loss: 70.43417943925141
Iteration: 3 || Loss: 70.43352444344616
Iteration: 4 || Loss: 70.43275683899076
Iteration: 5 || Loss: 70.43204955483503
Iteration: 6 || Loss: 70.43204955483503
saving ADAM checkpoint...
Sum of params:-105.762856
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.43204955483503
Iteration: 2 || Loss: 70.42568813863873
Iteration: 3 || Loss: 70.40155357199728
Iteration: 4 || Loss: 70.18877091942569
Iteration: 5 || Loss: 70.11873082153812
Iteration: 6 || Loss: 70.04433193959115
Iteration: 7 || Loss: 69.9799049708494
Iteration: 8 || Loss: 69.9579272922514
Iteration: 9 || Loss: 69.94793531033991
Iteration: 10 || Loss: 69.93550204411427
Iteration: 11 || Loss: 69.90871035728222
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.81244
Epoch 187 loss:69.90871035728222
MSE loss S0.9955193155429718
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.351224737244074
MSE loss S0.3502412762718082
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:120.2774029602243
MSE loss S2.30098981818223
waveform batch: 2/2
Test loss - extrapolation:60.01470831368003
MSE loss S1.3621726143241117
Epoch 187 mean train loss:3.4359592855355667
Epoch 187 mean test loss - interpolation:4.058537456207346
Epoch 187 mean test loss - extrapolation:15.024342606158692
Start training epoch 188
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.81244
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.456125503810961
Iteration: 2 || Loss: 6.455088879930823
Iteration: 3 || Loss: 6.454081409602657
Iteration: 4 || Loss: 6.453126852554673
Iteration: 5 || Loss: 6.452159491421142
Iteration: 6 || Loss: 6.452159491421142
saving ADAM checkpoint...
Sum of params:-105.81233
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.452159491421142
Iteration: 2 || Loss: 6.419252383794124
Iteration: 3 || Loss: 6.361158859142554
Iteration: 4 || Loss: 6.293017841919453
Iteration: 5 || Loss: 6.17567148882322
Iteration: 6 || Loss: 6.11879224405936
Iteration: 7 || Loss: 6.10988803730921
Iteration: 8 || Loss: 6.104043036996148
Iteration: 9 || Loss: 6.101715008965945
Iteration: 10 || Loss: 6.0960571241333765
Iteration: 11 || Loss: 6.080655607955566
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.82951
Epoch 188 loss:6.080655607955566
MSE loss S0.2093156232359867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.82951
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.165764404653924
Iteration: 2 || Loss: 24.165197714023716
Iteration: 3 || Loss: 24.164720789487518
Iteration: 4 || Loss: 24.164098170531066
Iteration: 5 || Loss: 24.163562671039355
Iteration: 6 || Loss: 24.163562671039355
saving ADAM checkpoint...
Sum of params:-105.82971
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.163562671039355
Iteration: 2 || Loss: 24.152600845363676
Iteration: 3 || Loss: 24.017556452351194
Iteration: 4 || Loss: 23.9694219170038
Iteration: 5 || Loss: 23.76717769433985
Iteration: 6 || Loss: 23.739312918836976
Iteration: 7 || Loss: 23.721393875911794
Iteration: 8 || Loss: 23.68067674488065
Iteration: 9 || Loss: 23.6726294409522
Iteration: 10 || Loss: 23.661312851810663
Iteration: 11 || Loss: 23.64645870782437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.78633
Epoch 188 loss:23.64645870782437
MSE loss S0.4141021714793155
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.78633
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.37781432975635
Iteration: 2 || Loss: 70.37708749983378
Iteration: 3 || Loss: 70.37643674028426
Iteration: 4 || Loss: 70.37576961961722
Iteration: 5 || Loss: 70.37515335125816
Iteration: 6 || Loss: 70.37515335125816
saving ADAM checkpoint...
Sum of params:-105.78643
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.37515335125816
Iteration: 2 || Loss: 70.36826837167861
Iteration: 3 || Loss: 70.34454956039123
Iteration: 4 || Loss: 70.14459669459673
Iteration: 5 || Loss: 70.06879467550696
Iteration: 6 || Loss: 69.99565214617007
Iteration: 7 || Loss: 69.93391223837968
Iteration: 8 || Loss: 69.9119369222687
Iteration: 9 || Loss: 69.90132352136948
Iteration: 10 || Loss: 69.88880765173994
Iteration: 11 || Loss: 69.86210002203902
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.83646
Epoch 188 loss:69.86210002203902
MSE loss S0.9953530912197551
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.341138978665093
MSE loss S0.3502057170985011
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:120.10911994294996
MSE loss S2.298541677630017
waveform batch: 2/2
Test loss - extrapolation:59.86331119660167
MSE loss S1.35979159939208
Epoch 188 mean train loss:3.4341108392351365
Epoch 188 mean test loss - interpolation:4.0568564964441824
Epoch 188 mean test loss - extrapolation:14.997702594962638
Start training epoch 189
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.83646
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.455984121755814
Iteration: 2 || Loss: 6.455058554707499
Iteration: 3 || Loss: 6.453995283263536
Iteration: 4 || Loss: 6.45304038998203
Iteration: 5 || Loss: 6.452100565013081
Iteration: 6 || Loss: 6.452100565013081
saving ADAM checkpoint...
Sum of params:-105.836365
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.452100565013081
Iteration: 2 || Loss: 6.4207131497776455
Iteration: 3 || Loss: 6.361492117519505
Iteration: 4 || Loss: 6.293433012507178
Iteration: 5 || Loss: 6.174936460837204
Iteration: 6 || Loss: 6.118251640827082
Iteration: 7 || Loss: 6.109021878168276
Iteration: 8 || Loss: 6.103006848152077
Iteration: 9 || Loss: 6.100682980166537
Iteration: 10 || Loss: 6.095100510632947
Iteration: 11 || Loss: 6.0794621383803715
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.85369
Epoch 189 loss:6.0794621383803715
MSE loss S0.2091821267640781
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.85369
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.16871159898146
Iteration: 2 || Loss: 24.168222720744346
Iteration: 3 || Loss: 24.167582750222955
Iteration: 4 || Loss: 24.167021991007637
Iteration: 5 || Loss: 24.16652310424941
Iteration: 6 || Loss: 24.16652310424941
saving ADAM checkpoint...
Sum of params:-105.85389
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.16652310424941
Iteration: 2 || Loss: 24.15549910590995
Iteration: 3 || Loss: 24.018071474744062
Iteration: 4 || Loss: 23.969858548314136
Iteration: 5 || Loss: 23.764404433092647
Iteration: 6 || Loss: 23.735238720798364
Iteration: 7 || Loss: 23.71751004461326
Iteration: 8 || Loss: 23.677465885598423
Iteration: 9 || Loss: 23.668121086570384
Iteration: 10 || Loss: 23.656715559750655
Iteration: 11 || Loss: 23.642099913073288
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.811005
Epoch 189 loss:23.642099913073288
MSE loss S0.4140589154119122
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.811005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.32946251996586
Iteration: 2 || Loss: 70.32874692113872
Iteration: 3 || Loss: 70.32801787658398
Iteration: 4 || Loss: 70.32739197936323
Iteration: 5 || Loss: 70.32676754768336
Iteration: 6 || Loss: 70.32676754768336
saving ADAM checkpoint...
Sum of params:-105.81113
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.32676754768336
Iteration: 2 || Loss: 70.32017263286689
Iteration: 3 || Loss: 70.29681579022767
Iteration: 4 || Loss: 70.09062294922312
Iteration: 5 || Loss: 70.02185535943286
Iteration: 6 || Loss: 69.948153762169
Iteration: 7 || Loss: 69.8869931440401
Iteration: 8 || Loss: 69.86514709741712
Iteration: 9 || Loss: 69.85481205570805
Iteration: 10 || Loss: 69.84252489977622
Iteration: 11 || Loss: 69.81613276966691
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.86022
Epoch 189 loss:69.81613276966691
MSE loss S0.99505814149635
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.33063722635808
MSE loss S0.35010913880115124
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:119.94036091620795
MSE loss S2.2961466522376206
waveform batch: 2/2
Test loss - extrapolation:59.71433011414475
MSE loss S1.3569247967550786
Epoch 189 mean train loss:3.4323343041765715
Epoch 189 mean test loss - interpolation:4.055106204393013
Epoch 189 mean test loss - extrapolation:14.97122425252939
Start training epoch 190
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.86022
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4573369739026045
Iteration: 2 || Loss: 6.456487587218465
Iteration: 3 || Loss: 6.4555406913238125
Iteration: 4 || Loss: 6.454510726108525
Iteration: 5 || Loss: 6.453562855998619
Iteration: 6 || Loss: 6.453562855998619
saving ADAM checkpoint...
Sum of params:-105.86012
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.453562855998619
Iteration: 2 || Loss: 6.422301040900517
Iteration: 3 || Loss: 6.362236044538262
Iteration: 4 || Loss: 6.293576633347969
Iteration: 5 || Loss: 6.173344871006524
Iteration: 6 || Loss: 6.116540138327922
Iteration: 7 || Loss: 6.107671368556166
Iteration: 8 || Loss: 6.101813802782435
Iteration: 9 || Loss: 6.099563919084929
Iteration: 10 || Loss: 6.093718688763634
Iteration: 11 || Loss: 6.078447916566434
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.877365
Epoch 190 loss:6.078447916566434
MSE loss S0.20949433879229207
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.877365
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.171430714193257
Iteration: 2 || Loss: 24.17087634419062
Iteration: 3 || Loss: 24.170383644302877
Iteration: 4 || Loss: 24.169850179638754
Iteration: 5 || Loss: 24.169262349610833
Iteration: 6 || Loss: 24.169262349610833
saving ADAM checkpoint...
Sum of params:-105.87758
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.169262349610833
Iteration: 2 || Loss: 24.15812209968492
Iteration: 3 || Loss: 24.012745171960315
Iteration: 4 || Loss: 23.96615622949497
Iteration: 5 || Loss: 23.758894718618492
Iteration: 6 || Loss: 23.729909203845242
Iteration: 7 || Loss: 23.712501753571935
Iteration: 8 || Loss: 23.672434871820144
Iteration: 9 || Loss: 23.662994985058617
Iteration: 10 || Loss: 23.65210199555243
Iteration: 11 || Loss: 23.637899825128173
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.83547
Epoch 190 loss:23.637899825128173
MSE loss S0.4137062179379706
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.83547
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.27233002767036
Iteration: 2 || Loss: 70.27159882109657
Iteration: 3 || Loss: 70.27098594474428
Iteration: 4 || Loss: 70.27028095763828
Iteration: 5 || Loss: 70.26959161420534
Iteration: 6 || Loss: 70.26959161420534
saving ADAM checkpoint...
Sum of params:-105.83559
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.26959161420534
Iteration: 2 || Loss: 70.2637225026711
Iteration: 3 || Loss: 70.2413842477325
Iteration: 4 || Loss: 70.03557706258287
Iteration: 5 || Loss: 69.97345716832564
Iteration: 6 || Loss: 69.90023886036198
Iteration: 7 || Loss: 69.84053968764131
Iteration: 8 || Loss: 69.81902087574383
Iteration: 9 || Loss: 69.80865537637563
Iteration: 10 || Loss: 69.79647677015197
Iteration: 11 || Loss: 69.77026812297719
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.88441
Epoch 190 loss:69.77026812297719
MSE loss S0.9948495076130868
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.320836628693044
MSE loss S0.35008462247456384
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:119.77051801339002
MSE loss S2.293697177362382
waveform batch: 2/2
Test loss - extrapolation:59.56471886904689
MSE loss S1.3542237933033074
Epoch 190 mean train loss:3.4305729608507516
Epoch 190 mean test loss - interpolation:4.053472771448841
Epoch 190 mean test loss - extrapolation:14.944603073536408
Start training epoch 191
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.88441
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.459465781671399
Iteration: 2 || Loss: 6.458426017797251
Iteration: 3 || Loss: 6.4574273927897154
Iteration: 4 || Loss: 6.456551065618452
Iteration: 5 || Loss: 6.455534840177088
Iteration: 6 || Loss: 6.455534840177088
saving ADAM checkpoint...
Sum of params:-105.88431
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.455534840177088
Iteration: 2 || Loss: 6.424966266954841
Iteration: 3 || Loss: 6.3632656428319265
Iteration: 4 || Loss: 6.293819986668222
Iteration: 5 || Loss: 6.172321952687621
Iteration: 6 || Loss: 6.115523880980556
Iteration: 7 || Loss: 6.106684162593724
Iteration: 8 || Loss: 6.100889185809346
Iteration: 9 || Loss: 6.098683609964395
Iteration: 10 || Loss: 6.09271434902161
Iteration: 11 || Loss: 6.077344553612297
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.90158
Epoch 191 loss:6.077344553612297
MSE loss S0.20951843949249893
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.90158
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.168749636156655
Iteration: 2 || Loss: 24.1681935982417
Iteration: 3 || Loss: 24.167611944034935
Iteration: 4 || Loss: 24.16706376294865
Iteration: 5 || Loss: 24.166460495057517
Iteration: 6 || Loss: 24.166460495057517
saving ADAM checkpoint...
Sum of params:-105.9018
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.166460495057517
Iteration: 2 || Loss: 24.15484010808483
Iteration: 3 || Loss: 24.011598996423302
Iteration: 4 || Loss: 23.962552736010693
Iteration: 5 || Loss: 23.75545396287826
Iteration: 6 || Loss: 23.725882720483163
Iteration: 7 || Loss: 23.70809159895597
Iteration: 8 || Loss: 23.668184733134837
Iteration: 9 || Loss: 23.658462463213745
Iteration: 10 || Loss: 23.647660291252738
Iteration: 11 || Loss: 23.633594987886656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.86012
Epoch 191 loss:23.633594987886656
MSE loss S0.4139787494493844
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.86012
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.21945339177076
Iteration: 2 || Loss: 70.21878571517081
Iteration: 3 || Loss: 70.21816382607791
Iteration: 4 || Loss: 70.217473646326
Iteration: 5 || Loss: 70.21687809567634
Iteration: 6 || Loss: 70.21687809567634
saving ADAM checkpoint...
Sum of params:-105.86024
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.21687809567634
Iteration: 2 || Loss: 70.21016251070135
Iteration: 3 || Loss: 70.18808150049968
Iteration: 4 || Loss: 69.98521886426428
Iteration: 5 || Loss: 69.92615460331304
Iteration: 6 || Loss: 69.8528051497182
Iteration: 7 || Loss: 69.79423811957002
Iteration: 8 || Loss: 69.77293937805884
Iteration: 9 || Loss: 69.762227712291
Iteration: 10 || Loss: 69.74950211646127
Iteration: 11 || Loss: 69.72438311378252
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.90873
Epoch 191 loss:69.72438311378252
MSE loss S0.9952317352219
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.309746744132738
MSE loss S0.3502267283447041
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:119.59879795761627
MSE loss S2.291822766760776
waveform batch: 2/2
Test loss - extrapolation:59.415835191133176
MSE loss S1.351952329278736
Epoch 191 mean train loss:3.428804229492465
Epoch 191 mean test loss - interpolation:4.051624457355456
Epoch 191 mean test loss - extrapolation:14.917886095729122
Start training epoch 192
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.90873
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.458286179666873
Iteration: 2 || Loss: 6.457295809339561
Iteration: 3 || Loss: 6.456330193243706
Iteration: 4 || Loss: 6.455380276341633
Iteration: 5 || Loss: 6.454520266091139
Iteration: 6 || Loss: 6.454520266091139
saving ADAM checkpoint...
Sum of params:-105.90864
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.454520266091139
Iteration: 2 || Loss: 6.425541455012181
Iteration: 3 || Loss: 6.362315185999336
Iteration: 4 || Loss: 6.29253039375412
Iteration: 5 || Loss: 6.170995402385065
Iteration: 6 || Loss: 6.114447432056761
Iteration: 7 || Loss: 6.105714338588614
Iteration: 8 || Loss: 6.100062493823103
Iteration: 9 || Loss: 6.097856435412387
Iteration: 10 || Loss: 6.0916774735344745
Iteration: 11 || Loss: 6.076729610602833
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.92549
Epoch 192 loss:6.076729610602833
MSE loss S0.20947672190653976
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.92549
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.15449124700795
Iteration: 2 || Loss: 24.153875901702506
Iteration: 3 || Loss: 24.153293124010762
Iteration: 4 || Loss: 24.1528030549744
Iteration: 5 || Loss: 24.152209189484296
Iteration: 6 || Loss: 24.152209189484296
saving ADAM checkpoint...
Sum of params:-105.92568
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.152209189484296
Iteration: 2 || Loss: 24.140951306910925
Iteration: 3 || Loss: 23.987711630259746
Iteration: 4 || Loss: 23.947749557849583
Iteration: 5 || Loss: 23.74734858300928
Iteration: 6 || Loss: 23.719193721568647
Iteration: 7 || Loss: 23.701647732137168
Iteration: 8 || Loss: 23.66157177144187
Iteration: 9 || Loss: 23.653204417675028
Iteration: 10 || Loss: 23.64219739483129
Iteration: 11 || Loss: 23.62826695471289
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.88443
Epoch 192 loss:23.62826695471289
MSE loss S0.41377007041465635
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.88443
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.17382899999551
Iteration: 2 || Loss: 70.17313911650824
Iteration: 3 || Loss: 70.1724637987103
Iteration: 4 || Loss: 70.17189067632164
Iteration: 5 || Loss: 70.17120404922936
Iteration: 6 || Loss: 70.17120404922936
saving ADAM checkpoint...
Sum of params:-105.884544
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.17120404922936
Iteration: 2 || Loss: 70.16439607725549
Iteration: 3 || Loss: 70.1422446080408
Iteration: 4 || Loss: 69.939700174418
Iteration: 5 || Loss: 69.87856327619107
Iteration: 6 || Loss: 69.80605137747635
Iteration: 7 || Loss: 69.74864562784431
Iteration: 8 || Loss: 69.72712406786414
Iteration: 9 || Loss: 69.71623526934533
Iteration: 10 || Loss: 69.70345838193732
Iteration: 11 || Loss: 69.67886014695884
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.932846
Epoch 192 loss:69.67886014695884
MSE loss S0.9950743784142433
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.299664656026863
MSE loss S0.3502290311366225
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:119.4251918684293
MSE loss S2.289102024134682
waveform batch: 2/2
Test loss - extrapolation:59.26773177435686
MSE loss S1.3494911413955772
Epoch 192 mean train loss:3.4270295418025714
Epoch 192 mean test loss - interpolation:4.049944109337811
Epoch 192 mean test loss - extrapolation:14.891076970232179
Start training epoch 193
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.932846
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4546221522611065
Iteration: 2 || Loss: 6.45366836225778
Iteration: 3 || Loss: 6.452761359788446
Iteration: 4 || Loss: 6.451888699588667
Iteration: 5 || Loss: 6.450998302938711
Iteration: 6 || Loss: 6.450998302938711
saving ADAM checkpoint...
Sum of params:-105.932755
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.450998302938711
Iteration: 2 || Loss: 6.4231530386956655
Iteration: 3 || Loss: 6.360791836881353
Iteration: 4 || Loss: 6.2921651408643635
Iteration: 5 || Loss: 6.1700375348816765
Iteration: 6 || Loss: 6.11369277448349
Iteration: 7 || Loss: 6.1048252943339385
Iteration: 8 || Loss: 6.099029054959612
Iteration: 9 || Loss: 6.096792138315552
Iteration: 10 || Loss: 6.090832088102824
Iteration: 11 || Loss: 6.075370939165966
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.94996
Epoch 193 loss:6.075370939165966
MSE loss S0.20908601581914324
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.94996
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.157224587515014
Iteration: 2 || Loss: 24.156630670147663
Iteration: 3 || Loss: 24.15595049407921
Iteration: 4 || Loss: 24.15542137136368
Iteration: 5 || Loss: 24.154854810845734
Iteration: 6 || Loss: 24.154854810845734
saving ADAM checkpoint...
Sum of params:-105.950134
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.154854810845734
Iteration: 2 || Loss: 24.14368837555912
Iteration: 3 || Loss: 23.99653851872617
Iteration: 4 || Loss: 23.95205087843318
Iteration: 5 || Loss: 23.745625519465467
Iteration: 6 || Loss: 23.716336204053253
Iteration: 7 || Loss: 23.69893784760468
Iteration: 8 || Loss: 23.65898155212538
Iteration: 9 || Loss: 23.64954248818712
Iteration: 10 || Loss: 23.637893629076192
Iteration: 11 || Loss: 23.623675799236384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.90883
Epoch 193 loss:23.623675799236384
MSE loss S0.4132056449292686
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.90883
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.13031278578573
Iteration: 2 || Loss: 70.12958991357259
Iteration: 3 || Loss: 70.1290309538481
Iteration: 4 || Loss: 70.12830758249517
Iteration: 5 || Loss: 70.12768102013897
Iteration: 6 || Loss: 70.12768102013897
saving ADAM checkpoint...
Sum of params:-105.90894
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.12768102013897
Iteration: 2 || Loss: 70.1213049538033
Iteration: 3 || Loss: 70.09959677049501
Iteration: 4 || Loss: 69.89580139755682
Iteration: 5 || Loss: 69.83454271331988
Iteration: 6 || Loss: 69.76162469989569
Iteration: 7 || Loss: 69.70372743345794
Iteration: 8 || Loss: 69.6823456844562
Iteration: 9 || Loss: 69.67167746983361
Iteration: 10 || Loss: 69.65927087895547
Iteration: 11 || Loss: 69.63493539196737
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.95652
Epoch 193 loss:69.63493539196737
MSE loss S0.994657633411378
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.289681512426856
MSE loss S0.350046512354882
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:119.25672467338767
MSE loss S2.286475051998539
waveform batch: 2/2
Test loss - extrapolation:59.124572103698796
MSE loss S1.3467372527595654
Epoch 193 mean train loss:3.4253097286334384
Epoch 193 mean test loss - interpolation:4.048280252071143
Epoch 193 mean test loss - extrapolation:14.865108064757203
Start training epoch 194
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.95652
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.452614867572339
Iteration: 2 || Loss: 6.451696599862638
Iteration: 3 || Loss: 6.450777174304122
Iteration: 4 || Loss: 6.449843809221429
Iteration: 5 || Loss: 6.4489596317717055
Iteration: 6 || Loss: 6.4489596317717055
saving ADAM checkpoint...
Sum of params:-105.956436
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4489596317717055
Iteration: 2 || Loss: 6.420765606605259
Iteration: 3 || Loss: 6.359662488473931
Iteration: 4 || Loss: 6.2916955115903255
Iteration: 5 || Loss: 6.168508044061288
Iteration: 6 || Loss: 6.112122925831714
Iteration: 7 || Loss: 6.103475000682913
Iteration: 8 || Loss: 6.097758895544641
Iteration: 9 || Loss: 6.095552039068533
Iteration: 10 || Loss: 6.089600501456608
Iteration: 11 || Loss: 6.074098490950275
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.97363
Epoch 194 loss:6.074098490950275
MSE loss S0.20795530334116885
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.97363
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.151438097091006
Iteration: 2 || Loss: 24.150957802694332
Iteration: 3 || Loss: 24.150439907573787
Iteration: 4 || Loss: 24.14988894832486
Iteration: 5 || Loss: 24.14935672754773
Iteration: 6 || Loss: 24.14935672754773
saving ADAM checkpoint...
Sum of params:-105.97383
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.14935672754773
Iteration: 2 || Loss: 24.139008967429294
Iteration: 3 || Loss: 23.996346827182567
Iteration: 4 || Loss: 23.953493562915845
Iteration: 5 || Loss: 23.73987033352659
Iteration: 6 || Loss: 23.711006865824952
Iteration: 7 || Loss: 23.694163580616586
Iteration: 8 || Loss: 23.65343058484396
Iteration: 9 || Loss: 23.64509555615085
Iteration: 10 || Loss: 23.632712829056565
Iteration: 11 || Loss: 23.6179775076708
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.93182
Epoch 194 loss:23.6179775076708
MSE loss S0.41255313653618286
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.93182
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.10220627951128
Iteration: 2 || Loss: 70.10160894348803
Iteration: 3 || Loss: 70.10086767674369
Iteration: 4 || Loss: 70.1001887447334
Iteration: 5 || Loss: 70.0995455258917
Iteration: 6 || Loss: 70.0995455258917
saving ADAM checkpoint...
Sum of params:-105.931946
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.0995455258917
Iteration: 2 || Loss: 70.09385719102927
Iteration: 3 || Loss: 70.0717643268445
Iteration: 4 || Loss: 69.86818711914364
Iteration: 5 || Loss: 69.79584712248106
Iteration: 6 || Loss: 69.72235137460538
Iteration: 7 || Loss: 69.66152250133129
Iteration: 8 || Loss: 69.63975836311224
Iteration: 9 || Loss: 69.62951284073205
Iteration: 10 || Loss: 69.61781833988836
Iteration: 11 || Loss: 69.59337529447863
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.978905
Epoch 194 loss:69.59337529447863
MSE loss S0.993536427020016
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.279901718568627
MSE loss S0.3494732766479607
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:119.09974596337958
MSE loss S2.28342842539224
waveform batch: 2/2
Test loss - extrapolation:58.9902183503726
MSE loss S1.3440714012252304
Epoch 194 mean train loss:3.423636251486197
Epoch 194 mean test loss - interpolation:4.046650286428105
Epoch 194 mean test loss - extrapolation:14.840830359479348
Start training epoch 195
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-105.978905
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.449932724804455
Iteration: 2 || Loss: 6.4489813966602405
Iteration: 3 || Loss: 6.448103709462622
Iteration: 4 || Loss: 6.447027798559793
Iteration: 5 || Loss: 6.446261794275316
Iteration: 6 || Loss: 6.446261794275316
saving ADAM checkpoint...
Sum of params:-105.978806
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.446261794275316
Iteration: 2 || Loss: 6.417034722894351
Iteration: 3 || Loss: 6.358204591094305
Iteration: 4 || Loss: 6.292091840229137
Iteration: 5 || Loss: 6.167088678383098
Iteration: 6 || Loss: 6.11049480695945
Iteration: 7 || Loss: 6.101794660284346
Iteration: 8 || Loss: 6.0959643254227505
Iteration: 9 || Loss: 6.093730459938126
Iteration: 10 || Loss: 6.088007970238972
Iteration: 11 || Loss: 6.072397359847352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.996376
Epoch 195 loss:6.072397359847352
MSE loss S0.20806464274723022
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-105.996376
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.1496796706246
Iteration: 2 || Loss: 24.149196596490196
Iteration: 3 || Loss: 24.14871235782753
Iteration: 4 || Loss: 24.14813142462443
Iteration: 5 || Loss: 24.147600062134412
Iteration: 6 || Loss: 24.147600062134412
saving ADAM checkpoint...
Sum of params:-105.99657
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.147600062134412
Iteration: 2 || Loss: 24.137011290575114
Iteration: 3 || Loss: 23.996037743264374
Iteration: 4 || Loss: 23.9494066484359
Iteration: 5 || Loss: 23.738017454830036
Iteration: 6 || Loss: 23.70734044952028
Iteration: 7 || Loss: 23.690612201598576
Iteration: 8 || Loss: 23.65139738613814
Iteration: 9 || Loss: 23.640675610969534
Iteration: 10 || Loss: 23.627797201001076
Iteration: 11 || Loss: 23.61320378864337
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.95499
Epoch 195 loss:23.61320378864337
MSE loss S0.41249899255200617
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.95499
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.05566261747263
Iteration: 2 || Loss: 70.05502147814654
Iteration: 3 || Loss: 70.05432089498247
Iteration: 4 || Loss: 70.05368575718188
Iteration: 5 || Loss: 70.05298097937408
Iteration: 6 || Loss: 70.05298097937408
saving ADAM checkpoint...
Sum of params:-105.95511
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.05298097937408
Iteration: 2 || Loss: 70.04736107142381
Iteration: 3 || Loss: 70.02573550918378
Iteration: 4 || Loss: 69.82441454567378
Iteration: 5 || Loss: 69.75485156179981
Iteration: 6 || Loss: 69.68070648068512
Iteration: 7 || Loss: 69.6206846009521
Iteration: 8 || Loss: 69.59906352180954
Iteration: 9 || Loss: 69.58873632367465
Iteration: 10 || Loss: 69.57677698373374
Iteration: 11 || Loss: 69.55302457593363
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.00178
Epoch 195 loss:69.55302457593363
MSE loss S0.9937051894045885
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.269495629885604
MSE loss S0.34943354554260586
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:118.94533593593803
MSE loss S2.2816165751538495
waveform batch: 2/2
Test loss - extrapolation:58.85977995464125
MSE loss S1.3419937313780073
Epoch 195 mean train loss:3.4220215767042883
Epoch 195 mean test loss - interpolation:4.044915938314268
Epoch 195 mean test loss - extrapolation:14.817092990881607
Start training epoch 196
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.00178
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.450119031576416
Iteration: 2 || Loss: 6.449202083129904
Iteration: 3 || Loss: 6.448189621823668
Iteration: 4 || Loss: 6.447298100282141
Iteration: 5 || Loss: 6.446358526009266
Iteration: 6 || Loss: 6.446358526009266
saving ADAM checkpoint...
Sum of params:-106.00171
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.446358526009266
Iteration: 2 || Loss: 6.417964325402306
Iteration: 3 || Loss: 6.358347178035465
Iteration: 4 || Loss: 6.29190178214637
Iteration: 5 || Loss: 6.165393454136314
Iteration: 6 || Loss: 6.10877016464218
Iteration: 7 || Loss: 6.1004194084314225
Iteration: 8 || Loss: 6.094661760180828
Iteration: 9 || Loss: 6.092476058631819
Iteration: 10 || Loss: 6.086549125432926
Iteration: 11 || Loss: 6.0712090716468925
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.019226
Epoch 196 loss:6.0712090716468925
MSE loss S0.20849033085803337
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.019226
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.150242827736434
Iteration: 2 || Loss: 24.149596640301993
Iteration: 3 || Loss: 24.149035192686807
Iteration: 4 || Loss: 24.14855117088722
Iteration: 5 || Loss: 24.147997783026707
Iteration: 6 || Loss: 24.147997783026707
saving ADAM checkpoint...
Sum of params:-106.019424
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.147997783026707
Iteration: 2 || Loss: 24.13695457187717
Iteration: 3 || Loss: 23.98747449677193
Iteration: 4 || Loss: 23.94228969823872
Iteration: 5 || Loss: 23.733131613447032
Iteration: 6 || Loss: 23.702502697198177
Iteration: 7 || Loss: 23.685708288457157
Iteration: 8 || Loss: 23.647342492107335
Iteration: 9 || Loss: 23.63567329726757
Iteration: 10 || Loss: 23.623746494971044
Iteration: 11 || Loss: 23.609641555573454
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-105.97924
Epoch 196 loss:23.609641555573454
MSE loss S0.41258589848432436
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-105.97924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.99489169827542
Iteration: 2 || Loss: 69.99419219770436
Iteration: 3 || Loss: 69.99356958139781
Iteration: 4 || Loss: 69.9929230926361
Iteration: 5 || Loss: 69.9923242869181
Iteration: 6 || Loss: 69.9923242869181
saving ADAM checkpoint...
Sum of params:-105.97938
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.9923242869181
Iteration: 2 || Loss: 69.98602913820537
Iteration: 3 || Loss: 69.96524208088255
Iteration: 4 || Loss: 69.7698632462447
Iteration: 5 || Loss: 69.70863546348122
Iteration: 6 || Loss: 69.63532226019511
Iteration: 7 || Loss: 69.57929425990329
Iteration: 8 || Loss: 69.55811925818334
Iteration: 9 || Loss: 69.54723421721597
Iteration: 10 || Loss: 69.53363526208102
Iteration: 11 || Loss: 69.51129034341817
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.02653
Epoch 196 loss:69.51129034341817
MSE loss S0.9945686301505932
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.259408012927953
MSE loss S0.34982337144138403
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:118.77874582139683
MSE loss S2.280009198747375
waveform batch: 2/2
Test loss - extrapolation:58.72081979133284
MSE loss S1.340194645110301
Epoch 196 mean train loss:3.420418654159949
Epoch 196 mean test loss - interpolation:4.043234668821325
Epoch 196 mean test loss - extrapolation:14.791630467727472
Start training epoch 197
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.02653
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.452396603433816
Iteration: 2 || Loss: 6.451549397147539
Iteration: 3 || Loss: 6.450658377453181
Iteration: 4 || Loss: 6.449781094455274
Iteration: 5 || Loss: 6.448860870287179
Iteration: 6 || Loss: 6.448860870287179
saving ADAM checkpoint...
Sum of params:-106.02643
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.448860870287179
Iteration: 2 || Loss: 6.4229972398998605
Iteration: 3 || Loss: 6.359333568767044
Iteration: 4 || Loss: 6.291376005139427
Iteration: 5 || Loss: 6.164608054439084
Iteration: 6 || Loss: 6.108367925181403
Iteration: 7 || Loss: 6.099911974561329
Iteration: 8 || Loss: 6.094257177419239
Iteration: 9 || Loss: 6.092106432814871
Iteration: 10 || Loss: 6.085800954250555
Iteration: 11 || Loss: 6.07030212295122
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.043755
Epoch 197 loss:6.07030212295122
MSE loss S0.20801458162923164
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.043755
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.14893610023085
Iteration: 2 || Loss: 24.148437306151042
Iteration: 3 || Loss: 24.14788806690121
Iteration: 4 || Loss: 24.147342555372827
Iteration: 5 || Loss: 24.146893006668336
Iteration: 6 || Loss: 24.146893006668336
saving ADAM checkpoint...
Sum of params:-106.04393
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.146893006668336
Iteration: 2 || Loss: 24.13594364590875
Iteration: 3 || Loss: 23.980858331568154
Iteration: 4 || Loss: 23.940073078780244
Iteration: 5 || Loss: 23.728550330152835
Iteration: 6 || Loss: 23.698292393304506
Iteration: 7 || Loss: 23.681679571232415
Iteration: 8 || Loss: 23.643064465934984
Iteration: 9 || Loss: 23.631649911881098
Iteration: 10 || Loss: 23.62005690619181
Iteration: 11 || Loss: 23.60578987352402
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.00435
Epoch 197 loss:23.60578987352402
MSE loss S0.4123816497480728
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.00435
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.94595934426877
Iteration: 2 || Loss: 69.94530283361682
Iteration: 3 || Loss: 69.9447289022033
Iteration: 4 || Loss: 69.94410120286756
Iteration: 5 || Loss: 69.94353000386218
Iteration: 6 || Loss: 69.94353000386218
saving ADAM checkpoint...
Sum of params:-106.00447
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.94353000386218
Iteration: 2 || Loss: 69.93712457646699
Iteration: 3 || Loss: 69.91677196289181
Iteration: 4 || Loss: 69.72136493358617
Iteration: 5 || Loss: 69.66346339765667
Iteration: 6 || Loss: 69.59098112406149
Iteration: 7 || Loss: 69.5366607623967
Iteration: 8 || Loss: 69.51563774653134
Iteration: 9 || Loss: 69.50448207112443
Iteration: 10 || Loss: 69.48953898538463
Iteration: 11 || Loss: 69.46830685196254
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.05175
Epoch 197 loss:69.46830685196254
MSE loss S0.9945800007595853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.249512234547996
MSE loss S0.34989262005735355
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:118.60226949026614
MSE loss S2.277549192761512
waveform batch: 2/2
Test loss - extrapolation:58.57585467765904
MSE loss S1.3379549626273424
Epoch 197 mean train loss:3.418772374084061
Epoch 197 mean test loss - interpolation:4.041585372424666
Epoch 197 mean test loss - extrapolation:14.764843680660432
Start training epoch 198
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.05175
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.45284360339343
Iteration: 2 || Loss: 6.451871817752329
Iteration: 3 || Loss: 6.451045519223414
Iteration: 4 || Loss: 6.450201461634564
Iteration: 5 || Loss: 6.449359002024971
Iteration: 6 || Loss: 6.449359002024971
saving ADAM checkpoint...
Sum of params:-106.05166
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.449359002024971
Iteration: 2 || Loss: 6.425195008063706
Iteration: 3 || Loss: 6.3588631902188
Iteration: 4 || Loss: 6.290251766394291
Iteration: 5 || Loss: 6.163902685268436
Iteration: 6 || Loss: 6.108282222095106
Iteration: 7 || Loss: 6.099349449906134
Iteration: 8 || Loss: 6.093704587389875
Iteration: 9 || Loss: 6.091564397233631
Iteration: 10 || Loss: 6.085197401432407
Iteration: 11 || Loss: 6.069563259495372
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.06866
Epoch 198 loss:6.069563259495372
MSE loss S0.20775099029027277
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.06866
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.139561278109383
Iteration: 2 || Loss: 24.139054379181005
Iteration: 3 || Loss: 24.138484681187077
Iteration: 4 || Loss: 24.137971094801838
Iteration: 5 || Loss: 24.13750134128033
Iteration: 6 || Loss: 24.13750134128033
saving ADAM checkpoint...
Sum of params:-106.06884
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.13750134128033
Iteration: 2 || Loss: 24.126739838901972
Iteration: 3 || Loss: 23.97051080618219
Iteration: 4 || Loss: 23.93415514557104
Iteration: 5 || Loss: 23.72348813456219
Iteration: 6 || Loss: 23.693850276478884
Iteration: 7 || Loss: 23.6770233606881
Iteration: 8 || Loss: 23.63713553757647
Iteration: 9 || Loss: 23.627470905631405
Iteration: 10 || Loss: 23.615057984883894
Iteration: 11 || Loss: 23.600864072003816
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.029106
Epoch 198 loss:23.600864072003816
MSE loss S0.41192372979690184
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.029106
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.91157530216648
Iteration: 2 || Loss: 69.91099502075599
Iteration: 3 || Loss: 69.91037438202636
Iteration: 4 || Loss: 69.90979320931352
Iteration: 5 || Loss: 69.90917358676069
Iteration: 6 || Loss: 69.90917358676069
saving ADAM checkpoint...
Sum of params:-106.02922
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.90917358676069
Iteration: 2 || Loss: 69.90269101576028
Iteration: 3 || Loss: 69.88248061437173
Iteration: 4 || Loss: 69.6824778304686
Iteration: 5 || Loss: 69.62211433019885
Iteration: 6 || Loss: 69.54958826453186
Iteration: 7 || Loss: 69.49398920032424
Iteration: 8 || Loss: 69.47269407893236
Iteration: 9 || Loss: 69.46163095943058
Iteration: 10 || Loss: 69.44705478134286
Iteration: 11 || Loss: 69.42650916258151
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.0757
Epoch 198 loss:69.42650916258151
MSE loss S0.9945306764579334
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.239432615726194
MSE loss S0.3498027976065391
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:118.4343687872247
MSE loss S2.2751043088225016
waveform batch: 2/2
Test loss - extrapolation:58.440946279935496
MSE loss S1.3356036140840004
Epoch 198 mean train loss:3.4171357411751964
Epoch 198 mean test loss - interpolation:4.039905435954366
Epoch 198 mean test loss - extrapolation:14.739609588930016
Start training epoch 199
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.0757
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4476461868522
Iteration: 2 || Loss: 6.446746955132075
Iteration: 3 || Loss: 6.445889551495859
Iteration: 4 || Loss: 6.445071663182603
Iteration: 5 || Loss: 6.444188827642261
Iteration: 6 || Loss: 6.444188827642261
saving ADAM checkpoint...
Sum of params:-106.075615
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.444188827642261
Iteration: 2 || Loss: 6.420199133895188
Iteration: 3 || Loss: 6.356290674745203
Iteration: 4 || Loss: 6.289220167755005
Iteration: 5 || Loss: 6.162307554453926
Iteration: 6 || Loss: 6.106792272650666
Iteration: 7 || Loss: 6.097993025088116
Iteration: 8 || Loss: 6.092282536252485
Iteration: 9 || Loss: 6.090183559941731
Iteration: 10 || Loss: 6.084016811876275
Iteration: 11 || Loss: 6.068209608084197
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.09292
Epoch 199 loss:6.068209608084197
MSE loss S0.20760525166582278
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.09292
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.138026578452564
Iteration: 2 || Loss: 24.137516213020717
Iteration: 3 || Loss: 24.136996107910242
Iteration: 4 || Loss: 24.136483577381842
Iteration: 5 || Loss: 24.135944482124646
Iteration: 6 || Loss: 24.135944482124646
saving ADAM checkpoint...
Sum of params:-106.09312
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.135944482124646
Iteration: 2 || Loss: 24.1251463639882
Iteration: 3 || Loss: 23.971161399410345
Iteration: 4 || Loss: 23.93154145599096
Iteration: 5 || Loss: 23.720712508422356
Iteration: 6 || Loss: 23.69019835022481
Iteration: 7 || Loss: 23.673581681910687
Iteration: 8 || Loss: 23.634918207188957
Iteration: 9 || Loss: 23.623512795476653
Iteration: 10 || Loss: 23.610714023611948
Iteration: 11 || Loss: 23.596482554011384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.053474
Epoch 199 loss:23.596482554011384
MSE loss S0.4119484206963857
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.053474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.86958622039604
Iteration: 2 || Loss: 69.86897094280083
Iteration: 3 || Loss: 69.86832338329042
Iteration: 4 || Loss: 69.86771953425296
Iteration: 5 || Loss: 69.86714291362821
Iteration: 6 || Loss: 69.86714291362821
saving ADAM checkpoint...
Sum of params:-106.05359
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.86714291362821
Iteration: 2 || Loss: 69.86064906714086
Iteration: 3 || Loss: 69.84048706076045
Iteration: 4 || Loss: 69.64239884752813
Iteration: 5 || Loss: 69.58147091585703
Iteration: 6 || Loss: 69.50834068545481
Iteration: 7 || Loss: 69.45287857150512
Iteration: 8 || Loss: 69.43162015680423
Iteration: 9 || Loss: 69.42064851372311
Iteration: 10 || Loss: 69.40603339499205
Iteration: 11 || Loss: 69.38591490017494
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.099655
Epoch 199 loss:69.38591490017494
MSE loss S0.9942530892348813
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.22994264724015
MSE loss S0.34968266422979793
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:118.2684004596168
MSE loss S2.27252169853151
waveform batch: 2/2
Test loss - extrapolation:58.3072735599625
MSE loss S1.3332041185383985
Epoch 199 mean train loss:3.4155381745610525
Epoch 199 mean test loss - interpolation:4.038323774540025
Epoch 199 mean test loss - extrapolation:14.71463950163161
Start training epoch 200
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.099655
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.446975466854505
Iteration: 2 || Loss: 6.446121921322568
Iteration: 3 || Loss: 6.445260622421513
Iteration: 4 || Loss: 6.44445175903528
Iteration: 5 || Loss: 6.443571919296248
Iteration: 6 || Loss: 6.443571919296248
saving ADAM checkpoint...
Sum of params:-106.09956
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.443571919296248
Iteration: 2 || Loss: 6.419893044552218
Iteration: 3 || Loss: 6.355857247785295
Iteration: 4 || Loss: 6.289069281256009
Iteration: 5 || Loss: 6.161078330253797
Iteration: 6 || Loss: 6.105472440879711
Iteration: 7 || Loss: 6.096851538704469
Iteration: 8 || Loss: 6.0912012595355165
Iteration: 9 || Loss: 6.089120055830232
Iteration: 10 || Loss: 6.082827639112544
Iteration: 11 || Loss: 6.0670122597619045
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.116936
Epoch 200 loss:6.0670122597619045
MSE loss S0.2073474817290622
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.116936
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.13900582218156
Iteration: 2 || Loss: 24.138452362290597
Iteration: 3 || Loss: 24.137946204712136
Iteration: 4 || Loss: 24.137453142447157
Iteration: 5 || Loss: 24.136975791814983
Iteration: 6 || Loss: 24.136975791814983
saving ADAM checkpoint...
Sum of params:-106.11712
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.136975791814983
Iteration: 2 || Loss: 24.126200669964838
Iteration: 3 || Loss: 23.971951920931602
Iteration: 4 || Loss: 23.932071875180576
Iteration: 5 || Loss: 23.7171302535018
Iteration: 6 || Loss: 23.686113878180308
Iteration: 7 || Loss: 23.669773842141836
Iteration: 8 || Loss: 23.63126473010978
Iteration: 9 || Loss: 23.619474498008824
Iteration: 10 || Loss: 23.60704615950417
Iteration: 11 || Loss: 23.592602576535302
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.07825
Epoch 200 loss:23.592602576535302
MSE loss S0.4118487001045563
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.07825
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.8203878578263
Iteration: 2 || Loss: 69.819824102932
Iteration: 3 || Loss: 69.8192309138436
Iteration: 4 || Loss: 69.81866447882577
Iteration: 5 || Loss: 69.81807398685572
Iteration: 6 || Loss: 69.81807398685572
saving ADAM checkpoint...
Sum of params:-106.07837
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.81807398685572
Iteration: 2 || Loss: 69.81135993933961
Iteration: 3 || Loss: 69.79158944032058
Iteration: 4 || Loss: 69.5996421045258
Iteration: 5 || Loss: 69.53794873257284
Iteration: 6 || Loss: 69.46556864530677
Iteration: 7 || Loss: 69.41237658102531
Iteration: 8 || Loss: 69.39115742936501
Iteration: 9 || Loss: 69.37981836677045
Iteration: 10 || Loss: 69.36209835567281
Iteration: 11 || Loss: 69.34357841447039
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.12591
Epoch 200 loss:69.34357841447039
MSE loss S0.993870470013971
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.218969180936817
MSE loss S0.3495353954835345
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:118.07282468820974
MSE loss S2.2696766682417575
waveform batch: 2/2
Test loss - extrapolation:58.15170434620925
MSE loss S1.331220226350925
Epoch 200 mean train loss:3.4139032155437103
Epoch 200 mean test loss - interpolation:4.0364948634894695
Epoch 200 mean test loss - extrapolation:14.685377419534916
Start training epoch 201
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.12591
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.445778565656886
Iteration: 2 || Loss: 6.444883363322838
Iteration: 3 || Loss: 6.444036897573248
Iteration: 4 || Loss: 6.443279218760823
Iteration: 5 || Loss: 6.4424669526313245
Iteration: 6 || Loss: 6.4424669526313245
saving ADAM checkpoint...
Sum of params:-106.12581
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4424669526313245
Iteration: 2 || Loss: 6.435354683571097
Iteration: 3 || Loss: 6.366460164056439
Iteration: 4 || Loss: 6.29692943311387
Iteration: 5 || Loss: 6.162496123166944
Iteration: 6 || Loss: 6.126112294875881
Iteration: 7 || Loss: 6.100558498521022
Iteration: 8 || Loss: 6.094612668109829
Iteration: 9 || Loss: 6.089415412825511
Iteration: 10 || Loss: 6.086655464423618
Iteration: 11 || Loss: 6.079516094045438
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.14024
Epoch 201 loss:6.079516094045438
MSE loss S0.21119845869559492
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.14024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.04294500079311
Iteration: 2 || Loss: 24.04247652145332
Iteration: 3 || Loss: 24.04199569049676
Iteration: 4 || Loss: 24.04147398634985
Iteration: 5 || Loss: 24.041020426962195
Iteration: 6 || Loss: 24.041020426962195
saving ADAM checkpoint...
Sum of params:-106.14032
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.041020426962195
Iteration: 2 || Loss: 24.03256304748329
Iteration: 3 || Loss: 23.95643447969156
Iteration: 4 || Loss: 23.894562121676557
Iteration: 5 || Loss: 23.674655176248972
Iteration: 6 || Loss: 23.643475810925842
Iteration: 7 || Loss: 23.630156268581498
Iteration: 8 || Loss: 23.601708023716363
Iteration: 9 || Loss: 23.59775089104018
Iteration: 10 || Loss: 23.589991970045542
Iteration: 11 || Loss: 23.57611399440485
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.100235
Epoch 201 loss:23.57611399440485
MSE loss S0.41140404873369274
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.100235
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.76104982641262
Iteration: 2 || Loss: 69.76066320590112
Iteration: 3 || Loss: 69.76035046237634
Iteration: 4 || Loss: 69.76011535125444
Iteration: 5 || Loss: 69.75983370691694
Iteration: 6 || Loss: 69.75983370691694
saving ADAM checkpoint...
Sum of params:-106.100296
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.75983370691694
Iteration: 2 || Loss: 69.7537483262228
Iteration: 3 || Loss: 69.73383587897148
Iteration: 4 || Loss: 69.53270970372746
Iteration: 5 || Loss: 69.45507724249181
Iteration: 6 || Loss: 69.40094434469903
Iteration: 7 || Loss: 69.36454607684863
Iteration: 8 || Loss: 69.3437423487087
Iteration: 9 || Loss: 69.3303046433959
Iteration: 10 || Loss: 69.30847558438764
Iteration: 11 || Loss: 69.2880400151388
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.15973
Epoch 201 loss:69.2880400151388
MSE loss S0.993697196393993
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.21682319731337
MSE loss S0.34943851763378275
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:117.77074204603461
MSE loss S2.262577924071871
waveform batch: 2/2
Test loss - extrapolation:57.96620576210536
MSE loss S1.3293598557713013
Epoch 201 mean train loss:3.4118506932272097
Epoch 201 mean test loss - interpolation:4.036137199552228
Epoch 201 mean test loss - extrapolation:14.644745650678331
Start training epoch 202
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.15973
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.399143383438293
Iteration: 2 || Loss: 6.398327229027666
Iteration: 3 || Loss: 6.397557267832263
Iteration: 4 || Loss: 6.396775076857232
Iteration: 5 || Loss: 6.395982709433592
Iteration: 6 || Loss: 6.395982709433592
saving ADAM checkpoint...
Sum of params:-106.15967
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.395982709433592
Iteration: 2 || Loss: 6.375961049519677
Iteration: 3 || Loss: 6.322271424049312
Iteration: 4 || Loss: 6.268604847459683
Iteration: 5 || Loss: 6.174759341812902
Iteration: 6 || Loss: 6.121949940163498
Iteration: 7 || Loss: 6.101337978049262
Iteration: 8 || Loss: 6.0954397981884965
Iteration: 9 || Loss: 6.091799270698465
Iteration: 10 || Loss: 6.088104989788714
Iteration: 11 || Loss: 6.069049830519949
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.17936
Epoch 202 loss:6.069049830519949
MSE loss S0.20719601095679835
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.17936
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.130172973807465
Iteration: 2 || Loss: 24.129695137872684
Iteration: 3 || Loss: 24.12911945500501
Iteration: 4 || Loss: 24.12859075516506
Iteration: 5 || Loss: 24.128088300770315
Iteration: 6 || Loss: 24.128088300770315
saving ADAM checkpoint...
Sum of params:-106.17955
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.128088300770315
Iteration: 2 || Loss: 24.117293717502953
Iteration: 3 || Loss: 23.984910535650787
Iteration: 4 || Loss: 23.926204155656723
Iteration: 5 || Loss: 23.716155915319728
Iteration: 6 || Loss: 23.681190087703143
Iteration: 7 || Loss: 23.663501536033746
Iteration: 8 || Loss: 23.630335259564216
Iteration: 9 || Loss: 23.613440824516946
Iteration: 10 || Loss: 23.60196459482358
Iteration: 11 || Loss: 23.58816464919415
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.14047
Epoch 202 loss:23.58816464919415
MSE loss S0.41302457619789734
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.14047
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.72474461546521
Iteration: 2 || Loss: 69.7240971686544
Iteration: 3 || Loss: 69.7235696341084
Iteration: 4 || Loss: 69.722969414723
Iteration: 5 || Loss: 69.72233356687416
Iteration: 6 || Loss: 69.72233356687416
saving ADAM checkpoint...
Sum of params:-106.14059
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.72233356687416
Iteration: 2 || Loss: 69.7152655856797
Iteration: 3 || Loss: 69.69425564141314
Iteration: 4 || Loss: 69.49479618422039
Iteration: 5 || Loss: 69.43536926428493
Iteration: 6 || Loss: 69.36277017825056
Iteration: 7 || Loss: 69.31148533897567
Iteration: 8 || Loss: 69.28968198705446
Iteration: 9 || Loss: 69.27937867920053
Iteration: 10 || Loss: 69.26363036201383
Iteration: 11 || Loss: 69.24533106841517
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.184074
Epoch 202 loss:69.24533106841517
MSE loss S0.9934159776628987
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.20181971231638
MSE loss S0.3507592572092799
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:117.65010186842433
MSE loss S2.262284262993354
waveform batch: 2/2
Test loss - extrapolation:57.838116115027496
MSE loss S1.3238685321872157
Epoch 202 mean train loss:3.4104326051079057
Epoch 202 mean test loss - interpolation:4.033636618719396
Epoch 202 mean test loss - extrapolation:14.624018165287652
Start training epoch 203
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.184074
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4436363714543905
Iteration: 2 || Loss: 6.442725525764558
Iteration: 3 || Loss: 6.441902838684348
Iteration: 4 || Loss: 6.441110345290733
Iteration: 5 || Loss: 6.440393156906941
Iteration: 6 || Loss: 6.440393156906941
saving ADAM checkpoint...
Sum of params:-106.18398
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.440393156906941
Iteration: 2 || Loss: 6.433569740743129
Iteration: 3 || Loss: 6.367785913133275
Iteration: 4 || Loss: 6.298896345861068
Iteration: 5 || Loss: 6.164107946418427
Iteration: 6 || Loss: 6.125206283857171
Iteration: 7 || Loss: 6.1025935490655225
Iteration: 8 || Loss: 6.096433380946769
Iteration: 9 || Loss: 6.091694913360885
Iteration: 10 || Loss: 6.088359309052735
Iteration: 11 || Loss: 6.080526341545649
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.1984
Epoch 203 loss:6.080526341545649
MSE loss S0.21200167900802694
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.1984
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.039981580549835
Iteration: 2 || Loss: 24.039538674314795
Iteration: 3 || Loss: 24.03905251120489
Iteration: 4 || Loss: 24.03857323866857
Iteration: 5 || Loss: 24.03815739610812
Iteration: 6 || Loss: 24.03815739610812
saving ADAM checkpoint...
Sum of params:-106.198494
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.03815739610812
Iteration: 2 || Loss: 24.02955960277858
Iteration: 3 || Loss: 23.95266408870195
Iteration: 4 || Loss: 23.888205464047406
Iteration: 5 || Loss: 23.66618470549709
Iteration: 6 || Loss: 23.635704306430114
Iteration: 7 || Loss: 23.622782562314395
Iteration: 8 || Loss: 23.59452991358009
Iteration: 9 || Loss: 23.590491131568452
Iteration: 10 || Loss: 23.582784864686595
Iteration: 11 || Loss: 23.568789169369317
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.15939
Epoch 203 loss:23.568789169369317
MSE loss S0.411686860779092
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.15939
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.66427786948537
Iteration: 2 || Loss: 69.66397848046402
Iteration: 3 || Loss: 69.66365631002878
Iteration: 4 || Loss: 69.6633888680079
Iteration: 5 || Loss: 69.6630511306303
Iteration: 6 || Loss: 69.6630511306303
saving ADAM checkpoint...
Sum of params:-106.15944
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.6630511306303
Iteration: 2 || Loss: 69.65704017653339
Iteration: 3 || Loss: 69.63696969321595
Iteration: 4 || Loss: 69.43954128469764
Iteration: 5 || Loss: 69.3571158111324
Iteration: 6 || Loss: 69.30307240564326
Iteration: 7 || Loss: 69.26737844967167
Iteration: 8 || Loss: 69.2469070555548
Iteration: 9 || Loss: 69.23380307177052
Iteration: 10 || Loss: 69.21256673715875
Iteration: 11 || Loss: 69.1933909939705
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.21613
Epoch 203 loss:69.1933909939705
MSE loss S0.9931427853327501
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.198562841970734
MSE loss S0.35006415558950466
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:117.36536122659301
MSE loss S2.25555967941759
waveform batch: 2/2
Test loss - extrapolation:57.66628221569223
MSE loss S1.3234886456927866
Epoch 203 mean train loss:3.4083691898236363
Epoch 203 mean test loss - interpolation:4.033093806995122
Epoch 203 mean test loss - extrapolation:14.585970286857103
Start training epoch 204
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.21613
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.393064402031281
Iteration: 2 || Loss: 6.392315638052271
Iteration: 3 || Loss: 6.391481446044945
Iteration: 4 || Loss: 6.3907763378349625
Iteration: 5 || Loss: 6.390011562168146
Iteration: 6 || Loss: 6.390011562168146
saving ADAM checkpoint...
Sum of params:-106.216064
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.390011562168146
Iteration: 2 || Loss: 6.370486087108566
Iteration: 3 || Loss: 6.319041208540074
Iteration: 4 || Loss: 6.266056604921564
Iteration: 5 || Loss: 6.175570932278888
Iteration: 6 || Loss: 6.122152361839641
Iteration: 7 || Loss: 6.1024605036055375
Iteration: 8 || Loss: 6.0965284798639
Iteration: 9 || Loss: 6.092604861689251
Iteration: 10 || Loss: 6.0890520681263265
Iteration: 11 || Loss: 6.069158972428515
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.235756
Epoch 204 loss:6.069158972428515
MSE loss S0.20748813479198525
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.235756
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.12449492236628
Iteration: 2 || Loss: 24.12395910290245
Iteration: 3 || Loss: 24.123453924817422
Iteration: 4 || Loss: 24.122994240677258
Iteration: 5 || Loss: 24.122472914691322
Iteration: 6 || Loss: 24.122472914691322
saving ADAM checkpoint...
Sum of params:-106.23594
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.122472914691322
Iteration: 2 || Loss: 24.111776428877256
Iteration: 3 || Loss: 23.975230957031272
Iteration: 4 || Loss: 23.919956663460148
Iteration: 5 || Loss: 23.711618949046013
Iteration: 6 || Loss: 23.675174311946552
Iteration: 7 || Loss: 23.657648402947046
Iteration: 8 || Loss: 23.62487524381422
Iteration: 9 || Loss: 23.60671514114331
Iteration: 10 || Loss: 23.594294700650085
Iteration: 11 || Loss: 23.580231117857647
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.197586
Epoch 204 loss:23.580231117857647
MSE loss S0.41300502671342193
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.197586
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.63601297520583
Iteration: 2 || Loss: 69.63534398921203
Iteration: 3 || Loss: 69.63477142569593
Iteration: 4 || Loss: 69.634200258268
Iteration: 5 || Loss: 69.63364240460332
Iteration: 6 || Loss: 69.63364240460332
saving ADAM checkpoint...
Sum of params:-106.1977
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.63364240460332
Iteration: 2 || Loss: 69.6262343266196
Iteration: 3 || Loss: 69.60477808945492
Iteration: 4 || Loss: 69.40263865184605
Iteration: 5 || Loss: 69.34096847554102
Iteration: 6 || Loss: 69.26918112984426
Iteration: 7 || Loss: 69.21872237912083
Iteration: 8 || Loss: 69.1967316663638
Iteration: 9 || Loss: 69.18650670907047
Iteration: 10 || Loss: 69.17072813638038
Iteration: 11 || Loss: 69.15349115311243
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.23978
Epoch 204 loss:69.15349115311243
MSE loss S0.9927686065373675
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.182901389758303
MSE loss S0.35118381422615896
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:117.24597366290499
MSE loss S2.255418423252218
waveform batch: 2/2
Test loss - extrapolation:57.539235716945974
MSE loss S1.3183308819898316
Epoch 204 mean train loss:3.406995904944779
Epoch 204 mean test loss - interpolation:4.0304835649597175
Epoch 204 mean test loss - extrapolation:14.56543411498758
Start training epoch 205
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.23978
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.434383986224566
Iteration: 2 || Loss: 6.433629702070005
Iteration: 3 || Loss: 6.432825687606683
Iteration: 4 || Loss: 6.432077894436468
Iteration: 5 || Loss: 6.431291722662311
Iteration: 6 || Loss: 6.431291722662311
saving ADAM checkpoint...
Sum of params:-106.23971
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.431291722662311
Iteration: 2 || Loss: 6.423945905003909
Iteration: 3 || Loss: 6.361374355431369
Iteration: 4 || Loss: 6.294422290124562
Iteration: 5 || Loss: 6.16416698530752
Iteration: 6 || Loss: 6.121886512459901
Iteration: 7 || Loss: 6.102979449380815
Iteration: 8 || Loss: 6.0968507580433515
Iteration: 9 || Loss: 6.0921192469746295
Iteration: 10 || Loss: 6.088541648519448
Iteration: 11 || Loss: 6.080765739918432
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.25402
Epoch 205 loss:6.080765739918432
MSE loss S0.2124222395432786
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.25402
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.026218756302164
Iteration: 2 || Loss: 24.02575870912174
Iteration: 3 || Loss: 24.025236241500956
Iteration: 4 || Loss: 24.024959292678435
Iteration: 5 || Loss: 24.02442180786239
Iteration: 6 || Loss: 24.02442180786239
saving ADAM checkpoint...
Sum of params:-106.25412
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.02442180786239
Iteration: 2 || Loss: 24.015955742600365
Iteration: 3 || Loss: 23.94219494305251
Iteration: 4 || Loss: 23.877309136779985
Iteration: 5 || Loss: 23.657143874628897
Iteration: 6 || Loss: 23.627019676552937
Iteration: 7 || Loss: 23.614114169522548
Iteration: 8 || Loss: 23.58659503536252
Iteration: 9 || Loss: 23.582573725172633
Iteration: 10 || Loss: 23.574787836393888
Iteration: 11 || Loss: 23.56096814957271
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.21615
Epoch 205 loss:23.56096814957271
MSE loss S0.4121355102105016
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.21615
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.55578986870697
Iteration: 2 || Loss: 69.55550259884295
Iteration: 3 || Loss: 69.55520703260095
Iteration: 4 || Loss: 69.55488697268869
Iteration: 5 || Loss: 69.55453719243414
Iteration: 6 || Loss: 69.55453719243414
saving ADAM checkpoint...
Sum of params:-106.2162
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.55453719243414
Iteration: 2 || Loss: 69.54823472948448
Iteration: 3 || Loss: 69.52975085488404
Iteration: 4 || Loss: 69.32973410495865
Iteration: 5 || Loss: 69.26036483194461
Iteration: 6 || Loss: 69.20894485694176
Iteration: 7 || Loss: 69.17529355414419
Iteration: 8 || Loss: 69.1556102183082
Iteration: 9 || Loss: 69.14258666814617
Iteration: 10 || Loss: 69.12164302709333
Iteration: 11 || Loss: 69.10378221747187
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.27124
Epoch 205 loss:69.10378221747187
MSE loss S0.9927624551410452
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.1836768798487
MSE loss S0.35053509666345384
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:116.99524850425449
MSE loss S2.2485120027047225
waveform batch: 2/2
Test loss - extrapolation:57.39487559713906
MSE loss S1.3175073661456733
Epoch 205 mean train loss:3.405017796791828
Epoch 205 mean test loss - interpolation:4.0306128133081165
Epoch 205 mean test loss - extrapolation:14.532510341782796
Start training epoch 206
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.27124
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.385036278865584
Iteration: 2 || Loss: 6.384241889400794
Iteration: 3 || Loss: 6.3833833839070895
Iteration: 4 || Loss: 6.38263608825889
Iteration: 5 || Loss: 6.381951434032725
Iteration: 6 || Loss: 6.381951434032725
saving ADAM checkpoint...
Sum of params:-106.27119
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.381951434032725
Iteration: 2 || Loss: 6.361577673126933
Iteration: 3 || Loss: 6.314042483544009
Iteration: 4 || Loss: 6.263250267403261
Iteration: 5 || Loss: 6.176256885446392
Iteration: 6 || Loss: 6.121309466275239
Iteration: 7 || Loss: 6.103038996146184
Iteration: 8 || Loss: 6.096932933274413
Iteration: 9 || Loss: 6.092668165548946
Iteration: 10 || Loss: 6.089173454341096
Iteration: 11 || Loss: 6.0686639320480005
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.29094
Epoch 206 loss:6.0686639320480005
MSE loss S0.2073431059504524
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.29094
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.115900415293087
Iteration: 2 || Loss: 24.115436664466174
Iteration: 3 || Loss: 24.114918271867623
Iteration: 4 || Loss: 24.114434037867547
Iteration: 5 || Loss: 24.11397962516949
Iteration: 6 || Loss: 24.11397962516949
saving ADAM checkpoint...
Sum of params:-106.29112
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.11397962516949
Iteration: 2 || Loss: 24.103454292419478
Iteration: 3 || Loss: 23.9689149011788
Iteration: 4 || Loss: 23.91472469646171
Iteration: 5 || Loss: 23.70637331224048
Iteration: 6 || Loss: 23.66853313407146
Iteration: 7 || Loss: 23.65065237092548
Iteration: 8 || Loss: 23.61812673218022
Iteration: 9 || Loss: 23.59966485174462
Iteration: 10 || Loss: 23.586649488683182
Iteration: 11 || Loss: 23.572366629218624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.25316
Epoch 206 loss:23.572366629218624
MSE loss S0.41333264616496634
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.25316
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.55228550197515
Iteration: 2 || Loss: 69.55174066658296
Iteration: 3 || Loss: 69.5511750590038
Iteration: 4 || Loss: 69.55064632666544
Iteration: 5 || Loss: 69.55012403675205
Iteration: 6 || Loss: 69.55012403675205
saving ADAM checkpoint...
Sum of params:-106.25327
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.55012403675205
Iteration: 2 || Loss: 69.54218315157428
Iteration: 3 || Loss: 69.52044171186638
Iteration: 4 || Loss: 69.31981074149913
Iteration: 5 || Loss: 69.25217543714488
Iteration: 6 || Loss: 69.18095999308616
Iteration: 7 || Loss: 69.13076092604013
Iteration: 8 || Loss: 69.1083793782834
Iteration: 9 || Loss: 69.09785850015993
Iteration: 10 || Loss: 69.07998133868405
Iteration: 11 || Loss: 69.06378599440836
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.29642
Epoch 206 loss:69.06378599440836
MSE loss S0.989125447751382
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.163028444374987
MSE loss S0.3501693222239311
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:116.79276956381246
MSE loss S2.246578654028198
waveform batch: 2/2
Test loss - extrapolation:57.20192659313582
MSE loss S1.3119255512099561
Epoch 206 mean train loss:3.403614363988793
Epoch 206 mean test loss - interpolation:4.027171407395831
Epoch 206 mean test loss - extrapolation:14.499558013079024
Start training epoch 207
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.29642
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.42804798822458
Iteration: 2 || Loss: 6.427273053736084
Iteration: 3 || Loss: 6.426578069579988
Iteration: 4 || Loss: 6.4257259864344665
Iteration: 5 || Loss: 6.425040151564992
Iteration: 6 || Loss: 6.425040151564992
saving ADAM checkpoint...
Sum of params:-106.29634
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.425040151564992
Iteration: 2 || Loss: 6.416222356948397
Iteration: 3 || Loss: 6.3512459273432285
Iteration: 4 || Loss: 6.284788381493265
Iteration: 5 || Loss: 6.1658086116105055
Iteration: 6 || Loss: 6.125632674688823
Iteration: 7 || Loss: 6.104142429148874
Iteration: 8 || Loss: 6.098261949767175
Iteration: 9 || Loss: 6.093084041976235
Iteration: 10 || Loss: 6.089522934969601
Iteration: 11 || Loss: 6.082265125577551
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.310295
Epoch 207 loss:6.082265125577551
MSE loss S0.21314404618967944
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.310295
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.008631561673038
Iteration: 2 || Loss: 24.0082649028717
Iteration: 3 || Loss: 24.007867903248027
Iteration: 4 || Loss: 24.007409220905377
Iteration: 5 || Loss: 24.007008181448008
Iteration: 6 || Loss: 24.007008181448008
saving ADAM checkpoint...
Sum of params:-106.31038
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.007008181448008
Iteration: 2 || Loss: 23.998799219595718
Iteration: 3 || Loss: 23.930257643776283
Iteration: 4 || Loss: 23.86661553262992
Iteration: 5 || Loss: 23.64655749508268
Iteration: 6 || Loss: 23.61667870191828
Iteration: 7 || Loss: 23.60338214214856
Iteration: 8 || Loss: 23.577743467326556
Iteration: 9 || Loss: 23.57395261026163
Iteration: 10 || Loss: 23.566201800781943
Iteration: 11 || Loss: 23.553177625473595
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.27253
Epoch 207 loss:23.553177625473595
MSE loss S0.41292301083248384
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.27253
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.45720300253208
Iteration: 2 || Loss: 69.45691070347263
Iteration: 3 || Loss: 69.4566095336678
Iteration: 4 || Loss: 69.45638461585358
Iteration: 5 || Loss: 69.45610286324467
Iteration: 6 || Loss: 69.45610286324467
saving ADAM checkpoint...
Sum of params:-106.272575
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.45610286324467
Iteration: 2 || Loss: 69.44963888695966
Iteration: 3 || Loss: 69.43247099289565
Iteration: 4 || Loss: 69.22623523531674
Iteration: 5 || Loss: 69.16889193111652
Iteration: 6 || Loss: 69.11844736276446
Iteration: 7 || Loss: 69.08455992391238
Iteration: 8 || Loss: 69.06527534862579
Iteration: 9 || Loss: 69.05238174382838
Iteration: 10 || Loss: 69.03208305792765
Iteration: 11 || Loss: 69.0153084262805
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.32662
Epoch 207 loss:69.0153084262805
MSE loss S0.9925748451711189
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.166819281443182
MSE loss S0.3511643366304099
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:116.61495458969608
MSE loss S2.2417352898711327
waveform batch: 2/2
Test loss - extrapolation:57.12030952228698
MSE loss S1.3120419959525873
Epoch 207 mean train loss:3.401750040597643
Epoch 207 mean test loss - interpolation:4.027803213573864
Epoch 207 mean test loss - extrapolation:14.477938675998589
Start training epoch 208
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.32662
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3770069693012505
Iteration: 2 || Loss: 6.376116456262133
Iteration: 3 || Loss: 6.3753803515711285
Iteration: 4 || Loss: 6.374580993533672
Iteration: 5 || Loss: 6.373891956760199
Iteration: 6 || Loss: 6.373891956760199
saving ADAM checkpoint...
Sum of params:-106.32654
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.373891956760199
Iteration: 2 || Loss: 6.353829049450777
Iteration: 3 || Loss: 6.310453556509694
Iteration: 4 || Loss: 6.262247416016558
Iteration: 5 || Loss: 6.1762103311403065
Iteration: 6 || Loss: 6.121002513620954
Iteration: 7 || Loss: 6.1033923051203445
Iteration: 8 || Loss: 6.097338475880534
Iteration: 9 || Loss: 6.092598282321079
Iteration: 10 || Loss: 6.088777104029881
Iteration: 11 || Loss: 6.068249847862469
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.34633
Epoch 208 loss:6.068249847862469
MSE loss S0.20756048822831763
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.34633
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.106494937271506
Iteration: 2 || Loss: 24.10601204272598
Iteration: 3 || Loss: 24.105531189829396
Iteration: 4 || Loss: 24.105132403486664
Iteration: 5 || Loss: 24.10462975754548
Iteration: 6 || Loss: 24.10462975754548
saving ADAM checkpoint...
Sum of params:-106.34651
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.10462975754548
Iteration: 2 || Loss: 24.09406095339858
Iteration: 3 || Loss: 23.959191031291372
Iteration: 4 || Loss: 23.9060252559971
Iteration: 5 || Loss: 23.700460320256525
Iteration: 6 || Loss: 23.661662066062796
Iteration: 7 || Loss: 23.643626490346143
Iteration: 8 || Loss: 23.61150153614152
Iteration: 9 || Loss: 23.59259546396608
Iteration: 10 || Loss: 23.579056095331524
Iteration: 11 || Loss: 23.564922752903833
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.30922
Epoch 208 loss:23.564922752903833
MSE loss S0.4138465167692008
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.30922
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.46353795435444
Iteration: 2 || Loss: 69.46307846924707
Iteration: 3 || Loss: 69.46254711992415
Iteration: 4 || Loss: 69.46202716211268
Iteration: 5 || Loss: 69.46145517147359
Iteration: 6 || Loss: 69.46145517147359
saving ADAM checkpoint...
Sum of params:-106.3093
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.46145517147359
Iteration: 2 || Loss: 69.45340969613383
Iteration: 3 || Loss: 69.43209751850769
Iteration: 4 || Loss: 69.23182723340679
Iteration: 5 || Loss: 69.16429615990982
Iteration: 6 || Loss: 69.09339665303523
Iteration: 7 || Loss: 69.04361175707163
Iteration: 8 || Loss: 69.02112706609401
Iteration: 9 || Loss: 69.0106288684599
Iteration: 10 || Loss: 68.99220180075991
Iteration: 11 || Loss: 68.97558449701766
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.35316
Epoch 208 loss:68.97558449701766
MSE loss S0.9799551769314313
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.152893764254433
MSE loss S0.3472342184399137
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:116.30878728806906
MSE loss S2.2330872579182417
waveform batch: 2/2
Test loss - extrapolation:56.837574749330244
MSE loss S1.300507051758736
Epoch 208 mean train loss:3.400301968889102
Epoch 208 mean test loss - interpolation:4.025482294042406
Epoch 208 mean test loss - extrapolation:14.428863503116608
Start training epoch 209
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.35316
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.429175426087822
Iteration: 2 || Loss: 6.428184468679454
Iteration: 3 || Loss: 6.427257879023929
Iteration: 4 || Loss: 6.426334746409567
Iteration: 5 || Loss: 6.425462467086648
Iteration: 6 || Loss: 6.425462467086648
saving ADAM checkpoint...
Sum of params:-106.35308
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.425462467086648
Iteration: 2 || Loss: 6.397651325569103
Iteration: 3 || Loss: 6.332023453297912
Iteration: 4 || Loss: 6.2709284778343335
Iteration: 5 || Loss: 6.1674575290801945
Iteration: 6 || Loss: 6.112586038122666
Iteration: 7 || Loss: 6.10127533165613
Iteration: 8 || Loss: 6.095474128347949
Iteration: 9 || Loss: 6.093029718445187
Iteration: 10 || Loss: 6.086927004638487
Iteration: 11 || Loss: 6.067316517682913
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.37005
Epoch 209 loss:6.067316517682913
MSE loss S0.20936957069531542
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.37005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.123093846155125
Iteration: 2 || Loss: 24.122606657590858
Iteration: 3 || Loss: 24.122167183757988
Iteration: 4 || Loss: 24.12174551588767
Iteration: 5 || Loss: 24.12126161660425
Iteration: 6 || Loss: 24.12126161660425
saving ADAM checkpoint...
Sum of params:-106.37016
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.12126161660425
Iteration: 2 || Loss: 24.10917062895442
Iteration: 3 || Loss: 23.948436809942123
Iteration: 4 || Loss: 23.904419929296076
Iteration: 5 || Loss: 23.683943229966065
Iteration: 6 || Loss: 23.653342591938983
Iteration: 7 || Loss: 23.63812557995721
Iteration: 8 || Loss: 23.60169205212015
Iteration: 9 || Loss: 23.586836935014762
Iteration: 10 || Loss: 23.574052483130597
Iteration: 11 || Loss: 23.560866595343143
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.33429
Epoch 209 loss:23.560866595343143
MSE loss S0.4117621590023008
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.33429
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.41233064775054
Iteration: 2 || Loss: 69.41175942179296
Iteration: 3 || Loss: 69.41115308602126
Iteration: 4 || Loss: 69.41064129390713
Iteration: 5 || Loss: 69.41008039375542
Iteration: 6 || Loss: 69.41008039375542
saving ADAM checkpoint...
Sum of params:-106.33443
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.41008039375542
Iteration: 2 || Loss: 69.4038039017605
Iteration: 3 || Loss: 69.3858300784743
Iteration: 4 || Loss: 69.15551277853888
Iteration: 5 || Loss: 69.12546845023762
Iteration: 6 || Loss: 69.0560616589581
Iteration: 7 || Loss: 69.00303179006994
Iteration: 8 || Loss: 68.98215066207604
Iteration: 9 || Loss: 68.97187707615191
Iteration: 10 || Loss: 68.95364766548255
Iteration: 11 || Loss: 68.93698742217549
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.37855
Epoch 209 loss:68.93698742217549
MSE loss S0.9836238477511237
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.140155890304378
MSE loss S0.3485709765302177
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:116.17862257309595
MSE loss S2.2333904097362485
waveform batch: 2/2
Test loss - extrapolation:56.75280735505481
MSE loss S1.3017473894291167
Epoch 209 mean train loss:3.3987989839724673
Epoch 209 mean test loss - interpolation:4.02335931505073
Epoch 209 mean test loss - extrapolation:14.410952494012562
Start training epoch 210
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.37855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.430539770586551
Iteration: 2 || Loss: 6.429715944230845
Iteration: 3 || Loss: 6.428826332212082
Iteration: 4 || Loss: 6.427985144442339
Iteration: 5 || Loss: 6.427273366762665
Iteration: 6 || Loss: 6.427273366762665
saving ADAM checkpoint...
Sum of params:-106.37845
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.427273366762665
Iteration: 2 || Loss: 6.404705650927575
Iteration: 3 || Loss: 6.336603456762049
Iteration: 4 || Loss: 6.2711261669000296
Iteration: 5 || Loss: 6.165063664894818
Iteration: 6 || Loss: 6.111089873665506
Iteration: 7 || Loss: 6.099238832588835
Iteration: 8 || Loss: 6.093714790701003
Iteration: 9 || Loss: 6.091421092036934
Iteration: 10 || Loss: 6.0852036011221795
Iteration: 11 || Loss: 6.066121554575383
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.395615
Epoch 210 loss:6.066121554575383
MSE loss S0.20964647605615702
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.395615
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.121224154599275
Iteration: 2 || Loss: 24.120722216026152
Iteration: 3 || Loss: 24.120355406747155
Iteration: 4 || Loss: 24.11986089447529
Iteration: 5 || Loss: 24.119442694482032
Iteration: 6 || Loss: 24.119442694482032
saving ADAM checkpoint...
Sum of params:-106.39572
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.119442694482032
Iteration: 2 || Loss: 24.106401836155793
Iteration: 3 || Loss: 23.95502335481435
Iteration: 4 || Loss: 23.898021142213754
Iteration: 5 || Loss: 23.678666369366184
Iteration: 6 || Loss: 23.647861448744017
Iteration: 7 || Loss: 23.632999347642055
Iteration: 8 || Loss: 23.596833037345867
Iteration: 9 || Loss: 23.582430355888928
Iteration: 10 || Loss: 23.56998678403789
Iteration: 11 || Loss: 23.556919205544293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.359695
Epoch 210 loss:23.556919205544293
MSE loss S0.4113010567894308
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.359695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.36781725312146
Iteration: 2 || Loss: 69.36721593041871
Iteration: 3 || Loss: 69.3666751955118
Iteration: 4 || Loss: 69.36610813985489
Iteration: 5 || Loss: 69.36557344063266
Iteration: 6 || Loss: 69.36557344063266
saving ADAM checkpoint...
Sum of params:-106.3598
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.36557344063266
Iteration: 2 || Loss: 69.3595647870391
Iteration: 3 || Loss: 69.34187224352081
Iteration: 4 || Loss: 69.1122375765201
Iteration: 5 || Loss: 69.08451676843458
Iteration: 6 || Loss: 69.0159548765375
Iteration: 7 || Loss: 68.96401456158964
Iteration: 8 || Loss: 68.94337147274261
Iteration: 9 || Loss: 68.93314711322152
Iteration: 10 || Loss: 68.91542222410075
Iteration: 11 || Loss: 68.8994855098715
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.40323
Epoch 210 loss:68.8994855098715
MSE loss S0.9860661071524939
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.130182685231468
MSE loss S0.3494830522150475
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:116.03685450193927
MSE loss S2.232628659250323
waveform batch: 2/2
Test loss - extrapolation:56.655433627868085
MSE loss S1.3010838495683745
Epoch 210 mean train loss:3.3973284920686613
Epoch 210 mean test loss - interpolation:4.021697114205245
Epoch 210 mean test loss - extrapolation:14.391024010817281
Start training epoch 211
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.40323
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.427024342020738
Iteration: 2 || Loss: 6.4262117609624445
Iteration: 3 || Loss: 6.425413128921688
Iteration: 4 || Loss: 6.424630427108596
Iteration: 5 || Loss: 6.423851126292446
Iteration: 6 || Loss: 6.423851126292446
saving ADAM checkpoint...
Sum of params:-106.40314
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.423851126292446
Iteration: 2 || Loss: 6.4155381688994355
Iteration: 3 || Loss: 6.348724894077943
Iteration: 4 || Loss: 6.282488970700131
Iteration: 5 || Loss: 6.163184434110747
Iteration: 6 || Loss: 6.124294255469155
Iteration: 7 || Loss: 6.101189831440127
Iteration: 8 || Loss: 6.095207010106719
Iteration: 9 || Loss: 6.090016424800235
Iteration: 10 || Loss: 6.086301827998241
Iteration: 11 || Loss: 6.079470846600728
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.41687
Epoch 211 loss:6.079470846600728
MSE loss S0.21297524852272198
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.41687
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.990858874297288
Iteration: 2 || Loss: 23.990511917438834
Iteration: 3 || Loss: 23.99011063931533
Iteration: 4 || Loss: 23.98972479323777
Iteration: 5 || Loss: 23.989378422298397
Iteration: 6 || Loss: 23.989378422298397
saving ADAM checkpoint...
Sum of params:-106.416885
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.989378422298397
Iteration: 2 || Loss: 23.98096164467047
Iteration: 3 || Loss: 23.914417084353186
Iteration: 4 || Loss: 23.851154686295292
Iteration: 5 || Loss: 23.63011176042301
Iteration: 6 || Loss: 23.600227548009105
Iteration: 7 || Loss: 23.587162017807703
Iteration: 8 || Loss: 23.562337584772877
Iteration: 9 || Loss: 23.55870136768944
Iteration: 10 || Loss: 23.550813191872106
Iteration: 11 || Loss: 23.538637430106967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.38006
Epoch 211 loss:23.538637430106967
MSE loss S0.4124177649511467
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.38006
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.28490975990339
Iteration: 2 || Loss: 69.28463461703379
Iteration: 3 || Loss: 69.28434974716771
Iteration: 4 || Loss: 69.28411594836449
Iteration: 5 || Loss: 69.28381302557975
Iteration: 6 || Loss: 69.28381302557975
saving ADAM checkpoint...
Sum of params:-106.3801
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.28381302557975
Iteration: 2 || Loss: 69.27747133382537
Iteration: 3 || Loss: 69.26136337093308
Iteration: 4 || Loss: 69.05675339955822
Iteration: 5 || Loss: 69.00333265066752
Iteration: 6 || Loss: 68.95302134021996
Iteration: 7 || Loss: 68.92010791882441
Iteration: 8 || Loss: 68.901251094753
Iteration: 9 || Loss: 68.88869988687058
Iteration: 10 || Loss: 68.86950490578889
Iteration: 11 || Loss: 68.85344985616736
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.43318
Epoch 211 loss:68.85344985616736
MSE loss S0.9907901402997106
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.134119597304053
MSE loss S0.35121002651164945
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:115.88014522504994
MSE loss S2.2283700950323224
waveform batch: 2/2
Test loss - extrapolation:56.591778699738796
MSE loss S1.3013994852974315
Epoch 211 mean train loss:3.39557097009914
Epoch 211 mean test loss - interpolation:4.022353266217342
Epoch 211 mean test loss - extrapolation:14.372660327065729
Start training epoch 212
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.43318
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.373405763450554
Iteration: 2 || Loss: 6.372631303890734
Iteration: 3 || Loss: 6.3717725392389015
Iteration: 4 || Loss: 6.371080235306506
Iteration: 5 || Loss: 6.370296684308637
Iteration: 6 || Loss: 6.370296684308637
saving ADAM checkpoint...
Sum of params:-106.433105
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.370296684308637
Iteration: 2 || Loss: 6.35092061241686
Iteration: 3 || Loss: 6.308619873095303
Iteration: 4 || Loss: 6.261925794217404
Iteration: 5 || Loss: 6.172653051921289
Iteration: 6 || Loss: 6.11732756955076
Iteration: 7 || Loss: 6.100418594165943
Iteration: 8 || Loss: 6.094417115122049
Iteration: 9 || Loss: 6.0895927432142685
Iteration: 10 || Loss: 6.085115582286375
Iteration: 11 || Loss: 6.064488185891716
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.45286
Epoch 212 loss:6.064488185891716
MSE loss S0.20708826481703413
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.45286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.09760483556593
Iteration: 2 || Loss: 24.097094211669567
Iteration: 3 || Loss: 24.096575315672748
Iteration: 4 || Loss: 24.096194857484903
Iteration: 5 || Loss: 24.095730102986643
Iteration: 6 || Loss: 24.095730102986643
saving ADAM checkpoint...
Sum of params:-106.45306
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.095730102986643
Iteration: 2 || Loss: 24.084996005661043
Iteration: 3 || Loss: 23.94633908173899
Iteration: 4 || Loss: 23.895101591475758
Iteration: 5 || Loss: 23.688975541684997
Iteration: 6 || Loss: 23.64878473827974
Iteration: 7 || Loss: 23.630623972045587
Iteration: 8 || Loss: 23.599012366183217
Iteration: 9 || Loss: 23.578903228660238
Iteration: 10 || Loss: 23.564684286841306
Iteration: 11 || Loss: 23.550804005211567
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.41685
Epoch 212 loss:23.550804005211567
MSE loss S0.4129355551432655
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.41685
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.29469122921395
Iteration: 2 || Loss: 69.2942105236004
Iteration: 3 || Loss: 69.29368834394549
Iteration: 4 || Loss: 69.29323732902343
Iteration: 5 || Loss: 69.29276055993337
Iteration: 6 || Loss: 69.29276055993337
saving ADAM checkpoint...
Sum of params:-106.41695
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.29276055993337
Iteration: 2 || Loss: 69.28799154077889
Iteration: 3 || Loss: 69.26793877372057
Iteration: 4 || Loss: 69.06985428631083
Iteration: 5 || Loss: 69.00301646856339
Iteration: 6 || Loss: 68.931953006697
Iteration: 7 || Loss: 68.88268847624875
Iteration: 8 || Loss: 68.86064828689616
Iteration: 9 || Loss: 68.85036132591448
Iteration: 10 || Loss: 68.83299825973454
Iteration: 11 || Loss: 68.81675174943743
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.45976
Epoch 212 loss:68.81675174943743
MSE loss S0.9885962793882191
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.113849012747394
MSE loss S0.35131208860585017
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:115.6532267166776
MSE loss S2.226669414993512
waveform batch: 2/2
Test loss - extrapolation:56.40639551139116
MSE loss S1.2980613425450427
Epoch 212 mean train loss:3.394208411742783
Epoch 212 mean test loss - interpolation:4.018974835457899
Epoch 212 mean test loss - extrapolation:14.338301852339063
Start training epoch 213
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.45976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4136916762749445
Iteration: 2 || Loss: 6.413051286652307
Iteration: 3 || Loss: 6.412346219890111
Iteration: 4 || Loss: 6.411605700292459
Iteration: 5 || Loss: 6.410995212144991
Iteration: 6 || Loss: 6.410995212144991
saving ADAM checkpoint...
Sum of params:-106.45967
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.410995212144991
Iteration: 2 || Loss: 6.403546045941784
Iteration: 3 || Loss: 6.340132743929812
Iteration: 4 || Loss: 6.280802396851479
Iteration: 5 || Loss: 6.165538932578818
Iteration: 6 || Loss: 6.11877124327037
Iteration: 7 || Loss: 6.101547291401175
Iteration: 8 || Loss: 6.095826641529653
Iteration: 9 || Loss: 6.090676117553251
Iteration: 10 || Loss: 6.086916324743372
Iteration: 11 || Loss: 6.079613185944557
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.472496
Epoch 213 loss:6.079613185944557
MSE loss S0.21275681804379182
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.472496
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.972640733431028
Iteration: 2 || Loss: 23.972328692087856
Iteration: 3 || Loss: 23.971955368133834
Iteration: 4 || Loss: 23.97167386661248
Iteration: 5 || Loss: 23.97124805070738
Iteration: 6 || Loss: 23.97124805070738
saving ADAM checkpoint...
Sum of params:-106.472595
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.97124805070738
Iteration: 2 || Loss: 23.96317884347117
Iteration: 3 || Loss: 23.881634760858073
Iteration: 4 || Loss: 23.838250156029716
Iteration: 5 || Loss: 23.620491302021833
Iteration: 6 || Loss: 23.591610428070343
Iteration: 7 || Loss: 23.578229544562635
Iteration: 8 || Loss: 23.554827031773065
Iteration: 9 || Loss: 23.55112595153391
Iteration: 10 || Loss: 23.543494490405322
Iteration: 11 || Loss: 23.531130528323622
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.43733
Epoch 213 loss:23.531130528323622
MSE loss S0.41224998128602064
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.43733
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.18651112390833
Iteration: 2 || Loss: 69.18627998187547
Iteration: 3 || Loss: 69.18601633876067
Iteration: 4 || Loss: 69.18578309168845
Iteration: 5 || Loss: 69.18547135453005
Iteration: 6 || Loss: 69.18547135453005
saving ADAM checkpoint...
Sum of params:-106.437355
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.18547135453005
Iteration: 2 || Loss: 69.17835843630944
Iteration: 3 || Loss: 69.16454369694013
Iteration: 4 || Loss: 68.96622010673094
Iteration: 5 || Loss: 68.91728801546189
Iteration: 6 || Loss: 68.86896517022177
Iteration: 7 || Loss: 68.83699258268058
Iteration: 8 || Loss: 68.81888533104491
Iteration: 9 || Loss: 68.80628995338928
Iteration: 10 || Loss: 68.78796619668482
Iteration: 11 || Loss: 68.77196482910782
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.48995
Epoch 213 loss:68.77196482910782
MSE loss S0.9895646657040925
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.117397261482473
MSE loss S0.3515677323139089
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:115.47406634888146
MSE loss S2.2205528349933097
waveform batch: 2/2
Test loss - extrapolation:56.316394485110614
MSE loss S1.2956090075811215
Epoch 213 mean train loss:3.3925071911508966
Epoch 213 mean test loss - interpolation:4.019566210247079
Epoch 213 mean test loss - extrapolation:14.315871736166008
Start training epoch 214
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.48995
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.37128812473167
Iteration: 2 || Loss: 6.3705697411224795
Iteration: 3 || Loss: 6.369812178733742
Iteration: 4 || Loss: 6.369072344872184
Iteration: 5 || Loss: 6.368306390840001
Iteration: 6 || Loss: 6.368306390840001
saving ADAM checkpoint...
Sum of params:-106.489876
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.368306390840001
Iteration: 2 || Loss: 6.350467216999006
Iteration: 3 || Loss: 6.309011027256347
Iteration: 4 || Loss: 6.262752337358517
Iteration: 5 || Loss: 6.172398986840007
Iteration: 6 || Loss: 6.11718467334844
Iteration: 7 || Loss: 6.100528643920318
Iteration: 8 || Loss: 6.094469944437012
Iteration: 9 || Loss: 6.089578553872614
Iteration: 10 || Loss: 6.084657859076807
Iteration: 11 || Loss: 6.0637895696726245
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.50972
Epoch 214 loss:6.0637895696726245
MSE loss S0.20719653010498518
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.50972
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.094309496835056
Iteration: 2 || Loss: 24.093878648119038
Iteration: 3 || Loss: 24.093382878197634
Iteration: 4 || Loss: 24.092914522257367
Iteration: 5 || Loss: 24.09253710790793
Iteration: 6 || Loss: 24.09253710790793
saving ADAM checkpoint...
Sum of params:-106.5099
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.09253710790793
Iteration: 2 || Loss: 24.081349860304574
Iteration: 3 || Loss: 23.941855132912966
Iteration: 4 || Loss: 23.890461249302668
Iteration: 5 || Loss: 23.68368681412828
Iteration: 6 || Loss: 23.642810277743116
Iteration: 7 || Loss: 23.62465410587082
Iteration: 8 || Loss: 23.593219164454624
Iteration: 9 || Loss: 23.57229886324023
Iteration: 10 || Loss: 23.557900874864796
Iteration: 11 || Loss: 23.544238748696497
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.47446
Epoch 214 loss:23.544238748696497
MSE loss S0.4133236539937267
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.47446
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.21168301090802
Iteration: 2 || Loss: 69.2112261222744
Iteration: 3 || Loss: 69.21078201852823
Iteration: 4 || Loss: 69.21025013338961
Iteration: 5 || Loss: 69.20983770516916
Iteration: 6 || Loss: 69.20983770516916
saving ADAM checkpoint...
Sum of params:-106.474556
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.20983770516916
Iteration: 2 || Loss: 69.20509804083657
Iteration: 3 || Loss: 69.18555517882716
Iteration: 4 || Loss: 68.98067051614387
Iteration: 5 || Loss: 68.92148772578902
Iteration: 6 || Loss: 68.85060960120323
Iteration: 7 || Loss: 68.80209249275873
Iteration: 8 || Loss: 68.78001229417336
Iteration: 9 || Loss: 68.76997683418148
Iteration: 10 || Loss: 68.75354854332103
Iteration: 11 || Loss: 68.73707254608114
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.51613
Epoch 214 loss:68.73707254608114
MSE loss S0.9883337681017597
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.100068681452647
MSE loss S0.3520139374516188
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:115.27971341991011
MSE loss S2.219602677023973
waveform batch: 2/2
Test loss - extrapolation:56.1624766400554
MSE loss S1.292686485021585
Epoch 214 mean train loss:3.391210374636216
Epoch 214 mean test loss - interpolation:4.016678113575441
Epoch 214 mean test loss - extrapolation:14.286849171663794
Start training epoch 215
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.51613
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.413778856783361
Iteration: 2 || Loss: 6.413000967573838
Iteration: 3 || Loss: 6.412383033439522
Iteration: 4 || Loss: 6.411759991028441
Iteration: 5 || Loss: 6.411111569382843
Iteration: 6 || Loss: 6.411111569382843
saving ADAM checkpoint...
Sum of params:-106.51602
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.411111569382843
Iteration: 2 || Loss: 6.404038013107237
Iteration: 3 || Loss: 6.34299570259112
Iteration: 4 || Loss: 6.284256927221264
Iteration: 5 || Loss: 6.164934997932412
Iteration: 6 || Loss: 6.116224674315938
Iteration: 7 || Loss: 6.101034306381488
Iteration: 8 || Loss: 6.095418828221193
Iteration: 9 || Loss: 6.090344299719733
Iteration: 10 || Loss: 6.08588442936639
Iteration: 11 || Loss: 6.078527522081109
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.5288
Epoch 215 loss:6.078527522081109
MSE loss S0.21264894145949165
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.5288
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.964083388645232
Iteration: 2 || Loss: 23.96369798193095
Iteration: 3 || Loss: 23.963388974291536
Iteration: 4 || Loss: 23.963148449724383
Iteration: 5 || Loss: 23.962810950726617
Iteration: 6 || Loss: 23.962810950726617
saving ADAM checkpoint...
Sum of params:-106.52888
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.962810950726617
Iteration: 2 || Loss: 23.954064558875537
Iteration: 3 || Loss: 23.868778690358933
Iteration: 4 || Loss: 23.829484799685442
Iteration: 5 || Loss: 23.613207627281454
Iteration: 6 || Loss: 23.584680665011657
Iteration: 7 || Loss: 23.57149402547646
Iteration: 8 || Loss: 23.548051488054693
Iteration: 9 || Loss: 23.544316403112397
Iteration: 10 || Loss: 23.53662716891543
Iteration: 11 || Loss: 23.523924994583922
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.494446
Epoch 215 loss:23.523924994583922
MSE loss S0.41206449831455927
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.494446
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.10003412150735
Iteration: 2 || Loss: 69.09976830662873
Iteration: 3 || Loss: 69.09948340424654
Iteration: 4 || Loss: 69.09924389106568
Iteration: 5 || Loss: 69.09906223348919
Iteration: 6 || Loss: 69.09906223348919
saving ADAM checkpoint...
Sum of params:-106.49451
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.09906223348919
Iteration: 2 || Loss: 69.09086252504918
Iteration: 3 || Loss: 69.07926294110473
Iteration: 4 || Loss: 68.8862608853775
Iteration: 5 || Loss: 68.83692273177107
Iteration: 6 || Loss: 68.78906269355369
Iteration: 7 || Loss: 68.75792619209665
Iteration: 8 || Loss: 68.74016625573404
Iteration: 9 || Loss: 68.72741096633784
Iteration: 10 || Loss: 68.7099695963232
Iteration: 11 || Loss: 68.69381239867457
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.54654
Epoch 215 loss:68.69381239867457
MSE loss S0.9883712909874629
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.10059983059902
MSE loss S0.3516661474894277
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:115.06126709106064
MSE loss S2.2130195978919294
waveform batch: 2/2
Test loss - extrapolation:56.04714776007527
MSE loss S1.290244100997922
Epoch 215 mean train loss:3.389526376391021
Epoch 215 mean test loss - interpolation:4.01676663843317
Epoch 215 mean test loss - extrapolation:14.259034570927993
Start training epoch 216
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.54654
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.36960618936192
Iteration: 2 || Loss: 6.368883309189682
Iteration: 3 || Loss: 6.368171720474358
Iteration: 4 || Loss: 6.367372925292467
Iteration: 5 || Loss: 6.366684240821784
Iteration: 6 || Loss: 6.366684240821784
saving ADAM checkpoint...
Sum of params:-106.54648
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.366684240821784
Iteration: 2 || Loss: 6.349676426097651
Iteration: 3 || Loss: 6.308097667229654
Iteration: 4 || Loss: 6.262421883119563
Iteration: 5 || Loss: 6.171499223199037
Iteration: 6 || Loss: 6.116822765537705
Iteration: 7 || Loss: 6.099905663106254
Iteration: 8 || Loss: 6.094056118891733
Iteration: 9 || Loss: 6.088989118871223
Iteration: 10 || Loss: 6.0836716187254645
Iteration: 11 || Loss: 6.062495850255644
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.56634
Epoch 216 loss:6.062495850255644
MSE loss S0.20717165110171787
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.56634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.093690926996885
Iteration: 2 || Loss: 24.093322688587506
Iteration: 3 || Loss: 24.092861674703315
Iteration: 4 || Loss: 24.09245627931093
Iteration: 5 || Loss: 24.092115356030956
Iteration: 6 || Loss: 24.092115356030956
saving ADAM checkpoint...
Sum of params:-106.566536
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.092115356030956
Iteration: 2 || Loss: 24.08019737266581
Iteration: 3 || Loss: 23.93774225131851
Iteration: 4 || Loss: 23.886703244583973
Iteration: 5 || Loss: 23.67918023563094
Iteration: 6 || Loss: 23.637583822128555
Iteration: 7 || Loss: 23.61957659749711
Iteration: 8 || Loss: 23.588066809210705
Iteration: 9 || Loss: 23.566012571877256
Iteration: 10 || Loss: 23.551760600088826
Iteration: 11 || Loss: 23.538527475703848
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.53227
Epoch 216 loss:23.538527475703848
MSE loss S0.412934325611029
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.53227
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.12473306604069
Iteration: 2 || Loss: 69.12425003351602
Iteration: 3 || Loss: 69.12381489557004
Iteration: 4 || Loss: 69.12335721780296
Iteration: 5 || Loss: 69.12293410765317
Iteration: 6 || Loss: 69.12293410765317
saving ADAM checkpoint...
Sum of params:-106.53238
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.12293410765317
Iteration: 2 || Loss: 69.1184337646528
Iteration: 3 || Loss: 69.09981142564865
Iteration: 4 || Loss: 68.88666085418672
Iteration: 5 || Loss: 68.84179337593935
Iteration: 6 || Loss: 68.77187048523584
Iteration: 7 || Loss: 68.7241259708503
Iteration: 8 || Loss: 68.70263944488522
Iteration: 9 || Loss: 68.69281036076283
Iteration: 10 || Loss: 68.67675249675499
Iteration: 11 || Loss: 68.6599120765578
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.57288
Epoch 216 loss:68.6599120765578
MSE loss S0.9874977631167506
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.085219759093068
MSE loss S0.35224719838342566
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:114.88172828586349
MSE loss S2.2122844660742436
waveform batch: 2/2
Test loss - extrapolation:55.9096862041123
MSE loss S1.2871196406383945
Epoch 216 mean train loss:3.3883081173281826
Epoch 216 mean test loss - interpolation:4.014203293182178
Epoch 216 mean test loss - extrapolation:14.232617874164648
Start training epoch 217
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.57288
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.414372249859286
Iteration: 2 || Loss: 6.413712286789486
Iteration: 3 || Loss: 6.413017874037775
Iteration: 4 || Loss: 6.41234905576526
Iteration: 5 || Loss: 6.411711211384973
Iteration: 6 || Loss: 6.411711211384973
saving ADAM checkpoint...
Sum of params:-106.57279
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.411711211384973
Iteration: 2 || Loss: 6.404738288068518
Iteration: 3 || Loss: 6.3434073998814595
Iteration: 4 || Loss: 6.284795599825078
Iteration: 5 || Loss: 6.164204616228898
Iteration: 6 || Loss: 6.115652841012295
Iteration: 7 || Loss: 6.100607617867213
Iteration: 8 || Loss: 6.0949776050821205
Iteration: 9 || Loss: 6.08986282944706
Iteration: 10 || Loss: 6.085412442836193
Iteration: 11 || Loss: 6.077633039451498
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.58549
Epoch 217 loss:6.077633039451498
MSE loss S0.212675101182921
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.58549
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.95795517592482
Iteration: 2 || Loss: 23.957653225065425
Iteration: 3 || Loss: 23.95729826450698
Iteration: 4 || Loss: 23.957027117173524
Iteration: 5 || Loss: 23.956659847591222
Iteration: 6 || Loss: 23.956659847591222
saving ADAM checkpoint...
Sum of params:-106.585594
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.956659847591222
Iteration: 2 || Loss: 23.94729255424145
Iteration: 3 || Loss: 23.864340197375185
Iteration: 4 || Loss: 23.82347504056668
Iteration: 5 || Loss: 23.605607237494407
Iteration: 6 || Loss: 23.57710029025809
Iteration: 7 || Loss: 23.56409459890936
Iteration: 8 || Loss: 23.540906372433362
Iteration: 9 || Loss: 23.537196789283648
Iteration: 10 || Loss: 23.529591145808755
Iteration: 11 || Loss: 23.516747403625477
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.55139
Epoch 217 loss:23.516747403625477
MSE loss S0.4119137662533695
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.55139
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.01961130763783
Iteration: 2 || Loss: 69.01931775232767
Iteration: 3 || Loss: 69.01910736873438
Iteration: 4 || Loss: 69.0188038377275
Iteration: 5 || Loss: 69.01854128758801
Iteration: 6 || Loss: 69.01854128758801
saving ADAM checkpoint...
Sum of params:-106.551476
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.01854128758801
Iteration: 2 || Loss: 69.0103569053178
Iteration: 3 || Loss: 68.99919997066894
Iteration: 4 || Loss: 68.81040787893413
Iteration: 5 || Loss: 68.75992554652491
Iteration: 6 || Loss: 68.71222316285986
Iteration: 7 || Loss: 68.68141137819468
Iteration: 8 || Loss: 68.66373268065196
Iteration: 9 || Loss: 68.65102482242501
Iteration: 10 || Loss: 68.63390268792094
Iteration: 11 || Loss: 68.61815631826826
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.60306
Epoch 217 loss:68.61815631826826
MSE loss S0.987787346520193
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.08471918093988
MSE loss S0.35182968755490024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:114.65900869143277
MSE loss S2.205959019092978
waveform batch: 2/2
Test loss - extrapolation:55.79648261469581
MSE loss S1.2853921638827426
Epoch 217 mean train loss:3.386639198667077
Epoch 217 mean test loss - interpolation:4.01411986348998
Epoch 217 mean test loss - extrapolation:14.204624275510715
Start training epoch 218
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.60306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.367535918320313
Iteration: 2 || Loss: 6.366731190758079
Iteration: 3 || Loss: 6.366073261938809
Iteration: 4 || Loss: 6.365328808220598
Iteration: 5 || Loss: 6.364610006066089
Iteration: 6 || Loss: 6.364610006066089
saving ADAM checkpoint...
Sum of params:-106.60297
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.364610006066089
Iteration: 2 || Loss: 6.347903705716153
Iteration: 3 || Loss: 6.30693814967195
Iteration: 4 || Loss: 6.261979678687917
Iteration: 5 || Loss: 6.170333991673887
Iteration: 6 || Loss: 6.115977164533856
Iteration: 7 || Loss: 6.0989552394875375
Iteration: 8 || Loss: 6.093117873986944
Iteration: 9 || Loss: 6.087959055199852
Iteration: 10 || Loss: 6.082050268460112
Iteration: 11 || Loss: 6.060907907644354
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.62285
Epoch 218 loss:6.060907907644354
MSE loss S0.2069723199997766
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.62285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.090488213407465
Iteration: 2 || Loss: 24.09010129836089
Iteration: 3 || Loss: 24.089678294508218
Iteration: 4 || Loss: 24.089273577871886
Iteration: 5 || Loss: 24.088923055798787
Iteration: 6 || Loss: 24.088923055798787
saving ADAM checkpoint...
Sum of params:-106.623055
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.088923055798787
Iteration: 2 || Loss: 24.07634089113312
Iteration: 3 || Loss: 23.9317811211203
Iteration: 4 || Loss: 23.881704222030635
Iteration: 5 || Loss: 23.674684298551718
Iteration: 6 || Loss: 23.632218784762507
Iteration: 7 || Loss: 23.614228150457127
Iteration: 8 || Loss: 23.58278406222025
Iteration: 9 || Loss: 23.55982461800627
Iteration: 10 || Loss: 23.545182331433644
Iteration: 11 || Loss: 23.532084316483036
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.58924
Epoch 218 loss:23.532084316483036
MSE loss S0.4128867206681419
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.58924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 69.05162444804027
Iteration: 2 || Loss: 69.05114718016794
Iteration: 3 || Loss: 69.05067761600496
Iteration: 4 || Loss: 69.05020996565541
Iteration: 5 || Loss: 69.04975283601333
Iteration: 6 || Loss: 69.04975283601333
saving ADAM checkpoint...
Sum of params:-106.58933
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 69.04975283601333
Iteration: 2 || Loss: 69.04551831849528
Iteration: 3 || Loss: 69.02749733082334
Iteration: 4 || Loss: 68.80890240366081
Iteration: 5 || Loss: 68.76751477289324
Iteration: 6 || Loss: 68.69734237949277
Iteration: 7 || Loss: 68.64957177302918
Iteration: 8 || Loss: 68.62791203318798
Iteration: 9 || Loss: 68.6181195002603
Iteration: 10 || Loss: 68.60229034713726
Iteration: 11 || Loss: 68.5855803729171
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.62951
Epoch 218 loss:68.5855803729171
MSE loss S0.9867781334660098
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.070273868277653
MSE loss S0.35232511194762584
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:114.48213499556445
MSE loss S2.2050495013350577
waveform batch: 2/2
Test loss - extrapolation:55.663741811923735
MSE loss S1.2820321803348933
Epoch 218 mean train loss:3.3854680205877408
Epoch 218 mean test loss - interpolation:4.0117123113796085
Epoch 218 mean test loss - extrapolation:14.178823067290681
Start training epoch 219
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.62951
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.412111989735887
Iteration: 2 || Loss: 6.411452048109423
Iteration: 3 || Loss: 6.410735979421428
Iteration: 4 || Loss: 6.4100923526164415
Iteration: 5 || Loss: 6.409415935689779
Iteration: 6 || Loss: 6.409415935689779
saving ADAM checkpoint...
Sum of params:-106.62942
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.409415935689779
Iteration: 2 || Loss: 6.402919861216576
Iteration: 3 || Loss: 6.3423707527533235
Iteration: 4 || Loss: 6.284903967241563
Iteration: 5 || Loss: 6.163001315457043
Iteration: 6 || Loss: 6.113904198139232
Iteration: 7 || Loss: 6.099438134881744
Iteration: 8 || Loss: 6.09393404198605
Iteration: 9 || Loss: 6.0888376342371515
Iteration: 10 || Loss: 6.083820769810918
Iteration: 11 || Loss: 6.0760516582923625
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.64189
Epoch 219 loss:6.0760516582923625
MSE loss S0.21242608493171883
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.64189
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.950525482839094
Iteration: 2 || Loss: 23.950185993045064
Iteration: 3 || Loss: 23.949901861920072
Iteration: 4 || Loss: 23.94959869917698
Iteration: 5 || Loss: 23.949321430098983
Iteration: 6 || Loss: 23.949321430098983
saving ADAM checkpoint...
Sum of params:-106.64199
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.949321430098983
Iteration: 2 || Loss: 23.938598473511295
Iteration: 3 || Loss: 23.850928030353153
Iteration: 4 || Loss: 23.815392182420986
Iteration: 5 || Loss: 23.59861415555982
Iteration: 6 || Loss: 23.570480178233264
Iteration: 7 || Loss: 23.557868244562744
Iteration: 8 || Loss: 23.534388156029344
Iteration: 9 || Loss: 23.530612153299483
Iteration: 10 || Loss: 23.522980901369895
Iteration: 11 || Loss: 23.509964253627185
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.60869
Epoch 219 loss:23.509964253627185
MSE loss S0.41143623147829245
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.60869
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.93786117169672
Iteration: 2 || Loss: 68.93756748037843
Iteration: 3 || Loss: 68.93735816746727
Iteration: 4 || Loss: 68.93714254977178
Iteration: 5 || Loss: 68.93687839331903
Iteration: 6 || Loss: 68.93687839331903
saving ADAM checkpoint...
Sum of params:-106.608734
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.93687839331903
Iteration: 2 || Loss: 68.9283894135663
Iteration: 3 || Loss: 68.91840808221463
Iteration: 4 || Loss: 68.73518020181247
Iteration: 5 || Loss: 68.6846122950408
Iteration: 6 || Loss: 68.63734080110193
Iteration: 7 || Loss: 68.60714738802191
Iteration: 8 || Loss: 68.58982547969123
Iteration: 9 || Loss: 68.57716861423509
Iteration: 10 || Loss: 68.56041125131868
Iteration: 11 || Loss: 68.54480105812416
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.660095
Epoch 219 loss:68.54480105812416
MSE loss S0.9869787697685749
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.069022902006143
MSE loss S0.351861537550639
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:114.24775307110855
MSE loss S2.198722260918103
waveform batch: 2/2
Test loss - extrapolation:55.54645565846073
MSE loss S1.28048409096708
Epoch 219 mean train loss:3.3838212748290935
Epoch 219 mean test loss - interpolation:4.0115038170010235
Epoch 219 mean test loss - extrapolation:14.149517394130774
Start training epoch 220
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.660095
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.366375262644505
Iteration: 2 || Loss: 6.365636625881555
Iteration: 3 || Loss: 6.364904704036183
Iteration: 4 || Loss: 6.3642398559226905
Iteration: 5 || Loss: 6.363567239949385
Iteration: 6 || Loss: 6.363567239949385
saving ADAM checkpoint...
Sum of params:-106.66
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.363567239949385
Iteration: 2 || Loss: 6.347175831103365
Iteration: 3 || Loss: 6.305702388646862
Iteration: 4 || Loss: 6.261441038509522
Iteration: 5 || Loss: 6.168958466338789
Iteration: 6 || Loss: 6.115081393826855
Iteration: 7 || Loss: 6.097816198888449
Iteration: 8 || Loss: 6.0920287824870565
Iteration: 9 || Loss: 6.086981995997221
Iteration: 10 || Loss: 6.081631409934955
Iteration: 11 || Loss: 6.059258448542861
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.67976
Epoch 220 loss:6.059258448542861
MSE loss S0.2066855729062927
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.67976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.088641185219277
Iteration: 2 || Loss: 24.0883528906155
Iteration: 3 || Loss: 24.087832909689006
Iteration: 4 || Loss: 24.087427176659602
Iteration: 5 || Loss: 24.087030928308227
Iteration: 6 || Loss: 24.087030928308227
saving ADAM checkpoint...
Sum of params:-106.679955
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.087030928308227
Iteration: 2 || Loss: 24.07411816129253
Iteration: 3 || Loss: 23.92491246561185
Iteration: 4 || Loss: 23.8776180954438
Iteration: 5 || Loss: 23.66982573456725
Iteration: 6 || Loss: 23.627043963763263
Iteration: 7 || Loss: 23.608963207481235
Iteration: 8 || Loss: 23.577433469269955
Iteration: 9 || Loss: 23.553673864059615
Iteration: 10 || Loss: 23.538743711513305
Iteration: 11 || Loss: 23.52560755830074
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.6465
Epoch 220 loss:23.52560755830074
MSE loss S0.41249829278966643
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.6465
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.97758854855974
Iteration: 2 || Loss: 68.9771024317695
Iteration: 3 || Loss: 68.97659646804945
Iteration: 4 || Loss: 68.97614296602296
Iteration: 5 || Loss: 68.97567049018446
Iteration: 6 || Loss: 68.97567049018446
saving ADAM checkpoint...
Sum of params:-106.6466
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.97567049018446
Iteration: 2 || Loss: 68.97156930082609
Iteration: 3 || Loss: 68.95375891933075
Iteration: 4 || Loss: 68.7362106986421
Iteration: 5 || Loss: 68.6954991210265
Iteration: 6 || Loss: 68.62490456048312
Iteration: 7 || Loss: 68.57673433148724
Iteration: 8 || Loss: 68.555127957774
Iteration: 9 || Loss: 68.54543667480206
Iteration: 10 || Loss: 68.52954936878295
Iteration: 11 || Loss: 68.51335929383944
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.68648
Epoch 220 loss:68.51335929383944
MSE loss S0.9864547646769453
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.05507050618232
MSE loss S0.35242278875189736
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:114.07911319891198
MSE loss S2.1980788984550403
waveform batch: 2/2
Test loss - extrapolation:55.42495137825184
MSE loss S1.2773301847392946
Epoch 220 mean train loss:3.3826974241614844
Epoch 220 mean test loss - interpolation:4.009178417697053
Epoch 220 mean test loss - extrapolation:14.125338714763652
Start training epoch 221
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.68648
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.409614173647785
Iteration: 2 || Loss: 6.408925447088635
Iteration: 3 || Loss: 6.40829973603253
Iteration: 4 || Loss: 6.407628726458446
Iteration: 5 || Loss: 6.406988382925286
Iteration: 6 || Loss: 6.406988382925286
saving ADAM checkpoint...
Sum of params:-106.68638
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.406988382925286
Iteration: 2 || Loss: 6.4007522395444205
Iteration: 3 || Loss: 6.340507769333335
Iteration: 4 || Loss: 6.28444224491615
Iteration: 5 || Loss: 6.1614783106700095
Iteration: 6 || Loss: 6.111697502385496
Iteration: 7 || Loss: 6.0980177450478985
Iteration: 8 || Loss: 6.092525955626848
Iteration: 9 || Loss: 6.087578394915366
Iteration: 10 || Loss: 6.082206883618244
Iteration: 11 || Loss: 6.074272135818635
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.69869
Epoch 221 loss:6.074272135818635
MSE loss S0.21208388377707343
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.69869
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.94222491648645
Iteration: 2 || Loss: 23.94193720804622
Iteration: 3 || Loss: 23.94163874724345
Iteration: 4 || Loss: 23.941404338871436
Iteration: 5 || Loss: 23.94110730859982
Iteration: 6 || Loss: 23.94110730859982
saving ADAM checkpoint...
Sum of params:-106.69879
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.94110730859982
Iteration: 2 || Loss: 23.92961146921825
Iteration: 3 || Loss: 23.841522458992525
Iteration: 4 || Loss: 23.808033530561254
Iteration: 5 || Loss: 23.591855700651767
Iteration: 6 || Loss: 23.563755652552402
Iteration: 7 || Loss: 23.551288057051952
Iteration: 8 || Loss: 23.527788225888166
Iteration: 9 || Loss: 23.523918339171125
Iteration: 10 || Loss: 23.5163120426054
Iteration: 11 || Loss: 23.502814915696582
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.66577
Epoch 221 loss:23.502814915696582
MSE loss S0.41124205103689226
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.66577
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.86325805668612
Iteration: 2 || Loss: 68.86305185089023
Iteration: 3 || Loss: 68.86279005210429
Iteration: 4 || Loss: 68.86253408594855
Iteration: 5 || Loss: 68.86228304746899
Iteration: 6 || Loss: 68.86228304746899
saving ADAM checkpoint...
Sum of params:-106.66582
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.86228304746899
Iteration: 2 || Loss: 68.85455012683602
Iteration: 3 || Loss: 68.84391900776266
Iteration: 4 || Loss: 68.662962063659
Iteration: 5 || Loss: 68.61282561941925
Iteration: 6 || Loss: 68.56567307770717
Iteration: 7 || Loss: 68.53578565638331
Iteration: 8 || Loss: 68.51831676740363
Iteration: 9 || Loss: 68.50562676754265
Iteration: 10 || Loss: 68.48910877957522
Iteration: 11 || Loss: 68.4739143169002
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.716995
Epoch 221 loss:68.4739143169002
MSE loss S0.9866865024818412
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.054109078694594
MSE loss S0.3519972164996377
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:113.8447913939761
MSE loss S2.1915844060250578
waveform batch: 2/2
Test loss - extrapolation:55.31295972868908
MSE loss S1.2758555513072705
Epoch 221 mean train loss:3.38106901270398
Epoch 221 mean test loss - interpolation:4.009018179782433
Epoch 221 mean test loss - extrapolation:14.096479260222097
Start training epoch 222
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.716995
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.364404971125697
Iteration: 2 || Loss: 6.363587917815558
Iteration: 3 || Loss: 6.362950578179213
Iteration: 4 || Loss: 6.3622274004158115
Iteration: 5 || Loss: 6.361540142188445
Iteration: 6 || Loss: 6.361540142188445
saving ADAM checkpoint...
Sum of params:-106.7169
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.361540142188445
Iteration: 2 || Loss: 6.345312104915371
Iteration: 3 || Loss: 6.30452373670664
Iteration: 4 || Loss: 6.261341774844668
Iteration: 5 || Loss: 6.167395728567648
Iteration: 6 || Loss: 6.113748786601535
Iteration: 7 || Loss: 6.096363938509684
Iteration: 8 || Loss: 6.090721309737655
Iteration: 9 || Loss: 6.085489760749459
Iteration: 10 || Loss: 6.078725617062827
Iteration: 11 || Loss: 6.0571236326104945
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.7367
Epoch 222 loss:6.0571236326104945
MSE loss S0.206440341797258
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.7367
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.08939268369188
Iteration: 2 || Loss: 24.088992163722097
Iteration: 3 || Loss: 24.088525258464188
Iteration: 4 || Loss: 24.088178745801834
Iteration: 5 || Loss: 24.087750792923988
Iteration: 6 || Loss: 24.087750792923988
saving ADAM checkpoint...
Sum of params:-106.736916
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.087750792923988
Iteration: 2 || Loss: 24.07404643053426
Iteration: 3 || Loss: 23.924707199020016
Iteration: 4 || Loss: 23.875237962551704
Iteration: 5 || Loss: 23.666933273578373
Iteration: 6 || Loss: 23.622743424003087
Iteration: 7 || Loss: 23.604618998352848
Iteration: 8 || Loss: 23.573226795877165
Iteration: 9 || Loss: 23.54779922665386
Iteration: 10 || Loss: 23.533136364399926
Iteration: 11 || Loss: 23.520343983216012
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.70463
Epoch 222 loss:23.520343983216012
MSE loss S0.4121920746423922
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.70463
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.9008395650791
Iteration: 2 || Loss: 68.90032802410535
Iteration: 3 || Loss: 68.89989610976612
Iteration: 4 || Loss: 68.89944733272307
Iteration: 5 || Loss: 68.8990083304727
Iteration: 6 || Loss: 68.8990083304727
saving ADAM checkpoint...
Sum of params:-106.70472
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.8990083304727
Iteration: 2 || Loss: 68.89514812120767
Iteration: 3 || Loss: 68.87833661256838
Iteration: 4 || Loss: 68.65037383131614
Iteration: 5 || Loss: 68.62193286009648
Iteration: 6 || Loss: 68.55350657413149
Iteration: 7 || Loss: 68.50660728720004
Iteration: 8 || Loss: 68.48542906391805
Iteration: 9 || Loss: 68.47572626987129
Iteration: 10 || Loss: 68.4598267333894
Iteration: 11 || Loss: 68.44299968481143
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.74406
Epoch 222 loss:68.44299968481143
MSE loss S0.985722203278779
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.041655717122516
MSE loss S0.3525228797162474
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:113.66249235282883
MSE loss S2.1902699331304127
waveform batch: 2/2
Test loss - extrapolation:55.18776827574228
MSE loss S1.2719916245012761
Epoch 222 mean train loss:3.3800161138151013
Epoch 222 mean test loss - interpolation:4.006942619520419
Epoch 222 mean test loss - extrapolation:14.070855052380926
Start training epoch 223
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.74406
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.410053350055619
Iteration: 2 || Loss: 6.409326563366282
Iteration: 3 || Loss: 6.40865787187934
Iteration: 4 || Loss: 6.408047555460723
Iteration: 5 || Loss: 6.407276436445861
Iteration: 6 || Loss: 6.407276436445861
saving ADAM checkpoint...
Sum of params:-106.743965
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.407276436445861
Iteration: 2 || Loss: 6.401495245043314
Iteration: 3 || Loss: 6.340664599474352
Iteration: 4 || Loss: 6.285284054926008
Iteration: 5 || Loss: 6.160530146646868
Iteration: 6 || Loss: 6.111270170182926
Iteration: 7 || Loss: 6.097046816521543
Iteration: 8 || Loss: 6.0916610859255975
Iteration: 9 || Loss: 6.08660600558286
Iteration: 10 || Loss: 6.080985211632529
Iteration: 11 || Loss: 6.072912973134528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.75612
Epoch 223 loss:6.072912973134528
MSE loss S0.21192662605019874
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.75612
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.93743249139144
Iteration: 2 || Loss: 23.93713947083098
Iteration: 3 || Loss: 23.936833666131136
Iteration: 4 || Loss: 23.93650753632411
Iteration: 5 || Loss: 23.93628064254952
Iteration: 6 || Loss: 23.93628064254952
saving ADAM checkpoint...
Sum of params:-106.75624
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.93628064254952
Iteration: 2 || Loss: 23.922832983016654
Iteration: 3 || Loss: 23.83404776357994
Iteration: 4 || Loss: 23.802035010142585
Iteration: 5 || Loss: 23.584831160696403
Iteration: 6 || Loss: 23.556924954782655
Iteration: 7 || Loss: 23.545128467319955
Iteration: 8 || Loss: 23.521210147545684
Iteration: 9 || Loss: 23.51731515875301
Iteration: 10 || Loss: 23.50976494938179
Iteration: 11 || Loss: 23.49624866473496
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.72368
Epoch 223 loss:23.49624866473496
MSE loss S0.4103936690268416
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.72368
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.78964072997869
Iteration: 2 || Loss: 68.7893821029161
Iteration: 3 || Loss: 68.78913885804332
Iteration: 4 || Loss: 68.7888355613797
Iteration: 5 || Loss: 68.78863384720412
Iteration: 6 || Loss: 68.78863384720412
saving ADAM checkpoint...
Sum of params:-106.723755
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.78863384720412
Iteration: 2 || Loss: 68.78247966101337
Iteration: 3 || Loss: 68.76978571153913
Iteration: 4 || Loss: 68.59482131978376
Iteration: 5 || Loss: 68.54145794791367
Iteration: 6 || Loss: 68.49471352551286
Iteration: 7 || Loss: 68.46546605079779
Iteration: 8 || Loss: 68.44820305205357
Iteration: 9 || Loss: 68.43572401449293
Iteration: 10 || Loss: 68.41908643652687
Iteration: 11 || Loss: 68.40451668231013
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.774574
Epoch 223 loss:68.40451668231013
MSE loss S0.986564190919413
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.040435737275285
MSE loss S0.3521573989021549
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:113.44488931953316
MSE loss S2.184446500321148
waveform batch: 2/2
Test loss - extrapolation:55.08996144076122
MSE loss S1.2712384046991398
Epoch 223 mean train loss:3.378402700695849
Epoch 223 mean test loss - interpolation:4.006739289545881
Epoch 223 mean test loss - extrapolation:14.044570896691198
Start training epoch 224
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.774574
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.363406531602645
Iteration: 2 || Loss: 6.362614173331523
Iteration: 3 || Loss: 6.361915197789722
Iteration: 4 || Loss: 6.361233400611968
Iteration: 5 || Loss: 6.360441942775757
Iteration: 6 || Loss: 6.360441942775757
saving ADAM checkpoint...
Sum of params:-106.77449
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.360441942775757
Iteration: 2 || Loss: 6.344063631328655
Iteration: 3 || Loss: 6.303424673163808
Iteration: 4 || Loss: 6.261186586050669
Iteration: 5 || Loss: 6.165816936337424
Iteration: 6 || Loss: 6.1118246756900465
Iteration: 7 || Loss: 6.094824875954417
Iteration: 8 || Loss: 6.0891326940032044
Iteration: 9 || Loss: 6.083927948027918
Iteration: 10 || Loss: 6.076927282032363
Iteration: 11 || Loss: 6.055231049594967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.79416
Epoch 224 loss:6.055231049594967
MSE loss S0.20617731629816877
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.79416
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.087811635499627
Iteration: 2 || Loss: 24.087394419584342
Iteration: 3 || Loss: 24.087035998195788
Iteration: 4 || Loss: 24.086649154033893
Iteration: 5 || Loss: 24.086194079305567
Iteration: 6 || Loss: 24.086194079305567
saving ADAM checkpoint...
Sum of params:-106.79434
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.086194079305567
Iteration: 2 || Loss: 24.07153199304936
Iteration: 3 || Loss: 23.918528632979648
Iteration: 4 || Loss: 23.8708832459214
Iteration: 5 || Loss: 23.662655593667512
Iteration: 6 || Loss: 23.617675808437035
Iteration: 7 || Loss: 23.599726337082608
Iteration: 8 || Loss: 23.568322812504775
Iteration: 9 || Loss: 23.54180479770561
Iteration: 10 || Loss: 23.526815964503733
Iteration: 11 || Loss: 23.513991638107253
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.76238
Epoch 224 loss:23.513991638107253
MSE loss S0.4115707521740536
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.76238
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.83138659366422
Iteration: 2 || Loss: 68.8308998103999
Iteration: 3 || Loss: 68.83050086483682
Iteration: 4 || Loss: 68.83003675097764
Iteration: 5 || Loss: 68.82962022582758
Iteration: 6 || Loss: 68.82962022582758
saving ADAM checkpoint...
Sum of params:-106.76247
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.82962022582758
Iteration: 2 || Loss: 68.82584661554398
Iteration: 3 || Loss: 68.80934083208517
Iteration: 4 || Loss: 68.58191345397677
Iteration: 5 || Loss: 68.5537178714957
Iteration: 6 || Loss: 68.4852523081544
Iteration: 7 || Loss: 68.43772917666061
Iteration: 8 || Loss: 68.41657266313402
Iteration: 9 || Loss: 68.40697031119365
Iteration: 10 || Loss: 68.39106088416115
Iteration: 11 || Loss: 68.37463248494555
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.80182
Epoch 224 loss:68.37463248494555
MSE loss S0.9852868093589462
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.0273045677449
MSE loss S0.35250575787023236
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:113.25007206641227
MSE loss S2.1829055777609083
waveform batch: 2/2
Test loss - extrapolation:54.96052213782068
MSE loss S1.2672528986082303
Epoch 224 mean train loss:3.377374316298199
Epoch 224 mean test loss - interpolation:4.004550761290816
Epoch 224 mean test loss - extrapolation:14.017549517019413
Start training epoch 225
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.80182
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.408556338707327
Iteration: 2 || Loss: 6.40790481439288
Iteration: 3 || Loss: 6.40717768208801
Iteration: 4 || Loss: 6.406520618173414
Iteration: 5 || Loss: 6.405817526487737
Iteration: 6 || Loss: 6.405817526487737
saving ADAM checkpoint...
Sum of params:-106.80171
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.405817526487737
Iteration: 2 || Loss: 6.397128530698991
Iteration: 3 || Loss: 6.336285236382994
Iteration: 4 || Loss: 6.2824909607392545
Iteration: 5 || Loss: 6.158510347998199
Iteration: 6 || Loss: 6.106840431632362
Iteration: 7 || Loss: 6.095040068856966
Iteration: 8 || Loss: 6.089985740142487
Iteration: 9 || Loss: 6.084968557417502
Iteration: 10 || Loss: 6.077629580889639
Iteration: 11 || Loss: 6.069265882016212
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.81604
Epoch 225 loss:6.069265882016212
MSE loss S0.21162778919633674
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.81604
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.975746130625758
Iteration: 2 || Loss: 23.97546099088449
Iteration: 3 || Loss: 23.975182789674054
Iteration: 4 || Loss: 23.974923644501526
Iteration: 5 || Loss: 23.974619568478737
Iteration: 6 || Loss: 23.974619568478737
saving ADAM checkpoint...
Sum of params:-106.81613
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.974619568478737
Iteration: 2 || Loss: 23.94972913767236
Iteration: 3 || Loss: 23.9003981078583
Iteration: 4 || Loss: 23.823096680820758
Iteration: 5 || Loss: 23.58609818593426
Iteration: 6 || Loss: 23.556663363867347
Iteration: 7 || Loss: 23.545613244814955
Iteration: 8 || Loss: 23.516676841958365
Iteration: 9 || Loss: 23.51168936147888
Iteration: 10 || Loss: 23.50415097980065
Iteration: 11 || Loss: 23.48864180418576
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.77889
Epoch 225 loss:23.48864180418576
MSE loss S0.40772051944079146
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.77889
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.80995277427168
Iteration: 2 || Loss: 68.80955685947501
Iteration: 3 || Loss: 68.80914996868954
Iteration: 4 || Loss: 68.8087219453364
Iteration: 5 || Loss: 68.80836570546255
Iteration: 6 || Loss: 68.80836570546255
saving ADAM checkpoint...
Sum of params:-106.77905
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.80836570546255
Iteration: 2 || Loss: 68.80365952623814
Iteration: 3 || Loss: 68.78324824479162
Iteration: 4 || Loss: 68.59370424771967
Iteration: 5 || Loss: 68.50263375695317
Iteration: 6 || Loss: 68.44400375662039
Iteration: 7 || Loss: 68.40506608105134
Iteration: 8 || Loss: 68.3837436029898
Iteration: 9 || Loss: 68.37148251609605
Iteration: 10 || Loss: 68.3551690713851
Iteration: 11 || Loss: 68.34265164127814
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.826065
Epoch 225 loss:68.34265164127814
MSE loss S0.9859566019659616
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.017326517001493
MSE loss S0.35247809704524263
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:113.05691841432667
MSE loss S2.1787681013454736
waveform batch: 2/2
Test loss - extrapolation:54.87006932216321
MSE loss S1.2681978292170188
Epoch 225 mean train loss:3.375881356120004
Epoch 225 mean test loss - interpolation:4.0028877528335824
Epoch 225 mean test loss - extrapolation:13.99391564470749
Start training epoch 226
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.826065
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3736152822371155
Iteration: 2 || Loss: 6.372901509835092
Iteration: 3 || Loss: 6.3723305644279025
Iteration: 4 || Loss: 6.371668960730552
Iteration: 5 || Loss: 6.371017162377245
Iteration: 6 || Loss: 6.371017162377245
saving ADAM checkpoint...
Sum of params:-106.82597
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.371017162377245
Iteration: 2 || Loss: 6.357636170449602
Iteration: 3 || Loss: 6.318215270780256
Iteration: 4 || Loss: 6.275309536980307
Iteration: 5 || Loss: 6.158812071899526
Iteration: 6 || Loss: 6.105587585172756
Iteration: 7 || Loss: 6.091135242580203
Iteration: 8 || Loss: 6.0857593161269445
Iteration: 9 || Loss: 6.08051028556935
Iteration: 10 || Loss: 6.0730385286628135
Iteration: 11 || Loss: 6.051810845520041
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.845634
Epoch 226 loss:6.051810845520041
MSE loss S0.20601809500562804
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.845634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.08390521875015
Iteration: 2 || Loss: 24.083547476489763
Iteration: 3 || Loss: 24.083096741749486
Iteration: 4 || Loss: 24.082736306709542
Iteration: 5 || Loss: 24.08244385681655
Iteration: 6 || Loss: 24.08244385681655
saving ADAM checkpoint...
Sum of params:-106.845825
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.08244385681655
Iteration: 2 || Loss: 24.06705991646148
Iteration: 3 || Loss: 23.911241176843312
Iteration: 4 || Loss: 23.86397243642196
Iteration: 5 || Loss: 23.65470558143845
Iteration: 6 || Loss: 23.612579791019776
Iteration: 7 || Loss: 23.59530786720469
Iteration: 8 || Loss: 23.563354960204165
Iteration: 9 || Loss: 23.534508090582033
Iteration: 10 || Loss: 23.518239282095646
Iteration: 11 || Loss: 23.505399424754245
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.814896
Epoch 226 loss:23.505399424754245
MSE loss S0.41024883136481033
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.814896
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.76513085518005
Iteration: 2 || Loss: 68.76466833139396
Iteration: 3 || Loss: 68.76420799303585
Iteration: 4 || Loss: 68.76375248556336
Iteration: 5 || Loss: 68.76328835067756
Iteration: 6 || Loss: 68.76328835067756
saving ADAM checkpoint...
Sum of params:-106.81499
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.76328835067756
Iteration: 2 || Loss: 68.7597640671683
Iteration: 3 || Loss: 68.74374881818558
Iteration: 4 || Loss: 68.53012881146898
Iteration: 5 || Loss: 68.49610551387141
Iteration: 6 || Loss: 68.42537112205208
Iteration: 7 || Loss: 68.37802304682968
Iteration: 8 || Loss: 68.35728948184241
Iteration: 9 || Loss: 68.3477073525464
Iteration: 10 || Loss: 68.331506841824
Iteration: 11 || Loss: 68.31504368317029
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.85571
Epoch 226 loss:68.31504368317029
MSE loss S0.985103562406821
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.01344447806856
MSE loss S0.35227244122679446
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:112.88031917467875
MSE loss S2.1763962855660983
waveform batch: 2/2
Test loss - extrapolation:54.763499547150666
MSE loss S1.263541049488068
Epoch 226 mean train loss:3.374905308739468
Epoch 226 mean test loss - interpolation:4.00224074634476
Epoch 226 mean test loss - extrapolation:13.970318226819117
Start training epoch 227
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.85571
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.409415892466994
Iteration: 2 || Loss: 6.408791243135434
Iteration: 3 || Loss: 6.40801478586288
Iteration: 4 || Loss: 6.4074490168191
Iteration: 5 || Loss: 6.4067451846781935
Iteration: 6 || Loss: 6.4067451846781935
saving ADAM checkpoint...
Sum of params:-106.85561
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4067451846781935
Iteration: 2 || Loss: 6.398309687666181
Iteration: 3 || Loss: 6.336064224577447
Iteration: 4 || Loss: 6.284078712820342
Iteration: 5 || Loss: 6.1563461452676265
Iteration: 6 || Loss: 6.104299037165484
Iteration: 7 || Loss: 6.092435384466193
Iteration: 8 || Loss: 6.087440359588638
Iteration: 9 || Loss: 6.082442884847082
Iteration: 10 || Loss: 6.074621787186553
Iteration: 11 || Loss: 6.066181646709581
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.86962
Epoch 227 loss:6.066181646709581
MSE loss S0.21060545017503984
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.86962
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.961652851715325
Iteration: 2 || Loss: 23.961401639233948
Iteration: 3 || Loss: 23.961026341429694
Iteration: 4 || Loss: 23.960733806748966
Iteration: 5 || Loss: 23.96055831475387
Iteration: 6 || Loss: 23.96055831475387
saving ADAM checkpoint...
Sum of params:-106.869736
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.96055831475387
Iteration: 2 || Loss: 23.93448292318246
Iteration: 3 || Loss: 23.88583038574035
Iteration: 4 || Loss: 23.812879992540335
Iteration: 5 || Loss: 23.579665578711605
Iteration: 6 || Loss: 23.549834690643618
Iteration: 7 || Loss: 23.538637472999493
Iteration: 8 || Loss: 23.509449193196744
Iteration: 9 || Loss: 23.503656644829377
Iteration: 10 || Loss: 23.496238203551414
Iteration: 11 || Loss: 23.48084901843875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.833084
Epoch 227 loss:23.48084901843875
MSE loss S0.4082132873250512
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.833084
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.73490248924732
Iteration: 2 || Loss: 68.73448118094058
Iteration: 3 || Loss: 68.73411354444976
Iteration: 4 || Loss: 68.73368471138208
Iteration: 5 || Loss: 68.73337081146627
Iteration: 6 || Loss: 68.73337081146627
saving ADAM checkpoint...
Sum of params:-106.83321
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.73337081146627
Iteration: 2 || Loss: 68.72875059065225
Iteration: 3 || Loss: 68.70902048312026
Iteration: 4 || Loss: 68.53136023345758
Iteration: 5 || Loss: 68.44188018834227
Iteration: 6 || Loss: 68.38349665404448
Iteration: 7 || Loss: 68.34561802923085
Iteration: 8 || Loss: 68.32468690511081
Iteration: 9 || Loss: 68.31238725465863
Iteration: 10 || Loss: 68.29543141731305
Iteration: 11 || Loss: 68.28259187063522
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.88164
Epoch 227 loss:68.28259187063522
MSE loss S0.9858792567842849
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.003945743483374
MSE loss S0.35225291907703055
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:112.66868768396188
MSE loss S2.172001363905416
waveform batch: 2/2
Test loss - extrapolation:54.66940734717646
MSE loss S1.2644296127808232
Epoch 227 mean train loss:3.373435259854605
Epoch 227 mean test loss - interpolation:4.000657623913896
Epoch 227 mean test loss - extrapolation:13.94484125259486
Start training epoch 228
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.88164
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.373523550653663
Iteration: 2 || Loss: 6.3727928600582775
Iteration: 3 || Loss: 6.37214268655242
Iteration: 4 || Loss: 6.3715208898158675
Iteration: 5 || Loss: 6.37085097811987
Iteration: 6 || Loss: 6.37085097811987
saving ADAM checkpoint...
Sum of params:-106.88156
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.37085097811987
Iteration: 2 || Loss: 6.357443897847256
Iteration: 3 || Loss: 6.3169108044868185
Iteration: 4 || Loss: 6.275103106175461
Iteration: 5 || Loss: 6.157027194172791
Iteration: 6 || Loss: 6.1045034979239
Iteration: 7 || Loss: 6.088744615389549
Iteration: 8 || Loss: 6.083410714092158
Iteration: 9 || Loss: 6.078131557183645
Iteration: 10 || Loss: 6.0707517402720335
Iteration: 11 || Loss: 6.048846718959954
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.90126
Epoch 228 loss:6.048846718959954
MSE loss S0.20523446532848766
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.90126
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.08608546517118
Iteration: 2 || Loss: 24.08570704682199
Iteration: 3 || Loss: 24.08533798373922
Iteration: 4 || Loss: 24.084824815978763
Iteration: 5 || Loss: 24.084475684316892
Iteration: 6 || Loss: 24.084475684316892
saving ADAM checkpoint...
Sum of params:-106.90145
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.084475684316892
Iteration: 2 || Loss: 24.069156149540714
Iteration: 3 || Loss: 23.90556799316502
Iteration: 4 || Loss: 23.862366467169316
Iteration: 5 || Loss: 23.652198314091553
Iteration: 6 || Loss: 23.608586070804048
Iteration: 7 || Loss: 23.591249548098602
Iteration: 8 || Loss: 23.559181374219122
Iteration: 9 || Loss: 23.528569416138495
Iteration: 10 || Loss: 23.51198654245817
Iteration: 11 || Loss: 23.4990156594442
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.87102
Epoch 228 loss:23.4990156594442
MSE loss S0.40966587682608835
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.87102
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.70254585439781
Iteration: 2 || Loss: 68.70208788288791
Iteration: 3 || Loss: 68.70167410219092
Iteration: 4 || Loss: 68.70120740741302
Iteration: 5 || Loss: 68.70081520579637
Iteration: 6 || Loss: 68.70081520579637
saving ADAM checkpoint...
Sum of params:-106.871086
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.70081520579637
Iteration: 2 || Loss: 68.69743898278595
Iteration: 3 || Loss: 68.68213251995097
Iteration: 4 || Loss: 68.47086410483158
Iteration: 5 || Loss: 68.43604710201082
Iteration: 6 || Loss: 68.36497757335512
Iteration: 7 || Loss: 68.31845767906502
Iteration: 8 || Loss: 68.29762840946462
Iteration: 9 || Loss: 68.28799716630756
Iteration: 10 || Loss: 68.27159120645753
Iteration: 11 || Loss: 68.2548627451663
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.91194
Epoch 228 loss:68.2548627451663
MSE loss S0.9846311847199226
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.00051716230259
MSE loss S0.35203169488238895
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:112.48561249430827
MSE loss S2.169274997620864
waveform batch: 2/2
Test loss - extrapolation:54.55978156796903
MSE loss S1.2591343878590986
Epoch 228 mean train loss:3.37250776288174
Epoch 228 mean test loss - interpolation:4.000086193717098
Epoch 228 mean test loss - extrapolation:13.920449505189774
Start training epoch 229
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.91194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.41259115069547
Iteration: 2 || Loss: 6.411891790786083
Iteration: 3 || Loss: 6.411184272387681
Iteration: 4 || Loss: 6.410452514551843
Iteration: 5 || Loss: 6.409772232223906
Iteration: 6 || Loss: 6.409772232223906
saving ADAM checkpoint...
Sum of params:-106.91184
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.409772232223906
Iteration: 2 || Loss: 6.402028224333135
Iteration: 3 || Loss: 6.336957148964286
Iteration: 4 || Loss: 6.2863910387672375
Iteration: 5 || Loss: 6.154370810550872
Iteration: 6 || Loss: 6.103378625169115
Iteration: 7 || Loss: 6.090326252777498
Iteration: 8 || Loss: 6.085392113687371
Iteration: 9 || Loss: 6.080315642922063
Iteration: 10 || Loss: 6.072437018524941
Iteration: 11 || Loss: 6.063936489937451
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.92549
Epoch 229 loss:6.063936489937451
MSE loss S0.2104826054065643
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.92549
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.95585840205177
Iteration: 2 || Loss: 23.955643642450422
Iteration: 3 || Loss: 23.95528296816676
Iteration: 4 || Loss: 23.95496912018948
Iteration: 5 || Loss: 23.954745675978167
Iteration: 6 || Loss: 23.954745675978167
saving ADAM checkpoint...
Sum of params:-106.925606
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.954745675978167
Iteration: 2 || Loss: 23.9278360009757
Iteration: 3 || Loss: 23.87517726117057
Iteration: 4 || Loss: 23.804571521571056
Iteration: 5 || Loss: 23.572629758074005
Iteration: 6 || Loss: 23.542602344905077
Iteration: 7 || Loss: 23.531656137467557
Iteration: 8 || Loss: 23.502320955370795
Iteration: 9 || Loss: 23.49627142011904
Iteration: 10 || Loss: 23.489321820626728
Iteration: 11 || Loss: 23.47380847827656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.8896
Epoch 229 loss:23.47380847827656
MSE loss S0.40906823812319076
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.8896
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.65080931256539
Iteration: 2 || Loss: 68.6504343694807
Iteration: 3 || Loss: 68.6500937106682
Iteration: 4 || Loss: 68.64978915871336
Iteration: 5 || Loss: 68.64948049282015
Iteration: 6 || Loss: 68.64948049282015
saving ADAM checkpoint...
Sum of params:-106.88974
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.64948049282015
Iteration: 2 || Loss: 68.64526758692021
Iteration: 3 || Loss: 68.62687509274042
Iteration: 4 || Loss: 68.46406288597846
Iteration: 5 || Loss: 68.37806220434005
Iteration: 6 || Loss: 68.3211426148836
Iteration: 7 || Loss: 68.28478872842012
Iteration: 8 || Loss: 68.2646270256914
Iteration: 9 || Loss: 68.25243013413726
Iteration: 10 || Loss: 68.23442607779693
Iteration: 11 || Loss: 68.22075219957527
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.940865
Epoch 229 loss:68.22075219957527
MSE loss S0.9860121203552588
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.993636702528253
MSE loss S0.3517729423739108
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:112.2514939893641
MSE loss S2.1648313304605864
waveform batch: 2/2
Test loss - extrapolation:54.47015640830346
MSE loss S1.2604527111257906
Epoch 229 mean train loss:3.3709826609582514
Epoch 229 mean test loss - interpolation:3.9989394504213753
Epoch 229 mean test loss - extrapolation:13.893470866472297
Start training epoch 230
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.940865
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.369789236393317
Iteration: 2 || Loss: 6.369058249423216
Iteration: 3 || Loss: 6.368283042763255
Iteration: 4 || Loss: 6.367553504102506
Iteration: 5 || Loss: 6.366885809537052
Iteration: 6 || Loss: 6.366885809537052
saving ADAM checkpoint...
Sum of params:-106.940796
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.366885809537052
Iteration: 2 || Loss: 6.350883117038419
Iteration: 3 || Loss: 6.308948225800158
Iteration: 4 || Loss: 6.269586583366547
Iteration: 5 || Loss: 6.15734912200118
Iteration: 6 || Loss: 6.106464429604908
Iteration: 7 || Loss: 6.08700255165239
Iteration: 8 || Loss: 6.08183698305336
Iteration: 9 || Loss: 6.07637246783746
Iteration: 10 || Loss: 6.069005884755548
Iteration: 11 || Loss: 6.045998147817365
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.96069
Epoch 230 loss:6.045998147817365
MSE loss S0.20425245402681352
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.96069
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.093510501312245
Iteration: 2 || Loss: 24.09310570429029
Iteration: 3 || Loss: 24.092649696908964
Iteration: 4 || Loss: 24.09226552573826
Iteration: 5 || Loss: 24.091808116070553
Iteration: 6 || Loss: 24.091808116070553
saving ADAM checkpoint...
Sum of params:-106.960884
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.091808116070553
Iteration: 2 || Loss: 24.076913175415587
Iteration: 3 || Loss: 23.90322513954327
Iteration: 4 || Loss: 23.86406103837358
Iteration: 5 || Loss: 23.65344118460788
Iteration: 6 || Loss: 23.606508378629925
Iteration: 7 || Loss: 23.588834040608653
Iteration: 8 || Loss: 23.556465005738353
Iteration: 9 || Loss: 23.52385194028738
Iteration: 10 || Loss: 23.50793679180564
Iteration: 11 || Loss: 23.4947473135907
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.931046
Epoch 230 loss:23.4947473135907
MSE loss S0.41054175247668523
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.931046
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.63855462522149
Iteration: 2 || Loss: 68.6381318100169
Iteration: 3 || Loss: 68.63765814364481
Iteration: 4 || Loss: 68.63721938225481
Iteration: 5 || Loss: 68.63675926473572
Iteration: 6 || Loss: 68.63675926473572
saving ADAM checkpoint...
Sum of params:-106.931145
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.63675926473572
Iteration: 2 || Loss: 68.63157277595398
Iteration: 3 || Loss: 68.6166662581385
Iteration: 4 || Loss: 68.39690224202799
Iteration: 5 || Loss: 68.3709268661963
Iteration: 6 || Loss: 68.30259198801943
Iteration: 7 || Loss: 68.25717008584175
Iteration: 8 || Loss: 68.23621236510373
Iteration: 9 || Loss: 68.22637272521386
Iteration: 10 || Loss: 68.20928194595433
Iteration: 11 || Loss: 68.19253446734076
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.97113
Epoch 230 loss:68.19253446734076
MSE loss S0.9766437961418799
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.993641872654454
MSE loss S0.3487062606368744
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:111.99673456003545
MSE loss S2.1567274731274333
waveform batch: 2/2
Test loss - extrapolation:54.28486287280412
MSE loss S1.2482395789664438
Epoch 230 mean train loss:3.3701131009913388
Epoch 230 mean test loss - interpolation:3.9989403121090756
Epoch 230 mean test loss - extrapolation:13.856799786069963
Start training epoch 231
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-106.97113
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.420428435750379
Iteration: 2 || Loss: 6.419501947149626
Iteration: 3 || Loss: 6.41857360743719
Iteration: 4 || Loss: 6.417690315501491
Iteration: 5 || Loss: 6.416826245631602
Iteration: 6 || Loss: 6.416826245631602
saving ADAM checkpoint...
Sum of params:-106.971054
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.416826245631602
Iteration: 2 || Loss: 6.39098744726774
Iteration: 3 || Loss: 6.320533094324917
Iteration: 4 || Loss: 6.276565801577687
Iteration: 5 || Loss: 6.152355955043875
Iteration: 6 || Loss: 6.0969007375122715
Iteration: 7 || Loss: 6.086322441392355
Iteration: 8 || Loss: 6.0800193691769255
Iteration: 9 || Loss: 6.076957962932314
Iteration: 10 || Loss: 6.068087859024621
Iteration: 11 || Loss: 6.0467024931873485
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.98633
Epoch 231 loss:6.0467024931873485
MSE loss S0.20640815584590985
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-106.98633
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.083801808556114
Iteration: 2 || Loss: 24.08338433839546
Iteration: 3 || Loss: 24.083166562235377
Iteration: 4 || Loss: 24.082780997567955
Iteration: 5 || Loss: 24.082402530249237
Iteration: 6 || Loss: 24.082402530249237
saving ADAM checkpoint...
Sum of params:-106.986435
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.082402530249237
Iteration: 2 || Loss: 24.04792785948932
Iteration: 3 || Loss: 23.88077621981897
Iteration: 4 || Loss: 23.851089506537722
Iteration: 5 || Loss: 23.62792572731579
Iteration: 6 || Loss: 23.591406420241935
Iteration: 7 || Loss: 23.575567857265387
Iteration: 8 || Loss: 23.543832545277983
Iteration: 9 || Loss: 23.51491291874619
Iteration: 10 || Loss: 23.504095397665562
Iteration: 11 || Loss: 23.48871734263414
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.95875
Epoch 231 loss:23.48871734263414
MSE loss S0.4080358042550183
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.95875
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.60589667248448
Iteration: 2 || Loss: 68.60552925341017
Iteration: 3 || Loss: 68.60519488999978
Iteration: 4 || Loss: 68.60483619484498
Iteration: 5 || Loss: 68.60450886256062
Iteration: 6 || Loss: 68.60450886256062
saving ADAM checkpoint...
Sum of params:-106.95885
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.60450886256062
Iteration: 2 || Loss: 68.60118818563284
Iteration: 3 || Loss: 68.58660004549071
Iteration: 4 || Loss: 68.3512908379499
Iteration: 5 || Loss: 68.32383760406799
Iteration: 6 || Loss: 68.26986059696205
Iteration: 7 || Loss: 68.22412464481158
Iteration: 8 || Loss: 68.2054766954631
Iteration: 9 || Loss: 68.19602872830535
Iteration: 10 || Loss: 68.17761193792781
Iteration: 11 || Loss: 68.16053499172264
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.00099
Epoch 231 loss:68.16053499172264
MSE loss S0.9832083475684704
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.98715242968221
MSE loss S0.3507129470927
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:111.84156408588102
MSE loss S2.1567782873931582
waveform batch: 2/2
Test loss - extrapolation:54.252760163827986
MSE loss S1.2521840275962288
Epoch 231 mean train loss:3.3688260285360045
Epoch 231 mean test loss - interpolation:3.9978587382803688
Epoch 231 mean test loss - extrapolation:13.841193687475752
Start training epoch 232
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.00099
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.408054950903294
Iteration: 2 || Loss: 6.407266825899969
Iteration: 3 || Loss: 6.4064339967257675
Iteration: 4 || Loss: 6.405578186362967
Iteration: 5 || Loss: 6.4048566839765755
Iteration: 6 || Loss: 6.4048566839765755
saving ADAM checkpoint...
Sum of params:-107.000916
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4048566839765755
Iteration: 2 || Loss: 6.384234979048726
Iteration: 3 || Loss: 6.311955232996
Iteration: 4 || Loss: 6.268459657389841
Iteration: 5 || Loss: 6.154507227574696
Iteration: 6 || Loss: 6.100721431110262
Iteration: 7 || Loss: 6.085213801554183
Iteration: 8 || Loss: 6.079044976054991
Iteration: 9 || Loss: 6.076452901386138
Iteration: 10 || Loss: 6.069021913872903
Iteration: 11 || Loss: 6.046640552605768
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.016655
Epoch 232 loss:6.046640552605768
MSE loss S0.20670662396562184
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.016655
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.087928912919413
Iteration: 2 || Loss: 24.087496016228805
Iteration: 3 || Loss: 24.08715885462226
Iteration: 4 || Loss: 24.086755500830932
Iteration: 5 || Loss: 24.08642772032073
Iteration: 6 || Loss: 24.08642772032073
saving ADAM checkpoint...
Sum of params:-107.01677
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.08642772032073
Iteration: 2 || Loss: 24.06257899794665
Iteration: 3 || Loss: 23.887265964621772
Iteration: 4 || Loss: 23.849919263615476
Iteration: 5 || Loss: 23.621157504095073
Iteration: 6 || Loss: 23.584089775220672
Iteration: 7 || Loss: 23.569574850863887
Iteration: 8 || Loss: 23.54033890519251
Iteration: 9 || Loss: 23.511945032795293
Iteration: 10 || Loss: 23.50087816306205
Iteration: 11 || Loss: 23.487442748331535
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-106.9879
Epoch 232 loss:23.487442748331535
MSE loss S0.40760793583916893
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-106.9879
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.57520611984316
Iteration: 2 || Loss: 68.57483290973076
Iteration: 3 || Loss: 68.57448252016043
Iteration: 4 || Loss: 68.57408360261235
Iteration: 5 || Loss: 68.57372625409604
Iteration: 6 || Loss: 68.57372625409604
saving ADAM checkpoint...
Sum of params:-106.98804
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.57372625409604
Iteration: 2 || Loss: 68.57051168814188
Iteration: 3 || Loss: 68.55645369832921
Iteration: 4 || Loss: 68.32238955048342
Iteration: 5 || Loss: 68.29591216442189
Iteration: 6 || Loss: 68.2374020203011
Iteration: 7 || Loss: 68.19099403266789
Iteration: 8 || Loss: 68.17166611944776
Iteration: 9 || Loss: 68.16256190701753
Iteration: 10 || Loss: 68.14497212767431
Iteration: 11 || Loss: 68.12769896911558
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.03087
Epoch 232 loss:68.12769896911558
MSE loss S0.9832359857323778
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.978588083682503
MSE loss S0.35119634755781287
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:111.5916886061972
MSE loss S2.152171595261171
waveform batch: 2/2
Test loss - extrapolation:54.12623969882665
MSE loss S1.2494603375114912
Epoch 232 mean train loss:3.3676476644845827
Epoch 232 mean test loss - interpolation:3.996431347280417
Epoch 232 mean test loss - extrapolation:13.809827358751988
Start training epoch 233
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.03087
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.407973339075892
Iteration: 2 || Loss: 6.407180049305581
Iteration: 3 || Loss: 6.406348633079873
Iteration: 4 || Loss: 6.40556280215226
Iteration: 5 || Loss: 6.404845553816603
Iteration: 6 || Loss: 6.404845553816603
saving ADAM checkpoint...
Sum of params:-107.03076
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.404845553816603
Iteration: 2 || Loss: 6.386579562126137
Iteration: 3 || Loss: 6.31570307920777
Iteration: 4 || Loss: 6.272202274923055
Iteration: 5 || Loss: 6.151450455500747
Iteration: 6 || Loss: 6.098271484235376
Iteration: 7 || Loss: 6.0837924704975626
Iteration: 8 || Loss: 6.077734047857512
Iteration: 9 || Loss: 6.074628689815129
Iteration: 10 || Loss: 6.067292947715786
Iteration: 11 || Loss: 6.045628910596654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.045906
Epoch 233 loss:6.045628910596654
MSE loss S0.20661923222760895
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.045906
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.078512303488374
Iteration: 2 || Loss: 24.078171085694102
Iteration: 3 || Loss: 24.077844513101276
Iteration: 4 || Loss: 24.077567353040468
Iteration: 5 || Loss: 24.077189590631587
Iteration: 6 || Loss: 24.077189590631587
saving ADAM checkpoint...
Sum of params:-107.046005
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.077189590631587
Iteration: 2 || Loss: 24.041627747663323
Iteration: 3 || Loss: 23.87240211082515
Iteration: 4 || Loss: 23.843904557727374
Iteration: 5 || Loss: 23.61930769776381
Iteration: 6 || Loss: 23.582456218765337
Iteration: 7 || Loss: 23.566701130360947
Iteration: 8 || Loss: 23.536706425585333
Iteration: 9 || Loss: 23.509011270768276
Iteration: 10 || Loss: 23.499410833133584
Iteration: 11 || Loss: 23.484332486967574
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.018265
Epoch 233 loss:23.484332486967574
MSE loss S0.40730583560854317
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.018265
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.53642978962563
Iteration: 2 || Loss: 68.53609668429071
Iteration: 3 || Loss: 68.53579368421167
Iteration: 4 || Loss: 68.53544620357171
Iteration: 5 || Loss: 68.53513546523362
Iteration: 6 || Loss: 68.53513546523362
saving ADAM checkpoint...
Sum of params:-107.01837
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.53513546523362
Iteration: 2 || Loss: 68.53190861210511
Iteration: 3 || Loss: 68.51766098912063
Iteration: 4 || Loss: 68.28752967435258
Iteration: 5 || Loss: 68.26380779965939
Iteration: 6 || Loss: 68.20194224247346
Iteration: 7 || Loss: 68.15710253737636
Iteration: 8 || Loss: 68.13862701472037
Iteration: 9 || Loss: 68.12946235915437
Iteration: 10 || Loss: 68.11115862025291
Iteration: 11 || Loss: 68.09442374999199
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.06189
Epoch 233 loss:68.09442374999199
MSE loss S0.9826949886903756
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.976192378979594
MSE loss S0.3506309843200148
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:111.37538530872483
MSE loss S2.1477371630399014
waveform batch: 2/2
Test loss - extrapolation:54.03224538898222
MSE loss S1.2465375213580183
Epoch 233 mean train loss:3.366358108536421
Epoch 233 mean test loss - interpolation:3.9960320631632658
Epoch 233 mean test loss - extrapolation:13.78396922480892
Start training epoch 234
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.06189
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.406152524402609
Iteration: 2 || Loss: 6.405334619567576
Iteration: 3 || Loss: 6.404563310144587
Iteration: 4 || Loss: 6.4036657033799695
Iteration: 5 || Loss: 6.402842756249665
Iteration: 6 || Loss: 6.402842756249665
saving ADAM checkpoint...
Sum of params:-107.06178
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.402842756249665
Iteration: 2 || Loss: 6.380948538215772
Iteration: 3 || Loss: 6.307499013627179
Iteration: 4 || Loss: 6.266615184846895
Iteration: 5 || Loss: 6.151776473494322
Iteration: 6 || Loss: 6.098505177070599
Iteration: 7 || Loss: 6.083012009906127
Iteration: 8 || Loss: 6.076907942020645
Iteration: 9 || Loss: 6.074063074322336
Iteration: 10 || Loss: 6.067162071188367
Iteration: 11 || Loss: 6.044681488706093
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.07717
Epoch 234 loss:6.044681488706093
MSE loss S0.2061674004455727
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.07717
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.07441263035581
Iteration: 2 || Loss: 24.074176581115157
Iteration: 3 || Loss: 24.073763999714863
Iteration: 4 || Loss: 24.073423246208915
Iteration: 5 || Loss: 24.073067481496945
Iteration: 6 || Loss: 24.073067481496945
saving ADAM checkpoint...
Sum of params:-107.07726
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.073067481496945
Iteration: 2 || Loss: 24.04480994258929
Iteration: 3 || Loss: 23.874714134277042
Iteration: 4 || Loss: 23.84101999058847
Iteration: 5 || Loss: 23.6155584959206
Iteration: 6 || Loss: 23.578664208746257
Iteration: 7 || Loss: 23.56392423089603
Iteration: 8 || Loss: 23.5346729023097
Iteration: 9 || Loss: 23.506747312354406
Iteration: 10 || Loss: 23.49591267072016
Iteration: 11 || Loss: 23.482445605125324
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.04882
Epoch 234 loss:23.482445605125324
MSE loss S0.4073157875929916
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.04882
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.5036753043138
Iteration: 2 || Loss: 68.50326349989156
Iteration: 3 || Loss: 68.5028905258007
Iteration: 4 || Loss: 68.50253331370295
Iteration: 5 || Loss: 68.50221927107128
Iteration: 6 || Loss: 68.50221927107128
saving ADAM checkpoint...
Sum of params:-107.04893
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.50221927107128
Iteration: 2 || Loss: 68.49902128345977
Iteration: 3 || Loss: 68.48492070560306
Iteration: 4 || Loss: 68.25399389224383
Iteration: 5 || Loss: 68.22912555427142
Iteration: 6 || Loss: 68.16945502733817
Iteration: 7 || Loss: 68.12378334424434
Iteration: 8 || Loss: 68.10490455406331
Iteration: 9 || Loss: 68.09597546938959
Iteration: 10 || Loss: 68.07840181650327
Iteration: 11 || Loss: 68.0613255666383
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.09288
Epoch 234 loss:68.0613255666383
MSE loss S0.9827256971320216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.96754369964601
MSE loss S0.351110239010969
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:111.11776471277756
MSE loss S2.1429728980536793
waveform batch: 2/2
Test loss - extrapolation:53.905714334238
MSE loss S1.244177683355305
Epoch 234 mean train loss:3.365119057257577
Epoch 234 mean test loss - interpolation:3.9945906166076686
Epoch 234 mean test loss - extrapolation:13.751956587251295
Start training epoch 235
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.09288
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4058799648841225
Iteration: 2 || Loss: 6.405069300769497
Iteration: 3 || Loss: 6.4043084962151715
Iteration: 4 || Loss: 6.403512775144422
Iteration: 5 || Loss: 6.402708630719225
Iteration: 6 || Loss: 6.402708630719225
saving ADAM checkpoint...
Sum of params:-107.09278
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.402708630719225
Iteration: 2 || Loss: 6.383518282377461
Iteration: 3 || Loss: 6.3110319156253425
Iteration: 4 || Loss: 6.270339640944237
Iteration: 5 || Loss: 6.149436018136896
Iteration: 6 || Loss: 6.096598823932359
Iteration: 7 || Loss: 6.08165766185607
Iteration: 8 || Loss: 6.075657136859394
Iteration: 9 || Loss: 6.072424376131259
Iteration: 10 || Loss: 6.065615841448047
Iteration: 11 || Loss: 6.0439043552179745
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.107834
Epoch 235 loss:6.0439043552179745
MSE loss S0.20634497526835188
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.107834
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.07177984225233
Iteration: 2 || Loss: 24.07132400931981
Iteration: 3 || Loss: 24.070999790580974
Iteration: 4 || Loss: 24.070661504315435
Iteration: 5 || Loss: 24.07027972424789
Iteration: 6 || Loss: 24.07027972424789
saving ADAM checkpoint...
Sum of params:-107.10797
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.07027972424789
Iteration: 2 || Loss: 24.03519634312101
Iteration: 3 || Loss: 23.869121562355794
Iteration: 4 || Loss: 23.837607067923386
Iteration: 5 || Loss: 23.61356238843407
Iteration: 6 || Loss: 23.577044992692258
Iteration: 7 || Loss: 23.561410039951973
Iteration: 8 || Loss: 23.531409872306906
Iteration: 9 || Loss: 23.504183546355637
Iteration: 10 || Loss: 23.494358769210553
Iteration: 11 || Loss: 23.47954672919156
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.08019
Epoch 235 loss:23.47954672919156
MSE loss S0.40669524725120265
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.08019
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.46729351560796
Iteration: 2 || Loss: 68.46694166513879
Iteration: 3 || Loss: 68.4665418486266
Iteration: 4 || Loss: 68.4662211283779
Iteration: 5 || Loss: 68.46594574290248
Iteration: 6 || Loss: 68.46594574290248
saving ADAM checkpoint...
Sum of params:-107.0803
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.46594574290248
Iteration: 2 || Loss: 68.46280982114114
Iteration: 3 || Loss: 68.44873291482764
Iteration: 4 || Loss: 68.2178719765292
Iteration: 5 || Loss: 68.19440503595898
Iteration: 6 || Loss: 68.13444432983817
Iteration: 7 || Loss: 68.0901492038707
Iteration: 8 || Loss: 68.07176342957236
Iteration: 9 || Loss: 68.0628156973192
Iteration: 10 || Loss: 68.04473140223159
Iteration: 11 || Loss: 68.02800161427125
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.12477
Epoch 235 loss:68.02800161427125
MSE loss S0.9822863864247564
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.964278843046927
MSE loss S0.35080576401450175
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:110.88693389968414
MSE loss S2.1382316524707194
waveform batch: 2/2
Test loss - extrapolation:53.80685146194042
MSE loss S1.2411600277966182
Epoch 235 mean train loss:3.3638431965062336
Epoch 235 mean test loss - interpolation:3.9940464738411543
Epoch 235 mean test loss - extrapolation:13.724482113468712
Start training epoch 236
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.12477
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.403549026402348
Iteration: 2 || Loss: 6.402683924423312
Iteration: 3 || Loss: 6.4017975845441555
Iteration: 4 || Loss: 6.401012272517974
Iteration: 5 || Loss: 6.400187433279296
Iteration: 6 || Loss: 6.400187433279296
saving ADAM checkpoint...
Sum of params:-107.124725
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.400187433279296
Iteration: 2 || Loss: 6.378299779869462
Iteration: 3 || Loss: 6.30504839177301
Iteration: 4 || Loss: 6.266452542560204
Iteration: 5 || Loss: 6.149437090537583
Iteration: 6 || Loss: 6.096520012870434
Iteration: 7 || Loss: 6.0810064811860025
Iteration: 8 || Loss: 6.074983759660213
Iteration: 9 || Loss: 6.0714963025697495
Iteration: 10 || Loss: 6.065149829129795
Iteration: 11 || Loss: 6.042961471896051
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.13962
Epoch 236 loss:6.042961471896051
MSE loss S0.20605064301714593
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.13962
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.0676576847211
Iteration: 2 || Loss: 24.067409997278947
Iteration: 3 || Loss: 24.066948088563873
Iteration: 4 || Loss: 24.066675312020834
Iteration: 5 || Loss: 24.066436679932092
Iteration: 6 || Loss: 24.066436679932092
saving ADAM checkpoint...
Sum of params:-107.13972
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.066436679932092
Iteration: 2 || Loss: 24.025590277441903
Iteration: 3 || Loss: 23.862610921402847
Iteration: 4 || Loss: 23.834419597575852
Iteration: 5 || Loss: 23.61452572946086
Iteration: 6 || Loss: 23.57628480029658
Iteration: 7 || Loss: 23.55995815448489
Iteration: 8 || Loss: 23.53078169832892
Iteration: 9 || Loss: 23.502099757205805
Iteration: 10 || Loss: 23.492534236139228
Iteration: 11 || Loss: 23.478105547790577
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.11241
Epoch 236 loss:23.478105547790577
MSE loss S0.4069529493031819
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.11241
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.43385025829123
Iteration: 2 || Loss: 68.43351332171736
Iteration: 3 || Loss: 68.433181949308
Iteration: 4 || Loss: 68.4328555369779
Iteration: 5 || Loss: 68.43256737280177
Iteration: 6 || Loss: 68.43256737280177
saving ADAM checkpoint...
Sum of params:-107.11252
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.43256737280177
Iteration: 2 || Loss: 68.42950852581329
Iteration: 3 || Loss: 68.41546231244175
Iteration: 4 || Loss: 68.19335567122687
Iteration: 5 || Loss: 68.16475914189544
Iteration: 6 || Loss: 68.10101263639531
Iteration: 7 || Loss: 68.05685834661085
Iteration: 8 || Loss: 68.03846399637428
Iteration: 9 || Loss: 68.02946935534989
Iteration: 10 || Loss: 68.01117694700716
Iteration: 11 || Loss: 67.99477537151694
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.15667
Epoch 236 loss:67.99477537151694
MSE loss S0.9819281391390333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.95901961830485
MSE loss S0.3508066460470261
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:110.63832526331042
MSE loss S2.133358400167551
waveform batch: 2/2
Test loss - extrapolation:53.69652022643151
MSE loss S1.2381833603559949
Epoch 236 mean train loss:3.3626152548690884
Epoch 236 mean test loss - interpolation:3.9931699363841417
Epoch 236 mean test loss - extrapolation:13.694570457478493
Start training epoch 237
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.15667
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.404275451766857
Iteration: 2 || Loss: 6.403396563134454
Iteration: 3 || Loss: 6.4025374951634495
Iteration: 4 || Loss: 6.401661859752126
Iteration: 5 || Loss: 6.400863661206561
Iteration: 6 || Loss: 6.400863661206561
saving ADAM checkpoint...
Sum of params:-107.15656
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.400863661206561
Iteration: 2 || Loss: 6.378186282265473
Iteration: 3 || Loss: 6.303681771423927
Iteration: 4 || Loss: 6.265882628229438
Iteration: 5 || Loss: 6.148599625892759
Iteration: 6 || Loss: 6.095593738640027
Iteration: 7 || Loss: 6.080308768906231
Iteration: 8 || Loss: 6.074236431594114
Iteration: 9 || Loss: 6.0709321541650905
Iteration: 10 || Loss: 6.064663317416873
Iteration: 11 || Loss: 6.04221846066495
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.171425
Epoch 237 loss:6.04221846066495
MSE loss S0.20589030426255467
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.171425
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.06756430959593
Iteration: 2 || Loss: 24.067149259198786
Iteration: 3 || Loss: 24.066901530044564
Iteration: 4 || Loss: 24.06654298587884
Iteration: 5 || Loss: 24.066142348674056
Iteration: 6 || Loss: 24.066142348674056
saving ADAM checkpoint...
Sum of params:-107.17156
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.066142348674056
Iteration: 2 || Loss: 24.03228879982276
Iteration: 3 || Loss: 23.865181360961028
Iteration: 4 || Loss: 23.835201854858585
Iteration: 5 || Loss: 23.610265299523032
Iteration: 6 || Loss: 23.572709067480236
Iteration: 7 || Loss: 23.557357014022585
Iteration: 8 || Loss: 23.528166194630867
Iteration: 9 || Loss: 23.499694171301687
Iteration: 10 || Loss: 23.489625537595533
Iteration: 11 || Loss: 23.47576624593568
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.14401
Epoch 237 loss:23.47576624593568
MSE loss S0.4067834657490467
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.14401
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.4022086454457
Iteration: 2 || Loss: 68.40179844117095
Iteration: 3 || Loss: 68.4014879668858
Iteration: 4 || Loss: 68.40117801254743
Iteration: 5 || Loss: 68.40084248061748
Iteration: 6 || Loss: 68.40084248061748
saving ADAM checkpoint...
Sum of params:-107.14411
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.40084248061748
Iteration: 2 || Loss: 68.39778398650012
Iteration: 3 || Loss: 68.38394555090083
Iteration: 4 || Loss: 68.15578658405445
Iteration: 5 || Loss: 68.13170154238477
Iteration: 6 || Loss: 68.06852916038238
Iteration: 7 || Loss: 68.02385228576462
Iteration: 8 || Loss: 68.00527372616553
Iteration: 9 || Loss: 67.99645097087895
Iteration: 10 || Loss: 67.97841339960851
Iteration: 11 || Loss: 67.9619219014274
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.18878
Epoch 237 loss:67.9619219014274
MSE loss S0.9818619897643245
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.952489270706028
MSE loss S0.35098947648139994
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:110.38584302907645
MSE loss S2.1285753999101864
waveform batch: 2/2
Test loss - extrapolation:53.58382045003281
MSE loss S1.2357964559863124
Epoch 237 mean train loss:3.361376089932001
Epoch 237 mean test loss - interpolation:3.992081545117671
Epoch 237 mean test loss - extrapolation:13.664138623259106
Start training epoch 238
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.18878
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.403940558172533
Iteration: 2 || Loss: 6.403055006638015
Iteration: 3 || Loss: 6.402236063317511
Iteration: 4 || Loss: 6.401380418874212
Iteration: 5 || Loss: 6.400492338616059
Iteration: 6 || Loss: 6.400492338616059
saving ADAM checkpoint...
Sum of params:-107.18869
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.400492338616059
Iteration: 2 || Loss: 6.378768729460931
Iteration: 3 || Loss: 6.303450108863954
Iteration: 4 || Loss: 6.2667580892186345
Iteration: 5 || Loss: 6.147482025476891
Iteration: 6 || Loss: 6.0945735856839995
Iteration: 7 || Loss: 6.079262808332718
Iteration: 8 || Loss: 6.073320402638644
Iteration: 9 || Loss: 6.069904837165772
Iteration: 10 || Loss: 6.063664015883551
Iteration: 11 || Loss: 6.04150982199778
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.20334
Epoch 238 loss:6.04150982199778
MSE loss S0.20607043192047406
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.20334
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.06285703230166
Iteration: 2 || Loss: 24.062524331563658
Iteration: 3 || Loss: 24.062212339197227
Iteration: 4 || Loss: 24.061867805760865
Iteration: 5 || Loss: 24.06150715687015
Iteration: 6 || Loss: 24.06150715687015
saving ADAM checkpoint...
Sum of params:-107.20346
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.06150715687015
Iteration: 2 || Loss: 24.030786538071126
Iteration: 3 || Loss: 23.85786081112257
Iteration: 4 || Loss: 23.83026772161907
Iteration: 5 || Loss: 23.606006455809574
Iteration: 6 || Loss: 23.569060019226896
Iteration: 7 || Loss: 23.55411063534438
Iteration: 8 || Loss: 23.52490098012327
Iteration: 9 || Loss: 23.496969457190104
Iteration: 10 || Loss: 23.486386790311094
Iteration: 11 || Loss: 23.473077579445352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.175934
Epoch 238 loss:23.473077579445352
MSE loss S0.40647556709085475
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.175934
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.37009059646435
Iteration: 2 || Loss: 68.36964307562087
Iteration: 3 || Loss: 68.3693758409481
Iteration: 4 || Loss: 68.36903955136953
Iteration: 5 || Loss: 68.36868673749308
Iteration: 6 || Loss: 68.36868673749308
saving ADAM checkpoint...
Sum of params:-107.176056
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.36868673749308
Iteration: 2 || Loss: 68.36576891405247
Iteration: 3 || Loss: 68.35198529661432
Iteration: 4 || Loss: 68.12092420578642
Iteration: 5 || Loss: 68.09718766328912
Iteration: 6 || Loss: 68.03562350517122
Iteration: 7 || Loss: 67.99076479952005
Iteration: 8 || Loss: 67.97220044583626
Iteration: 9 || Loss: 67.96342649686785
Iteration: 10 || Loss: 67.94542718346035
Iteration: 11 || Loss: 67.92900759121065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.22128
Epoch 238 loss:67.92900759121065
MSE loss S0.981679649379051
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.9471406616543
MSE loss S0.3510616326830208
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:110.13850278762281
MSE loss S2.123706845628857
waveform batch: 2/2
Test loss - extrapolation:53.477670680955114
MSE loss S1.2331697962740789
Epoch 238 mean train loss:3.360123965263923
Epoch 238 mean test loss - interpolation:3.9911901102757166
Epoch 238 mean test loss - extrapolation:13.634681122381494
Start training epoch 239
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.22128
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.402513713472702
Iteration: 2 || Loss: 6.401618518264093
Iteration: 3 || Loss: 6.4007232937331855
Iteration: 4 || Loss: 6.399888051886462
Iteration: 5 || Loss: 6.399062540720555
Iteration: 6 || Loss: 6.399062540720555
saving ADAM checkpoint...
Sum of params:-107.22119
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.399062540720555
Iteration: 2 || Loss: 6.376961636377905
Iteration: 3 || Loss: 6.3017742097923195
Iteration: 4 || Loss: 6.26641780152011
Iteration: 5 || Loss: 6.146643117597326
Iteration: 6 || Loss: 6.093773415876781
Iteration: 7 || Loss: 6.078449196568298
Iteration: 8 || Loss: 6.072605190243006
Iteration: 9 || Loss: 6.069141385036755
Iteration: 10 || Loss: 6.062934899302918
Iteration: 11 || Loss: 6.040687123807251
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.23585
Epoch 239 loss:6.040687123807251
MSE loss S0.2057084310192973
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.23585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.056461574319677
Iteration: 2 || Loss: 24.05605558988841
Iteration: 3 || Loss: 24.05578030924741
Iteration: 4 || Loss: 24.055428964841468
Iteration: 5 || Loss: 24.055161456906944
Iteration: 6 || Loss: 24.055161456906944
saving ADAM checkpoint...
Sum of params:-107.23596
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.055161456906944
Iteration: 2 || Loss: 24.02403798159505
Iteration: 3 || Loss: 23.85493739540619
Iteration: 4 || Loss: 23.82717505259988
Iteration: 5 || Loss: 23.603978819322045
Iteration: 6 || Loss: 23.566876057695833
Iteration: 7 || Loss: 23.551866650914082
Iteration: 8 || Loss: 23.522534792577343
Iteration: 9 || Loss: 23.494573850020153
Iteration: 10 || Loss: 23.48329632293074
Iteration: 11 || Loss: 23.47037980515551
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.208374
Epoch 239 loss:23.47037980515551
MSE loss S0.4064920705519475
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.208374
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.33566556670623
Iteration: 2 || Loss: 68.33530198500455
Iteration: 3 || Loss: 68.33496778169985
Iteration: 4 || Loss: 68.33461776634815
Iteration: 5 || Loss: 68.33435703195818
Iteration: 6 || Loss: 68.33435703195818
saving ADAM checkpoint...
Sum of params:-107.20848
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.33435703195818
Iteration: 2 || Loss: 68.33136636576027
Iteration: 3 || Loss: 68.31767385188179
Iteration: 4 || Loss: 68.08669385009362
Iteration: 5 || Loss: 68.06242493971074
Iteration: 6 || Loss: 68.00263622636464
Iteration: 7 || Loss: 67.95780387362112
Iteration: 8 || Loss: 67.93923576529747
Iteration: 9 || Loss: 67.93050408525866
Iteration: 10 || Loss: 67.91260374167845
Iteration: 11 || Loss: 67.89623664155224
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.25412
Epoch 239 loss:67.89623664155224
MSE loss S0.9815451971594253
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.94156171046378
MSE loss S0.35118433827192397
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:109.88696128018537
MSE loss S2.118760332902986
waveform batch: 2/2
Test loss - extrapolation:53.37044873148563
MSE loss S1.2306384009695093
Epoch 239 mean train loss:3.35887253691431
Epoch 239 mean test loss - interpolation:3.9902602850772966
Epoch 239 mean test loss - extrapolation:13.60478416763925
Start training epoch 240
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.25412
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4013018316281505
Iteration: 2 || Loss: 6.400438434761254
Iteration: 3 || Loss: 6.39956626245959
Iteration: 4 || Loss: 6.398637577306384
Iteration: 5 || Loss: 6.397940756281338
Iteration: 6 || Loss: 6.397940756281338
saving ADAM checkpoint...
Sum of params:-107.25404
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.397940756281338
Iteration: 2 || Loss: 6.37566716998416
Iteration: 3 || Loss: 6.300526055718823
Iteration: 4 || Loss: 6.2664780551192125
Iteration: 5 || Loss: 6.145802670367596
Iteration: 6 || Loss: 6.092959933623236
Iteration: 7 || Loss: 6.077626709161613
Iteration: 8 || Loss: 6.07178239545188
Iteration: 9 || Loss: 6.068019282225728
Iteration: 10 || Loss: 6.061997961170719
Iteration: 11 || Loss: 6.039849606358743
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.26855
Epoch 240 loss:6.039849606358743
MSE loss S0.20557608731853844
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.26855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.054394459716473
Iteration: 2 || Loss: 24.05396527017065
Iteration: 3 || Loss: 24.05360331385317
Iteration: 4 || Loss: 24.053282510755377
Iteration: 5 || Loss: 24.052960038134405
Iteration: 6 || Loss: 24.052960038134405
saving ADAM checkpoint...
Sum of params:-107.26868
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.052960038134405
Iteration: 2 || Loss: 24.017159045840003
Iteration: 3 || Loss: 23.851959573630072
Iteration: 4 || Loss: 23.825334692368443
Iteration: 5 || Loss: 23.60304984794285
Iteration: 6 || Loss: 23.56550531637235
Iteration: 7 || Loss: 23.549898531525706
Iteration: 8 || Loss: 23.520475594868845
Iteration: 9 || Loss: 23.492189455347557
Iteration: 10 || Loss: 23.48146557061194
Iteration: 11 || Loss: 23.46799323685048
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.24158
Epoch 240 loss:23.46799323685048
MSE loss S0.40619810929906597
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.24158
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.29909018586002
Iteration: 2 || Loss: 68.29873298302623
Iteration: 3 || Loss: 68.29841920088909
Iteration: 4 || Loss: 68.2981632543215
Iteration: 5 || Loss: 68.297839029197
Iteration: 6 || Loss: 68.297839029197
saving ADAM checkpoint...
Sum of params:-107.24168
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.297839029197
Iteration: 2 || Loss: 68.29485372955145
Iteration: 3 || Loss: 68.28121821137213
Iteration: 4 || Loss: 68.05427749134303
Iteration: 5 || Loss: 68.03077321529695
Iteration: 6 || Loss: 67.96890339008509
Iteration: 7 || Loss: 67.9249063012862
Iteration: 8 || Loss: 67.90654372018298
Iteration: 9 || Loss: 67.89781948403832
Iteration: 10 || Loss: 67.87980478544442
Iteration: 11 || Loss: 67.86359328193933
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.28721
Epoch 240 loss:67.86359328193933
MSE loss S0.9811761135714019
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.936335126165872
MSE loss S0.35119694994500383
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:109.63348745575775
MSE loss S2.113676096451551
waveform batch: 2/2
Test loss - extrapolation:53.264055089551874
MSE loss S1.2277689143701909
Epoch 240 mean train loss:3.357635728453398
Epoch 240 mean test loss - interpolation:3.989389187694312
Epoch 240 mean test loss - extrapolation:13.574795212109136
Start training epoch 241
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.28721
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.401572795776521
Iteration: 2 || Loss: 6.4007299448533
Iteration: 3 || Loss: 6.399844344348084
Iteration: 4 || Loss: 6.399082354088868
Iteration: 5 || Loss: 6.398164437298447
Iteration: 6 || Loss: 6.398164437298447
saving ADAM checkpoint...
Sum of params:-107.28713
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.398164437298447
Iteration: 2 || Loss: 6.375340512636954
Iteration: 3 || Loss: 6.299114289889643
Iteration: 4 || Loss: 6.266083684531144
Iteration: 5 || Loss: 6.145204787716944
Iteration: 6 || Loss: 6.092090562100614
Iteration: 7 || Loss: 6.077005048525864
Iteration: 8 || Loss: 6.071132135797784
Iteration: 9 || Loss: 6.06717252527584
Iteration: 10 || Loss: 6.061231017798081
Iteration: 11 || Loss: 6.039009483509982
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.301476
Epoch 241 loss:6.039009483509982
MSE loss S0.2055216846026011
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.301476
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.05148128302498
Iteration: 2 || Loss: 24.051100382958943
Iteration: 3 || Loss: 24.05083739081187
Iteration: 4 || Loss: 24.050549817214396
Iteration: 5 || Loss: 24.05024748371456
Iteration: 6 || Loss: 24.05024748371456
saving ADAM checkpoint...
Sum of params:-107.301605
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.05024748371456
Iteration: 2 || Loss: 23.995092063676694
Iteration: 3 || Loss: 23.849426290363418
Iteration: 4 || Loss: 23.82329819135123
Iteration: 5 || Loss: 23.608271192176744
Iteration: 6 || Loss: 23.566363501306867
Iteration: 7 || Loss: 23.548242213995188
Iteration: 8 || Loss: 23.519169596282612
Iteration: 9 || Loss: 23.489680044264716
Iteration: 10 || Loss: 23.480383106169334
Iteration: 11 || Loss: 23.465981060317397
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.27528
Epoch 241 loss:23.465981060317397
MSE loss S0.4062070009901232
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.27528
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.26475330772143
Iteration: 2 || Loss: 68.26439074129527
Iteration: 3 || Loss: 68.26407634535786
Iteration: 4 || Loss: 68.26382114231376
Iteration: 5 || Loss: 68.26349709236592
Iteration: 6 || Loss: 68.26349709236592
saving ADAM checkpoint...
Sum of params:-107.275375
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.26349709236592
Iteration: 2 || Loss: 68.26057483087403
Iteration: 3 || Loss: 68.24680175241251
Iteration: 4 || Loss: 68.0279089863534
Iteration: 5 || Loss: 67.99814358617535
Iteration: 6 || Loss: 67.93504232028393
Iteration: 7 || Loss: 67.89242444700416
Iteration: 8 || Loss: 67.87428370438532
Iteration: 9 || Loss: 67.86531230453905
Iteration: 10 || Loss: 67.8470820943286
Iteration: 11 || Loss: 67.83133448590051
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.320366
Epoch 241 loss:67.83133448590051
MSE loss S0.9807101699001983
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.931973323822916
MSE loss S0.35112286385581554
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:109.38271500357261
MSE loss S2.1085812007685387
waveform batch: 2/2
Test loss - extrapolation:53.16094100712162
MSE loss S1.2246627158786134
Epoch 241 mean train loss:3.3564250010250998
Epoch 241 mean test loss - interpolation:3.9886622206371527
Epoch 241 mean test loss - extrapolation:13.545304667557852
Start training epoch 242
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.320366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.403364792376605
Iteration: 2 || Loss: 6.402475700736275
Iteration: 3 || Loss: 6.4016046907452075
Iteration: 4 || Loss: 6.400705814651136
Iteration: 5 || Loss: 6.399736404012828
Iteration: 6 || Loss: 6.399736404012828
saving ADAM checkpoint...
Sum of params:-107.32029
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.399736404012828
Iteration: 2 || Loss: 6.375748072526988
Iteration: 3 || Loss: 6.297484130670906
Iteration: 4 || Loss: 6.265379861045185
Iteration: 5 || Loss: 6.144597824364654
Iteration: 6 || Loss: 6.090911279338464
Iteration: 7 || Loss: 6.076382458251264
Iteration: 8 || Loss: 6.070437518502218
Iteration: 9 || Loss: 6.066699367405193
Iteration: 10 || Loss: 6.06071668635647
Iteration: 11 || Loss: 6.038298865034898
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.33444
Epoch 242 loss:6.038298865034898
MSE loss S0.20544017058235642
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.33444
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.05215168756254
Iteration: 2 || Loss: 24.05178382052164
Iteration: 3 || Loss: 24.05155175896024
Iteration: 4 || Loss: 24.0511052275148
Iteration: 5 || Loss: 24.050789879671125
Iteration: 6 || Loss: 24.050789879671125
saving ADAM checkpoint...
Sum of params:-107.33458
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.050789879671125
Iteration: 2 || Loss: 24.001490928183166
Iteration: 3 || Loss: 23.84544676037906
Iteration: 4 || Loss: 23.821319107525607
Iteration: 5 || Loss: 23.603161127043137
Iteration: 6 || Loss: 23.56320294864816
Iteration: 7 || Loss: 23.54603649538448
Iteration: 8 || Loss: 23.517205818690627
Iteration: 9 || Loss: 23.487282109647783
Iteration: 10 || Loss: 23.477952883125866
Iteration: 11 || Loss: 23.463788719319123
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.30833
Epoch 242 loss:23.463788719319123
MSE loss S0.4061368807683798
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.30833
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.23202484188116
Iteration: 2 || Loss: 68.23167229977531
Iteration: 3 || Loss: 68.23135506287048
Iteration: 4 || Loss: 68.23105223017139
Iteration: 5 || Loss: 68.2307307683439
Iteration: 6 || Loss: 68.2307307683439
saving ADAM checkpoint...
Sum of params:-107.3084
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.2307307683439
Iteration: 2 || Loss: 68.22791804132603
Iteration: 3 || Loss: 68.21422786698093
Iteration: 4 || Loss: 67.99765953466748
Iteration: 5 || Loss: 67.96682981900635
Iteration: 6 || Loss: 67.90313392000333
Iteration: 7 || Loss: 67.86036748463569
Iteration: 8 || Loss: 67.84215096103604
Iteration: 9 || Loss: 67.83325750876983
Iteration: 10 || Loss: 67.8150024160029
Iteration: 11 || Loss: 67.79930770118588
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.353546
Epoch 242 loss:67.79930770118588
MSE loss S0.9804681058076349
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.92631355141736
MSE loss S0.35121005115794984
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:109.12494001013515
MSE loss S2.1035026233294527
waveform batch: 2/2
Test loss - extrapolation:53.054331514868785
MSE loss S1.2220929826836207
Epoch 242 mean train loss:3.355220527087583
Epoch 242 mean test loss - interpolation:3.9877189252362264
Epoch 242 mean test loss - extrapolation:13.514939293750329
Start training epoch 243
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.353546
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.403121442342896
Iteration: 2 || Loss: 6.402242806469176
Iteration: 3 || Loss: 6.401303332575922
Iteration: 4 || Loss: 6.400443392498891
Iteration: 5 || Loss: 6.39953050514515
Iteration: 6 || Loss: 6.39953050514515
saving ADAM checkpoint...
Sum of params:-107.353455
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.39953050514515
Iteration: 2 || Loss: 6.3754935170484375
Iteration: 3 || Loss: 6.296494419751797
Iteration: 4 || Loss: 6.265268946914905
Iteration: 5 || Loss: 6.143746810937426
Iteration: 6 || Loss: 6.090150481749217
Iteration: 7 || Loss: 6.075624838955581
Iteration: 8 || Loss: 6.069751231731775
Iteration: 9 || Loss: 6.0660053783868
Iteration: 10 || Loss: 6.05996258409778
Iteration: 11 || Loss: 6.037666273931986
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.367485
Epoch 243 loss:6.037666273931986
MSE loss S0.20544894934728958
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.367485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.050317221751314
Iteration: 2 || Loss: 24.05007677267593
Iteration: 3 || Loss: 24.0498565369536
Iteration: 4 || Loss: 24.04947306179917
Iteration: 5 || Loss: 24.04918122155598
Iteration: 6 || Loss: 24.04918122155598
saving ADAM checkpoint...
Sum of params:-107.36768
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.04918122155598
Iteration: 2 || Loss: 23.986557337325316
Iteration: 3 || Loss: 23.84398278851922
Iteration: 4 || Loss: 23.820159719620438
Iteration: 5 || Loss: 23.606132678675266
Iteration: 6 || Loss: 23.56209169934881
Iteration: 7 || Loss: 23.543543029605633
Iteration: 8 || Loss: 23.514734961550186
Iteration: 9 || Loss: 23.484709288501037
Iteration: 10 || Loss: 23.475828285383095
Iteration: 11 || Loss: 23.461308814051648
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.34193
Epoch 243 loss:23.461308814051648
MSE loss S0.40604582102388537
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.34193
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.19825296935426
Iteration: 2 || Loss: 68.19794329537413
Iteration: 3 || Loss: 68.1976194382104
Iteration: 4 || Loss: 68.19726998353158
Iteration: 5 || Loss: 68.1970020245017
Iteration: 6 || Loss: 68.1970020245017
saving ADAM checkpoint...
Sum of params:-107.341995
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.1970020245017
Iteration: 2 || Loss: 68.19409473827567
Iteration: 3 || Loss: 68.18012583714723
Iteration: 4 || Loss: 67.96237084639237
Iteration: 5 || Loss: 67.93255679607549
Iteration: 6 || Loss: 67.86994690549082
Iteration: 7 || Loss: 67.82840134444261
Iteration: 8 || Loss: 67.81025682218456
Iteration: 9 || Loss: 67.80127642000336
Iteration: 10 || Loss: 67.78297534346865
Iteration: 11 || Loss: 67.76734265996828
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.3871
Epoch 243 loss:67.76734265996828
MSE loss S0.9802127983695648
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.920744528998405
MSE loss S0.351348799430412
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:108.86315411196833
MSE loss S2.098201256002755
waveform batch: 2/2
Test loss - extrapolation:52.946453995720134
MSE loss S1.2194070727997877
Epoch 243 mean train loss:3.354010956825928
Epoch 243 mean test loss - interpolation:3.9867907548330677
Epoch 243 mean test loss - extrapolation:13.484134008974038
Start training epoch 244
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.3871
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.402319986707304
Iteration: 2 || Loss: 6.401476242760134
Iteration: 3 || Loss: 6.400509849385222
Iteration: 4 || Loss: 6.399610588654562
Iteration: 5 || Loss: 6.398715785349434
Iteration: 6 || Loss: 6.398715785349434
saving ADAM checkpoint...
Sum of params:-107.387024
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.398715785349434
Iteration: 2 || Loss: 6.374906071893034
Iteration: 3 || Loss: 6.295464621978245
Iteration: 4 || Loss: 6.265229935614004
Iteration: 5 || Loss: 6.143084970231468
Iteration: 6 || Loss: 6.08950992399907
Iteration: 7 || Loss: 6.074931465539063
Iteration: 8 || Loss: 6.0690842370408316
Iteration: 9 || Loss: 6.065245451741517
Iteration: 10 || Loss: 6.059132311944894
Iteration: 11 || Loss: 6.0368791621592175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.40089
Epoch 244 loss:6.0368791621592175
MSE loss S0.2054471311691901
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.40089
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.044754168309492
Iteration: 2 || Loss: 24.044353169365785
Iteration: 3 || Loss: 24.044106638168852
Iteration: 4 || Loss: 24.043823727888523
Iteration: 5 || Loss: 24.043505271372556
Iteration: 6 || Loss: 24.043505271372556
saving ADAM checkpoint...
Sum of params:-107.4011
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.043505271372556
Iteration: 2 || Loss: 23.973974907717835
Iteration: 3 || Loss: 23.838841272230688
Iteration: 4 || Loss: 23.815285359873464
Iteration: 5 || Loss: 23.607103426602638
Iteration: 6 || Loss: 23.560346437544954
Iteration: 7 || Loss: 23.541154021173313
Iteration: 8 || Loss: 23.51258709831267
Iteration: 9 || Loss: 23.482102577303337
Iteration: 10 || Loss: 23.472091132624943
Iteration: 11 || Loss: 23.458652034310376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.37547
Epoch 244 loss:23.458652034310376
MSE loss S0.40564925864128076
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.37547
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.16412688983456
Iteration: 2 || Loss: 68.16376071340783
Iteration: 3 || Loss: 68.16345310557965
Iteration: 4 || Loss: 68.16311482713719
Iteration: 5 || Loss: 68.16286924494017
Iteration: 6 || Loss: 68.16286924494017
saving ADAM checkpoint...
Sum of params:-107.375534
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.16286924494017
Iteration: 2 || Loss: 68.1598821895821
Iteration: 3 || Loss: 68.1459958496669
Iteration: 4 || Loss: 67.93202072675403
Iteration: 5 || Loss: 67.8999763728195
Iteration: 6 || Loss: 67.83763088728584
Iteration: 7 || Loss: 67.79621394224282
Iteration: 8 || Loss: 67.7782545334769
Iteration: 9 || Loss: 67.76931391277824
Iteration: 10 || Loss: 67.75117455853683
Iteration: 11 || Loss: 67.73552072848409
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.42092
Epoch 244 loss:67.73552072848409
MSE loss S0.9800153093832691
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.915405266618496
MSE loss S0.3514844971350374
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:108.60501455508768
MSE loss S2.0930296038229255
waveform batch: 2/2
Test loss - extrapolation:52.841514590040916
MSE loss S1.2168353811579475
Epoch 244 mean train loss:3.35279489396392
Epoch 244 mean test loss - interpolation:3.9859008777697493
Epoch 244 mean test loss - extrapolation:13.453877428760714
Start training epoch 245
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.42092
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.401071670715173
Iteration: 2 || Loss: 6.40016132207024
Iteration: 3 || Loss: 6.399299154243432
Iteration: 4 || Loss: 6.398377526524351
Iteration: 5 || Loss: 6.397530678374574
Iteration: 6 || Loss: 6.397530678374574
saving ADAM checkpoint...
Sum of params:-107.420845
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.397530678374574
Iteration: 2 || Loss: 6.373866408089359
Iteration: 3 || Loss: 6.294287838362506
Iteration: 4 || Loss: 6.2651516184087
Iteration: 5 || Loss: 6.142277655586634
Iteration: 6 || Loss: 6.088839526575353
Iteration: 7 || Loss: 6.074262347645534
Iteration: 8 || Loss: 6.06838176366278
Iteration: 9 || Loss: 6.064380589263107
Iteration: 10 || Loss: 6.058458092006662
Iteration: 11 || Loss: 6.036117632645672
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.434654
Epoch 245 loss:6.036117632645672
MSE loss S0.20510178079937086
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.434654
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.041526845433943
Iteration: 2 || Loss: 24.041317472072546
Iteration: 3 || Loss: 24.04099092794686
Iteration: 4 || Loss: 24.040720371319683
Iteration: 5 || Loss: 24.040428215229685
Iteration: 6 || Loss: 24.040428215229685
saving ADAM checkpoint...
Sum of params:-107.43485
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.040428215229685
Iteration: 2 || Loss: 23.971444894088695
Iteration: 3 || Loss: 23.836535579196575
Iteration: 4 || Loss: 23.814125689237365
Iteration: 5 || Loss: 23.604967353004497
Iteration: 6 || Loss: 23.558395125638448
Iteration: 7 || Loss: 23.539388966312945
Iteration: 8 || Loss: 23.510466587487794
Iteration: 9 || Loss: 23.479903190352257
Iteration: 10 || Loss: 23.469802433902082
Iteration: 11 || Loss: 23.45623595612052
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.40932
Epoch 245 loss:23.45623595612052
MSE loss S0.4054931518300048
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.40932
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.1328240370181
Iteration: 2 || Loss: 68.13243103926177
Iteration: 3 || Loss: 68.13210160660095
Iteration: 4 || Loss: 68.13179234672062
Iteration: 5 || Loss: 68.13147327246843
Iteration: 6 || Loss: 68.13147327246843
saving ADAM checkpoint...
Sum of params:-107.40941
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.13147327246843
Iteration: 2 || Loss: 68.12848481951814
Iteration: 3 || Loss: 68.11466237105205
Iteration: 4 || Loss: 67.89371105713107
Iteration: 5 || Loss: 67.86780279722974
Iteration: 6 || Loss: 67.80586687263785
Iteration: 7 || Loss: 67.76465569944678
Iteration: 8 || Loss: 67.74651594617465
Iteration: 9 || Loss: 67.73765140732895
Iteration: 10 || Loss: 67.71953556578784
Iteration: 11 || Loss: 67.70379388209066
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.45494
Epoch 245 loss:67.70379388209066
MSE loss S0.9799682165208737
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.909628149460996
MSE loss S0.3517149177898693
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:108.34453523872963
MSE loss S2.0878909697196653
waveform batch: 2/2
Test loss - extrapolation:52.73622098800429
MSE loss S1.2143950886761417
Epoch 245 mean train loss:3.3515912920985125
Epoch 245 mean test loss - interpolation:3.984938024910166
Epoch 245 mean test loss - extrapolation:13.423396352227826
Start training epoch 246
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.45494
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.399421215466298
Iteration: 2 || Loss: 6.398517376439455
Iteration: 3 || Loss: 6.397550743357869
Iteration: 4 || Loss: 6.396724807372011
Iteration: 5 || Loss: 6.395932109844483
Iteration: 6 || Loss: 6.395932109844483
saving ADAM checkpoint...
Sum of params:-107.45483
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.395932109844483
Iteration: 2 || Loss: 6.372731202920244
Iteration: 3 || Loss: 6.293554176801363
Iteration: 4 || Loss: 6.265323113596586
Iteration: 5 || Loss: 6.141493452431623
Iteration: 6 || Loss: 6.088245778756958
Iteration: 7 || Loss: 6.073506968010875
Iteration: 8 || Loss: 6.067723783349583
Iteration: 9 || Loss: 6.063395932172264
Iteration: 10 || Loss: 6.0574527921676395
Iteration: 11 || Loss: 6.035337244788013
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.46858
Epoch 246 loss:6.035337244788013
MSE loss S0.2051346886491161
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.46858
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.041601439751155
Iteration: 2 || Loss: 24.041208335159254
Iteration: 3 || Loss: 24.040980061348815
Iteration: 4 || Loss: 24.040719755249793
Iteration: 5 || Loss: 24.040478850021486
Iteration: 6 || Loss: 24.040478850021486
saving ADAM checkpoint...
Sum of params:-107.46878
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.040478850021486
Iteration: 2 || Loss: 23.9467395955932
Iteration: 3 || Loss: 23.839778415346167
Iteration: 4 || Loss: 23.81645269576365
Iteration: 5 || Loss: 23.616106364163382
Iteration: 6 || Loss: 23.557939314409673
Iteration: 7 || Loss: 23.537487874388052
Iteration: 8 || Loss: 23.508805152528364
Iteration: 9 || Loss: 23.477638660821682
Iteration: 10 || Loss: 23.46857287316311
Iteration: 11 || Loss: 23.454188073157756
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.443924
Epoch 246 loss:23.454188073157756
MSE loss S0.40610381668977696
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.443924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.10686211238269
Iteration: 2 || Loss: 68.10650253551398
Iteration: 3 || Loss: 68.10616000396017
Iteration: 4 || Loss: 68.10590200669029
Iteration: 5 || Loss: 68.1056271009075
Iteration: 6 || Loss: 68.1056271009075
saving ADAM checkpoint...
Sum of params:-107.443985
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.1056271009075
Iteration: 2 || Loss: 68.10258587464403
Iteration: 3 || Loss: 68.08856026483122
Iteration: 4 || Loss: 67.85772060510055
Iteration: 5 || Loss: 67.83328242130564
Iteration: 6 || Loss: 67.77261331659113
Iteration: 7 || Loss: 67.73411656095122
Iteration: 8 || Loss: 67.7152448556177
Iteration: 9 || Loss: 67.70611333296661
Iteration: 10 || Loss: 67.68780604847011
Iteration: 11 || Loss: 67.67230570776529
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.48896
Epoch 246 loss:67.67230570776529
MSE loss S0.9796888765267997
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.904450113043325
MSE loss S0.351780566358692
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:108.08253516957261
MSE loss S2.0825267987166707
waveform batch: 2/2
Test loss - extrapolation:52.63206681275477
MSE loss S1.211656640334433
Epoch 246 mean train loss:3.3504079664038295
Epoch 246 mean test loss - interpolation:3.9840750188405543
Epoch 246 mean test loss - extrapolation:13.392883498527283
Start training epoch 247
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.48896
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.399697184923943
Iteration: 2 || Loss: 6.39888408135661
Iteration: 3 || Loss: 6.397996991403776
Iteration: 4 || Loss: 6.397033065680901
Iteration: 5 || Loss: 6.396312339204627
Iteration: 6 || Loss: 6.396312339204627
saving ADAM checkpoint...
Sum of params:-107.488884
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.396312339204627
Iteration: 2 || Loss: 6.373047837242723
Iteration: 3 || Loss: 6.292478267486615
Iteration: 4 || Loss: 6.264886896930313
Iteration: 5 || Loss: 6.141013847402594
Iteration: 6 || Loss: 6.087442704339161
Iteration: 7 || Loss: 6.072995750173344
Iteration: 8 || Loss: 6.067158204719329
Iteration: 9 || Loss: 6.062829489451354
Iteration: 10 || Loss: 6.056900825400326
Iteration: 11 || Loss: 6.03463386712946
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.50237
Epoch 247 loss:6.03463386712946
MSE loss S0.20506845162383097
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.50237
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.037707188899535
Iteration: 2 || Loss: 24.03740411062579
Iteration: 3 || Loss: 24.03719276062837
Iteration: 4 || Loss: 24.036824272094027
Iteration: 5 || Loss: 24.036574655082543
Iteration: 6 || Loss: 24.036574655082543
saving ADAM checkpoint...
Sum of params:-107.50257
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.036574655082543
Iteration: 2 || Loss: 23.94841741058137
Iteration: 3 || Loss: 23.83263260846341
Iteration: 4 || Loss: 23.811148541894234
Iteration: 5 || Loss: 23.610320480190143
Iteration: 6 || Loss: 23.55530736143442
Iteration: 7 || Loss: 23.535173774057895
Iteration: 8 || Loss: 23.506514805472474
Iteration: 9 || Loss: 23.475079880100175
Iteration: 10 || Loss: 23.464727045477456
Iteration: 11 || Loss: 23.45157304407079
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.47775
Epoch 247 loss:23.45157304407079
MSE loss S0.4052787054044431
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.47775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.0736403543355
Iteration: 2 || Loss: 68.07329781196287
Iteration: 3 || Loss: 68.07293887128313
Iteration: 4 || Loss: 68.07263460127879
Iteration: 5 || Loss: 68.07227112613246
Iteration: 6 || Loss: 68.07227112613246
saving ADAM checkpoint...
Sum of params:-107.47782
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.07227112613246
Iteration: 2 || Loss: 68.06925187588193
Iteration: 3 || Loss: 68.05510024790088
Iteration: 4 || Loss: 67.82435793348141
Iteration: 5 || Loss: 67.80078397272759
Iteration: 6 || Loss: 67.74166257346083
Iteration: 7 || Loss: 67.70244574291108
Iteration: 8 || Loss: 67.68379074107862
Iteration: 9 || Loss: 67.67486767547454
Iteration: 10 || Loss: 67.65680068825515
Iteration: 11 || Loss: 67.64106963555567
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.523186
Epoch 247 loss:67.64106963555567
MSE loss S0.9796985151063975
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.89822563862076
MSE loss S0.35212981613766436
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:107.82048307855294
MSE loss S2.0773577036210096
waveform batch: 2/2
Test loss - extrapolation:52.524995367989604
MSE loss S1.2093391846561587
Epoch 247 mean train loss:3.349216432646756
Epoch 247 mean test loss - interpolation:3.9830376064367936
Epoch 247 mean test loss - extrapolation:13.362123203878545
Start training epoch 248
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.523186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.397365266954928
Iteration: 2 || Loss: 6.396441999914897
Iteration: 3 || Loss: 6.395616501887436
Iteration: 4 || Loss: 6.394786494347114
Iteration: 5 || Loss: 6.393924648464985
Iteration: 6 || Loss: 6.393924648464985
saving ADAM checkpoint...
Sum of params:-107.5231
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.393924648464985
Iteration: 2 || Loss: 6.371980359791946
Iteration: 3 || Loss: 6.2926444686919645
Iteration: 4 || Loss: 6.265930708274289
Iteration: 5 || Loss: 6.1398358956381935
Iteration: 6 || Loss: 6.086802471200288
Iteration: 7 || Loss: 6.07223356513364
Iteration: 8 || Loss: 6.066458585746557
Iteration: 9 || Loss: 6.062011651717932
Iteration: 10 || Loss: 6.055939914054016
Iteration: 11 || Loss: 6.0338675475384465
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.536575
Epoch 248 loss:6.0338675475384465
MSE loss S0.2050414821027277
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.536575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.035607749482814
Iteration: 2 || Loss: 24.035313672651096
Iteration: 3 || Loss: 24.034949623309615
Iteration: 4 || Loss: 24.03480171188328
Iteration: 5 || Loss: 24.034438185205786
Iteration: 6 || Loss: 24.034438185205786
saving ADAM checkpoint...
Sum of params:-107.53676
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.034438185205786
Iteration: 2 || Loss: 23.931828630590612
Iteration: 3 || Loss: 23.83268881769661
Iteration: 4 || Loss: 23.810884692327207
Iteration: 5 || Loss: 23.615135376475457
Iteration: 6 || Loss: 23.553711845307056
Iteration: 7 || Loss: 23.53304517311242
Iteration: 8 || Loss: 23.50419449173167
Iteration: 9 || Loss: 23.472684683041134
Iteration: 10 || Loss: 23.463369698460873
Iteration: 11 || Loss: 23.449404855865108
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.51221
Epoch 248 loss:23.449404855865108
MSE loss S0.40598095309812476
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.51221
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.0449583970954
Iteration: 2 || Loss: 68.04462877430673
Iteration: 3 || Loss: 68.04431910804459
Iteration: 4 || Loss: 68.0440387740418
Iteration: 5 || Loss: 68.04375787478641
Iteration: 6 || Loss: 68.04375787478641
saving ADAM checkpoint...
Sum of params:-107.51224
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.04375787478641
Iteration: 2 || Loss: 68.040899391951
Iteration: 3 || Loss: 68.02697462695667
Iteration: 4 || Loss: 67.7951007777222
Iteration: 5 || Loss: 67.77045659100611
Iteration: 6 || Loss: 67.709579949734
Iteration: 7 || Loss: 67.67176103006034
Iteration: 8 || Loss: 67.6528595567368
Iteration: 9 || Loss: 67.64364548594112
Iteration: 10 || Loss: 67.62528794105124
Iteration: 11 || Loss: 67.6100391159747
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.55735
Epoch 248 loss:67.6100391159747
MSE loss S0.979282910999239
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.894599673536256
MSE loss S0.3519678391284397
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:107.57700448952995
MSE loss S2.07228820900891
waveform batch: 2/2
Test loss - extrapolation:52.43331003241414
MSE loss S1.2064699177740812
Epoch 248 mean train loss:3.348045224806147
Epoch 248 mean test loss - interpolation:3.9824332789227093
Epoch 248 mean test loss - extrapolation:13.334192876828675
Start training epoch 249
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.55735
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3982280199017865
Iteration: 2 || Loss: 6.397369412265524
Iteration: 3 || Loss: 6.396469955454601
Iteration: 4 || Loss: 6.395536381185243
Iteration: 5 || Loss: 6.394658303023313
Iteration: 6 || Loss: 6.394658303023313
saving ADAM checkpoint...
Sum of params:-107.55724
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.394658303023313
Iteration: 2 || Loss: 6.370981774107392
Iteration: 3 || Loss: 6.28953642153806
Iteration: 4 || Loss: 6.263988290726211
Iteration: 5 || Loss: 6.139580999604207
Iteration: 6 || Loss: 6.085791503558243
Iteration: 7 || Loss: 6.071551159598974
Iteration: 8 || Loss: 6.065807973370544
Iteration: 9 || Loss: 6.0614140172287465
Iteration: 10 || Loss: 6.05539277601726
Iteration: 11 || Loss: 6.033105247605376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.57054
Epoch 249 loss:6.033105247605376
MSE loss S0.20489930835455344
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.57054
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.0341322100944
Iteration: 2 || Loss: 24.033831047670926
Iteration: 3 || Loss: 24.033571191686242
Iteration: 4 || Loss: 24.03321408797622
Iteration: 5 || Loss: 24.032938361950645
Iteration: 6 || Loss: 24.032938361950645
saving ADAM checkpoint...
Sum of params:-107.57073
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.032938361950645
Iteration: 2 || Loss: 23.910132261654585
Iteration: 3 || Loss: 23.831728686595888
Iteration: 4 || Loss: 23.811479396904396
Iteration: 5 || Loss: 23.62060785920006
Iteration: 6 || Loss: 23.552126437447768
Iteration: 7 || Loss: 23.531197053755452
Iteration: 8 || Loss: 23.502800226794964
Iteration: 9 || Loss: 23.470694235952664
Iteration: 10 || Loss: 23.464343909937426
Iteration: 11 || Loss: 23.44777088933644
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.54614
Epoch 249 loss:23.44777088933644
MSE loss S0.4062659668118454
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.54614
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.01733025005485
Iteration: 2 || Loss: 68.01700540481376
Iteration: 3 || Loss: 68.01667860000056
Iteration: 4 || Loss: 68.01640635538953
Iteration: 5 || Loss: 68.01609129315372
Iteration: 6 || Loss: 68.01609129315372
saving ADAM checkpoint...
Sum of params:-107.54614
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.01609129315372
Iteration: 2 || Loss: 68.01353762219202
Iteration: 3 || Loss: 67.99991863325945
Iteration: 4 || Loss: 67.77320038302543
Iteration: 5 || Loss: 67.74352680868611
Iteration: 6 || Loss: 67.67987624367778
Iteration: 7 || Loss: 67.64133375196442
Iteration: 8 || Loss: 67.62226996789383
Iteration: 9 || Loss: 67.61284510709865
Iteration: 10 || Loss: 67.59443260927081
Iteration: 11 || Loss: 67.57967276363865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.59094
Epoch 249 loss:67.57967276363865
MSE loss S0.9790176464033276
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.89040765795552
MSE loss S0.3518712441915699
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:107.34023614130373
MSE loss S2.067559988082156
waveform batch: 2/2
Test loss - extrapolation:52.34309880512164
MSE loss S1.2037191565755405
Epoch 249 mean train loss:3.3469154793303613
Epoch 249 mean test loss - interpolation:3.9817346096592536
Epoch 249 mean test loss - extrapolation:13.30694457886878
Start training epoch 250
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.59094
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.399748287325354
Iteration: 2 || Loss: 6.398918554719452
Iteration: 3 || Loss: 6.397991831475398
Iteration: 4 || Loss: 6.396996338036065
Iteration: 5 || Loss: 6.396141749812651
Iteration: 6 || Loss: 6.396141749812651
saving ADAM checkpoint...
Sum of params:-107.590866
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.396141749812651
Iteration: 2 || Loss: 6.3713588440166955
Iteration: 3 || Loss: 6.287571784901744
Iteration: 4 || Loss: 6.263209300951857
Iteration: 5 || Loss: 6.138741114681401
Iteration: 6 || Loss: 6.084095529619658
Iteration: 7 || Loss: 6.070785395461314
Iteration: 8 || Loss: 6.064938270008644
Iteration: 9 || Loss: 6.060880175506431
Iteration: 10 || Loss: 6.054725244563805
Iteration: 11 || Loss: 6.032383563237158
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.60398
Epoch 250 loss:6.032383563237158
MSE loss S0.2046290021272537
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.60398
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.03008676285968
Iteration: 2 || Loss: 24.02978549249724
Iteration: 3 || Loss: 24.029492017061365
Iteration: 4 || Loss: 24.029210342398372
Iteration: 5 || Loss: 24.02892222183429
Iteration: 6 || Loss: 24.02892222183429
saving ADAM checkpoint...
Sum of params:-107.60418
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.02892222183429
Iteration: 2 || Loss: 23.9146515445385
Iteration: 3 || Loss: 23.825457645647894
Iteration: 4 || Loss: 23.807789812912638
Iteration: 5 || Loss: 23.61456617028424
Iteration: 6 || Loss: 23.54927032181385
Iteration: 7 || Loss: 23.528767926821853
Iteration: 8 || Loss: 23.500401365065994
Iteration: 9 || Loss: 23.467992444347733
Iteration: 10 || Loss: 23.46000878356162
Iteration: 11 || Loss: 23.44507668483677
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.57971
Epoch 250 loss:23.44507668483677
MSE loss S0.405815836976156
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.57971
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.98300404433607
Iteration: 2 || Loss: 67.98269263781559
Iteration: 3 || Loss: 67.98244703483316
Iteration: 4 || Loss: 67.98209276645505
Iteration: 5 || Loss: 67.98179069905
Iteration: 6 || Loss: 67.98179069905
saving ADAM checkpoint...
Sum of params:-107.57972
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.98179069905
Iteration: 2 || Loss: 67.9791920778168
Iteration: 3 || Loss: 67.96572597683345
Iteration: 4 || Loss: 67.73607510538658
Iteration: 5 || Loss: 67.71088908097548
Iteration: 6 || Loss: 67.64891158586086
Iteration: 7 || Loss: 67.61095281439869
Iteration: 8 || Loss: 67.59202973799168
Iteration: 9 || Loss: 67.58269932479558
Iteration: 10 || Loss: 67.56424316216355
Iteration: 11 || Loss: 67.5494032934207
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.62473
Epoch 250 loss:67.5494032934207
MSE loss S0.9789013046427041
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.88523838148406
MSE loss S0.3520281226189535
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:107.08924954871144
MSE loss S2.0624403279334813
waveform batch: 2/2
Test loss - extrapolation:52.2445212584261
MSE loss S1.201407227285074
Epoch 250 mean train loss:3.3457539152239524
Epoch 250 mean test loss - interpolation:3.980873063580677
Epoch 250 mean test loss - extrapolation:13.277814233928128
Start training epoch 251
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.62473
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.397550358459007
Iteration: 2 || Loss: 6.3965801893410985
Iteration: 3 || Loss: 6.395702520862755
Iteration: 4 || Loss: 6.394797586786576
Iteration: 5 || Loss: 6.393923974575674
Iteration: 6 || Loss: 6.393923974575674
saving ADAM checkpoint...
Sum of params:-107.62466
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.393923974575674
Iteration: 2 || Loss: 6.369360739079315
Iteration: 3 || Loss: 6.286139018041636
Iteration: 4 || Loss: 6.262808553110838
Iteration: 5 || Loss: 6.137995605016084
Iteration: 6 || Loss: 6.083717991430105
Iteration: 7 || Loss: 6.070043291150048
Iteration: 8 || Loss: 6.064204312700661
Iteration: 9 || Loss: 6.060113589165232
Iteration: 10 || Loss: 6.053879772374708
Iteration: 11 || Loss: 6.031572746709566
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.63774
Epoch 251 loss:6.031572746709566
MSE loss S0.20436849008735433
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.63774
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.026442308683503
Iteration: 2 || Loss: 24.0261291925039
Iteration: 3 || Loss: 24.025875016001464
Iteration: 4 || Loss: 24.02563082500531
Iteration: 5 || Loss: 24.025326542629706
Iteration: 6 || Loss: 24.025326542629706
saving ADAM checkpoint...
Sum of params:-107.63794
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.025326542629706
Iteration: 2 || Loss: 23.88824135505499
Iteration: 3 || Loss: 23.824273328563375
Iteration: 4 || Loss: 23.807790543572057
Iteration: 5 || Loss: 23.618763854361482
Iteration: 6 || Loss: 23.547726254289678
Iteration: 7 || Loss: 23.52696024250308
Iteration: 8 || Loss: 23.49861761410729
Iteration: 9 || Loss: 23.46589605794633
Iteration: 10 || Loss: 23.459435040757338
Iteration: 11 || Loss: 23.442829031711398
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.613266
Epoch 251 loss:23.442829031711398
MSE loss S0.4059264714688087
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.613266
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.95484241132309
Iteration: 2 || Loss: 67.95453753795499
Iteration: 3 || Loss: 67.95422580744818
Iteration: 4 || Loss: 67.95393436532743
Iteration: 5 || Loss: 67.95360525101464
Iteration: 6 || Loss: 67.95360525101464
saving ADAM checkpoint...
Sum of params:-107.6133
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.95360525101464
Iteration: 2 || Loss: 67.95111916805295
Iteration: 3 || Loss: 67.9377307366751
Iteration: 4 || Loss: 67.70828198875668
Iteration: 5 || Loss: 67.68259159345253
Iteration: 6 || Loss: 67.61959976033083
Iteration: 7 || Loss: 67.58089625798968
Iteration: 8 || Loss: 67.56187814063851
Iteration: 9 || Loss: 67.55257193018102
Iteration: 10 || Loss: 67.5341966055698
Iteration: 11 || Loss: 67.5195463798554
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.65847
Epoch 251 loss:67.5195463798554
MSE loss S0.9788015480381691
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.88066163826398
MSE loss S0.3520853311554011
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:106.85191495783428
MSE loss S2.057672305984239
waveform batch: 2/2
Test loss - extrapolation:52.1532939924811
MSE loss S1.1989640558904135
Epoch 251 mean train loss:3.34461890200953
Epoch 251 mean test loss - interpolation:3.9801102730439966
Epoch 251 mean test loss - extrapolation:13.25043407919295
Start training epoch 252
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.65847
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.396901148295084
Iteration: 2 || Loss: 6.395974823398385
Iteration: 3 || Loss: 6.395025327976051
Iteration: 4 || Loss: 6.394095052641799
Iteration: 5 || Loss: 6.393279050495157
Iteration: 6 || Loss: 6.393279050495157
saving ADAM checkpoint...
Sum of params:-107.6584
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.393279050495157
Iteration: 2 || Loss: 6.368316612432518
Iteration: 3 || Loss: 6.28445502755672
Iteration: 4 || Loss: 6.262372054699688
Iteration: 5 || Loss: 6.137195870728268
Iteration: 6 || Loss: 6.082544914508929
Iteration: 7 || Loss: 6.069211116273714
Iteration: 8 || Loss: 6.063435952465619
Iteration: 9 || Loss: 6.059140239260816
Iteration: 10 || Loss: 6.0527310582742775
Iteration: 11 || Loss: 6.03058351614425
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.671394
Epoch 252 loss:6.03058351614425
MSE loss S0.20443645443252853
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.671394
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.022952967286084
Iteration: 2 || Loss: 24.022650158298116
Iteration: 3 || Loss: 24.022361704571463
Iteration: 4 || Loss: 24.022063881372112
Iteration: 5 || Loss: 24.021806591418006
Iteration: 6 || Loss: 24.021806591418006
saving ADAM checkpoint...
Sum of params:-107.67158
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.021806591418006
Iteration: 2 || Loss: 23.86810994772437
Iteration: 3 || Loss: 23.827151656416277
Iteration: 4 || Loss: 23.80450023499462
Iteration: 5 || Loss: 23.61550831410001
Iteration: 6 || Loss: 23.546170997500116
Iteration: 7 || Loss: 23.525142598206614
Iteration: 8 || Loss: 23.49681301804438
Iteration: 9 || Loss: 23.463667446537883
Iteration: 10 || Loss: 23.45726775899897
Iteration: 11 || Loss: 23.4406960312645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.647
Epoch 252 loss:23.4406960312645
MSE loss S0.40562244130412073
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.647
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.92725099103687
Iteration: 2 || Loss: 67.92684848943577
Iteration: 3 || Loss: 67.92654287850605
Iteration: 4 || Loss: 67.92632158569103
Iteration: 5 || Loss: 67.92604240818353
Iteration: 6 || Loss: 67.92604240818353
saving ADAM checkpoint...
Sum of params:-107.647026
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.92604240818353
Iteration: 2 || Loss: 67.9235951250185
Iteration: 3 || Loss: 67.91007779438443
Iteration: 4 || Loss: 67.67824623555433
Iteration: 5 || Loss: 67.65272284766212
Iteration: 6 || Loss: 67.5900345156787
Iteration: 7 || Loss: 67.55131923381411
Iteration: 8 || Loss: 67.53219044553053
Iteration: 9 || Loss: 67.52280793202924
Iteration: 10 || Loss: 67.504446649354
Iteration: 11 || Loss: 67.4900047883673
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.69199
Epoch 252 loss:67.4900047883673
MSE loss S0.9786970518067757
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.876031229579546
MSE loss S0.3521598374320959
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:106.61451458902032
MSE loss S2.0528714307900424
waveform batch: 2/2
Test loss - extrapolation:52.0617367116667
MSE loss S1.196536988814095
Epoch 252 mean train loss:3.3434925633026227
Epoch 252 mean test loss - interpolation:3.979338538263258
Epoch 252 mean test loss - extrapolation:13.223020941723917
Start training epoch 253
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.69199
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.396399121377144
Iteration: 2 || Loss: 6.395482976704478
Iteration: 3 || Loss: 6.394578625729945
Iteration: 4 || Loss: 6.393658708669538
Iteration: 5 || Loss: 6.392793191124984
Iteration: 6 || Loss: 6.392793191124984
saving ADAM checkpoint...
Sum of params:-107.69194
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.392793191124984
Iteration: 2 || Loss: 6.36764925626068
Iteration: 3 || Loss: 6.283301745748016
Iteration: 4 || Loss: 6.262315571490785
Iteration: 5 || Loss: 6.136349562517488
Iteration: 6 || Loss: 6.081517736001819
Iteration: 7 || Loss: 6.068482766936344
Iteration: 8 || Loss: 6.062640475834505
Iteration: 9 || Loss: 6.058176428826628
Iteration: 10 || Loss: 6.051732880375709
Iteration: 11 || Loss: 6.029728314802736
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.70476
Epoch 253 loss:6.029728314802736
MSE loss S0.2043323672343746
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.70476
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.02473585836713
Iteration: 2 || Loss: 24.02448109537895
Iteration: 3 || Loss: 24.02422445247668
Iteration: 4 || Loss: 24.023900882502254
Iteration: 5 || Loss: 24.02365976465837
Iteration: 6 || Loss: 24.02365976465837
saving ADAM checkpoint...
Sum of params:-107.70497
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.02365976465837
Iteration: 2 || Loss: 23.853393279162443
Iteration: 3 || Loss: 23.82168247069722
Iteration: 4 || Loss: 23.801909639408077
Iteration: 5 || Loss: 23.61155754913819
Iteration: 6 || Loss: 23.544646880442855
Iteration: 7 || Loss: 23.52362253589774
Iteration: 8 || Loss: 23.49563652386367
Iteration: 9 || Loss: 23.46152333152379
Iteration: 10 || Loss: 23.454752948074034
Iteration: 11 || Loss: 23.438573653387355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.68051
Epoch 253 loss:23.438573653387355
MSE loss S0.4053777153685837
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.68051
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.89710130804538
Iteration: 2 || Loss: 67.89673921640889
Iteration: 3 || Loss: 67.89642323611248
Iteration: 4 || Loss: 67.89612974616054
Iteration: 5 || Loss: 67.89578868478333
Iteration: 6 || Loss: 67.89578868478333
saving ADAM checkpoint...
Sum of params:-107.680565
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.89578868478333
Iteration: 2 || Loss: 67.89340653867102
Iteration: 3 || Loss: 67.88000699898899
Iteration: 4 || Loss: 67.64813475405074
Iteration: 5 || Loss: 67.62313737689179
Iteration: 6 || Loss: 67.5607850174447
Iteration: 7 || Loss: 67.52205154311886
Iteration: 8 || Loss: 67.50302310476299
Iteration: 9 || Loss: 67.49350943262839
Iteration: 10 || Loss: 67.47507392301233
Iteration: 11 || Loss: 67.46073678104632
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.72543
Epoch 253 loss:67.46073678104632
MSE loss S0.9784979994665388
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.87101792632393
MSE loss S0.3521999979481747
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:106.37589495374911
MSE loss S2.0480624089218424
waveform batch: 2/2
Test loss - extrapolation:51.969168674472485
MSE loss S1.1941566408860667
Epoch 253 mean train loss:3.3423806465253936
Epoch 253 mean test loss - interpolation:3.978502987720655
Epoch 253 mean test loss - extrapolation:13.195421969018467
Start training epoch 254
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.72543
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.39570836272161
Iteration: 2 || Loss: 6.394866011062361
Iteration: 3 || Loss: 6.393913389873947
Iteration: 4 || Loss: 6.39302876404107
Iteration: 5 || Loss: 6.392093138235372
Iteration: 6 || Loss: 6.392093138235372
saving ADAM checkpoint...
Sum of params:-107.72537
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.392093138235372
Iteration: 2 || Loss: 6.3670138910529
Iteration: 3 || Loss: 6.281967884877298
Iteration: 4 || Loss: 6.261937844219874
Iteration: 5 || Loss: 6.135641412719515
Iteration: 6 || Loss: 6.0808117829338055
Iteration: 7 || Loss: 6.067683137542764
Iteration: 8 || Loss: 6.061859442298561
Iteration: 9 || Loss: 6.057290485553131
Iteration: 10 || Loss: 6.050843313752059
Iteration: 11 || Loss: 6.028897023340603
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.73813
Epoch 254 loss:6.028897023340603
MSE loss S0.20416778954388143
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.73813
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.02073372142631
Iteration: 2 || Loss: 24.02046085335709
Iteration: 3 || Loss: 24.020198125073257
Iteration: 4 || Loss: 24.019974262108708
Iteration: 5 || Loss: 24.01975739726536
Iteration: 6 || Loss: 24.01975739726536
saving ADAM checkpoint...
Sum of params:-107.73831
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.01975739726536
Iteration: 2 || Loss: 23.838414966732515
Iteration: 3 || Loss: 23.816280332138966
Iteration: 4 || Loss: 23.79556793593678
Iteration: 5 || Loss: 23.59960044940222
Iteration: 6 || Loss: 23.541966044366834
Iteration: 7 || Loss: 23.521482758745236
Iteration: 8 || Loss: 23.49352112935809
Iteration: 9 || Loss: 23.45911478204927
Iteration: 10 || Loss: 23.45190650648167
Iteration: 11 || Loss: 23.436079507035966
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.713715
Epoch 254 loss:23.436079507035966
MSE loss S0.4048542285221663
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.713715
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.86734923202829
Iteration: 2 || Loss: 67.8669364962351
Iteration: 3 || Loss: 67.86663234247757
Iteration: 4 || Loss: 67.86633172497224
Iteration: 5 || Loss: 67.86604283007878
Iteration: 6 || Loss: 67.86604283007878
saving ADAM checkpoint...
Sum of params:-107.71376
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.86604283007878
Iteration: 2 || Loss: 67.86365975647459
Iteration: 3 || Loss: 67.85018506857469
Iteration: 4 || Loss: 67.61607835495028
Iteration: 5 || Loss: 67.58911688524346
Iteration: 6 || Loss: 67.53226799250592
Iteration: 7 || Loss: 67.49275133293892
Iteration: 8 || Loss: 67.4737696901711
Iteration: 9 || Loss: 67.46447229147525
Iteration: 10 || Loss: 67.44609775773407
Iteration: 11 || Loss: 67.43179479734738
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.758835
Epoch 254 loss:67.43179479734738
MSE loss S0.9785249320356062
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.8665913285929
MSE loss S0.352327302229989
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:106.14527274413939
MSE loss S2.0434033617710234
waveform batch: 2/2
Test loss - extrapolation:51.880990950468195
MSE loss S1.1919623226782758
Epoch 254 mean train loss:3.341267976818067
Epoch 254 mean test loss - interpolation:3.97776522143215
Epoch 254 mean test loss - extrapolation:13.168855307883966
Start training epoch 255
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.758835
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.394017839480358
Iteration: 2 || Loss: 6.393045238663554
Iteration: 3 || Loss: 6.392127261134849
Iteration: 4 || Loss: 6.391295085020524
Iteration: 5 || Loss: 6.390353039862321
Iteration: 6 || Loss: 6.390353039862321
saving ADAM checkpoint...
Sum of params:-107.758766
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.390353039862321
Iteration: 2 || Loss: 6.365017885583418
Iteration: 3 || Loss: 6.280522267057538
Iteration: 4 || Loss: 6.26170243524533
Iteration: 5 || Loss: 6.134841112395266
Iteration: 6 || Loss: 6.080010443751166
Iteration: 7 || Loss: 6.066824449267628
Iteration: 8 || Loss: 6.061052556518366
Iteration: 9 || Loss: 6.056326559926158
Iteration: 10 || Loss: 6.049710494080221
Iteration: 11 || Loss: 6.027976404328479
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.7715
Epoch 255 loss:6.027976404328479
MSE loss S0.20402580545937649
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.7715
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.020284504516397
Iteration: 2 || Loss: 24.019962401054915
Iteration: 3 || Loss: 24.019666401806024
Iteration: 4 || Loss: 24.019475851994628
Iteration: 5 || Loss: 24.01921307102522
Iteration: 6 || Loss: 24.01921307102522
saving ADAM checkpoint...
Sum of params:-107.77166
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.01921307102522
Iteration: 2 || Loss: 23.83199182058692
Iteration: 3 || Loss: 23.81294662389958
Iteration: 4 || Loss: 23.792649715856765
Iteration: 5 || Loss: 23.593656563298882
Iteration: 6 || Loss: 23.54033530930416
Iteration: 7 || Loss: 23.520101324893464
Iteration: 8 || Loss: 23.492074849705023
Iteration: 9 || Loss: 23.456971210149977
Iteration: 10 || Loss: 23.44927888634568
Iteration: 11 || Loss: 23.43384003079426
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.74704
Epoch 255 loss:23.43384003079426
MSE loss S0.40435754597920454
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.74704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.83704550769357
Iteration: 2 || Loss: 67.83666649949441
Iteration: 3 || Loss: 67.83632958090897
Iteration: 4 || Loss: 67.8360146328892
Iteration: 5 || Loss: 67.83572397261226
Iteration: 6 || Loss: 67.83572397261226
saving ADAM checkpoint...
Sum of params:-107.74707
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.83572397261226
Iteration: 2 || Loss: 67.83344154168276
Iteration: 3 || Loss: 67.81993818817577
Iteration: 4 || Loss: 67.58692162611858
Iteration: 5 || Loss: 67.55603218972833
Iteration: 6 || Loss: 67.50376698592181
Iteration: 7 || Loss: 67.46379486606848
Iteration: 8 || Loss: 67.44490949955193
Iteration: 9 || Loss: 67.43572940357093
Iteration: 10 || Loss: 67.41731639577813
Iteration: 11 || Loss: 67.40310876537028
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.79213
Epoch 255 loss:67.40310876537028
MSE loss S0.9784274854768765
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.861871345710906
MSE loss S0.35238436573842946
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:105.9161700859728
MSE loss S2.0387745678965055
waveform batch: 2/2
Test loss - extrapolation:51.792915341064514
MSE loss S1.1896712956649853
Epoch 255 mean train loss:3.3401698344997595
Epoch 255 mean test loss - interpolation:3.976978557618484
Epoch 255 mean test loss - extrapolation:13.142423785586443
Start training epoch 256
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.79213
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.393093770977188
Iteration: 2 || Loss: 6.39211887309774
Iteration: 3 || Loss: 6.391178453336198
Iteration: 4 || Loss: 6.390355450202056
Iteration: 5 || Loss: 6.389407570912301
Iteration: 6 || Loss: 6.389407570912301
saving ADAM checkpoint...
Sum of params:-107.79207
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.389407570912301
Iteration: 2 || Loss: 6.3639941545885055
Iteration: 3 || Loss: 6.2792144476437715
Iteration: 4 || Loss: 6.261582406336811
Iteration: 5 || Loss: 6.133937074334842
Iteration: 6 || Loss: 6.0790785261943565
Iteration: 7 || Loss: 6.066072561877614
Iteration: 8 || Loss: 6.060299723767939
Iteration: 9 || Loss: 6.055800806196148
Iteration: 10 || Loss: 6.048933383098775
Iteration: 11 || Loss: 6.02696447672046
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.80483
Epoch 256 loss:6.02696447672046
MSE loss S0.20377752976364802
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.80483
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.0208610039047
Iteration: 2 || Loss: 24.02057811574328
Iteration: 3 || Loss: 24.02036509098097
Iteration: 4 || Loss: 24.020032615956556
Iteration: 5 || Loss: 24.019815804911218
Iteration: 6 || Loss: 24.019815804911218
saving ADAM checkpoint...
Sum of params:-107.80502
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.019815804911218
Iteration: 2 || Loss: 23.8586153282847
Iteration: 3 || Loss: 23.825339234334965
Iteration: 4 || Loss: 23.800358171245048
Iteration: 5 || Loss: 23.606800535554818
Iteration: 6 || Loss: 23.53987615649944
Iteration: 7 || Loss: 23.5187808462904
Iteration: 8 || Loss: 23.49009661722966
Iteration: 9 || Loss: 23.45494962477314
Iteration: 10 || Loss: 23.44781803303022
Iteration: 11 || Loss: 23.431659966303204
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.780594
Epoch 256 loss:23.431659966303204
MSE loss S0.40491174871521285
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.780594
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.80935358798962
Iteration: 2 || Loss: 67.80902771102153
Iteration: 3 || Loss: 67.8087149137582
Iteration: 4 || Loss: 67.80833742290646
Iteration: 5 || Loss: 67.80805247109149
Iteration: 6 || Loss: 67.80805247109149
saving ADAM checkpoint...
Sum of params:-107.780624
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.80805247109149
Iteration: 2 || Loss: 67.80579934159181
Iteration: 3 || Loss: 67.79239272909295
Iteration: 4 || Loss: 67.55854784345269
Iteration: 5 || Loss: 67.53131534536604
Iteration: 6 || Loss: 67.47457280322769
Iteration: 7 || Loss: 67.43566432131527
Iteration: 8 || Loss: 67.41664806591321
Iteration: 9 || Loss: 67.40716385452822
Iteration: 10 || Loss: 67.388853321448
Iteration: 11 || Loss: 67.3747023765346
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.82535
Epoch 256 loss:67.3747023765346
MSE loss S0.9779876079481986
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.857238579485042
MSE loss S0.35235118373044855
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:105.68395165980972
MSE loss S2.0338893319334375
waveform batch: 2/2
Test loss - extrapolation:51.70141981062757
MSE loss S1.1871780061547992
Epoch 256 mean train loss:3.3390802351571818
Epoch 256 mean test loss - interpolation:3.9762064299141735
Epoch 256 mean test loss - extrapolation:13.115447622536442
Start training epoch 257
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.82535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.393590678092767
Iteration: 2 || Loss: 6.392691368717221
Iteration: 3 || Loss: 6.391757207484856
Iteration: 4 || Loss: 6.390834687014995
Iteration: 5 || Loss: 6.3899029104991065
Iteration: 6 || Loss: 6.3899029104991065
saving ADAM checkpoint...
Sum of params:-107.82527
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3899029104991065
Iteration: 2 || Loss: 6.3644890005385655
Iteration: 3 || Loss: 6.278487109895997
Iteration: 4 || Loss: 6.261707303994191
Iteration: 5 || Loss: 6.133146284516022
Iteration: 6 || Loss: 6.078172073233764
Iteration: 7 || Loss: 6.065207140278671
Iteration: 8 || Loss: 6.059457082994431
Iteration: 9 || Loss: 6.0549231440516085
Iteration: 10 || Loss: 6.047970750510543
Iteration: 11 || Loss: 6.025997516177339
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.83787
Epoch 257 loss:6.025997516177339
MSE loss S0.203500644713183
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.83787
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.018216825280692
Iteration: 2 || Loss: 24.018001597943353
Iteration: 3 || Loss: 24.017694545553773
Iteration: 4 || Loss: 24.017527205370403
Iteration: 5 || Loss: 24.017224049655944
Iteration: 6 || Loss: 24.017224049655944
saving ADAM checkpoint...
Sum of params:-107.838066
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.017224049655944
Iteration: 2 || Loss: 23.825088557630963
Iteration: 3 || Loss: 23.811157501360032
Iteration: 4 || Loss: 23.789564806741662
Iteration: 5 || Loss: 23.580771923381462
Iteration: 6 || Loss: 23.53583471911801
Iteration: 7 || Loss: 23.517022911709468
Iteration: 8 || Loss: 23.488650436689444
Iteration: 9 || Loss: 23.452597656479774
Iteration: 10 || Loss: 23.44404952628629
Iteration: 11 || Loss: 23.42934888424298
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.813324
Epoch 257 loss:23.42934888424298
MSE loss S0.4041583077211229
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.813324
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.78102912205563
Iteration: 2 || Loss: 67.78070057164943
Iteration: 3 || Loss: 67.78038704792472
Iteration: 4 || Loss: 67.78008552160786
Iteration: 5 || Loss: 67.77978704804889
Iteration: 6 || Loss: 67.77978704804889
saving ADAM checkpoint...
Sum of params:-107.813385
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.77978704804889
Iteration: 2 || Loss: 67.77763198428569
Iteration: 3 || Loss: 67.76391659143167
Iteration: 4 || Loss: 67.53203092992615
Iteration: 5 || Loss: 67.48989673470273
Iteration: 6 || Loss: 67.44809581157311
Iteration: 7 || Loss: 67.40717603305761
Iteration: 8 || Loss: 67.38834548222809
Iteration: 9 || Loss: 67.37913141341222
Iteration: 10 || Loss: 67.36093763020138
Iteration: 11 || Loss: 67.34683504610237
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.85841
Epoch 257 loss:67.34683504610237
MSE loss S0.9781967409792874
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.852929939786744
MSE loss S0.3524517465640419
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:105.46930903347881
MSE loss S2.029699307192344
waveform batch: 2/2
Test loss - extrapolation:51.62027429826244
MSE loss S1.1851135137990312
Epoch 257 mean train loss:3.338006256776645
Epoch 257 mean test loss - interpolation:3.9754883232977907
Epoch 257 mean test loss - extrapolation:13.090798610978437
Start training epoch 258
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.85841
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.391772267509017
Iteration: 2 || Loss: 6.390903332660655
Iteration: 3 || Loss: 6.389937727154429
Iteration: 4 || Loss: 6.389025000302476
Iteration: 5 || Loss: 6.388127395454898
Iteration: 6 || Loss: 6.388127395454898
saving ADAM checkpoint...
Sum of params:-107.85833
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.388127395454898
Iteration: 2 || Loss: 6.362276592643122
Iteration: 3 || Loss: 6.276785077850194
Iteration: 4 || Loss: 6.261426497050923
Iteration: 5 || Loss: 6.132099602805491
Iteration: 6 || Loss: 6.0768883836610605
Iteration: 7 || Loss: 6.064241137661654
Iteration: 8 || Loss: 6.058509965841574
Iteration: 9 || Loss: 6.053925290182899
Iteration: 10 || Loss: 6.04676763235566
Iteration: 11 || Loss: 6.0249075430164
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.870895
Epoch 258 loss:6.0249075430164
MSE loss S0.20345332667348454
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.870895
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.020067301727536
Iteration: 2 || Loss: 24.019799371328933
Iteration: 3 || Loss: 24.019527640657376
Iteration: 4 || Loss: 24.019191186865513
Iteration: 5 || Loss: 24.01897609827443
Iteration: 6 || Loss: 24.01897609827443
saving ADAM checkpoint...
Sum of params:-107.87112
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.01897609827443
Iteration: 2 || Loss: 23.828944820157208
Iteration: 3 || Loss: 23.809452534317078
Iteration: 4 || Loss: 23.789915366109316
Iteration: 5 || Loss: 23.59144478357448
Iteration: 6 || Loss: 23.53661807156322
Iteration: 7 || Loss: 23.516185856756415
Iteration: 8 || Loss: 23.488034730164973
Iteration: 9 || Loss: 23.450673878459174
Iteration: 10 || Loss: 23.442692327352994
Iteration: 11 || Loss: 23.4270734421855
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.84673
Epoch 258 loss:23.4270734421855
MSE loss S0.4042147117414916
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.84673
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.75033041641257
Iteration: 2 || Loss: 67.74996670109397
Iteration: 3 || Loss: 67.74969112896777
Iteration: 4 || Loss: 67.74940228636014
Iteration: 5 || Loss: 67.74906367695296
Iteration: 6 || Loss: 67.74906367695296
saving ADAM checkpoint...
Sum of params:-107.846794
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.74906367695296
Iteration: 2 || Loss: 67.74675036696003
Iteration: 3 || Loss: 67.73347333622002
Iteration: 4 || Loss: 67.50100903388274
Iteration: 5 || Loss: 67.4495649247796
Iteration: 6 || Loss: 67.41871124241905
Iteration: 7 || Loss: 67.37952412117464
Iteration: 8 || Loss: 67.36073945901047
Iteration: 9 || Loss: 67.35132834979667
Iteration: 10 || Loss: 67.33302505091426
Iteration: 11 || Loss: 67.31896910620382
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.89145
Epoch 258 loss:67.31896910620382
MSE loss S0.9777798210202648
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.848247477176134
MSE loss S0.35246972844310853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:105.23692092785649
MSE loss S2.024765062117943
waveform batch: 2/2
Test loss - extrapolation:51.52834892778269
MSE loss S1.1827239615422123
Epoch 258 mean train loss:3.336929313496749
Epoch 258 mean test loss - interpolation:3.974707912862689
Epoch 258 mean test loss - extrapolation:13.063772487969933
Start training epoch 259
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.89145
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.391162687664444
Iteration: 2 || Loss: 6.390205695741573
Iteration: 3 || Loss: 6.389303243313192
Iteration: 4 || Loss: 6.388440311997875
Iteration: 5 || Loss: 6.3875018314263965
Iteration: 6 || Loss: 6.3875018314263965
saving ADAM checkpoint...
Sum of params:-107.89138
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3875018314263965
Iteration: 2 || Loss: 6.361694851050566
Iteration: 3 || Loss: 6.275963850321736
Iteration: 4 || Loss: 6.2614180410114795
Iteration: 5 || Loss: 6.131410614816541
Iteration: 6 || Loss: 6.076561276499708
Iteration: 7 || Loss: 6.06353755885853
Iteration: 8 || Loss: 6.057755074892779
Iteration: 9 || Loss: 6.052897396816984
Iteration: 10 || Loss: 6.045598216909289
Iteration: 11 || Loss: 6.023921591044207
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.90387
Epoch 259 loss:6.023921591044207
MSE loss S0.20320784376398496
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.90387
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.017041882471208
Iteration: 2 || Loss: 24.016840355570128
Iteration: 3 || Loss: 24.016499342655603
Iteration: 4 || Loss: 24.01631770489157
Iteration: 5 || Loss: 24.016024067859643
Iteration: 6 || Loss: 24.016024067859643
saving ADAM checkpoint...
Sum of params:-107.90405
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.016024067859643
Iteration: 2 || Loss: 23.817311590499834
Iteration: 3 || Loss: 23.80379256277405
Iteration: 4 || Loss: 23.784479896107303
Iteration: 5 || Loss: 23.57861818253995
Iteration: 6 || Loss: 23.533600078570498
Iteration: 7 || Loss: 23.514529706902227
Iteration: 8 || Loss: 23.48642083054989
Iteration: 9 || Loss: 23.44843021027819
Iteration: 10 || Loss: 23.439662671377693
Iteration: 11 || Loss: 23.424975555389196
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.87952
Epoch 259 loss:23.424975555389196
MSE loss S0.40391343060584795
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.87952
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.72401347503195
Iteration: 2 || Loss: 67.72367194495871
Iteration: 3 || Loss: 67.72332175157375
Iteration: 4 || Loss: 67.72307756666183
Iteration: 5 || Loss: 67.72277358192818
Iteration: 6 || Loss: 67.72277358192818
saving ADAM checkpoint...
Sum of params:-107.87955
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.72277358192818
Iteration: 2 || Loss: 67.72063491528097
Iteration: 3 || Loss: 67.70694643608898
Iteration: 4 || Loss: 67.47660916451372
Iteration: 5 || Loss: 67.42633709126707
Iteration: 6 || Loss: 67.39260733441166
Iteration: 7 || Loss: 67.35176041599864
Iteration: 8 || Loss: 67.33309927271331
Iteration: 9 || Loss: 67.3237560393438
Iteration: 10 || Loss: 67.30561225885748
Iteration: 11 || Loss: 67.29151330343063
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.924385
Epoch 259 loss:67.29151330343063
MSE loss S0.9777273853486679
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.843869772149734
MSE loss S0.35248013475383205
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:105.02510541042719
MSE loss S2.020545238165072
waveform batch: 2/2
Test loss - extrapolation:51.44688406182826
MSE loss S1.1805222182808017
Epoch 259 mean train loss:3.3358762224091048
Epoch 259 mean test loss - interpolation:3.973978295358289
Epoch 259 mean test loss - extrapolation:13.039332456021286
Start training epoch 260
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.924385
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.390833532890151
Iteration: 2 || Loss: 6.389864644988365
Iteration: 3 || Loss: 6.38895074820957
Iteration: 4 || Loss: 6.3880555760172415
Iteration: 5 || Loss: 6.3871869755459025
Iteration: 6 || Loss: 6.3871869755459025
saving ADAM checkpoint...
Sum of params:-107.924286
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3871869755459025
Iteration: 2 || Loss: 6.361105334888693
Iteration: 3 || Loss: 6.274752054609138
Iteration: 4 || Loss: 6.261383060263431
Iteration: 5 || Loss: 6.130424029278192
Iteration: 6 || Loss: 6.0752780530984944
Iteration: 7 || Loss: 6.062590014291256
Iteration: 8 || Loss: 6.056859898024731
Iteration: 9 || Loss: 6.052169327249596
Iteration: 10 || Loss: 6.044556625823275
Iteration: 11 || Loss: 6.02287079451612
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.93678
Epoch 260 loss:6.02287079451612
MSE loss S0.2030751139916962
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.93678
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.019849368063735
Iteration: 2 || Loss: 24.019495798721376
Iteration: 3 || Loss: 24.01913039836757
Iteration: 4 || Loss: 24.018967265616443
Iteration: 5 || Loss: 24.018695426969842
Iteration: 6 || Loss: 24.018695426969842
saving ADAM checkpoint...
Sum of params:-107.93701
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.018695426969842
Iteration: 2 || Loss: 23.830144697250944
Iteration: 3 || Loss: 23.809250531361215
Iteration: 4 || Loss: 23.788511919924666
Iteration: 5 || Loss: 23.591310971450717
Iteration: 6 || Loss: 23.534391379068733
Iteration: 7 || Loss: 23.513593250637605
Iteration: 8 || Loss: 23.48522870688965
Iteration: 9 || Loss: 23.44652317835346
Iteration: 10 || Loss: 23.43921211872864
Iteration: 11 || Loss: 23.422702729422248
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.91278
Epoch 260 loss:23.422702729422248
MSE loss S0.4038912805611723
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.91278
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.69491674927049
Iteration: 2 || Loss: 67.69460217605027
Iteration: 3 || Loss: 67.69431346273137
Iteration: 4 || Loss: 67.69400310548244
Iteration: 5 || Loss: 67.69369267726574
Iteration: 6 || Loss: 67.69369267726574
saving ADAM checkpoint...
Sum of params:-107.9128
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.69369267726574
Iteration: 2 || Loss: 67.69157091891974
Iteration: 3 || Loss: 67.67813098692415
Iteration: 4 || Loss: 67.44677671813763
Iteration: 5 || Loss: 67.38840840311249
Iteration: 6 || Loss: 67.3638926766604
Iteration: 7 || Loss: 67.32478322332469
Iteration: 8 || Loss: 67.30606946209862
Iteration: 9 || Loss: 67.29647908704135
Iteration: 10 || Loss: 67.27835445986295
Iteration: 11 || Loss: 67.2643094749952
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.957214
Epoch 260 loss:67.2643094749952
MSE loss S0.9773664364928865
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.839345772837742
MSE loss S0.3524667630986765
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:104.80089781304909
MSE loss S2.0157973064219363
waveform batch: 2/2
Test loss - extrapolation:51.35835393366919
MSE loss S1.1781721375421272
Epoch 260 mean train loss:3.3348235516873643
Epoch 260 mean test loss - interpolation:3.973224295472957
Epoch 260 mean test loss - extrapolation:13.013270978893189
Start training epoch 261
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.957214
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.390688679084361
Iteration: 2 || Loss: 6.389719167484902
Iteration: 3 || Loss: 6.388770139552696
Iteration: 4 || Loss: 6.387966066265913
Iteration: 5 || Loss: 6.387004893629652
Iteration: 6 || Loss: 6.387004893629652
saving ADAM checkpoint...
Sum of params:-107.95712
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.387004893629652
Iteration: 2 || Loss: 6.360954487040575
Iteration: 3 || Loss: 6.273923796678746
Iteration: 4 || Loss: 6.261350505514411
Iteration: 5 || Loss: 6.129617915615691
Iteration: 6 || Loss: 6.074535329803076
Iteration: 7 || Loss: 6.061805901888808
Iteration: 8 || Loss: 6.056045964758693
Iteration: 9 || Loss: 6.05151623784128
Iteration: 10 || Loss: 6.043717870646265
Iteration: 11 || Loss: 6.021823520203306
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.96954
Epoch 261 loss:6.021823520203306
MSE loss S0.20288821085131747
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-107.96954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.02201203109673
Iteration: 2 || Loss: 24.021791442775996
Iteration: 3 || Loss: 24.02158577274523
Iteration: 4 || Loss: 24.021270934828614
Iteration: 5 || Loss: 24.021078456574163
Iteration: 6 || Loss: 24.021078456574163
saving ADAM checkpoint...
Sum of params:-107.96975
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.021078456574163
Iteration: 2 || Loss: 23.82708170991265
Iteration: 3 || Loss: 23.807195071345095
Iteration: 4 || Loss: 23.78779013588724
Iteration: 5 || Loss: 23.58942430408721
Iteration: 6 || Loss: 23.53350660376513
Iteration: 7 || Loss: 23.512619527410795
Iteration: 8 || Loss: 23.484274126010632
Iteration: 9 || Loss: 23.44473684258191
Iteration: 10 || Loss: 23.43833625221016
Iteration: 11 || Loss: 23.420747520020296
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.94558
Epoch 261 loss:23.420747520020296
MSE loss S0.40403738715860693
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.94558
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.67053799341382
Iteration: 2 || Loss: 67.67021771407637
Iteration: 3 || Loss: 67.66993386664373
Iteration: 4 || Loss: 67.66963596586034
Iteration: 5 || Loss: 67.66931594350804
Iteration: 6 || Loss: 67.66931594350804
saving ADAM checkpoint...
Sum of params:-107.945625
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.66931594350804
Iteration: 2 || Loss: 67.66721759973525
Iteration: 3 || Loss: 67.65354830332247
Iteration: 4 || Loss: 67.42179368183493
Iteration: 5 || Loss: 67.37613712686702
Iteration: 6 || Loss: 67.33759880499943
Iteration: 7 || Loss: 67.29796490142584
Iteration: 8 || Loss: 67.27916979675994
Iteration: 9 || Loss: 67.26954927896774
Iteration: 10 || Loss: 67.25149402667938
Iteration: 11 || Loss: 67.23751047646292
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.98992
Epoch 261 loss:67.23751047646292
MSE loss S0.9771037614807399
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.834833476547676
MSE loss S0.35241719482337563
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:104.58800709109597
MSE loss S2.0114405256241605
waveform batch: 2/2
Test loss - extrapolation:51.27473323170489
MSE loss S1.1758951780617757
Epoch 261 mean train loss:3.3337959143685003
Epoch 261 mean test loss - interpolation:3.972472246091279
Epoch 261 mean test loss - extrapolation:12.988561693566739
Start training epoch 262
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-107.98992
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.390894553189944
Iteration: 2 || Loss: 6.3899685569851705
Iteration: 3 || Loss: 6.389003597877468
Iteration: 4 || Loss: 6.388046950361863
Iteration: 5 || Loss: 6.387163478039026
Iteration: 6 || Loss: 6.387163478039026
saving ADAM checkpoint...
Sum of params:-107.989845
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.387163478039026
Iteration: 2 || Loss: 6.36106877148903
Iteration: 3 || Loss: 6.272881873858847
Iteration: 4 || Loss: 6.261263494557183
Iteration: 5 || Loss: 6.128634063856249
Iteration: 6 || Loss: 6.0734661323828965
Iteration: 7 || Loss: 6.060900738161073
Iteration: 8 || Loss: 6.055135777422279
Iteration: 9 || Loss: 6.050413002461087
Iteration: 10 || Loss: 6.042528094543596
Iteration: 11 || Loss: 6.020670276476905
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.00218
Epoch 262 loss:6.020670276476905
MSE loss S0.2025374281128677
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.00218
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.020722379768753
Iteration: 2 || Loss: 24.020542863773276
Iteration: 3 || Loss: 24.02020502812786
Iteration: 4 || Loss: 24.019992663496893
Iteration: 5 || Loss: 24.019736754530577
Iteration: 6 || Loss: 24.019736754530577
saving ADAM checkpoint...
Sum of params:-108.002365
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.019736754530577
Iteration: 2 || Loss: 23.81386901340499
Iteration: 3 || Loss: 23.801196382054613
Iteration: 4 || Loss: 23.78320846065716
Iteration: 5 || Loss: 23.575211548413
Iteration: 6 || Loss: 23.530434840878513
Iteration: 7 || Loss: 23.511477644724465
Iteration: 8 || Loss: 23.4830499668402
Iteration: 9 || Loss: 23.44242140608748
Iteration: 10 || Loss: 23.43378301744703
Iteration: 11 || Loss: 23.418426818001933
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-107.97807
Epoch 262 loss:23.418426818001933
MSE loss S0.4036551204605002
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-107.97807
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.64189517949498
Iteration: 2 || Loss: 67.64163456831795
Iteration: 3 || Loss: 67.64138044037804
Iteration: 4 || Loss: 67.64099613246462
Iteration: 5 || Loss: 67.64071257319245
Iteration: 6 || Loss: 67.64071257319245
saving ADAM checkpoint...
Sum of params:-107.9781
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.64071257319245
Iteration: 2 || Loss: 67.63865252564361
Iteration: 3 || Loss: 67.62491522041262
Iteration: 4 || Loss: 67.39609795149367
Iteration: 5 || Loss: 67.31704758061626
Iteration: 6 || Loss: 67.28521502003146
Iteration: 7 || Loss: 67.27099607323294
Iteration: 8 || Loss: 67.2524651396695
Iteration: 9 || Loss: 67.24291800616726
Iteration: 10 || Loss: 67.22500853724948
Iteration: 11 || Loss: 67.21093440456228
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.02255
Epoch 262 loss:67.21093440456228
MSE loss S0.9771648422827204
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.830977491873313
MSE loss S0.3525087090370881
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:104.381217859352
MSE loss S2.0072269524209796
waveform batch: 2/2
Test loss - extrapolation:51.195341471229696
MSE loss S1.173899598928538
Epoch 262 mean train loss:3.332759706863487
Epoch 262 mean test loss - interpolation:3.9718295819788856
Epoch 262 mean test loss - extrapolation:12.964713277548475
Start training epoch 263
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.02255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.389037364588481
Iteration: 2 || Loss: 6.388153728972416
Iteration: 3 || Loss: 6.3871852040962125
Iteration: 4 || Loss: 6.386281996050229
Iteration: 5 || Loss: 6.385345814852184
Iteration: 6 || Loss: 6.385345814852184
saving ADAM checkpoint...
Sum of params:-108.022484
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.385345814852184
Iteration: 2 || Loss: 6.358938382074533
Iteration: 3 || Loss: 6.271862986492987
Iteration: 4 || Loss: 6.261415838675448
Iteration: 5 || Loss: 6.12762538752236
Iteration: 6 || Loss: 6.072478307022219
Iteration: 7 || Loss: 6.059866969196242
Iteration: 8 || Loss: 6.054180717502356
Iteration: 9 || Loss: 6.049567832535402
Iteration: 10 || Loss: 6.041147190814821
Iteration: 11 || Loss: 6.0192843186907385
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.03485
Epoch 263 loss:6.0192843186907385
MSE loss S0.2021042811095755
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.03485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.028790675048167
Iteration: 2 || Loss: 24.028516634733016
Iteration: 3 || Loss: 24.028271497491353
Iteration: 4 || Loss: 24.027921127719612
Iteration: 5 || Loss: 24.027683416151614
Iteration: 6 || Loss: 24.027683416151614
saving ADAM checkpoint...
Sum of params:-108.035065
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.027683416151614
Iteration: 2 || Loss: 23.84400979879831
Iteration: 3 || Loss: 23.81697227381951
Iteration: 4 || Loss: 23.7970076624417
Iteration: 5 || Loss: 23.600102720569588
Iteration: 6 || Loss: 23.53417126091309
Iteration: 7 || Loss: 23.51236747684959
Iteration: 8 || Loss: 23.482912318558633
Iteration: 9 || Loss: 23.441397233744095
Iteration: 10 || Loss: 23.43491609651465
Iteration: 11 || Loss: 23.416675433405896
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.01118
Epoch 263 loss:23.416675433405896
MSE loss S0.4038810194278752
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.01118
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.6160480052284
Iteration: 2 || Loss: 67.61575424673234
Iteration: 3 || Loss: 67.61541654543966
Iteration: 4 || Loss: 67.6151157389377
Iteration: 5 || Loss: 67.61478074923092
Iteration: 6 || Loss: 67.61478074923092
saving ADAM checkpoint...
Sum of params:-108.011185
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.61478074923092
Iteration: 2 || Loss: 67.6127663170308
Iteration: 3 || Loss: 67.5991276914849
Iteration: 4 || Loss: 67.36780494045168
Iteration: 5 || Loss: 67.30905680275234
Iteration: 6 || Loss: 67.2838735307325
Iteration: 7 || Loss: 67.24517221599457
Iteration: 8 || Loss: 67.22646431597131
Iteration: 9 || Loss: 67.21667393014434
Iteration: 10 || Loss: 67.19866536712145
Iteration: 11 || Loss: 67.18462027814859
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.05502
Epoch 263 loss:67.18462027814859
MSE loss S0.9765380037673455
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.826142530712506
MSE loss S0.3523362550457677
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:104.16633089540197
MSE loss S2.0027104491305847
waveform batch: 2/2
Test loss - extrapolation:51.108701375308684
MSE loss S1.171473096426699
Epoch 263 mean train loss:3.3317441389739733
Epoch 263 mean test loss - interpolation:3.971023755118751
Epoch 263 mean test loss - extrapolation:12.93958602255922
Start training epoch 264
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.05502
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3908406548400505
Iteration: 2 || Loss: 6.3898902718373725
Iteration: 3 || Loss: 6.388984906573566
Iteration: 4 || Loss: 6.387992869701163
Iteration: 5 || Loss: 6.387043213791435
Iteration: 6 || Loss: 6.387043213791435
saving ADAM checkpoint...
Sum of params:-108.05495
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.387043213791435
Iteration: 2 || Loss: 6.360710152340188
Iteration: 3 || Loss: 6.271042012457611
Iteration: 4 || Loss: 6.26132913079999
Iteration: 5 || Loss: 6.1266851003762035
Iteration: 6 || Loss: 6.071721361655456
Iteration: 7 || Loss: 6.058974804980609
Iteration: 8 || Loss: 6.053244867008298
Iteration: 9 || Loss: 6.048472554675952
Iteration: 10 || Loss: 6.040125522867521
Iteration: 11 || Loss: 6.018271826813088
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.06719
Epoch 264 loss:6.018271826813088
MSE loss S0.20204513953086867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.06719
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.02683836798018
Iteration: 2 || Loss: 24.026544373060478
Iteration: 3 || Loss: 24.026321334101624
Iteration: 4 || Loss: 24.026058167332472
Iteration: 5 || Loss: 24.025756885779455
Iteration: 6 || Loss: 24.025756885779455
saving ADAM checkpoint...
Sum of params:-108.06739
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.025756885779455
Iteration: 2 || Loss: 23.812364763024824
Iteration: 3 || Loss: 23.799941297248747
Iteration: 4 || Loss: 23.78370143852446
Iteration: 5 || Loss: 23.574869849555963
Iteration: 6 || Loss: 23.52980658272541
Iteration: 7 || Loss: 23.510593239292252
Iteration: 8 || Loss: 23.48200602032645
Iteration: 9 || Loss: 23.438852025173016
Iteration: 10 || Loss: 23.430729337081907
Iteration: 11 || Loss: 23.414488769013012
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.04324
Epoch 264 loss:23.414488769013012
MSE loss S0.40350422069210207
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.04324
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.58991229619839
Iteration: 2 || Loss: 67.58960245157981
Iteration: 3 || Loss: 67.58928498589987
Iteration: 4 || Loss: 67.58899786691826
Iteration: 5 || Loss: 67.58875947473557
Iteration: 6 || Loss: 67.58875947473557
saving ADAM checkpoint...
Sum of params:-108.04329
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.58875947473557
Iteration: 2 || Loss: 67.58663182211588
Iteration: 3 || Loss: 67.57273089635444
Iteration: 4 || Loss: 67.34515291335671
Iteration: 5 || Loss: 67.26535196536439
Iteration: 6 || Loss: 67.2478760944554
Iteration: 7 || Loss: 67.21881288833656
Iteration: 8 || Loss: 67.20025083720401
Iteration: 9 || Loss: 67.19063809420642
Iteration: 10 || Loss: 67.1727570945961
Iteration: 11 || Loss: 67.15871286300472
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.08743
Epoch 264 loss:67.15871286300472
MSE loss S0.9766352936856001
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.822489587448892
MSE loss S0.35236892113144597
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:103.96836734062502
MSE loss S1.9987269077063385
waveform batch: 2/2
Test loss - extrapolation:51.032999667076076
MSE loss S1.169532487152865
Epoch 264 mean train loss:3.3307404640976146
Epoch 264 mean test loss - interpolation:3.970414931241482
Epoch 264 mean test loss - extrapolation:12.916780583975092
Start training epoch 265
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.08743
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.390050463917979
Iteration: 2 || Loss: 6.389055944250488
Iteration: 3 || Loss: 6.388186357882825
Iteration: 4 || Loss: 6.387189931600624
Iteration: 5 || Loss: 6.386243893448677
Iteration: 6 || Loss: 6.386243893448677
saving ADAM checkpoint...
Sum of params:-108.087326
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.386243893448677
Iteration: 2 || Loss: 6.359078190101462
Iteration: 3 || Loss: 6.270190689302394
Iteration: 4 || Loss: 6.261613577219961
Iteration: 5 || Loss: 6.125634608642816
Iteration: 6 || Loss: 6.070647852359625
Iteration: 7 || Loss: 6.057953845322656
Iteration: 8 || Loss: 6.052242578724817
Iteration: 9 || Loss: 6.047497890227012
Iteration: 10 || Loss: 6.038775824801864
Iteration: 11 || Loss: 6.016900591149119
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.099556
Epoch 265 loss:6.016900591149119
MSE loss S0.20175335952014506
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.099556
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.03035410252157
Iteration: 2 || Loss: 24.030068150881213
Iteration: 3 || Loss: 24.0297686167967
Iteration: 4 || Loss: 24.029481887086842
Iteration: 5 || Loss: 24.029254894235578
Iteration: 6 || Loss: 24.029254894235578
saving ADAM checkpoint...
Sum of params:-108.09976
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.029254894235578
Iteration: 2 || Loss: 23.83514196779873
Iteration: 3 || Loss: 23.808624344691456
Iteration: 4 || Loss: 23.79184176815225
Iteration: 5 || Loss: 23.599620166874296
Iteration: 6 || Loss: 23.533282493455737
Iteration: 7 || Loss: 23.51097813812947
Iteration: 8 || Loss: 23.481633269662254
Iteration: 9 || Loss: 23.437754141687094
Iteration: 10 || Loss: 23.431574615042884
Iteration: 11 || Loss: 23.41242833158993
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.07596
Epoch 265 loss:23.41242833158993
MSE loss S0.40360727653276174
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.07596
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.56405420856628
Iteration: 2 || Loss: 67.56375647566911
Iteration: 3 || Loss: 67.5634366547612
Iteration: 4 || Loss: 67.56313862246644
Iteration: 5 || Loss: 67.56281862876638
Iteration: 6 || Loss: 67.56281862876638
saving ADAM checkpoint...
Sum of params:-108.076
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.56281862876638
Iteration: 2 || Loss: 67.56086167929686
Iteration: 3 || Loss: 67.54694808779891
Iteration: 4 || Loss: 67.317275793158
Iteration: 5 || Loss: 67.2376573825461
Iteration: 6 || Loss: 67.20225571904955
Iteration: 7 || Loss: 67.19331433339954
Iteration: 8 || Loss: 67.17469921352148
Iteration: 9 || Loss: 67.16487884167425
Iteration: 10 || Loss: 67.14685382014883
Iteration: 11 || Loss: 67.13291637003807
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.11966
Epoch 265 loss:67.13291637003807
MSE loss S0.9762303671443306
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.817718243315973
MSE loss S0.35226019722878793
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:103.75970208271971
MSE loss S1.9943060194059774
waveform batch: 2/2
Test loss - extrapolation:50.94944268364294
MSE loss S1.1672835080267505
Epoch 265 mean train loss:3.329732596302659
Epoch 265 mean test loss - interpolation:3.9696197072193287
Epoch 265 mean test loss - extrapolation:12.892428730530222
Start training epoch 266
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.11966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.390028356496213
Iteration: 2 || Loss: 6.389045241642318
Iteration: 3 || Loss: 6.388100115581024
Iteration: 4 || Loss: 6.387205925212512
Iteration: 5 || Loss: 6.386261513452544
Iteration: 6 || Loss: 6.386261513452544
saving ADAM checkpoint...
Sum of params:-108.11958
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.386261513452544
Iteration: 2 || Loss: 6.359456148798774
Iteration: 3 || Loss: 6.269002853998433
Iteration: 4 || Loss: 6.261132435001874
Iteration: 5 || Loss: 6.124784363218218
Iteration: 6 || Loss: 6.069743704918313
Iteration: 7 || Loss: 6.0569985989804165
Iteration: 8 || Loss: 6.051321223258816
Iteration: 9 || Loss: 6.046652271438239
Iteration: 10 || Loss: 6.037593367726549
Iteration: 11 || Loss: 6.015569629262217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.13187
Epoch 266 loss:6.015569629262217
MSE loss S0.20140724817195765
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.13187
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.035019662049535
Iteration: 2 || Loss: 24.034733614455295
Iteration: 3 || Loss: 24.034386109233765
Iteration: 4 || Loss: 24.034221086942377
Iteration: 5 || Loss: 24.033934420205885
Iteration: 6 || Loss: 24.033934420205885
saving ADAM checkpoint...
Sum of params:-108.132065
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.033934420205885
Iteration: 2 || Loss: 23.831726557060378
Iteration: 3 || Loss: 23.80842909843362
Iteration: 4 || Loss: 23.791801617435556
Iteration: 5 || Loss: 23.596436950402566
Iteration: 6 || Loss: 23.53321320905507
Iteration: 7 || Loss: 23.51089944241542
Iteration: 8 || Loss: 23.481356718856432
Iteration: 9 || Loss: 23.436093467770963
Iteration: 10 || Loss: 23.430054121945886
Iteration: 11 || Loss: 23.41053288278684
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.10829
Epoch 266 loss:23.41053288278684
MSE loss S0.40345822659010133
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.10829
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.53835030243422
Iteration: 2 || Loss: 67.5379865819291
Iteration: 3 || Loss: 67.53767718482594
Iteration: 4 || Loss: 67.53739793870425
Iteration: 5 || Loss: 67.53710687526032
Iteration: 6 || Loss: 67.53710687526032
saving ADAM checkpoint...
Sum of params:-108.108315
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.53710687526032
Iteration: 2 || Loss: 67.53513372995712
Iteration: 3 || Loss: 67.52113377220131
Iteration: 4 || Loss: 67.29243189225318
Iteration: 5 || Loss: 67.21340626864215
Iteration: 6 || Loss: 67.2027415852884
Iteration: 7 || Loss: 67.16789541283552
Iteration: 8 || Loss: 67.14930877357448
Iteration: 9 || Loss: 67.13943687234199
Iteration: 10 || Loss: 67.12140425612766
Iteration: 11 || Loss: 67.10752741477876
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.15189
Epoch 266 loss:67.10752741477876
MSE loss S0.9759399100356425
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.813526102773235
MSE loss S0.35214213812137374
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:103.561022489816
MSE loss S1.990218129139055
waveform batch: 2/2
Test loss - extrapolation:50.870813119054624
MSE loss S1.1651187903457605
Epoch 266 mean train loss:3.3287458595457866
Epoch 266 mean test loss - interpolation:3.9689210171288725
Epoch 266 mean test loss - extrapolation:12.869319634072552
Start training epoch 267
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.15189
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.390861768694425
Iteration: 2 || Loss: 6.389888379820097
Iteration: 3 || Loss: 6.388928769405363
Iteration: 4 || Loss: 6.38800909063312
Iteration: 5 || Loss: 6.387071359469847
Iteration: 6 || Loss: 6.387071359469847
saving ADAM checkpoint...
Sum of params:-108.15181
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.387071359469847
Iteration: 2 || Loss: 6.359972477715308
Iteration: 3 || Loss: 6.268168610536979
Iteration: 4 || Loss: 6.261079551192613
Iteration: 5 || Loss: 6.123671833577101
Iteration: 6 || Loss: 6.068792943928833
Iteration: 7 || Loss: 6.055948637280291
Iteration: 8 || Loss: 6.050375502566293
Iteration: 9 || Loss: 6.045739511925784
Iteration: 10 || Loss: 6.036416685412682
Iteration: 11 || Loss: 6.014359496624147
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.16403
Epoch 267 loss:6.014359496624147
MSE loss S0.2011878300302744
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.16403
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.04127688409992
Iteration: 2 || Loss: 24.04102407592168
Iteration: 3 || Loss: 24.04072561481332
Iteration: 4 || Loss: 24.04048249292051
Iteration: 5 || Loss: 24.040212617995795
Iteration: 6 || Loss: 24.040212617995795
saving ADAM checkpoint...
Sum of params:-108.16422
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.040212617995795
Iteration: 2 || Loss: 23.82619893394235
Iteration: 3 || Loss: 23.807629962490267
Iteration: 4 || Loss: 23.79186949357967
Iteration: 5 || Loss: 23.589521592791723
Iteration: 6 || Loss: 23.53277810290618
Iteration: 7 || Loss: 23.51080003106098
Iteration: 8 || Loss: 23.481271971789056
Iteration: 9 || Loss: 23.4342026620383
Iteration: 10 || Loss: 23.42774374136762
Iteration: 11 || Loss: 23.408563668465437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.14041
Epoch 267 loss:23.408563668465437
MSE loss S0.40313642093652174
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.14041
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.51149237430062
Iteration: 2 || Loss: 67.51117272655395
Iteration: 3 || Loss: 67.51077576673401
Iteration: 4 || Loss: 67.51055833084082
Iteration: 5 || Loss: 67.51019929256917
Iteration: 6 || Loss: 67.51019929256917
saving ADAM checkpoint...
Sum of params:-108.14044
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.51019929256917
Iteration: 2 || Loss: 67.50835031878935
Iteration: 3 || Loss: 67.49428016255503
Iteration: 4 || Loss: 67.26873972344538
Iteration: 5 || Loss: 67.21579779474605
Iteration: 6 || Loss: 67.18210101791851
Iteration: 7 || Loss: 67.1425584317265
Iteration: 8 || Loss: 67.12402608481396
Iteration: 9 || Loss: 67.11420895853819
Iteration: 10 || Loss: 67.09637888215903
Iteration: 11 || Loss: 67.08232906971455
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.18394
Epoch 267 loss:67.08232906971455
MSE loss S0.9757258701182318
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.80936291749327
MSE loss S0.3520673230207093
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:103.36629511229306
MSE loss S1.9862394059975526
waveform batch: 2/2
Test loss - extrapolation:50.7935905669375
MSE loss S1.1630150473165832
Epoch 267 mean train loss:3.3277673184415217
Epoch 267 mean test loss - interpolation:3.9682271529155453
Epoch 267 mean test loss - extrapolation:12.84665713993588
Start training epoch 268
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.18394
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.390928646308304
Iteration: 2 || Loss: 6.389935639533929
Iteration: 3 || Loss: 6.38903322482889
Iteration: 4 || Loss: 6.3880387473297215
Iteration: 5 || Loss: 6.38708852544534
Iteration: 6 || Loss: 6.38708852544534
saving ADAM checkpoint...
Sum of params:-108.183846
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.38708852544534
Iteration: 2 || Loss: 6.359705531804832
Iteration: 3 || Loss: 6.26740626471905
Iteration: 4 || Loss: 6.261062303377113
Iteration: 5 || Loss: 6.122528825853925
Iteration: 6 || Loss: 6.067740375277115
Iteration: 7 || Loss: 6.054914299256374
Iteration: 8 || Loss: 6.049268726172813
Iteration: 9 || Loss: 6.0448385748961595
Iteration: 10 || Loss: 6.035178642504513
Iteration: 11 || Loss: 6.012842317906263
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.19599
Epoch 268 loss:6.012842317906263
MSE loss S0.20085461484897466
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.19599
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.044083607179676
Iteration: 2 || Loss: 24.043862069446675
Iteration: 3 || Loss: 24.043498230867822
Iteration: 4 || Loss: 24.043177515855714
Iteration: 5 || Loss: 24.04294011928046
Iteration: 6 || Loss: 24.04294011928046
saving ADAM checkpoint...
Sum of params:-108.1962
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.04294011928046
Iteration: 2 || Loss: 23.828446575042648
Iteration: 3 || Loss: 23.80335257778283
Iteration: 4 || Loss: 23.792079631400217
Iteration: 5 || Loss: 23.60241212586636
Iteration: 6 || Loss: 23.53480233604448
Iteration: 7 || Loss: 23.51151692750317
Iteration: 8 || Loss: 23.48181153521743
Iteration: 9 || Loss: 23.433189568186943
Iteration: 10 || Loss: 23.427326853250264
Iteration: 11 || Loss: 23.406725592390963
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.17258
Epoch 268 loss:23.406725592390963
MSE loss S0.4034139295577529
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.17258
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.48696524385464
Iteration: 2 || Loss: 67.48662212026557
Iteration: 3 || Loss: 67.48630579395996
Iteration: 4 || Loss: 67.48596677939493
Iteration: 5 || Loss: 67.48565884988273
Iteration: 6 || Loss: 67.48565884988273
saving ADAM checkpoint...
Sum of params:-108.172615
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.48565884988273
Iteration: 2 || Loss: 67.48379215336416
Iteration: 3 || Loss: 67.46985543561487
Iteration: 4 || Loss: 67.24369685680395
Iteration: 5 || Loss: 67.18676592775212
Iteration: 6 || Loss: 67.15671549688332
Iteration: 7 || Loss: 67.11791632972614
Iteration: 8 || Loss: 67.09925852752542
Iteration: 9 || Loss: 67.08937422502851
Iteration: 10 || Loss: 67.07133295910229
Iteration: 11 || Loss: 67.0575061691227
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.2159
Epoch 268 loss:67.0575061691227
MSE loss S0.975465205081283
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.805202441723218
MSE loss S0.35192916736403845
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:103.17195692300305
MSE loss S1.9822153739685038
waveform batch: 2/2
Test loss - extrapolation:50.716753430755034
MSE loss S1.1609964982763754
Epoch 268 mean train loss:3.326795657911032
Epoch 268 mean test loss - interpolation:3.967533740287203
Epoch 268 mean test loss - extrapolation:12.824059196146507
Start training epoch 269
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.2159
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.391767175231351
Iteration: 2 || Loss: 6.3907592160250495
Iteration: 3 || Loss: 6.389817743944562
Iteration: 4 || Loss: 6.388896264840827
Iteration: 5 || Loss: 6.387961899821623
Iteration: 6 || Loss: 6.387961899821623
saving ADAM checkpoint...
Sum of params:-108.21581
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.387961899821623
Iteration: 2 || Loss: 6.3603524401065865
Iteration: 3 || Loss: 6.266569136355383
Iteration: 4 || Loss: 6.260792141184817
Iteration: 5 || Loss: 6.121654377491436
Iteration: 6 || Loss: 6.066895317459158
Iteration: 7 || Loss: 6.053838178050657
Iteration: 8 || Loss: 6.048182777544074
Iteration: 9 || Loss: 6.044038774744008
Iteration: 10 || Loss: 6.033979194074592
Iteration: 11 || Loss: 6.011592998582252
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.22797
Epoch 269 loss:6.011592998582252
MSE loss S0.2007176506676033
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.22797
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.05275999763
Iteration: 2 || Loss: 24.05243317559885
Iteration: 3 || Loss: 24.052201107440823
Iteration: 4 || Loss: 24.05187142702174
Iteration: 5 || Loss: 24.051597059347973
Iteration: 6 || Loss: 24.051597059347973
saving ADAM checkpoint...
Sum of params:-108.22818
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.051597059347973
Iteration: 2 || Loss: 23.814358097872603
Iteration: 3 || Loss: 23.80051854401614
Iteration: 4 || Loss: 23.78889140435436
Iteration: 5 || Loss: 23.582178214917906
Iteration: 6 || Loss: 23.532465493005926
Iteration: 7 || Loss: 23.511128003968036
Iteration: 8 || Loss: 23.48154364683941
Iteration: 9 || Loss: 23.430911819649406
Iteration: 10 || Loss: 23.42436065203918
Iteration: 11 || Loss: 23.404708092131308
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.20456
Epoch 269 loss:23.404708092131308
MSE loss S0.4030732763211176
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.20456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.45656358184787
Iteration: 2 || Loss: 67.45623001372212
Iteration: 3 || Loss: 67.45593183635359
Iteration: 4 || Loss: 67.45558568251536
Iteration: 5 || Loss: 67.45532533588116
Iteration: 6 || Loss: 67.45532533588116
saving ADAM checkpoint...
Sum of params:-108.204575
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.45532533588116
Iteration: 2 || Loss: 67.45345953803213
Iteration: 3 || Loss: 67.43950940321433
Iteration: 4 || Loss: 67.21974896041269
Iteration: 5 || Loss: 67.1735390434418
Iteration: 6 || Loss: 67.13213060403022
Iteration: 7 || Loss: 67.09278838753569
Iteration: 8 || Loss: 67.07450849398803
Iteration: 9 || Loss: 67.06462183624288
Iteration: 10 || Loss: 67.04687485483366
Iteration: 11 || Loss: 67.03272787214613
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.24781
Epoch 269 loss:67.03272787214613
MSE loss S0.9751486873064521
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.80124867423584
MSE loss S0.3518671588083874
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:102.97991560734661
MSE loss S1.9781488751308502
waveform batch: 2/2
Test loss - extrapolation:50.63869264419328
MSE loss S1.1587861156766976
Epoch 269 mean train loss:3.325828584926196
Epoch 269 mean test loss - interpolation:3.9668747790393066
Epoch 269 mean test loss - extrapolation:12.801550687628323
Start training epoch 270
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.24781
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.392295558631306
Iteration: 2 || Loss: 6.391332177475082
Iteration: 3 || Loss: 6.390413873408132
Iteration: 4 || Loss: 6.389482125639308
Iteration: 5 || Loss: 6.388513599071952
Iteration: 6 || Loss: 6.388513599071952
saving ADAM checkpoint...
Sum of params:-108.24774
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.388513599071952
Iteration: 2 || Loss: 6.360719950676054
Iteration: 3 || Loss: 6.266871852321302
Iteration: 4 || Loss: 6.261387133756355
Iteration: 5 || Loss: 6.120354548967844
Iteration: 6 || Loss: 6.065924850005598
Iteration: 7 || Loss: 6.052714586733276
Iteration: 8 || Loss: 6.047149997723696
Iteration: 9 || Loss: 6.043091479032478
Iteration: 10 || Loss: 6.032613923911574
Iteration: 11 || Loss: 6.009635041348512
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.259964
Epoch 270 loss:6.009635041348512
MSE loss S0.20011180471035472
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.259964
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.06314187315378
Iteration: 2 || Loss: 24.062862356789303
Iteration: 3 || Loss: 24.062506222401638
Iteration: 4 || Loss: 24.0623292030199
Iteration: 5 || Loss: 24.061997510680822
Iteration: 6 || Loss: 24.061997510680822
saving ADAM checkpoint...
Sum of params:-108.26016
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.061997510680822
Iteration: 2 || Loss: 23.82558852847449
Iteration: 3 || Loss: 23.80625830834051
Iteration: 4 || Loss: 23.795706813262495
Iteration: 5 || Loss: 23.598862827249814
Iteration: 6 || Loss: 23.538268511668353
Iteration: 7 || Loss: 23.514714085641682
Iteration: 8 || Loss: 23.484244795006738
Iteration: 9 || Loss: 23.43067408397304
Iteration: 10 || Loss: 23.42456295581194
Iteration: 11 || Loss: 23.403210844700574
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.236786
Epoch 270 loss:23.403210844700574
MSE loss S0.40304088307966757
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.236786
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.43205865563928
Iteration: 2 || Loss: 67.43170760007149
Iteration: 3 || Loss: 67.4313522379164
Iteration: 4 || Loss: 67.43106057108369
Iteration: 5 || Loss: 67.4307996234303
Iteration: 6 || Loss: 67.4307996234303
saving ADAM checkpoint...
Sum of params:-108.23681
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.4307996234303
Iteration: 2 || Loss: 67.42896199688037
Iteration: 3 || Loss: 67.4149582781699
Iteration: 4 || Loss: 67.19571897891151
Iteration: 5 || Loss: 67.15595448121978
Iteration: 6 || Loss: 67.1073942549365
Iteration: 7 || Loss: 67.06887449429625
Iteration: 8 || Loss: 67.05060955268304
Iteration: 9 || Loss: 67.04050281178446
Iteration: 10 || Loss: 67.02244065207978
Iteration: 11 || Loss: 67.00853914005735
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.27953
Epoch 270 loss:67.00853914005735
MSE loss S0.9749088631846909
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.79711088470765
MSE loss S0.3516735435220817
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:102.79157393628358
MSE loss S1.9743336273040324
waveform batch: 2/2
Test loss - extrapolation:50.56395993648703
MSE loss S1.1567844820908595
Epoch 270 mean train loss:3.324875345727808
Epoch 270 mean test loss - interpolation:3.966185147451275
Epoch 270 mean test loss - extrapolation:12.779627822730886
Start training epoch 271
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.27953
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3935512394774685
Iteration: 2 || Loss: 6.392636303575183
Iteration: 3 || Loss: 6.391628566118688
Iteration: 4 || Loss: 6.390637589786861
Iteration: 5 || Loss: 6.389672542901616
Iteration: 6 || Loss: 6.389672542901616
saving ADAM checkpoint...
Sum of params:-108.279434
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.389672542901616
Iteration: 2 || Loss: 6.361424142376597
Iteration: 3 || Loss: 6.265937870557165
Iteration: 4 || Loss: 6.260684498412638
Iteration: 5 || Loss: 6.119248132292386
Iteration: 6 || Loss: 6.064777222587324
Iteration: 7 || Loss: 6.051548992731261
Iteration: 8 || Loss: 6.046029241233898
Iteration: 9 || Loss: 6.0425257314742336
Iteration: 10 || Loss: 6.03126018873823
Iteration: 11 || Loss: 6.007991789906728
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.291595
Epoch 271 loss:6.007991789906728
MSE loss S0.19954126636275069
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.291595
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.06886663091735
Iteration: 2 || Loss: 24.06860946595451
Iteration: 3 || Loss: 24.068286523573992
Iteration: 4 || Loss: 24.068053515878137
Iteration: 5 || Loss: 24.067649558325332
Iteration: 6 || Loss: 24.067649558325332
saving ADAM checkpoint...
Sum of params:-108.291794
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.067649558325332
Iteration: 2 || Loss: 23.883127149772804
Iteration: 3 || Loss: 23.81382064959964
Iteration: 4 || Loss: 23.806040068761746
Iteration: 5 || Loss: 23.62670775789882
Iteration: 6 || Loss: 23.541789451913527
Iteration: 7 || Loss: 23.516438375943935
Iteration: 8 || Loss: 23.48515794899096
Iteration: 9 || Loss: 23.430877082581194
Iteration: 10 || Loss: 23.4244492480846
Iteration: 11 || Loss: 23.401734559833297
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.268524
Epoch 271 loss:23.401734559833297
MSE loss S0.4029755397285121
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.268524
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.40503925508409
Iteration: 2 || Loss: 67.40465711147507
Iteration: 3 || Loss: 67.4042919493438
Iteration: 4 || Loss: 67.40389590242877
Iteration: 5 || Loss: 67.40361976299964
Iteration: 6 || Loss: 67.40361976299964
saving ADAM checkpoint...
Sum of params:-108.268555
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.40361976299964
Iteration: 2 || Loss: 67.40194582796192
Iteration: 3 || Loss: 67.38797453644708
Iteration: 4 || Loss: 67.16952984756747
Iteration: 5 || Loss: 67.10303908931901
Iteration: 6 || Loss: 67.08251633603705
Iteration: 7 || Loss: 67.04542334961593
Iteration: 8 || Loss: 67.02697615834514
Iteration: 9 || Loss: 67.01687029264298
Iteration: 10 || Loss: 66.99816328630399
Iteration: 11 || Loss: 66.98451423171115
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.311195
Epoch 271 loss:66.98451423171115
MSE loss S0.9747423471399981
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.791889771399823
MSE loss S0.35150617260186834
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:102.5978385946227
MSE loss S1.9705699289223768
waveform batch: 2/2
Test loss - extrapolation:50.48632472744837
MSE loss S1.1550596128302266
Epoch 271 mean train loss:3.323939330394868
Epoch 271 mean test loss - interpolation:3.9653149618999706
Epoch 271 mean test loss - extrapolation:12.75701361017259
Start training epoch 272
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.311195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.393414009327061
Iteration: 2 || Loss: 6.392430506668709
Iteration: 3 || Loss: 6.391530691829757
Iteration: 4 || Loss: 6.390516204947995
Iteration: 5 || Loss: 6.389551127840262
Iteration: 6 || Loss: 6.389551127840262
saving ADAM checkpoint...
Sum of params:-108.31109
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.389551127840262
Iteration: 2 || Loss: 6.36167700928798
Iteration: 3 || Loss: 6.264367358893403
Iteration: 4 || Loss: 6.258848598194916
Iteration: 5 || Loss: 6.118301144780758
Iteration: 6 || Loss: 6.064876229558246
Iteration: 7 || Loss: 6.050374066385427
Iteration: 8 || Loss: 6.0449752391599105
Iteration: 9 || Loss: 6.041569253799302
Iteration: 10 || Loss: 6.030296512931531
Iteration: 11 || Loss: 6.007339172375146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.32316
Epoch 272 loss:6.007339172375146
MSE loss S0.19961043866502826
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.32316
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.06953653556066
Iteration: 2 || Loss: 24.06913659922933
Iteration: 3 || Loss: 24.068974090553187
Iteration: 4 || Loss: 24.068703559374136
Iteration: 5 || Loss: 24.06838459353242
Iteration: 6 || Loss: 24.06838459353242
saving ADAM checkpoint...
Sum of params:-108.323364
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.06838459353242
Iteration: 2 || Loss: 23.810244369223096
Iteration: 3 || Loss: 23.797172052007312
Iteration: 4 || Loss: 23.790470813232414
Iteration: 5 || Loss: 23.58512572953419
Iteration: 6 || Loss: 23.534873042567423
Iteration: 7 || Loss: 23.51263338172979
Iteration: 8 || Loss: 23.482270221137508
Iteration: 9 || Loss: 23.426664068959575
Iteration: 10 || Loss: 23.42081212204409
Iteration: 11 || Loss: 23.399054214101483
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.29996
Epoch 272 loss:23.399054214101483
MSE loss S0.40280534988889904
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.29996
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.38255632446628
Iteration: 2 || Loss: 67.38217737731689
Iteration: 3 || Loss: 67.38191098614908
Iteration: 4 || Loss: 67.38158848880856
Iteration: 5 || Loss: 67.38130128673092
Iteration: 6 || Loss: 67.38130128673092
saving ADAM checkpoint...
Sum of params:-108.299995
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.38130128673092
Iteration: 2 || Loss: 67.37945995127394
Iteration: 3 || Loss: 67.36523603395474
Iteration: 4 || Loss: 67.15165150847226
Iteration: 5 || Loss: 67.12399208010206
Iteration: 6 || Loss: 67.0596549605838
Iteration: 7 || Loss: 67.02038044678879
Iteration: 8 || Loss: 67.00225998978294
Iteration: 9 || Loss: 66.99224036158252
Iteration: 10 || Loss: 66.97468843673796
Iteration: 11 || Loss: 66.96045787873865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.34299
Epoch 272 loss:66.96045787873865
MSE loss S0.9744333988612215
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.789501520837714
MSE loss S0.3514792271623033
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:102.42155328878093
MSE loss S1.9665480347690845
waveform batch: 2/2
Test loss - extrapolation:50.41347004384393
MSE loss S1.1526927096721338
Epoch 272 mean train loss:3.32299487121432
Epoch 272 mean test loss - interpolation:3.964916920139619
Epoch 272 mean test loss - extrapolation:12.736251944385407
Start training epoch 273
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.34299
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.395102409931989
Iteration: 2 || Loss: 6.394117700044767
Iteration: 3 || Loss: 6.393143165627973
Iteration: 4 || Loss: 6.392149490543115
Iteration: 5 || Loss: 6.391208971387687
Iteration: 6 || Loss: 6.391208971387687
saving ADAM checkpoint...
Sum of params:-108.342896
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.391208971387687
Iteration: 2 || Loss: 6.362598934002487
Iteration: 3 || Loss: 6.26658608567025
Iteration: 4 || Loss: 6.258161102397063
Iteration: 5 || Loss: 6.116907935439549
Iteration: 6 || Loss: 6.062681958742102
Iteration: 7 || Loss: 6.049257609417705
Iteration: 8 || Loss: 6.0440924316581235
Iteration: 9 || Loss: 6.04163479131922
Iteration: 10 || Loss: 6.028670195505327
Iteration: 11 || Loss: 6.00425034059638
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.35536
Epoch 273 loss:6.00425034059638
MSE loss S0.19888243655833487
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.35536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.107884297991777
Iteration: 2 || Loss: 24.107511615478217
Iteration: 3 || Loss: 24.10718018505925
Iteration: 4 || Loss: 24.107013660496516
Iteration: 5 || Loss: 24.106682989132835
Iteration: 6 || Loss: 24.106682989132835
saving ADAM checkpoint...
Sum of params:-108.35558
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.106682989132835
Iteration: 2 || Loss: 23.913169550797342
Iteration: 3 || Loss: 23.816559281915893
Iteration: 4 || Loss: 23.81095623073102
Iteration: 5 || Loss: 23.640961873007434
Iteration: 6 || Loss: 23.552243439397298
Iteration: 7 || Loss: 23.52457727387987
Iteration: 8 || Loss: 23.492063839515392
Iteration: 9 || Loss: 23.436163130199645
Iteration: 10 || Loss: 23.424083241180412
Iteration: 11 || Loss: 23.400589438064355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.332695
Epoch 273 loss:23.400589438064355
MSE loss S0.40334108283333225
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.332695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.34125594823497
Iteration: 2 || Loss: 67.34084747299691
Iteration: 3 || Loss: 67.3404756971612
Iteration: 4 || Loss: 67.34003519415386
Iteration: 5 || Loss: 67.33962570152482
Iteration: 6 || Loss: 67.33962570152482
saving ADAM checkpoint...
Sum of params:-108.33275
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.33962570152482
Iteration: 2 || Loss: 67.33776220790472
Iteration: 3 || Loss: 67.32110823676832
Iteration: 4 || Loss: 67.1381536438003
Iteration: 5 || Loss: 67.10357954920372
Iteration: 6 || Loss: 67.03417329382398
Iteration: 7 || Loss: 66.99957517829414
Iteration: 8 || Loss: 66.98131227126783
Iteration: 9 || Loss: 66.97046522441916
Iteration: 10 || Loss: 66.95068265786523
Iteration: 11 || Loss: 66.93741923761547
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.37369
Epoch 273 loss:66.93741923761547
MSE loss S0.9744261295812549
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.780439442210668
MSE loss S0.3512259629809004
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:102.20780159000178
MSE loss S1.9632195753685469
waveform batch: 2/2
Test loss - extrapolation:50.32474227203533
MSE loss S1.150958887445509
Epoch 273 mean train loss:3.3221468626302144
Epoch 273 mean test loss - interpolation:3.963406573701778
Epoch 273 mean test loss - extrapolation:12.711045321836425
Start training epoch 274
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.37369
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.394823055978468
Iteration: 2 || Loss: 6.393888569741895
Iteration: 3 || Loss: 6.392925709951101
Iteration: 4 || Loss: 6.392044452220751
Iteration: 5 || Loss: 6.391076501587001
Iteration: 6 || Loss: 6.391076501587001
saving ADAM checkpoint...
Sum of params:-108.37359
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.391076501587001
Iteration: 2 || Loss: 6.36510226427336
Iteration: 3 || Loss: 6.26273601157239
Iteration: 4 || Loss: 6.253697688095012
Iteration: 5 || Loss: 6.115253773088792
Iteration: 6 || Loss: 6.062924430973553
Iteration: 7 || Loss: 6.047878120602316
Iteration: 8 || Loss: 6.042742166090924
Iteration: 9 || Loss: 6.04015916268092
Iteration: 10 || Loss: 6.02789162815765
Iteration: 11 || Loss: 6.005669918861531
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.385254
Epoch 274 loss:6.005669918861531
MSE loss S0.19969976815803578
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.385254
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.069506467894787
Iteration: 2 || Loss: 24.069232155045288
Iteration: 3 || Loss: 24.068937145716276
Iteration: 4 || Loss: 24.06850370476072
Iteration: 5 || Loss: 24.068357303799033
Iteration: 6 || Loss: 24.068357303799033
saving ADAM checkpoint...
Sum of params:-108.385376
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.068357303799033
Iteration: 2 || Loss: 23.786930669270976
Iteration: 3 || Loss: 23.785293369982117
Iteration: 4 || Loss: 23.77968738401467
Iteration: 5 || Loss: 23.562377277048196
Iteration: 6 || Loss: 23.516242859887274
Iteration: 7 || Loss: 23.497724812901506
Iteration: 8 || Loss: 23.46909443488136
Iteration: 9 || Loss: 23.420341074740584
Iteration: 10 || Loss: 23.408144125091145
Iteration: 11 || Loss: 23.393989663768703
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.3621
Epoch 274 loss:23.393989663768703
MSE loss S0.4013743802139763
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.3621
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.33800267124981
Iteration: 2 || Loss: 67.3376363825425
Iteration: 3 || Loss: 67.33738857208432
Iteration: 4 || Loss: 67.33706922220655
Iteration: 5 || Loss: 67.33679297830565
Iteration: 6 || Loss: 67.33679297830565
saving ADAM checkpoint...
Sum of params:-108.362114
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.33679297830565
Iteration: 2 || Loss: 67.3349900566881
Iteration: 3 || Loss: 67.3199144111475
Iteration: 4 || Loss: 67.12370049118341
Iteration: 5 || Loss: 67.08676855777523
Iteration: 6 || Loss: 67.01309714361706
Iteration: 7 || Loss: 66.97276560547648
Iteration: 8 || Loss: 66.95489639552501
Iteration: 9 || Loss: 66.94505728596043
Iteration: 10 || Loss: 66.92810838910374
Iteration: 11 || Loss: 66.91351960536475
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.40564
Epoch 274 loss:66.91351960536475
MSE loss S0.974051571696875
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.781922105830276
MSE loss S0.35124226032038164
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:102.06109064154214
MSE loss S1.9590136151150026
waveform batch: 2/2
Test loss - extrapolation:50.26475546826364
MSE loss S1.148932050201443
Epoch 274 mean train loss:3.3211441099308616
Epoch 274 mean test loss - interpolation:3.963653684305046
Epoch 274 mean test loss - extrapolation:12.693820509150482
Start training epoch 275
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.40564
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.395477409924479
Iteration: 2 || Loss: 6.394431561191302
Iteration: 3 || Loss: 6.39343484684659
Iteration: 4 || Loss: 6.3925187411705755
Iteration: 5 || Loss: 6.3915712880456255
Iteration: 6 || Loss: 6.3915712880456255
saving ADAM checkpoint...
Sum of params:-108.405556
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3915712880456255
Iteration: 2 || Loss: 6.362531221625304
Iteration: 3 || Loss: 6.267164890136387
Iteration: 4 || Loss: 6.1717728189969945
Iteration: 5 || Loss: 6.11444356868771
Iteration: 6 || Loss: 6.060691364734579
Iteration: 7 || Loss: 6.046657452810618
Iteration: 8 || Loss: 6.041278236491925
Iteration: 9 || Loss: 6.037216931091633
Iteration: 10 || Loss: 6.025791247169298
Iteration: 11 || Loss: 5.998826102507748
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.41855
Epoch 275 loss:5.998826102507748
MSE loss S0.19547377510252123
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.41855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.13954131468419
Iteration: 2 || Loss: 24.13930644287752
Iteration: 3 || Loss: 24.138989224882067
Iteration: 4 || Loss: 24.138691864498277
Iteration: 5 || Loss: 24.138356030449003
Iteration: 6 || Loss: 24.138356030449003
saving ADAM checkpoint...
Sum of params:-108.418724
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.138356030449003
Iteration: 2 || Loss: 23.827356118897306
Iteration: 3 || Loss: 23.817872671804796
Iteration: 4 || Loss: 23.81169880092934
Iteration: 5 || Loss: 23.611318246553175
Iteration: 6 || Loss: 23.56055617115321
Iteration: 7 || Loss: 23.53871233362301
Iteration: 8 || Loss: 23.502019771409895
Iteration: 9 || Loss: 23.430007657963955
Iteration: 10 || Loss: 23.42304565035105
Iteration: 11 || Loss: 23.399814641691687
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.39434
Epoch 275 loss:23.399814641691687
MSE loss S0.404498744989767
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.39434
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.31654768814906
Iteration: 2 || Loss: 67.31610588879575
Iteration: 3 || Loss: 67.31559253618808
Iteration: 4 || Loss: 67.31510560538386
Iteration: 5 || Loss: 67.31460849525808
Iteration: 6 || Loss: 67.31460849525808
saving ADAM checkpoint...
Sum of params:-108.394356
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.31460849525808
Iteration: 2 || Loss: 67.30651349997204
Iteration: 3 || Loss: 67.29183194586673
Iteration: 4 || Loss: 67.09185154545668
Iteration: 5 || Loss: 67.00673753783235
Iteration: 6 || Loss: 66.99345531529111
Iteration: 7 || Loss: 66.95379227447486
Iteration: 8 || Loss: 66.93445270196462
Iteration: 9 || Loss: 66.92509316139154
Iteration: 10 || Loss: 66.90831417248144
Iteration: 11 || Loss: 66.89263821182611
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.43538
Epoch 275 loss:66.89263821182611
MSE loss S0.9697598686298718
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.769444653944152
MSE loss S0.34998159085324954
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:101.82235801061036
MSE loss S1.9538759739366682
waveform batch: 2/2
Test loss - extrapolation:50.12369663392494
MSE loss S1.1440878857102037
Epoch 275 mean train loss:3.3203889295181224
Epoch 275 mean test loss - interpolation:3.961574108990692
Epoch 275 mean test loss - extrapolation:12.662171220377942
Start training epoch 276
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.43538
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.410375222685917
Iteration: 2 || Loss: 6.409398630472477
Iteration: 3 || Loss: 6.408462739464484
Iteration: 4 || Loss: 6.407550659337694
Iteration: 5 || Loss: 6.406629600311438
Iteration: 6 || Loss: 6.406629600311438
saving ADAM checkpoint...
Sum of params:-108.435295
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.406629600311438
Iteration: 2 || Loss: 6.380186559252014
Iteration: 3 || Loss: 6.278067278331864
Iteration: 4 || Loss: 6.258114201878716
Iteration: 5 || Loss: 6.108215808314862
Iteration: 6 || Loss: 6.0566056609872145
Iteration: 7 || Loss: 6.044386994933782
Iteration: 8 || Loss: 6.039681297555317
Iteration: 9 || Loss: 6.037087997105177
Iteration: 10 || Loss: 6.023644658222308
Iteration: 11 || Loss: 5.99635000205285
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.44878
Epoch 276 loss:5.99635000205285
MSE loss S0.1954234426541248
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.44878
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.19161595678183
Iteration: 2 || Loss: 24.1913294598849
Iteration: 3 || Loss: 24.19106119999231
Iteration: 4 || Loss: 24.190644283364357
Iteration: 5 || Loss: 24.1902244948053
Iteration: 6 || Loss: 24.1902244948053
saving ADAM checkpoint...
Sum of params:-108.44889
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.1902244948053
Iteration: 2 || Loss: 24.085124693068217
Iteration: 3 || Loss: 23.839092927850125
Iteration: 4 || Loss: 23.83365563617925
Iteration: 5 || Loss: 23.630790151796052
Iteration: 6 || Loss: 23.57459253560713
Iteration: 7 || Loss: 23.545809883562352
Iteration: 8 || Loss: 23.508054339261506
Iteration: 9 || Loss: 23.432365287782705
Iteration: 10 || Loss: 23.424046503839747
Iteration: 11 || Loss: 23.399086982074408
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.42576
Epoch 276 loss:23.399086982074408
MSE loss S0.40527194289010887
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.42576
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.25743598582777
Iteration: 2 || Loss: 67.25698885379371
Iteration: 3 || Loss: 67.25658826792487
Iteration: 4 || Loss: 67.25614439207787
Iteration: 5 || Loss: 67.25571415245388
Iteration: 6 || Loss: 67.25571415245388
saving ADAM checkpoint...
Sum of params:-108.42586
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.25571415245388
Iteration: 2 || Loss: 67.24948707530056
Iteration: 3 || Loss: 67.23712496164565
Iteration: 4 || Loss: 67.06188249498919
Iteration: 5 || Loss: 67.03709481226174
Iteration: 6 || Loss: 66.96821565778082
Iteration: 7 || Loss: 66.93488914101367
Iteration: 8 || Loss: 66.91737502122116
Iteration: 9 || Loss: 66.90565052972752
Iteration: 10 || Loss: 66.88539141054639
Iteration: 11 || Loss: 66.87192559611242
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.46455
Epoch 276 loss:66.87192559611242
MSE loss S0.9716715191928224
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.766827626679444
MSE loss S0.3499345975168746
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:101.66046719795803
MSE loss S1.9514812408388011
waveform batch: 2/2
Test loss - extrapolation:50.081366079395174
MSE loss S1.143627507547675
Epoch 276 mean train loss:3.3195642269048165
Epoch 276 mean test loss - interpolation:3.9611379377799074
Epoch 276 mean test loss - extrapolation:12.645152773112768
Start training epoch 277
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.46455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.405799574289647
Iteration: 2 || Loss: 6.404832458655095
Iteration: 3 || Loss: 6.403786902059981
Iteration: 4 || Loss: 6.402924638482036
Iteration: 5 || Loss: 6.402005360236845
Iteration: 6 || Loss: 6.402005360236845
saving ADAM checkpoint...
Sum of params:-108.464485
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.402005360236845
Iteration: 2 || Loss: 6.37505016898429
Iteration: 3 || Loss: 6.268608941520009
Iteration: 4 || Loss: 6.117054156771143
Iteration: 5 || Loss: 6.108918953389021
Iteration: 6 || Loss: 6.058953235192169
Iteration: 7 || Loss: 6.0428075099555425
Iteration: 8 || Loss: 6.03813975972758
Iteration: 9 || Loss: 6.035666240159239
Iteration: 10 || Loss: 6.022828257296243
Iteration: 11 || Loss: 5.9996075935272914
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.47564
Epoch 277 loss:5.9996075935272914
MSE loss S0.19733526166640233
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.47564
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.056020459615674
Iteration: 2 || Loss: 24.055682098115085
Iteration: 3 || Loss: 24.05536849086493
Iteration: 4 || Loss: 24.05488442793792
Iteration: 5 || Loss: 24.054610048630295
Iteration: 6 || Loss: 24.054610048630295
saving ADAM checkpoint...
Sum of params:-108.4758
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.054610048630295
Iteration: 2 || Loss: 24.014642143659035
Iteration: 3 || Loss: 23.78279750611169
Iteration: 4 || Loss: 23.77524816172752
Iteration: 5 || Loss: 23.56673253205884
Iteration: 6 || Loss: 23.522533827972996
Iteration: 7 || Loss: 23.50554418721195
Iteration: 8 || Loss: 23.474241953708447
Iteration: 9 || Loss: 23.41568866439932
Iteration: 10 || Loss: 23.406587830325776
Iteration: 11 || Loss: 23.386484946264453
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.45314
Epoch 277 loss:23.386484946264453
MSE loss S0.4009713913841615
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.45314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.25315150639469
Iteration: 2 || Loss: 67.25287661917737
Iteration: 3 || Loss: 67.25264452107857
Iteration: 4 || Loss: 67.25237814620266
Iteration: 5 || Loss: 67.25214990980965
Iteration: 6 || Loss: 67.25214990980965
saving ADAM checkpoint...
Sum of params:-108.45316
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.25214990980965
Iteration: 2 || Loss: 67.25053133710992
Iteration: 3 || Loss: 67.23506679884527
Iteration: 4 || Loss: 67.06949831171488
Iteration: 5 || Loss: 67.01868706578742
Iteration: 6 || Loss: 66.94504078200504
Iteration: 7 || Loss: 66.90669663024762
Iteration: 8 || Loss: 66.88983654021908
Iteration: 9 || Loss: 66.879927890092
Iteration: 10 || Loss: 66.86402486327624
Iteration: 11 || Loss: 66.8479657756291
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.4976
Epoch 277 loss:66.8479657756291
MSE loss S0.9716205158740187
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.76888169744072
MSE loss S0.3499920386752145
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:101.53203189542077
MSE loss S1.9475699249510743
waveform batch: 2/2
Test loss - extrapolation:50.03151070756508
MSE loss S1.1427027507445335
Epoch 277 mean train loss:3.3184158039800296
Epoch 277 mean test loss - interpolation:3.9614802829067863
Epoch 277 mean test loss - extrapolation:12.630295216915487
Start training epoch 278
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.4976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4050306151656216
Iteration: 2 || Loss: 6.404022277384582
Iteration: 3 || Loss: 6.403058199127552
Iteration: 4 || Loss: 6.402102612110927
Iteration: 5 || Loss: 6.40111252664714
Iteration: 6 || Loss: 6.40111252664714
saving ADAM checkpoint...
Sum of params:-108.49752
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.40111252664714
Iteration: 2 || Loss: 6.371447000666778
Iteration: 3 || Loss: 6.272039955344589
Iteration: 4 || Loss: 6.266046814244307
Iteration: 5 || Loss: 6.110006950656405
Iteration: 6 || Loss: 6.058906895296277
Iteration: 7 || Loss: 6.042037084867367
Iteration: 8 || Loss: 6.036930146278172
Iteration: 9 || Loss: 6.034353004543769
Iteration: 10 || Loss: 6.021396248416584
Iteration: 11 || Loss: 5.994361683706218
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.51093
Epoch 278 loss:5.994361683706218
MSE loss S0.19381239993530985
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.51093
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.166035530351145
Iteration: 2 || Loss: 24.165611723401142
Iteration: 3 || Loss: 24.165399085369394
Iteration: 4 || Loss: 24.165106224836645
Iteration: 5 || Loss: 24.164648031072637
Iteration: 6 || Loss: 24.164648031072637
saving ADAM checkpoint...
Sum of params:-108.51105
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.164648031072637
Iteration: 2 || Loss: 23.929350351502716
Iteration: 3 || Loss: 23.83174444490698
Iteration: 4 || Loss: 23.826259010588995
Iteration: 5 || Loss: 23.669621107023115
Iteration: 6 || Loss: 23.572569514124833
Iteration: 7 || Loss: 23.540021173003442
Iteration: 8 || Loss: 23.501966167076848
Iteration: 9 || Loss: 23.43827689189018
Iteration: 10 || Loss: 23.421939527051702
Iteration: 11 || Loss: 23.397227093408965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.48796
Epoch 278 loss:23.397227093408965
MSE loss S0.40574527391670123
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.48796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.2293531887562
Iteration: 2 || Loss: 67.2288424333807
Iteration: 3 || Loss: 67.22830963168948
Iteration: 4 || Loss: 67.22783825731058
Iteration: 5 || Loss: 67.22734173740396
Iteration: 6 || Loss: 67.22734173740396
saving ADAM checkpoint...
Sum of params:-108.48802
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.22734173740396
Iteration: 2 || Loss: 67.21880596142204
Iteration: 3 || Loss: 67.20383548377511
Iteration: 4 || Loss: 67.07432547685576
Iteration: 5 || Loss: 66.99488086220455
Iteration: 6 || Loss: 66.92472674321536
Iteration: 7 || Loss: 66.89070803063511
Iteration: 8 || Loss: 66.87166944391451
Iteration: 9 || Loss: 66.85978692863546
Iteration: 10 || Loss: 66.8401482221028
Iteration: 11 || Loss: 66.82683088127098
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.52664
Epoch 278 loss:66.82683088127098
MSE loss S0.9697865765659583
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.756485577468208
MSE loss S0.34920443356687797
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:101.28670241299324
MSE loss S1.9431560372746148
waveform batch: 2/2
Test loss - extrapolation:49.91241660059398
MSE loss S1.138019720163864
Epoch 278 mean train loss:3.31787653994435
Epoch 278 mean test loss - interpolation:3.959414262911368
Epoch 278 mean test loss - extrapolation:12.599926584465601
Start training epoch 279
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.52664
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.405908376061683
Iteration: 2 || Loss: 6.404922152663933
Iteration: 3 || Loss: 6.403883584888945
Iteration: 4 || Loss: 6.402977610144616
Iteration: 5 || Loss: 6.402048286537701
Iteration: 6 || Loss: 6.402048286537701
saving ADAM checkpoint...
Sum of params:-108.52655
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.402048286537701
Iteration: 2 || Loss: 6.37453020581482
Iteration: 3 || Loss: 6.269909335802904
Iteration: 4 || Loss: 6.148757585787917
Iteration: 5 || Loss: 6.10436274121101
Iteration: 6 || Loss: 6.055266998603422
Iteration: 7 || Loss: 6.0402439825665475
Iteration: 8 || Loss: 6.036865865275987
Iteration: 9 || Loss: 6.03343928839656
Iteration: 10 || Loss: 6.019636220586451
Iteration: 11 || Loss: 5.994521854100217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.53859
Epoch 279 loss:5.994521854100217
MSE loss S0.19637781302682142
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.53859
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.12318824009908
Iteration: 2 || Loss: 24.12279960729345
Iteration: 3 || Loss: 24.122307464740135
Iteration: 4 || Loss: 24.12189762931065
Iteration: 5 || Loss: 24.1213910681445
Iteration: 6 || Loss: 24.1213910681445
saving ADAM checkpoint...
Sum of params:-108.53872
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.1213910681445
Iteration: 2 || Loss: 24.096131417679885
Iteration: 3 || Loss: 23.806169415318248
Iteration: 4 || Loss: 23.800584037686637
Iteration: 5 || Loss: 23.584513363243037
Iteration: 6 || Loss: 23.540161748545568
Iteration: 7 || Loss: 23.521785939784966
Iteration: 8 || Loss: 23.486027112728816
Iteration: 9 || Loss: 23.417342982174695
Iteration: 10 || Loss: 23.409900368028897
Iteration: 11 || Loss: 23.38747852843335
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.51541
Epoch 279 loss:23.38747852843335
MSE loss S0.40274531536125
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.51541
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.20442446360866
Iteration: 2 || Loss: 67.20396621947843
Iteration: 3 || Loss: 67.2036344429522
Iteration: 4 || Loss: 67.20323615760442
Iteration: 5 || Loss: 67.20289105588353
Iteration: 6 || Loss: 67.20289105588353
saving ADAM checkpoint...
Sum of params:-108.51545
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.20289105588353
Iteration: 2 || Loss: 67.20074118395556
Iteration: 3 || Loss: 67.18574321371428
Iteration: 4 || Loss: 67.02170709563404
Iteration: 5 || Loss: 66.97968777257303
Iteration: 6 || Loss: 66.9015183036182
Iteration: 7 || Loss: 66.86477797730163
Iteration: 8 || Loss: 66.8475065416987
Iteration: 9 || Loss: 66.83768500611025
Iteration: 10 || Loss: 66.81920872502559
Iteration: 11 || Loss: 66.80501884472697
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.55796
Epoch 279 loss:66.80501884472697
MSE loss S0.9743490234011752
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.76113824842721
MSE loss S0.3503951712602228
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:101.23480030983625
MSE loss S1.9433117500857628
waveform batch: 2/2
Test loss - extrapolation:49.92760698335133
MSE loss S1.140732345366812
Epoch 279 mean train loss:3.31679376645726
Epoch 279 mean test loss - interpolation:3.9601897080712014
Epoch 279 mean test loss - extrapolation:12.596867274432299
Start training epoch 280
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.55796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4049504760667855
Iteration: 2 || Loss: 6.403958978151193
Iteration: 3 || Loss: 6.402975484641472
Iteration: 4 || Loss: 6.402037846178126
Iteration: 5 || Loss: 6.4010715531692455
Iteration: 6 || Loss: 6.4010715531692455
saving ADAM checkpoint...
Sum of params:-108.557884
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4010715531692455
Iteration: 2 || Loss: 6.3726381572411785
Iteration: 3 || Loss: 6.271868912036528
Iteration: 4 || Loss: 6.265214172873747
Iteration: 5 || Loss: 6.107525997258362
Iteration: 6 || Loss: 6.055330143453237
Iteration: 7 || Loss: 6.039061296401072
Iteration: 8 || Loss: 6.033927532923182
Iteration: 9 || Loss: 6.030984627267173
Iteration: 10 || Loss: 6.018587561298986
Iteration: 11 || Loss: 5.991754166170673
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.5708
Epoch 280 loss:5.991754166170673
MSE loss S0.19225661377859188
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.5708
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.145594876905214
Iteration: 2 || Loss: 24.14520526318873
Iteration: 3 || Loss: 24.144912178732774
Iteration: 4 || Loss: 24.144633881623037
Iteration: 5 || Loss: 24.144396660745592
Iteration: 6 || Loss: 24.144396660745592
saving ADAM checkpoint...
Sum of params:-108.571014
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.144396660745592
Iteration: 2 || Loss: 23.876505397677015
Iteration: 3 || Loss: 23.828155730701077
Iteration: 4 || Loss: 23.821661244190768
Iteration: 5 || Loss: 23.659060656835315
Iteration: 6 || Loss: 23.568191531706567
Iteration: 7 || Loss: 23.535893153300147
Iteration: 8 || Loss: 23.49853287498812
Iteration: 9 || Loss: 23.43470065582866
Iteration: 10 || Loss: 23.41690541919164
Iteration: 11 || Loss: 23.392745774975438
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.54703
Epoch 280 loss:23.392745774975438
MSE loss S0.4035665590308539
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.54703
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.18804710588309
Iteration: 2 || Loss: 67.18747478017073
Iteration: 3 || Loss: 67.18687467662384
Iteration: 4 || Loss: 67.1863201745125
Iteration: 5 || Loss: 67.18577333108053
Iteration: 6 || Loss: 67.18577333108053
saving ADAM checkpoint...
Sum of params:-108.54707
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.18577333108053
Iteration: 2 || Loss: 67.17564278378968
Iteration: 3 || Loss: 67.15661222576198
Iteration: 4 || Loss: 66.99654159184811
Iteration: 5 || Loss: 66.95592841219124
Iteration: 6 || Loss: 66.88245851328718
Iteration: 7 || Loss: 66.84699540143322
Iteration: 8 || Loss: 66.82773403056852
Iteration: 9 || Loss: 66.81763730145454
Iteration: 10 || Loss: 66.80053913094723
Iteration: 11 || Loss: 66.78553211406012
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.58616
Epoch 280 loss:66.78553211406012
MSE loss S0.9696318597320424
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.751249235874393
MSE loss S0.34921334873852994
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.98575541364174
MSE loss S1.9372768454089568
waveform batch: 2/2
Test loss - extrapolation:49.78213113269401
MSE loss S1.1339334282080669
Epoch 280 mean train loss:3.3162080019036635
Epoch 280 mean test loss - interpolation:3.958541539312399
Epoch 280 mean test loss - extrapolation:12.563990545527979
Start training epoch 281
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.58616
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.420500262398097
Iteration: 2 || Loss: 6.419559375501514
Iteration: 3 || Loss: 6.418599033689254
Iteration: 4 || Loss: 6.417607610043199
Iteration: 5 || Loss: 6.416643897941382
Iteration: 6 || Loss: 6.416643897941382
saving ADAM checkpoint...
Sum of params:-108.58605
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.416643897941382
Iteration: 2 || Loss: 6.388067184808218
Iteration: 3 || Loss: 6.283886798750685
Iteration: 4 || Loss: 6.2476618131806845
Iteration: 5 || Loss: 6.098768892356214
Iteration: 6 || Loss: 6.048434533714462
Iteration: 7 || Loss: 6.03689737564859
Iteration: 8 || Loss: 6.033962608413175
Iteration: 9 || Loss: 6.030176598544572
Iteration: 10 || Loss: 6.017147136734342
Iteration: 11 || Loss: 6.004489070203775
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.59477
Epoch 281 loss:6.004489070203775
MSE loss S0.199650397303537
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.59477
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.96304778588377
Iteration: 2 || Loss: 23.96284737842593
Iteration: 3 || Loss: 23.962445549001792
Iteration: 4 || Loss: 23.962144467712857
Iteration: 5 || Loss: 23.961912049715657
Iteration: 6 || Loss: 23.961912049715657
saving ADAM checkpoint...
Sum of params:-108.59494
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.961912049715657
Iteration: 2 || Loss: 23.86357735473297
Iteration: 3 || Loss: 23.698883919682377
Iteration: 4 || Loss: 23.693002063210233
Iteration: 5 || Loss: 23.503781962259325
Iteration: 6 || Loss: 23.468059065162805
Iteration: 7 || Loss: 23.45060892559521
Iteration: 8 || Loss: 23.4292440776191
Iteration: 9 || Loss: 23.395028542564884
Iteration: 10 || Loss: 23.388639230712972
Iteration: 11 || Loss: 23.377159765740217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.57196
Epoch 281 loss:23.377159765740217
MSE loss S0.3998528368130699
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.57196
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.18391966723439
Iteration: 2 || Loss: 67.18362674551763
Iteration: 3 || Loss: 67.18330814415029
Iteration: 4 || Loss: 67.18311385917411
Iteration: 5 || Loss: 67.18283649557115
Iteration: 6 || Loss: 67.18283649557115
saving ADAM checkpoint...
Sum of params:-108.57198
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.18283649557115
Iteration: 2 || Loss: 67.18134908537455
Iteration: 3 || Loss: 67.15553560922594
Iteration: 4 || Loss: 66.9802928552677
Iteration: 5 || Loss: 66.94908185832014
Iteration: 6 || Loss: 66.86609639316875
Iteration: 7 || Loss: 66.81560947139447
Iteration: 8 || Loss: 66.79880504316505
Iteration: 9 || Loss: 66.78851755795671
Iteration: 10 || Loss: 66.77519839857091
Iteration: 11 || Loss: 66.75778635119043
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.617584
Epoch 281 loss:66.75778635119043
MSE loss S0.9710970770130289
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.754387938755798
MSE loss S0.34985439159188
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.85938906192087
MSE loss S1.9329330868504202
waveform batch: 2/2
Test loss - extrapolation:49.764354489693254
MSE loss S1.1349704882475882
Epoch 281 mean train loss:3.315152937487394
Epoch 281 mean test loss - interpolation:3.9590646564592995
Epoch 281 mean test loss - extrapolation:12.551978629301177
Start training epoch 282
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.617584
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.406491231147026
Iteration: 2 || Loss: 6.405436879897479
Iteration: 3 || Loss: 6.404499223075852
Iteration: 4 || Loss: 6.403530521456612
Iteration: 5 || Loss: 6.402568489308464
Iteration: 6 || Loss: 6.402568489308464
saving ADAM checkpoint...
Sum of params:-108.61747
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.402568489308464
Iteration: 2 || Loss: 6.373146225240256
Iteration: 3 || Loss: 6.273906404106589
Iteration: 4 || Loss: 6.267602229721828
Iteration: 5 || Loss: 6.106569917101879
Iteration: 6 || Loss: 6.055449689890259
Iteration: 7 || Loss: 6.038708408606025
Iteration: 8 || Loss: 6.033682481416429
Iteration: 9 || Loss: 6.030904462703094
Iteration: 10 || Loss: 6.01888017307869
Iteration: 11 || Loss: 5.9906881513717645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.63137
Epoch 282 loss:5.9906881513717645
MSE loss S0.19354402884461164
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.63137
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.17627634894911
Iteration: 2 || Loss: 24.175910134434773
Iteration: 3 || Loss: 24.175565773091307
Iteration: 4 || Loss: 24.175240845726957
Iteration: 5 || Loss: 24.174928015822502
Iteration: 6 || Loss: 24.174928015822502
saving ADAM checkpoint...
Sum of params:-108.63151
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.174928015822502
Iteration: 2 || Loss: 23.89069330883237
Iteration: 3 || Loss: 23.84172683701815
Iteration: 4 || Loss: 23.828650489559017
Iteration: 5 || Loss: 23.668785429807055
Iteration: 6 || Loss: 23.570478821089452
Iteration: 7 || Loss: 23.537123789740477
Iteration: 8 || Loss: 23.500211893007243
Iteration: 9 || Loss: 23.44582314922986
Iteration: 10 || Loss: 23.414663508765518
Iteration: 11 || Loss: 23.392342115975396
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.607056
Epoch 282 loss:23.392342115975396
MSE loss S0.4011384005101052
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.607056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.13844470966245
Iteration: 2 || Loss: 67.13777459936215
Iteration: 3 || Loss: 67.13702753001384
Iteration: 4 || Loss: 67.13631145980263
Iteration: 5 || Loss: 67.1355867646943
Iteration: 6 || Loss: 67.1355867646943
saving ADAM checkpoint...
Sum of params:-108.60711
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.1355867646943
Iteration: 2 || Loss: 67.11907758097658
Iteration: 3 || Loss: 67.09790316207557
Iteration: 4 || Loss: 66.96678717355904
Iteration: 5 || Loss: 66.91052098094467
Iteration: 6 || Loss: 66.8379184784827
Iteration: 7 || Loss: 66.80133861716422
Iteration: 8 || Loss: 66.78170003950801
Iteration: 9 || Loss: 66.77121989133543
Iteration: 10 || Loss: 66.75310025196386
Iteration: 11 || Loss: 66.74144877722867
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.64481
Epoch 282 loss:66.74144877722867
MSE loss S0.9673318958751022
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.739585724115003
MSE loss S0.3478412610467845
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.65380933425267
MSE loss S1.9301078904765379
waveform batch: 2/2
Test loss - extrapolation:49.632579742152494
MSE loss S1.1282698958705235
Epoch 282 mean train loss:3.314637208433649
Epoch 282 mean test loss - interpolation:3.9565976206858338
Epoch 282 mean test loss - extrapolation:12.523865756367096
Start training epoch 283
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.64481
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.415224970088829
Iteration: 2 || Loss: 6.414222509336347
Iteration: 3 || Loss: 6.413246373115124
Iteration: 4 || Loss: 6.412278639422159
Iteration: 5 || Loss: 6.411297886822266
Iteration: 6 || Loss: 6.411297886822266
saving ADAM checkpoint...
Sum of params:-108.64476
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.411297886822266
Iteration: 2 || Loss: 6.381247891141924
Iteration: 3 || Loss: 6.275997481974972
Iteration: 4 || Loss: 6.264849809900482
Iteration: 5 || Loss: 6.094935143638816
Iteration: 6 || Loss: 6.042483151646523
Iteration: 7 || Loss: 6.034056315213041
Iteration: 8 || Loss: 6.029695567120466
Iteration: 9 || Loss: 6.027183557561028
Iteration: 10 || Loss: 6.015757195172393
Iteration: 11 || Loss: 5.996644057625424
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.65549
Epoch 283 loss:5.996644057625424
MSE loss S0.19578481692723865
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.65549
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.02770521660378
Iteration: 2 || Loss: 24.027422276097028
Iteration: 3 || Loss: 24.027209177235076
Iteration: 4 || Loss: 24.026813710391533
Iteration: 5 || Loss: 24.026630446842574
Iteration: 6 || Loss: 24.026630446842574
saving ADAM checkpoint...
Sum of params:-108.655624
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.026630446842574
Iteration: 2 || Loss: 23.76954380343361
Iteration: 3 || Loss: 23.75750944590976
Iteration: 4 || Loss: 23.75279315072859
Iteration: 5 || Loss: 23.547936643542986
Iteration: 6 || Loss: 23.501657176832126
Iteration: 7 || Loss: 23.480237076297154
Iteration: 8 || Loss: 23.45289739124142
Iteration: 9 || Loss: 23.400147875805413
Iteration: 10 || Loss: 23.394699166625173
Iteration: 11 || Loss: 23.375307671455676
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.63182
Epoch 283 loss:23.375307671455676
MSE loss S0.4007813707189752
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.63182
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.12514046453744
Iteration: 2 || Loss: 67.12482372263322
Iteration: 3 || Loss: 67.12453046092541
Iteration: 4 || Loss: 67.12432603129018
Iteration: 5 || Loss: 67.12407023746472
Iteration: 6 || Loss: 67.12407023746472
saving ADAM checkpoint...
Sum of params:-108.63187
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.12407023746472
Iteration: 2 || Loss: 67.12257127870171
Iteration: 3 || Loss: 67.10315675419557
Iteration: 4 || Loss: 66.94638619772837
Iteration: 5 || Loss: 66.89266019670129
Iteration: 6 || Loss: 66.81544019850885
Iteration: 7 || Loss: 66.77491769820521
Iteration: 8 || Loss: 66.7580232267851
Iteration: 9 || Loss: 66.7481080674258
Iteration: 10 || Loss: 66.73298566303339
Iteration: 11 || Loss: 66.71699316525014
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.67554
Epoch 283 loss:66.71699316525014
MSE loss S0.9709514194102795
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.746595575646737
MSE loss S0.3493931973137885
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.57783793459394
MSE loss S1.927669459271267
waveform batch: 2/2
Test loss - extrapolation:49.639807183071376
MSE loss S1.1318540047004861
Epoch 283 mean train loss:3.313411892907974
Epoch 283 mean test loss - interpolation:3.9577659292744563
Epoch 283 mean test loss - extrapolation:12.518137093138776
Start training epoch 284
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.67554
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.406128874877332
Iteration: 2 || Loss: 6.40513700369468
Iteration: 3 || Loss: 6.404155257411293
Iteration: 4 || Loss: 6.403139434062327
Iteration: 5 || Loss: 6.402179573823711
Iteration: 6 || Loss: 6.402179573823711
saving ADAM checkpoint...
Sum of params:-108.67546
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.402179573823711
Iteration: 2 || Loss: 6.372604864853855
Iteration: 3 || Loss: 6.274570543714147
Iteration: 4 || Loss: 6.266625993969421
Iteration: 5 || Loss: 6.104191583629776
Iteration: 6 || Loss: 6.053177689073324
Iteration: 7 || Loss: 6.035974096896887
Iteration: 8 || Loss: 6.031077431458531
Iteration: 9 || Loss: 6.02823839771627
Iteration: 10 || Loss: 6.01612423565267
Iteration: 11 || Loss: 5.98835972279881
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.688835
Epoch 284 loss:5.98835972279881
MSE loss S0.19199338751846934
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.688835
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.157158817622076
Iteration: 2 || Loss: 24.15683889947529
Iteration: 3 || Loss: 24.15642577261449
Iteration: 4 || Loss: 24.15618932722553
Iteration: 5 || Loss: 24.15573770380635
Iteration: 6 || Loss: 24.15573770380635
saving ADAM checkpoint...
Sum of params:-108.688965
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.15573770380635
Iteration: 2 || Loss: 24.021721577707936
Iteration: 3 || Loss: 23.821946442246194
Iteration: 4 || Loss: 23.81267865819707
Iteration: 5 || Loss: 23.631372315002217
Iteration: 6 || Loss: 23.566549798978293
Iteration: 7 || Loss: 23.534832315542186
Iteration: 8 || Loss: 23.49605565626967
Iteration: 9 || Loss: 23.418999241514864
Iteration: 10 || Loss: 23.410965348504735
Iteration: 11 || Loss: 23.385231682169596
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.66749
Epoch 284 loss:23.385231682169596
MSE loss S0.40472224214546076
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.66749
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.08425959175462
Iteration: 2 || Loss: 67.08379437834706
Iteration: 3 || Loss: 67.08348971251878
Iteration: 4 || Loss: 67.08309779876622
Iteration: 5 || Loss: 67.08268071493931
Iteration: 6 || Loss: 67.08268071493931
saving ADAM checkpoint...
Sum of params:-108.66753
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.08268071493931
Iteration: 2 || Loss: 67.08119460532447
Iteration: 3 || Loss: 67.06360306938606
Iteration: 4 || Loss: 66.90009733304164
Iteration: 5 || Loss: 66.86329133756018
Iteration: 6 || Loss: 66.79022522831305
Iteration: 7 || Loss: 66.75878136034432
Iteration: 8 || Loss: 66.74182103642221
Iteration: 9 || Loss: 66.72933708949861
Iteration: 10 || Loss: 66.70838194315417
Iteration: 11 || Loss: 66.69650983036307
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.70482
Epoch 284 loss:66.69650983036307
MSE loss S0.9725718879501515
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.738486060692605
MSE loss S0.3494639363687423
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.41603927707058
MSE loss S1.9263270087027138
waveform batch: 2/2
Test loss - extrapolation:49.586711947401
MSE loss S1.1299304360320532
Epoch 284 mean train loss:3.3127621115631545
Epoch 284 mean test loss - interpolation:3.9564143434487673
Epoch 284 mean test loss - extrapolation:12.500229268705965
Start training epoch 285
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.70482
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.404348993734197
Iteration: 2 || Loss: 6.403406506554402
Iteration: 3 || Loss: 6.402440748236323
Iteration: 4 || Loss: 6.401462685332538
Iteration: 5 || Loss: 6.4005607268259626
Iteration: 6 || Loss: 6.4005607268259626
saving ADAM checkpoint...
Sum of params:-108.704735
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4005607268259626
Iteration: 2 || Loss: 6.372525021938304
Iteration: 3 || Loss: 6.2665327276654486
Iteration: 4 || Loss: 6.2601076051798294
Iteration: 5 || Loss: 6.101464946567535
Iteration: 6 || Loss: 6.051144903482505
Iteration: 7 || Loss: 6.034646999492756
Iteration: 8 || Loss: 6.029624171242371
Iteration: 9 || Loss: 6.02621807356374
Iteration: 10 || Loss: 6.014122227087682
Iteration: 11 || Loss: 5.98802331081077
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.717026
Epoch 285 loss:5.98802331081077
MSE loss S0.1924047133180276
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.717026
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.134511490877916
Iteration: 2 || Loss: 24.1342417191578
Iteration: 3 || Loss: 24.133841627247442
Iteration: 4 || Loss: 24.133659467514168
Iteration: 5 || Loss: 24.133331209745055
Iteration: 6 || Loss: 24.133331209745055
saving ADAM checkpoint...
Sum of params:-108.71715
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.133331209745055
Iteration: 2 || Loss: 23.810771335966017
Iteration: 3 || Loss: 23.797209238848673
Iteration: 4 || Loss: 23.792174730607737
Iteration: 5 || Loss: 23.601846811602424
Iteration: 6 || Loss: 23.550735113556307
Iteration: 7 || Loss: 23.524467724907954
Iteration: 8 || Loss: 23.488147766601152
Iteration: 9 || Loss: 23.410189208281516
Iteration: 10 || Loss: 23.403842835327296
Iteration: 11 || Loss: 23.380744896565187
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.692726
Epoch 285 loss:23.380744896565187
MSE loss S0.4017326072956356
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.692726
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.09165596755821
Iteration: 2 || Loss: 67.09109770135528
Iteration: 3 || Loss: 67.09054397400251
Iteration: 4 || Loss: 67.09004306559117
Iteration: 5 || Loss: 67.08955256949038
Iteration: 6 || Loss: 67.08955256949038
saving ADAM checkpoint...
Sum of params:-108.69275
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.08955256949038
Iteration: 2 || Loss: 67.08021169252113
Iteration: 3 || Loss: 67.05901591365563
Iteration: 4 || Loss: 66.90163942966942
Iteration: 5 || Loss: 66.86179396986566
Iteration: 6 || Loss: 66.77622555064573
Iteration: 7 || Loss: 66.7364706789894
Iteration: 8 || Loss: 66.71748563555522
Iteration: 9 || Loss: 66.70903163584032
Iteration: 10 || Loss: 66.69402272322375
Iteration: 11 || Loss: 66.67806646451413
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.732956
Epoch 285 loss:66.67806646451413
MSE loss S0.9669540171053053
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.734054798269373
MSE loss S0.3478889257537858
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.27707083024511
MSE loss S1.9216917765750183
waveform batch: 2/2
Test loss - extrapolation:49.477711916119596
MSE loss S1.1255471148665914
Epoch 285 mean train loss:3.3119598162720716
Epoch 285 mean test loss - interpolation:3.955675799711562
Epoch 285 mean test loss - extrapolation:12.479565228863725
Start training epoch 286
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.732956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.434500023475468
Iteration: 2 || Loss: 6.433535414680392
Iteration: 3 || Loss: 6.432573542969832
Iteration: 4 || Loss: 6.431545792185926
Iteration: 5 || Loss: 6.430615613483977
Iteration: 6 || Loss: 6.430615613483977
saving ADAM checkpoint...
Sum of params:-108.73285
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.430615613483977
Iteration: 2 || Loss: 6.402171266296838
Iteration: 3 || Loss: 6.290543347157155
Iteration: 4 || Loss: 6.282694348834136
Iteration: 5 || Loss: 6.095697872624265
Iteration: 6 || Loss: 6.043981197450617
Iteration: 7 || Loss: 6.0318300469941
Iteration: 8 || Loss: 6.027465598179149
Iteration: 9 || Loss: 6.024459541999958
Iteration: 10 || Loss: 6.014567821936507
Iteration: 11 || Loss: 5.995673107238944
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.74352
Epoch 286 loss:5.995673107238944
MSE loss S0.19546707767953356
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.74352
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.004732514000448
Iteration: 2 || Loss: 24.00450152492019
Iteration: 3 || Loss: 24.004152010256135
Iteration: 4 || Loss: 24.00389723414731
Iteration: 5 || Loss: 24.003566746879603
Iteration: 6 || Loss: 24.003566746879603
saving ADAM checkpoint...
Sum of params:-108.74368
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.003566746879603
Iteration: 2 || Loss: 23.78951295117376
Iteration: 3 || Loss: 23.767287952438487
Iteration: 4 || Loss: 23.76008083015437
Iteration: 5 || Loss: 23.55317329922015
Iteration: 6 || Loss: 23.488878073685836
Iteration: 7 || Loss: 23.465885153140224
Iteration: 8 || Loss: 23.438265553468568
Iteration: 9 || Loss: 23.393796922810818
Iteration: 10 || Loss: 23.388005529773892
Iteration: 11 || Loss: 23.37051597962397
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.71995
Epoch 286 loss:23.37051597962397
MSE loss S0.39875334849160704
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.71995
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.0789117821328
Iteration: 2 || Loss: 67.07860524064158
Iteration: 3 || Loss: 67.07831291333366
Iteration: 4 || Loss: 67.07808630261184
Iteration: 5 || Loss: 67.07782203390002
Iteration: 6 || Loss: 67.07782203390002
saving ADAM checkpoint...
Sum of params:-108.72002
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.07782203390002
Iteration: 2 || Loss: 67.0762458100523
Iteration: 3 || Loss: 67.05119888581649
Iteration: 4 || Loss: 66.88323191773135
Iteration: 5 || Loss: 66.84234712135105
Iteration: 6 || Loss: 66.7557390794984
Iteration: 7 || Loss: 66.71012428952885
Iteration: 8 || Loss: 66.69314574052409
Iteration: 9 || Loss: 66.6826954271977
Iteration: 10 || Loss: 66.6685996920583
Iteration: 11 || Loss: 66.6519402526763
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.76399
Epoch 286 loss:66.6519402526763
MSE loss S0.970624453103887
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.736414257283077
MSE loss S0.3494309585698881
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.1151960903684
MSE loss S1.9174508942639517
waveform batch: 2/2
Test loss - extrapolation:49.44962472028875
MSE loss S1.1260827621542615
Epoch 286 mean train loss:3.3109699772254904
Epoch 286 mean test loss - interpolation:3.9560690428805128
Epoch 286 mean test loss - extrapolation:12.463735067554763
Start training epoch 287
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.76399
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.403678404180976
Iteration: 2 || Loss: 6.402661457966634
Iteration: 3 || Loss: 6.4015709864958374
Iteration: 4 || Loss: 6.400658956166058
Iteration: 5 || Loss: 6.39971257759547
Iteration: 6 || Loss: 6.39971257759547
saving ADAM checkpoint...
Sum of params:-108.7639
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.39971257759547
Iteration: 2 || Loss: 6.36967751191901
Iteration: 3 || Loss: 6.276334776103789
Iteration: 4 || Loss: 6.267389096849562
Iteration: 5 || Loss: 6.101352717152163
Iteration: 6 || Loss: 6.050588941271077
Iteration: 7 || Loss: 6.0339705947342805
Iteration: 8 || Loss: 6.0290518492226095
Iteration: 9 || Loss: 6.026216553069821
Iteration: 10 || Loss: 6.014625036816242
Iteration: 11 || Loss: 5.989428049237383
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.776596
Epoch 287 loss:5.989428049237383
MSE loss S0.19246836435957226
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.776596
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.094767810985843
Iteration: 2 || Loss: 24.094344644197612
Iteration: 3 || Loss: 24.094073391433543
Iteration: 4 || Loss: 24.09375674566868
Iteration: 5 || Loss: 24.093507734686884
Iteration: 6 || Loss: 24.093507734686884
saving ADAM checkpoint...
Sum of params:-108.77669
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.093507734686884
Iteration: 2 || Loss: 23.830741536215257
Iteration: 3 || Loss: 23.7961346350514
Iteration: 4 || Loss: 23.787442898009175
Iteration: 5 || Loss: 23.617464015900477
Iteration: 6 || Loss: 23.536497941023306
Iteration: 7 || Loss: 23.50651023747212
Iteration: 8 || Loss: 23.473539815366916
Iteration: 9 || Loss: 23.40874737783182
Iteration: 10 || Loss: 23.399132320270567
Iteration: 11 || Loss: 23.374671702639315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.75368
Epoch 287 loss:23.374671702639315
MSE loss S0.40217791071853737
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.75368
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.02891262650786
Iteration: 2 || Loss: 67.02848639998503
Iteration: 3 || Loss: 67.02807091329069
Iteration: 4 || Loss: 67.02766113397243
Iteration: 5 || Loss: 67.0272414534367
Iteration: 6 || Loss: 67.0272414534367
saving ADAM checkpoint...
Sum of params:-108.753685
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.0272414534367
Iteration: 2 || Loss: 67.02590036569217
Iteration: 3 || Loss: 67.00691789546092
Iteration: 4 || Loss: 66.82496607197515
Iteration: 5 || Loss: 66.79790693184738
Iteration: 6 || Loss: 66.72438817190772
Iteration: 7 || Loss: 66.68922832373642
Iteration: 8 || Loss: 66.67185312564592
Iteration: 9 || Loss: 66.6619944682267
Iteration: 10 || Loss: 66.64325083990815
Iteration: 11 || Loss: 66.6295701630693
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.79468
Epoch 287 loss:66.6295701630693
MSE loss S0.9718422889573075
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.730009253935407
MSE loss S0.3494954230944419
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.97127561015344
MSE loss S1.9159230883059908
waveform batch: 2/2
Test loss - extrapolation:49.39428842545296
MSE loss S1.1253265568108033
Epoch 287 mean train loss:3.3101265487912412
Epoch 287 mean test loss - interpolation:3.955001542322568
Epoch 287 mean test loss - extrapolation:12.447130336300534
Start training epoch 288
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.79468
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3991024920165565
Iteration: 2 || Loss: 6.39816947595976
Iteration: 3 || Loss: 6.3971452325857046
Iteration: 4 || Loss: 6.396225527306075
Iteration: 5 || Loss: 6.395214735002131
Iteration: 6 || Loss: 6.395214735002131
saving ADAM checkpoint...
Sum of params:-108.7946
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.395214735002131
Iteration: 2 || Loss: 6.366826606349775
Iteration: 3 || Loss: 6.2709307052871175
Iteration: 4 || Loss: 6.261497477106333
Iteration: 5 || Loss: 6.100942545896077
Iteration: 6 || Loss: 6.049945928697255
Iteration: 7 || Loss: 6.033006837782529
Iteration: 8 || Loss: 6.028124770489333
Iteration: 9 || Loss: 6.024829300423657
Iteration: 10 || Loss: 6.013266621127713
Iteration: 11 || Loss: 5.988812583151623
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.80715
Epoch 288 loss:5.988812583151623
MSE loss S0.19253961070265818
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.80715
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.0935743718678
Iteration: 2 || Loss: 24.09338024134964
Iteration: 3 || Loss: 24.09304210805041
Iteration: 4 || Loss: 24.09268330942031
Iteration: 5 || Loss: 24.092536139206793
Iteration: 6 || Loss: 24.092536139206793
saving ADAM checkpoint...
Sum of params:-108.80727
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.092536139206793
Iteration: 2 || Loss: 23.7996506555955
Iteration: 3 || Loss: 23.78217098600041
Iteration: 4 || Loss: 23.770413841448338
Iteration: 5 || Loss: 23.583002366124948
Iteration: 6 || Loss: 23.52843541444475
Iteration: 7 || Loss: 23.502330358051154
Iteration: 8 || Loss: 23.470173236759248
Iteration: 9 || Loss: 23.40052140021288
Iteration: 10 || Loss: 23.394455043779345
Iteration: 11 || Loss: 23.37309688843618
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.782776
Epoch 288 loss:23.37309688843618
MSE loss S0.40099544310129187
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.782776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 67.02232536403676
Iteration: 2 || Loss: 67.02186799204934
Iteration: 3 || Loss: 67.02137705424067
Iteration: 4 || Loss: 67.02090502777145
Iteration: 5 || Loss: 67.02049764600599
Iteration: 6 || Loss: 67.02049764600599
saving ADAM checkpoint...
Sum of params:-108.7828
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 67.02049764600599
Iteration: 2 || Loss: 67.01329596733798
Iteration: 3 || Loss: 66.99151989917773
Iteration: 4 || Loss: 66.82131392499686
Iteration: 5 || Loss: 66.78846417039527
Iteration: 6 || Loss: 66.70594593514659
Iteration: 7 || Loss: 66.66687750201324
Iteration: 8 || Loss: 66.64822365328725
Iteration: 9 || Loss: 66.639869241616
Iteration: 10 || Loss: 66.62365888512508
Iteration: 11 || Loss: 66.60821256891063
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.824005
Epoch 288 loss:66.60821256891063
MSE loss S0.9680868565218911
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.723779754154076
MSE loss S0.3487932995929502
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.80174564612251
MSE loss S1.9113153635070987
waveform batch: 2/2
Test loss - extrapolation:49.28893630019061
MSE loss S1.1215253661876066
Epoch 288 mean train loss:3.3093145531206356
Epoch 288 mean test loss - interpolation:3.953963292359013
Epoch 288 mean test loss - extrapolation:12.424223495526093
Start training epoch 289
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.824005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.41830556921254
Iteration: 2 || Loss: 6.417365174987356
Iteration: 3 || Loss: 6.416367995217948
Iteration: 4 || Loss: 6.415424578028269
Iteration: 5 || Loss: 6.414499050132143
Iteration: 6 || Loss: 6.414499050132143
saving ADAM checkpoint...
Sum of params:-108.82391
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.414499050132143
Iteration: 2 || Loss: 6.3878515885653355
Iteration: 3 || Loss: 6.286797613512389
Iteration: 4 || Loss: 6.277253792205417
Iteration: 5 || Loss: 6.096878549822893
Iteration: 6 || Loss: 6.0469246103816925
Iteration: 7 || Loss: 6.03174800475479
Iteration: 8 || Loss: 6.0270875441366805
Iteration: 9 || Loss: 6.023506029214259
Iteration: 10 || Loss: 6.013071080192547
Iteration: 11 || Loss: 5.992137407618198
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.83552
Epoch 289 loss:5.992137407618198
MSE loss S0.1941694456829473
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.83552
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.026580401511367
Iteration: 2 || Loss: 24.026304707717795
Iteration: 3 || Loss: 24.025985689499958
Iteration: 4 || Loss: 24.02574664743161
Iteration: 5 || Loss: 24.025560211306285
Iteration: 6 || Loss: 24.025560211306285
saving ADAM checkpoint...
Sum of params:-108.835625
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.025560211306285
Iteration: 2 || Loss: 23.767729028878787
Iteration: 3 || Loss: 23.756342192764887
Iteration: 4 || Loss: 23.751873325093552
Iteration: 5 || Loss: 23.538229424177118
Iteration: 6 || Loss: 23.493721009575978
Iteration: 7 || Loss: 23.47353410270783
Iteration: 8 || Loss: 23.445597376471753
Iteration: 9 || Loss: 23.391170334984
Iteration: 10 || Loss: 23.38504005507605
Iteration: 11 || Loss: 23.366163744307478
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.81195
Epoch 289 loss:23.366163744307478
MSE loss S0.400003606697735
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.81195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.9994116014135
Iteration: 2 || Loss: 66.99908986473697
Iteration: 3 || Loss: 66.99875861417415
Iteration: 4 || Loss: 66.99845257035165
Iteration: 5 || Loss: 66.9981565810374
Iteration: 6 || Loss: 66.9981565810374
saving ADAM checkpoint...
Sum of params:-108.812004
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.9981565810374
Iteration: 2 || Loss: 66.99666336558373
Iteration: 3 || Loss: 66.97224059308097
Iteration: 4 || Loss: 66.81042693797671
Iteration: 5 || Loss: 66.77040483099476
Iteration: 6 || Loss: 66.68436340322626
Iteration: 7 || Loss: 66.64116311133141
Iteration: 8 || Loss: 66.62456362868626
Iteration: 9 || Loss: 66.6144803501265
Iteration: 10 || Loss: 66.59976951532795
Iteration: 11 || Loss: 66.58423619123982
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.854355
Epoch 289 loss:66.58423619123982
MSE loss S0.970710724591604
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.7256030439885
MSE loss S0.34948598161228084
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.67774924066994
MSE loss S1.9084845468494354
waveform batch: 2/2
Test loss - extrapolation:49.26520525341421
MSE loss S1.1211224313315675
Epoch 289 mean train loss:3.3083633566608794
Epoch 289 mean test loss - interpolation:3.9542671739980833
Epoch 289 mean test loss - extrapolation:12.411912874507012
Start training epoch 290
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.854355
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.399286460776459
Iteration: 2 || Loss: 6.398360068994453
Iteration: 3 || Loss: 6.397389405870375
Iteration: 4 || Loss: 6.396380210865113
Iteration: 5 || Loss: 6.395358480729464
Iteration: 6 || Loss: 6.395358480729464
saving ADAM checkpoint...
Sum of params:-108.85429
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.395358480729464
Iteration: 2 || Loss: 6.36575252698988
Iteration: 3 || Loss: 6.275641287183807
Iteration: 4 || Loss: 6.26544516416888
Iteration: 5 || Loss: 6.099576821308739
Iteration: 6 || Loss: 6.04708168114269
Iteration: 7 || Loss: 6.032135225126992
Iteration: 8 || Loss: 6.027337282050469
Iteration: 9 || Loss: 6.0241581432044455
Iteration: 10 || Loss: 6.013247399602919
Iteration: 11 || Loss: 5.990716477272522
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.866486
Epoch 290 loss:5.990716477272522
MSE loss S0.19360250299177617
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.866486
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.046578909671588
Iteration: 2 || Loss: 24.046357048227808
Iteration: 3 || Loss: 24.046156101821374
Iteration: 4 || Loss: 24.045966476967497
Iteration: 5 || Loss: 24.045603325858412
Iteration: 6 || Loss: 24.045603325858412
saving ADAM checkpoint...
Sum of params:-108.8666
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.045603325858412
Iteration: 2 || Loss: 23.79700423795601
Iteration: 3 || Loss: 23.765971178779136
Iteration: 4 || Loss: 23.750034854873146
Iteration: 5 || Loss: 23.576821814201395
Iteration: 6 || Loss: 23.508032856017714
Iteration: 7 || Loss: 23.48168082339555
Iteration: 8 || Loss: 23.452872098323663
Iteration: 9 || Loss: 23.393156339485955
Iteration: 10 || Loss: 23.387234664091995
Iteration: 11 || Loss: 23.367576716302764
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.84212
Epoch 290 loss:23.367576716302764
MSE loss S0.40001203619836406
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.84212
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.97836236364067
Iteration: 2 || Loss: 66.97787782220976
Iteration: 3 || Loss: 66.97744771881707
Iteration: 4 || Loss: 66.97700777291858
Iteration: 5 || Loss: 66.9766135128645
Iteration: 6 || Loss: 66.9766135128645
saving ADAM checkpoint...
Sum of params:-108.842155
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.9766135128645
Iteration: 2 || Loss: 66.97517597997593
Iteration: 3 || Loss: 66.9518587966019
Iteration: 4 || Loss: 66.77227625241248
Iteration: 5 || Loss: 66.74206337538692
Iteration: 6 || Loss: 66.65960742072124
Iteration: 7 || Loss: 66.6202184578823
Iteration: 8 || Loss: 66.60145176554987
Iteration: 9 || Loss: 66.59307926434823
Iteration: 10 || Loss: 66.57563442737514
Iteration: 11 || Loss: 66.56139829941266
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.88389
Epoch 290 loss:66.56139829941266
MSE loss S0.968277886067739
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.718269280869276
MSE loss S0.3489833335122236
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.49947429334487
MSE loss S1.9044623367191802
waveform batch: 2/2
Test loss - extrapolation:49.16980813420887
MSE loss S1.1186136541231604
Epoch 290 mean train loss:3.307575568723722
Epoch 290 mean test loss - interpolation:3.9530448801448794
Epoch 290 mean test loss - extrapolation:12.389106868962813
Start training epoch 291
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.88389
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.404361232974716
Iteration: 2 || Loss: 6.403368522734138
Iteration: 3 || Loss: 6.402419965549008
Iteration: 4 || Loss: 6.4015120212569645
Iteration: 5 || Loss: 6.40051707211812
Iteration: 6 || Loss: 6.40051707211812
saving ADAM checkpoint...
Sum of params:-108.883804
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.40051707211812
Iteration: 2 || Loss: 6.372586346027855
Iteration: 3 || Loss: 6.28090987199643
Iteration: 4 || Loss: 6.269383239626257
Iteration: 5 || Loss: 6.0985753171879376
Iteration: 6 || Loss: 6.0491356515377515
Iteration: 7 || Loss: 6.031848702946622
Iteration: 8 || Loss: 6.0270414366221114
Iteration: 9 || Loss: 6.023522448925045
Iteration: 10 || Loss: 6.013025638002682
Iteration: 11 || Loss: 5.990660019142156
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.895905
Epoch 291 loss:5.990660019142156
MSE loss S0.19381128770306824
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.895905
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.039342615099873
Iteration: 2 || Loss: 24.039018901805
Iteration: 3 || Loss: 24.038859957987018
Iteration: 4 || Loss: 24.038474753398397
Iteration: 5 || Loss: 24.038195078837255
Iteration: 6 || Loss: 24.038195078837255
saving ADAM checkpoint...
Sum of params:-108.89602
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.038195078837255
Iteration: 2 || Loss: 23.800916377655597
Iteration: 3 || Loss: 23.762797345103152
Iteration: 4 || Loss: 23.727015826305475
Iteration: 5 || Loss: 23.578399065304314
Iteration: 6 || Loss: 23.504152938605106
Iteration: 7 || Loss: 23.477488502299515
Iteration: 8 || Loss: 23.44886535974182
Iteration: 9 || Loss: 23.390928937175715
Iteration: 10 || Loss: 23.384668167484058
Iteration: 11 || Loss: 23.364812431790263
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.87184
Epoch 291 loss:23.364812431790263
MSE loss S0.3993233956486331
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.87184
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.94563872700118
Iteration: 2 || Loss: 66.94518559270877
Iteration: 3 || Loss: 66.94476686876759
Iteration: 4 || Loss: 66.94433853915457
Iteration: 5 || Loss: 66.94391797591805
Iteration: 6 || Loss: 66.94391797591805
saving ADAM checkpoint...
Sum of params:-108.87187
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.94391797591805
Iteration: 2 || Loss: 66.94221125526877
Iteration: 3 || Loss: 66.92158823322224
Iteration: 4 || Loss: 66.74687977018192
Iteration: 5 || Loss: 66.71382255289676
Iteration: 6 || Loss: 66.63507859887841
Iteration: 7 || Loss: 66.59630824014864
Iteration: 8 || Loss: 66.57809296526612
Iteration: 9 || Loss: 66.5697506399571
Iteration: 10 || Loss: 66.55199415581329
Iteration: 11 || Loss: 66.53859896698664
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.91359
Epoch 291 loss:66.53859896698664
MSE loss S0.9697347028088709
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.71549614495477
MSE loss S0.3493980541930013
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.36866566912299
MSE loss S1.902123571914991
waveform batch: 2/2
Test loss - extrapolation:49.12725178557081
MSE loss S1.1180999134483822
Epoch 291 mean train loss:3.3066921178592783
Epoch 291 mean test loss - interpolation:3.952582690825795
Epoch 291 mean test loss - extrapolation:12.374659787891149
Start training epoch 292
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.91359
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.395878536810315
Iteration: 2 || Loss: 6.394900021749724
Iteration: 3 || Loss: 6.393924515136246
Iteration: 4 || Loss: 6.393002915374367
Iteration: 5 || Loss: 6.392023172251598
Iteration: 6 || Loss: 6.392023172251598
saving ADAM checkpoint...
Sum of params:-108.9135
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.392023172251598
Iteration: 2 || Loss: 6.3641634009978265
Iteration: 3 || Loss: 6.275758963052859
Iteration: 4 || Loss: 6.26400911723202
Iteration: 5 || Loss: 6.099175243471392
Iteration: 6 || Loss: 6.04930708613659
Iteration: 7 || Loss: 6.03159363501074
Iteration: 8 || Loss: 6.026732735867472
Iteration: 9 || Loss: 6.022829094597202
Iteration: 10 || Loss: 6.012449483369814
Iteration: 11 || Loss: 5.989608361374639
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.92591
Epoch 292 loss:5.989608361374639
MSE loss S0.19344675464077501
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.92591
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.04533208695795
Iteration: 2 || Loss: 24.045088272026664
Iteration: 3 || Loss: 24.044807461154743
Iteration: 4 || Loss: 24.044463970409602
Iteration: 5 || Loss: 24.044225040812186
Iteration: 6 || Loss: 24.044225040812186
saving ADAM checkpoint...
Sum of params:-108.92602
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.044225040812186
Iteration: 2 || Loss: 23.798576769699093
Iteration: 3 || Loss: 23.7623272563738
Iteration: 4 || Loss: 23.65355761366635
Iteration: 5 || Loss: 23.58041842042039
Iteration: 6 || Loss: 23.50702353998205
Iteration: 7 || Loss: 23.47992497129199
Iteration: 8 || Loss: 23.450622542177385
Iteration: 9 || Loss: 23.38980327694077
Iteration: 10 || Loss: 23.38356709971222
Iteration: 11 || Loss: 23.36321109865992
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.90224
Epoch 292 loss:23.36321109865992
MSE loss S0.4000518788123732
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.90224
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.91900526718423
Iteration: 2 || Loss: 66.91856714700388
Iteration: 3 || Loss: 66.9181287127635
Iteration: 4 || Loss: 66.91774164926608
Iteration: 5 || Loss: 66.91736271344436
Iteration: 6 || Loss: 66.91736271344436
saving ADAM checkpoint...
Sum of params:-108.90228
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.91736271344436
Iteration: 2 || Loss: 66.91542302413828
Iteration: 3 || Loss: 66.89757660727632
Iteration: 4 || Loss: 66.71707274880656
Iteration: 5 || Loss: 66.6872551256774
Iteration: 6 || Loss: 66.6112559942559
Iteration: 7 || Loss: 66.57365638116912
Iteration: 8 || Loss: 66.55575137509362
Iteration: 9 || Loss: 66.54725358443335
Iteration: 10 || Loss: 66.52957922151678
Iteration: 11 || Loss: 66.51634583217184
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.94276
Epoch 292 loss:66.51634583217184
MSE loss S0.9701616509353997
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.712121419746794
MSE loss S0.3495742565468401
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.23165504950329
MSE loss S1.8993008692566917
waveform batch: 2/2
Test loss - extrapolation:49.07248514764982
MSE loss S1.1167744556583106
Epoch 292 mean train loss:3.305833285938151
Epoch 292 mean test loss - interpolation:3.9520202366244654
Epoch 292 mean test loss - extrapolation:12.35867834976276
Start training epoch 293
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.94276
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.391988360879724
Iteration: 2 || Loss: 6.390991164778255
Iteration: 3 || Loss: 6.390021745816706
Iteration: 4 || Loss: 6.389074192521993
Iteration: 5 || Loss: 6.388179190387534
Iteration: 6 || Loss: 6.388179190387534
saving ADAM checkpoint...
Sum of params:-108.942665
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.388179190387534
Iteration: 2 || Loss: 6.360706585047685
Iteration: 3 || Loss: 6.27302372274384
Iteration: 4 || Loss: 6.261875194860627
Iteration: 5 || Loss: 6.098928424619942
Iteration: 6 || Loss: 6.04872849758022
Iteration: 7 || Loss: 6.031182341285481
Iteration: 8 || Loss: 6.026305737846099
Iteration: 9 || Loss: 6.022716796548706
Iteration: 10 || Loss: 6.01193155924432
Iteration: 11 || Loss: 5.989617828607808
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.954926
Epoch 293 loss:5.989617828607808
MSE loss S0.19352137003629738
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.954926
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.032258178141678
Iteration: 2 || Loss: 24.031890297114444
Iteration: 3 || Loss: 24.031639324152202
Iteration: 4 || Loss: 24.031386065265913
Iteration: 5 || Loss: 24.031166919920555
Iteration: 6 || Loss: 24.031166919920555
saving ADAM checkpoint...
Sum of params:-108.954994
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.031166919920555
Iteration: 2 || Loss: 23.787631229404283
Iteration: 3 || Loss: 23.75478222475251
Iteration: 4 || Loss: 23.709869713200113
Iteration: 5 || Loss: 23.57074536264531
Iteration: 6 || Loss: 23.501426264346897
Iteration: 7 || Loss: 23.475173647128045
Iteration: 8 || Loss: 23.446424667935318
Iteration: 9 || Loss: 23.388360328359056
Iteration: 10 || Loss: 23.380963432654642
Iteration: 11 || Loss: 23.361828546915905
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.93085
Epoch 293 loss:23.361828546915905
MSE loss S0.3994021239978165
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.93085
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.89691312516297
Iteration: 2 || Loss: 66.89641658411037
Iteration: 3 || Loss: 66.89593074819521
Iteration: 4 || Loss: 66.89555017082087
Iteration: 5 || Loss: 66.89513528692497
Iteration: 6 || Loss: 66.89513528692497
saving ADAM checkpoint...
Sum of params:-108.93089
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.89513528692497
Iteration: 2 || Loss: 66.89382770287928
Iteration: 3 || Loss: 66.87207122797352
Iteration: 4 || Loss: 66.70183160182239
Iteration: 5 || Loss: 66.66969070023062
Iteration: 6 || Loss: 66.5902538041428
Iteration: 7 || Loss: 66.55183617786157
Iteration: 8 || Loss: 66.53358778177305
Iteration: 9 || Loss: 66.52536223786922
Iteration: 10 || Loss: 66.50756536129235
Iteration: 11 || Loss: 66.49437079966008
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.97138
Epoch 293 loss:66.49437079966008
MSE loss S0.9685408820163333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.707787915951027
MSE loss S0.3492447413733952
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.07806641635953
MSE loss S1.8954770469415907
waveform batch: 2/2
Test loss - extrapolation:48.99555437738443
MSE loss S1.1143671800575001
Epoch 293 mean train loss:3.3050281784546134
Epoch 293 mean test loss - interpolation:3.951297985991838
Epoch 293 mean test loss - extrapolation:12.339468399478664
Start training epoch 294
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-108.97138
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.395605684070853
Iteration: 2 || Loss: 6.39462095935918
Iteration: 3 || Loss: 6.393732878353708
Iteration: 4 || Loss: 6.39272570538296
Iteration: 5 || Loss: 6.3917694963693945
Iteration: 6 || Loss: 6.3917694963693945
saving ADAM checkpoint...
Sum of params:-108.971306
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3917694963693945
Iteration: 2 || Loss: 6.36452948829746
Iteration: 3 || Loss: 6.276433808379121
Iteration: 4 || Loss: 6.2646714077777546
Iteration: 5 || Loss: 6.097996243570505
Iteration: 6 || Loss: 6.048510853700983
Iteration: 7 || Loss: 6.03076061422032
Iteration: 8 || Loss: 6.025987428270456
Iteration: 9 || Loss: 6.022433636319385
Iteration: 10 || Loss: 6.0116935401102145
Iteration: 11 || Loss: 5.989444523497805
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.98363
Epoch 294 loss:5.989444523497805
MSE loss S0.1935147632953526
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-108.98363
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.030040742469406
Iteration: 2 || Loss: 24.02983345753461
Iteration: 3 || Loss: 24.029587447809643
Iteration: 4 || Loss: 24.029374867543844
Iteration: 5 || Loss: 24.029073102322727
Iteration: 6 || Loss: 24.029073102322727
saving ADAM checkpoint...
Sum of params:-108.983696
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.029073102322727
Iteration: 2 || Loss: 23.74877234348935
Iteration: 3 || Loss: 23.747267228807814
Iteration: 4 || Loss: 23.737878469007157
Iteration: 5 || Loss: 23.529942550393084
Iteration: 6 || Loss: 23.481900776477747
Iteration: 7 || Loss: 23.46451842944577
Iteration: 8 || Loss: 23.436967710414468
Iteration: 9 || Loss: 23.38220020516147
Iteration: 10 || Loss: 23.371708651615208
Iteration: 11 || Loss: 23.357260446677348
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.96103
Epoch 294 loss:23.357260446677348
MSE loss S0.3999675689346802
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.96103
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.86615146310092
Iteration: 2 || Loss: 66.86579752940966
Iteration: 3 || Loss: 66.86550980482129
Iteration: 4 || Loss: 66.86525875478841
Iteration: 5 || Loss: 66.86501324723284
Iteration: 6 || Loss: 66.86501324723284
saving ADAM checkpoint...
Sum of params:-108.961044
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.86501324723284
Iteration: 2 || Loss: 66.86359478431258
Iteration: 3 || Loss: 66.84131919509568
Iteration: 4 || Loss: 66.70061802328864
Iteration: 5 || Loss: 66.64646143630294
Iteration: 6 || Loss: 66.5669953415238
Iteration: 7 || Loss: 66.52796430429274
Iteration: 8 || Loss: 66.51168650211716
Iteration: 9 || Loss: 66.50171548853686
Iteration: 10 || Loss: 66.48730898992709
Iteration: 11 || Loss: 66.47247883091532
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.00041
Epoch 294 loss:66.47247883091532
MSE loss S0.9686854539362489
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.706030996695855
MSE loss S0.3493675890750789
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.9422770048212
MSE loss S1.8920161514323475
waveform batch: 2/2
Test loss - extrapolation:48.93975090200831
MSE loss S1.1125938469016523
Epoch 294 mean train loss:3.3041097862444992
Epoch 294 mean test loss - interpolation:3.951005166115976
Epoch 294 mean test loss - extrapolation:12.323502325569125
Start training epoch 295
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.00041
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.391327942338398
Iteration: 2 || Loss: 6.390372898965346
Iteration: 3 || Loss: 6.389480082687193
Iteration: 4 || Loss: 6.388443164547132
Iteration: 5 || Loss: 6.387532922359575
Iteration: 6 || Loss: 6.387532922359575
saving ADAM checkpoint...
Sum of params:-109.00032
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.387532922359575
Iteration: 2 || Loss: 6.359528011126818
Iteration: 3 || Loss: 6.272386931784799
Iteration: 4 || Loss: 6.262468666593158
Iteration: 5 || Loss: 6.097790948747285
Iteration: 6 || Loss: 6.047213040776581
Iteration: 7 || Loss: 6.030463334986722
Iteration: 8 || Loss: 6.025681931555822
Iteration: 9 || Loss: 6.02241057187732
Iteration: 10 || Loss: 6.01134495448994
Iteration: 11 || Loss: 5.989816159847622
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.01217
Epoch 295 loss:5.989816159847622
MSE loss S0.19386933953309554
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.01217
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.007452086911268
Iteration: 2 || Loss: 24.007148546287326
Iteration: 3 || Loss: 24.006829200215066
Iteration: 4 || Loss: 24.006564572938135
Iteration: 5 || Loss: 24.006398596128587
Iteration: 6 || Loss: 24.006398596128587
saving ADAM checkpoint...
Sum of params:-109.01229
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.006398596128587
Iteration: 2 || Loss: 23.748752941827064
Iteration: 3 || Loss: 23.73274870014494
Iteration: 4 || Loss: 23.728328424393023
Iteration: 5 || Loss: 23.536674759188884
Iteration: 6 || Loss: 23.488377631933464
Iteration: 7 || Loss: 23.465756227506567
Iteration: 8 || Loss: 23.437809630021537
Iteration: 9 || Loss: 23.379788435721718
Iteration: 10 || Loss: 23.37386788330257
Iteration: 11 || Loss: 23.355603501622195
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-108.989655
Epoch 295 loss:23.355603501622195
MSE loss S0.39953151608751536
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-108.989655
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.8533080731377
Iteration: 2 || Loss: 66.85296168483993
Iteration: 3 || Loss: 66.85266058817767
Iteration: 4 || Loss: 66.85234720799691
Iteration: 5 || Loss: 66.85204010351927
Iteration: 6 || Loss: 66.85204010351927
saving ADAM checkpoint...
Sum of params:-108.98966
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.85204010351927
Iteration: 2 || Loss: 66.85065965988548
Iteration: 3 || Loss: 66.82385259525971
Iteration: 4 || Loss: 66.67112502218481
Iteration: 5 || Loss: 66.63726142151745
Iteration: 6 || Loss: 66.54916099657804
Iteration: 7 || Loss: 66.50604611616993
Iteration: 8 || Loss: 66.49010294595737
Iteration: 9 || Loss: 66.47988404278611
Iteration: 10 || Loss: 66.4656895533258
Iteration: 11 || Loss: 66.45096077447504
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.028694
Epoch 295 loss:66.45096077447504
MSE loss S0.9693034066629285
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.703108803009243
MSE loss S0.34960096318380285
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.8208054713166
MSE loss S1.8896401033887527
waveform batch: 2/2
Test loss - extrapolation:48.89110500482095
MSE loss S1.1110826517052546
Epoch 295 mean train loss:3.3033234633084434
Epoch 295 mean test loss - interpolation:3.950518133834874
Epoch 295 mean test loss - extrapolation:12.309325873011462
Start training epoch 296
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.028694
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.387448068044361
Iteration: 2 || Loss: 6.386424252048504
Iteration: 3 || Loss: 6.385442158551037
Iteration: 4 || Loss: 6.384490465367457
Iteration: 5 || Loss: 6.3836395604115515
Iteration: 6 || Loss: 6.3836395604115515
saving ADAM checkpoint...
Sum of params:-109.02861
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3836395604115515
Iteration: 2 || Loss: 6.355679414756092
Iteration: 3 || Loss: 6.270447368613163
Iteration: 4 || Loss: 6.2604892704205275
Iteration: 5 || Loss: 6.097156140588517
Iteration: 6 || Loss: 6.04450110024553
Iteration: 7 || Loss: 6.030086051854058
Iteration: 8 || Loss: 6.02524251594363
Iteration: 9 || Loss: 6.0217786880859805
Iteration: 10 || Loss: 6.011327070392582
Iteration: 11 || Loss: 5.990300227097618
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.04049
Epoch 296 loss:5.990300227097618
MSE loss S0.19438392336197718
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.04049
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.003903104608153
Iteration: 2 || Loss: 24.003696362492672
Iteration: 3 || Loss: 24.003400829124814
Iteration: 4 || Loss: 24.003135066701905
Iteration: 5 || Loss: 24.0028548908904
Iteration: 6 || Loss: 24.0028548908904
saving ADAM checkpoint...
Sum of params:-109.040565
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.0028548908904
Iteration: 2 || Loss: 23.733309382607732
Iteration: 3 || Loss: 23.73143203548268
Iteration: 4 || Loss: 23.7270990900803
Iteration: 5 || Loss: 23.515471669253085
Iteration: 6 || Loss: 23.46857173142798
Iteration: 7 || Loss: 23.450287916789577
Iteration: 8 || Loss: 23.411442163746983
Iteration: 9 || Loss: 23.376023526381744
Iteration: 10 || Loss: 23.364286036870563
Iteration: 11 || Loss: 23.35309665780625
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.01806
Epoch 296 loss:23.35309665780625
MSE loss S0.39868669017617653
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.01806
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.82474765320106
Iteration: 2 || Loss: 66.82443328362281
Iteration: 3 || Loss: 66.82416438787453
Iteration: 4 || Loss: 66.82386368232935
Iteration: 5 || Loss: 66.82356368791591
Iteration: 6 || Loss: 66.82356368791591
saving ADAM checkpoint...
Sum of params:-109.01812
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.82356368791591
Iteration: 2 || Loss: 66.82228759503644
Iteration: 3 || Loss: 66.79205472876404
Iteration: 4 || Loss: 66.66956020460115
Iteration: 5 || Loss: 66.61864666979514
Iteration: 6 || Loss: 66.52876880331563
Iteration: 7 || Loss: 66.48406004620864
Iteration: 8 || Loss: 66.46814633449802
Iteration: 9 || Loss: 66.45792073758302
Iteration: 10 || Loss: 66.44467021497522
Iteration: 11 || Loss: 66.4291878820773
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.05664
Epoch 296 loss:66.4291878820773
MSE loss S0.9678220823660535
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.698562749660706
MSE loss S0.3493187117499194
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.66526322734067
MSE loss S1.8856577196596296
waveform batch: 2/2
Test loss - extrapolation:48.81727881358277
MSE loss S1.1091894529967266
Epoch 296 mean train loss:3.3025029229993508
Epoch 296 mean test loss - interpolation:3.9497604582767845
Epoch 296 mean test loss - extrapolation:12.290211836743621
Start training epoch 297
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.05664
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.387297761097653
Iteration: 2 || Loss: 6.386323960280052
Iteration: 3 || Loss: 6.385375871146625
Iteration: 4 || Loss: 6.384486699725589
Iteration: 5 || Loss: 6.383537930132563
Iteration: 6 || Loss: 6.383537930132563
saving ADAM checkpoint...
Sum of params:-109.05656
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.383537930132563
Iteration: 2 || Loss: 6.35558763552098
Iteration: 3 || Loss: 6.269667959670206
Iteration: 4 || Loss: 6.260548410504386
Iteration: 5 || Loss: 6.097127782413681
Iteration: 6 || Loss: 6.047308096333606
Iteration: 7 || Loss: 6.029954122706171
Iteration: 8 || Loss: 6.025185694840924
Iteration: 9 || Loss: 6.021856117004521
Iteration: 10 || Loss: 6.010869595388815
Iteration: 11 || Loss: 5.989270081897688
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.06869
Epoch 297 loss:5.989270081897688
MSE loss S0.19387630620201296
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.06869
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.00616908146002
Iteration: 2 || Loss: 24.005937321824014
Iteration: 3 || Loss: 24.005621090552577
Iteration: 4 || Loss: 24.00538084676066
Iteration: 5 || Loss: 24.00509780694931
Iteration: 6 || Loss: 24.00509780694931
saving ADAM checkpoint...
Sum of params:-109.06873
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.00509780694931
Iteration: 2 || Loss: 23.73770119279216
Iteration: 3 || Loss: 23.731125409680562
Iteration: 4 || Loss: 23.71456057738714
Iteration: 5 || Loss: 23.52149879744729
Iteration: 6 || Loss: 23.47756942038709
Iteration: 7 || Loss: 23.461151710766387
Iteration: 8 || Loss: 23.433324249487093
Iteration: 9 || Loss: 23.375117202153607
Iteration: 10 || Loss: 23.36608900760723
Iteration: 11 || Loss: 23.351404791699874
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.04681
Epoch 297 loss:23.351404791699874
MSE loss S0.3998385629914243
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.04681
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.79566419603039
Iteration: 2 || Loss: 66.79535433964334
Iteration: 3 || Loss: 66.79517311197209
Iteration: 4 || Loss: 66.79489262720887
Iteration: 5 || Loss: 66.79469988719104
Iteration: 6 || Loss: 66.79469988719104
saving ADAM checkpoint...
Sum of params:-109.04684
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.79469988719104
Iteration: 2 || Loss: 66.79338105053769
Iteration: 3 || Loss: 66.76892828036972
Iteration: 4 || Loss: 66.62989782132652
Iteration: 5 || Loss: 66.59169473192968
Iteration: 6 || Loss: 66.50646620387663
Iteration: 7 || Loss: 66.46288727813905
Iteration: 8 || Loss: 66.44667545499154
Iteration: 9 || Loss: 66.4367349341607
Iteration: 10 || Loss: 66.42310393875199
Iteration: 11 || Loss: 66.40820478303449
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.0847
Epoch 297 loss:66.40820478303449
MSE loss S0.9669563489019598
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.693582519957243
MSE loss S0.348973690557644
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.52106231483351
MSE loss S1.882537149502134
waveform batch: 2/2
Test loss - extrapolation:48.75091733297649
MSE loss S1.107628706176758
Epoch 297 mean train loss:3.3016855054011054
Epoch 297 mean test loss - interpolation:3.9489304199928736
Epoch 297 mean test loss - extrapolation:12.272664970650835
Start training epoch 298
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.0847
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.385717002598357
Iteration: 2 || Loss: 6.384706599490704
Iteration: 3 || Loss: 6.3836717228479865
Iteration: 4 || Loss: 6.382732394905975
Iteration: 5 || Loss: 6.38175980332435
Iteration: 6 || Loss: 6.38175980332435
saving ADAM checkpoint...
Sum of params:-109.084625
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.38175980332435
Iteration: 2 || Loss: 6.353264718512916
Iteration: 3 || Loss: 6.267080469448039
Iteration: 4 || Loss: 6.258583722807313
Iteration: 5 || Loss: 6.096516197767329
Iteration: 6 || Loss: 6.048308753309479
Iteration: 7 || Loss: 6.029497396615002
Iteration: 8 || Loss: 6.024798606960095
Iteration: 9 || Loss: 6.02143614307432
Iteration: 10 || Loss: 6.010160835833576
Iteration: 11 || Loss: 5.988620828717472
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.097046
Epoch 298 loss:5.988620828717472
MSE loss S0.19392545523892327
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.097046
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.023777738706524
Iteration: 2 || Loss: 24.02351902836881
Iteration: 3 || Loss: 24.023292892157855
Iteration: 4 || Loss: 24.023075753009426
Iteration: 5 || Loss: 24.02271602288019
Iteration: 6 || Loss: 24.02271602288019
saving ADAM checkpoint...
Sum of params:-109.09701
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.02271602288019
Iteration: 2 || Loss: 23.748627568244622
Iteration: 3 || Loss: 23.745577062559956
Iteration: 4 || Loss: 23.74118819288165
Iteration: 5 || Loss: 23.519529902522077
Iteration: 6 || Loss: 23.47375620791964
Iteration: 7 || Loss: 23.459390617494844
Iteration: 8 || Loss: 23.431481863728564
Iteration: 9 || Loss: 23.373560451395164
Iteration: 10 || Loss: 23.363563112135267
Iteration: 11 || Loss: 23.34977938686017
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.07493
Epoch 298 loss:23.34977938686017
MSE loss S0.39966387264381054
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.07493
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.77236238560859
Iteration: 2 || Loss: 66.77210147342142
Iteration: 3 || Loss: 66.77183755914142
Iteration: 4 || Loss: 66.77158989462622
Iteration: 5 || Loss: 66.77135885309609
Iteration: 6 || Loss: 66.77135885309609
saving ADAM checkpoint...
Sum of params:-109.07497
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.77135885309609
Iteration: 2 || Loss: 66.76999021882212
Iteration: 3 || Loss: 66.74767856994823
Iteration: 4 || Loss: 66.60874061903704
Iteration: 5 || Loss: 66.56207027073046
Iteration: 6 || Loss: 66.48227224435583
Iteration: 7 || Loss: 66.44185352663632
Iteration: 8 || Loss: 66.42560206366232
Iteration: 9 || Loss: 66.41583736067807
Iteration: 10 || Loss: 66.40189933714538
Iteration: 11 || Loss: 66.38740100986809
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.112305
Epoch 298 loss:66.38740100986809
MSE loss S0.9668868347416641
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.690117485785425
MSE loss S0.34901791761403633
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.39543759954314
MSE loss S1.8798517081297637
waveform batch: 2/2
Test loss - extrapolation:48.6934655595146
MSE loss S1.1061891946207631
Epoch 298 mean train loss:3.3008896974291635
Epoch 298 mean test loss - interpolation:3.948352914297571
Epoch 298 mean test loss - extrapolation:12.257408596588144
Start training epoch 299
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.112305
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.383865829900262
Iteration: 2 || Loss: 6.382959278595045
Iteration: 3 || Loss: 6.38197448751227
Iteration: 4 || Loss: 6.381018910068974
Iteration: 5 || Loss: 6.380073378206712
Iteration: 6 || Loss: 6.380073378206712
saving ADAM checkpoint...
Sum of params:-109.11221
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.380073378206712
Iteration: 2 || Loss: 6.352028667520724
Iteration: 3 || Loss: 6.266666657342611
Iteration: 4 || Loss: 6.257789909322605
Iteration: 5 || Loss: 6.096036676034695
Iteration: 6 || Loss: 6.047525486082525
Iteration: 7 || Loss: 6.029090263674071
Iteration: 8 || Loss: 6.024323277248252
Iteration: 9 || Loss: 6.020718694510057
Iteration: 10 || Loss: 6.009787289338075
Iteration: 11 || Loss: 5.988760452381517
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.12437
Epoch 299 loss:5.988760452381517
MSE loss S0.19441634869544355
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.12437
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.014386795234415
Iteration: 2 || Loss: 24.014151156226443
Iteration: 3 || Loss: 24.013842249368807
Iteration: 4 || Loss: 24.01357052204632
Iteration: 5 || Loss: 24.013351035523932
Iteration: 6 || Loss: 24.013351035523932
saving ADAM checkpoint...
Sum of params:-109.12433
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.013351035523932
Iteration: 2 || Loss: 23.750196077472598
Iteration: 3 || Loss: 23.735851230857428
Iteration: 4 || Loss: 23.73104043310478
Iteration: 5 || Loss: 23.52555702407084
Iteration: 6 || Loss: 23.479144071773213
Iteration: 7 || Loss: 23.45720215189886
Iteration: 8 || Loss: 23.42901970228954
Iteration: 9 || Loss: 23.37154035254653
Iteration: 10 || Loss: 23.36568948874055
Iteration: 11 || Loss: 23.34840686344163
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.10203
Epoch 299 loss:23.34840686344163
MSE loss S0.39913502464539385
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.10203
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.7618476853993
Iteration: 2 || Loss: 66.76153129918764
Iteration: 3 || Loss: 66.76125300501374
Iteration: 4 || Loss: 66.76093355317248
Iteration: 5 || Loss: 66.76063363128446
Iteration: 6 || Loss: 66.76063363128446
saving ADAM checkpoint...
Sum of params:-109.10206
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.76063363128446
Iteration: 2 || Loss: 66.75925321987542
Iteration: 3 || Loss: 66.73193504780531
Iteration: 4 || Loss: 66.58577649443434
Iteration: 5 || Loss: 66.55145557506982
Iteration: 6 || Loss: 66.4637516533464
Iteration: 7 || Loss: 66.42084219319922
Iteration: 8 || Loss: 66.40523196885628
Iteration: 9 || Loss: 66.39498844058747
Iteration: 10 || Loss: 66.38137103050668
Iteration: 11 || Loss: 66.36693615385468
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.13924
Epoch 299 loss:66.36693615385468
MSE loss S0.9681722669286201
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.688801882685
MSE loss S0.3495764645878803
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.29487928931418
MSE loss S1.877923694017587
waveform batch: 2/2
Test loss - extrapolation:48.65361119335206
MSE loss S1.1048991542278475
Epoch 299 mean train loss:3.300141498954408
Epoch 299 mean test loss - interpolation:3.9481336471141666
Epoch 299 mean test loss - extrapolation:12.245707540222186
Start training epoch 300
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.13924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.380501778876068
Iteration: 2 || Loss: 6.379487455370209
Iteration: 3 || Loss: 6.378571127427097
Iteration: 4 || Loss: 6.377603059251671
Iteration: 5 || Loss: 6.376656262547914
Iteration: 6 || Loss: 6.376656262547914
saving ADAM checkpoint...
Sum of params:-109.13916
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.376656262547914
Iteration: 2 || Loss: 6.349452362039623
Iteration: 3 || Loss: 6.267060383333043
Iteration: 4 || Loss: 6.257250996073699
Iteration: 5 || Loss: 6.095473251322996
Iteration: 6 || Loss: 6.043596207597072
Iteration: 7 || Loss: 6.028745390526905
Iteration: 8 || Loss: 6.024010297672279
Iteration: 9 || Loss: 6.020565811800911
Iteration: 10 || Loss: 6.009988672484849
Iteration: 11 || Loss: 5.9892314984023205
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.15115
Epoch 300 loss:5.9892314984023205
MSE loss S0.1943882613096922
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.15115
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.994318724108837
Iteration: 2 || Loss: 23.994163563129867
Iteration: 3 || Loss: 23.99378429799033
Iteration: 4 || Loss: 23.993550752513872
Iteration: 5 || Loss: 23.9932383399788
Iteration: 6 || Loss: 23.9932383399788
saving ADAM checkpoint...
Sum of params:-109.15114
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.9932383399788
Iteration: 2 || Loss: 23.757645229935765
Iteration: 3 || Loss: 23.73464088249605
Iteration: 4 || Loss: 23.72995494414266
Iteration: 5 || Loss: 23.531169552983865
Iteration: 6 || Loss: 23.473718872341575
Iteration: 7 || Loss: 23.45059426900563
Iteration: 8 || Loss: 23.422877873419356
Iteration: 9 || Loss: 23.368629630227147
Iteration: 10 || Loss: 23.362763861793532
Iteration: 11 || Loss: 23.346291937046583
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.12869
Epoch 300 loss:23.346291937046583
MSE loss S0.39899144593085073
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.12869
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.73807776953424
Iteration: 2 || Loss: 66.73767105848512
Iteration: 3 || Loss: 66.73740564680145
Iteration: 4 || Loss: 66.73712877418531
Iteration: 5 || Loss: 66.73677213617879
Iteration: 6 || Loss: 66.73677213617879
saving ADAM checkpoint...
Sum of params:-109.128746
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.73677213617879
Iteration: 2 || Loss: 66.7354546397192
Iteration: 3 || Loss: 66.70750096655023
Iteration: 4 || Loss: 66.56435321065479
Iteration: 5 || Loss: 66.53134358212596
Iteration: 6 || Loss: 66.4430996286955
Iteration: 7 || Loss: 66.39998272334607
Iteration: 8 || Loss: 66.38454461226331
Iteration: 9 || Loss: 66.37436410353948
Iteration: 10 || Loss: 66.36089804828117
Iteration: 11 || Loss: 66.34654127030652
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.165764
Epoch 300 loss:66.34654127030652
MSE loss S0.9680352085739296
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.685280561195594
MSE loss S0.3496386051356424
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.16862746150618
MSE loss S1.875139816269546
waveform batch: 2/2
Test loss - extrapolation:48.596891040237004
MSE loss S1.1034612649346136
Epoch 300 mean train loss:3.299381541577773
Epoch 300 mean test loss - interpolation:3.9475467601992658
Epoch 300 mean test loss - extrapolation:12.230459875145264
Start training epoch 301
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.165764
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.378154863791224
Iteration: 2 || Loss: 6.377171461250851
Iteration: 3 || Loss: 6.3762349417849915
Iteration: 4 || Loss: 6.375362401011143
Iteration: 5 || Loss: 6.3743980952638015
Iteration: 6 || Loss: 6.3743980952638015
saving ADAM checkpoint...
Sum of params:-109.16568
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3743980952638015
Iteration: 2 || Loss: 6.347567268158814
Iteration: 3 || Loss: 6.266118289193088
Iteration: 4 || Loss: 6.2564681657504275
Iteration: 5 || Loss: 6.09523489748268
Iteration: 6 || Loss: 6.043313032038629
Iteration: 7 || Loss: 6.028461768033697
Iteration: 8 || Loss: 6.023700931302226
Iteration: 9 || Loss: 6.020251262271549
Iteration: 10 || Loss: 6.0098602703178425
Iteration: 11 || Loss: 5.989057447010948
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.177536
Epoch 301 loss:5.989057447010948
MSE loss S0.19445245108730488
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.177536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.98112739849219
Iteration: 2 || Loss: 23.980825597964284
Iteration: 3 || Loss: 23.980590187727458
Iteration: 4 || Loss: 23.98030054950905
Iteration: 5 || Loss: 23.980033232287617
Iteration: 6 || Loss: 23.980033232287617
saving ADAM checkpoint...
Sum of params:-109.177536
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.980033232287617
Iteration: 2 || Loss: 23.721753206068993
Iteration: 3 || Loss: 23.717071488611197
Iteration: 4 || Loss: 23.695709516968176
Iteration: 5 || Loss: 23.503941807917226
Iteration: 6 || Loss: 23.460258395575103
Iteration: 7 || Loss: 23.44617981083336
Iteration: 8 || Loss: 23.419677254007212
Iteration: 9 || Loss: 23.365715963795072
Iteration: 10 || Loss: 23.35456730848645
Iteration: 11 || Loss: 23.34343269769661
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.15593
Epoch 301 loss:23.34343269769661
MSE loss S0.39980465328099096
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.15593
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.70503667786511
Iteration: 2 || Loss: 66.70474699828651
Iteration: 3 || Loss: 66.70459316202233
Iteration: 4 || Loss: 66.70435128118547
Iteration: 5 || Loss: 66.70418616228925
Iteration: 6 || Loss: 66.70418616228925
saving ADAM checkpoint...
Sum of params:-109.15596
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.70418616228925
Iteration: 2 || Loss: 66.70294341412519
Iteration: 3 || Loss: 66.6788526273185
Iteration: 4 || Loss: 66.55239749774016
Iteration: 5 || Loss: 66.50837605991649
Iteration: 6 || Loss: 66.42489126580126
Iteration: 7 || Loss: 66.38029451744421
Iteration: 8 || Loss: 66.36310232764437
Iteration: 9 || Loss: 66.35410675130629
Iteration: 10 || Loss: 66.33937084827559
Iteration: 11 || Loss: 66.32656981744042
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.192604
Epoch 301 loss:66.32656981744042
MSE loss S0.9653201589143732
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.677684821879097
MSE loss S0.3483298102883534
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.00278108852567
MSE loss S1.871439723803452
waveform batch: 2/2
Test loss - extrapolation:48.51808295906052
MSE loss S1.1019509612303282
Epoch 301 mean train loss:3.2985882745568267
Epoch 301 mean test loss - interpolation:3.9462808036465162
Epoch 301 mean test loss - extrapolation:12.210072003965516
Start training epoch 302
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.192604
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.378281888652627
Iteration: 2 || Loss: 6.377306683572424
Iteration: 3 || Loss: 6.376298170906274
Iteration: 4 || Loss: 6.375300164441075
Iteration: 5 || Loss: 6.374304598870992
Iteration: 6 || Loss: 6.374304598870992
saving ADAM checkpoint...
Sum of params:-109.19252
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.374304598870992
Iteration: 2 || Loss: 6.343476399685809
Iteration: 3 || Loss: 6.259615327039991
Iteration: 4 || Loss: 6.252307068313874
Iteration: 5 || Loss: 6.094318846086032
Iteration: 6 || Loss: 6.047761099830739
Iteration: 7 || Loss: 6.027903495109989
Iteration: 8 || Loss: 6.023249882154431
Iteration: 9 || Loss: 6.019989891430577
Iteration: 10 || Loss: 6.00923054148047
Iteration: 11 || Loss: 5.989166526139642
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.204
Epoch 302 loss:5.989166526139642
MSE loss S0.19493129206393617
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.204
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.978298787040895
Iteration: 2 || Loss: 23.97807557639812
Iteration: 3 || Loss: 23.977832689628954
Iteration: 4 || Loss: 23.977482366585324
Iteration: 5 || Loss: 23.97723771935633
Iteration: 6 || Loss: 23.97723771935633
saving ADAM checkpoint...
Sum of params:-109.204
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.97723771935633
Iteration: 2 || Loss: 23.720554987358458
Iteration: 3 || Loss: 23.708938167914177
Iteration: 4 || Loss: 23.684445904214417
Iteration: 5 || Loss: 23.505456425382476
Iteration: 6 || Loss: 23.46381825004891
Iteration: 7 || Loss: 23.44396641325192
Iteration: 8 || Loss: 23.416127695182087
Iteration: 9 || Loss: 23.363339630942164
Iteration: 10 || Loss: 23.357704404726217
Iteration: 11 || Loss: 23.34141584863067
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.18259
Epoch 302 loss:23.34141584863067
MSE loss S0.3982670633642587
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.18259
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.69433587589603
Iteration: 2 || Loss: 66.69407373761251
Iteration: 3 || Loss: 66.6938073968645
Iteration: 4 || Loss: 66.69357044499444
Iteration: 5 || Loss: 66.69330228271744
Iteration: 6 || Loss: 66.69330228271744
saving ADAM checkpoint...
Sum of params:-109.1826
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.69330228271744
Iteration: 2 || Loss: 66.69201900135094
Iteration: 3 || Loss: 66.66297590118559
Iteration: 4 || Loss: 66.53825154347172
Iteration: 5 || Loss: 66.4943557540569
Iteration: 6 || Loss: 66.40555677797762
Iteration: 7 || Loss: 66.3592667867764
Iteration: 8 || Loss: 66.34315323070507
Iteration: 9 || Loss: 66.333431482058
Iteration: 10 || Loss: 66.32064378584491
Iteration: 11 || Loss: 66.30606655977738
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.21919
Epoch 302 loss:66.30606655977738
MSE loss S0.9661235507284344
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.67730456996601
MSE loss S0.3491539095224975
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.90622054481229
MSE loss S1.8688975178597884
waveform batch: 2/2
Test loss - extrapolation:48.471773685907095
MSE loss S1.1004882553381794
Epoch 302 mean train loss:3.2978154805016446
Epoch 302 mean test loss - interpolation:3.9462174283276688
Epoch 302 mean test loss - extrapolation:12.19816618589328
Start training epoch 303
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.21919
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.376186171783731
Iteration: 2 || Loss: 6.3751880625442405
Iteration: 3 || Loss: 6.374202921129926
Iteration: 4 || Loss: 6.373244547785389
Iteration: 5 || Loss: 6.372317953029236
Iteration: 6 || Loss: 6.372317953029236
saving ADAM checkpoint...
Sum of params:-109.21911
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.372317953029236
Iteration: 2 || Loss: 6.344438812496663
Iteration: 3 || Loss: 6.264235911787437
Iteration: 4 || Loss: 6.255286186441751
Iteration: 5 || Loss: 6.094881820623763
Iteration: 6 || Loss: 6.0467874284822205
Iteration: 7 || Loss: 6.027936464475511
Iteration: 8 || Loss: 6.023147999587775
Iteration: 9 || Loss: 6.019544918841227
Iteration: 10 || Loss: 6.008931335823059
Iteration: 11 || Loss: 5.988004806933331
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.23106
Epoch 303 loss:5.988004806933331
MSE loss S0.1942178142803403
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.23106
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.975921410165455
Iteration: 2 || Loss: 23.97568673940664
Iteration: 3 || Loss: 23.975487018023355
Iteration: 4 || Loss: 23.97520438769332
Iteration: 5 || Loss: 23.97494805908778
Iteration: 6 || Loss: 23.97494805908778
saving ADAM checkpoint...
Sum of params:-109.231094
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.97494805908778
Iteration: 2 || Loss: 23.70594013205699
Iteration: 3 || Loss: 23.705381317353172
Iteration: 4 || Loss: 23.69367622828543
Iteration: 5 || Loss: 23.50043076937973
Iteration: 6 || Loss: 23.455557138774047
Iteration: 7 || Loss: 23.437786139861853
Iteration: 8 || Loss: 23.397760159245205
Iteration: 9 || Loss: 23.362141411548645
Iteration: 10 || Loss: 23.350705613033867
Iteration: 11 || Loss: 23.33971756642607
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.2101
Epoch 303 loss:23.33971756642607
MSE loss S0.39784318976339517
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.2101
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.66825088309794
Iteration: 2 || Loss: 66.66799964597979
Iteration: 3 || Loss: 66.66774269705539
Iteration: 4 || Loss: 66.66749370292169
Iteration: 5 || Loss: 66.66731747638606
Iteration: 6 || Loss: 66.66731747638606
saving ADAM checkpoint...
Sum of params:-109.21013
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.66731747638606
Iteration: 2 || Loss: 66.6660777958052
Iteration: 3 || Loss: 66.63725639328013
Iteration: 4 || Loss: 66.53024070693121
Iteration: 5 || Loss: 66.46990575651529
Iteration: 6 || Loss: 66.38478839423759
Iteration: 7 || Loss: 66.33945588181568
Iteration: 8 || Loss: 66.32250796647062
Iteration: 9 || Loss: 66.31359899641899
Iteration: 10 || Loss: 66.29914693289169
Iteration: 11 || Loss: 66.28642012379754
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.24592
Epoch 303 loss:66.28642012379754
MSE loss S0.9648222437254451
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.67141064617963
MSE loss S0.3483737788071736
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.76836590840924
MSE loss S1.8660775284798896
waveform batch: 2/2
Test loss - extrapolation:48.409312685912745
MSE loss S1.0990802669790365
Epoch 303 mean train loss:3.2970393964536875
Epoch 303 mean test loss - interpolation:3.9452351076966052
Epoch 303 mean test loss - extrapolation:12.181473216193497
Start training epoch 304
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.24592
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.375082608326833
Iteration: 2 || Loss: 6.374042348168347
Iteration: 3 || Loss: 6.373003531833023
Iteration: 4 || Loss: 6.372079982430014
Iteration: 5 || Loss: 6.3710416695131515
Iteration: 6 || Loss: 6.3710416695131515
saving ADAM checkpoint...
Sum of params:-109.245834
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3710416695131515
Iteration: 2 || Loss: 6.340395805422154
Iteration: 3 || Loss: 6.258604805625475
Iteration: 4 || Loss: 6.2513593301339645
Iteration: 5 || Loss: 6.093880891210393
Iteration: 6 || Loss: 6.047411442301926
Iteration: 7 || Loss: 6.02742080943546
Iteration: 8 || Loss: 6.022792218915774
Iteration: 9 || Loss: 6.019697258167163
Iteration: 10 || Loss: 6.008955025443236
Iteration: 11 || Loss: 5.989103886570662
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.256775
Epoch 304 loss:5.989103886570662
MSE loss S0.19475903898378305
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.256775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.944296333155012
Iteration: 2 || Loss: 23.9440709536926
Iteration: 3 || Loss: 23.943842998195052
Iteration: 4 || Loss: 23.943569663899186
Iteration: 5 || Loss: 23.943291121796445
Iteration: 6 || Loss: 23.943291121796445
saving ADAM checkpoint...
Sum of params:-109.25684
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.943291121796445
Iteration: 2 || Loss: 23.695444782758887
Iteration: 3 || Loss: 23.686040608971062
Iteration: 4 || Loss: 23.681692366060815
Iteration: 5 || Loss: 23.49342522273829
Iteration: 6 || Loss: 23.454514817874934
Iteration: 7 || Loss: 23.43676366284191
Iteration: 8 || Loss: 23.4090553452461
Iteration: 9 || Loss: 23.35852728139275
Iteration: 10 || Loss: 23.351238653492533
Iteration: 11 || Loss: 23.337437852682562
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.23648
Epoch 304 loss:23.337437852682562
MSE loss S0.39789126340446257
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.23648
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.64890216318409
Iteration: 2 || Loss: 66.64869980397263
Iteration: 3 || Loss: 66.64844556215468
Iteration: 4 || Loss: 66.64828527343946
Iteration: 5 || Loss: 66.64806035186668
Iteration: 6 || Loss: 66.64806035186668
saving ADAM checkpoint...
Sum of params:-109.23651
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.64806035186668
Iteration: 2 || Loss: 66.6468842175356
Iteration: 3 || Loss: 66.61449523287577
Iteration: 4 || Loss: 66.51242615470065
Iteration: 5 || Loss: 66.46932579757386
Iteration: 6 || Loss: 66.37367550070152
Iteration: 7 || Loss: 66.32018682828753
Iteration: 8 || Loss: 66.30237340911486
Iteration: 9 || Loss: 66.29358489128619
Iteration: 10 || Loss: 66.27900885902766
Iteration: 11 || Loss: 66.26669783944831
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.27184
Epoch 304 loss:66.26669783944831
MSE loss S0.964789790327977
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.66869356769975
MSE loss S0.3483812360100266
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.64959424819835
MSE loss S1.8636900227556719
waveform batch: 2/2
Test loss - extrapolation:48.3592221502513
MSE loss S1.0980128773947517
Epoch 304 mean train loss:3.296318606162122
Epoch 304 mean test loss - interpolation:3.944782261283292
Epoch 304 mean test loss - extrapolation:12.16740136653747
Start training epoch 305
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.27184
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.374349911046805
Iteration: 2 || Loss: 6.373366502651173
Iteration: 3 || Loss: 6.372293569962032
Iteration: 4 || Loss: 6.371273757837643
Iteration: 5 || Loss: 6.37022876768829
Iteration: 6 || Loss: 6.37022876768829
saving ADAM checkpoint...
Sum of params:-109.27176
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.37022876768829
Iteration: 2 || Loss: 6.3382854635761205
Iteration: 3 || Loss: 6.258163968245103
Iteration: 4 || Loss: 6.251061576831509
Iteration: 5 || Loss: 6.093644287920172
Iteration: 6 || Loss: 6.047644757206042
Iteration: 7 || Loss: 6.027139771074414
Iteration: 8 || Loss: 6.022531322711058
Iteration: 9 || Loss: 6.019492909354357
Iteration: 10 || Loss: 6.008666898072862
Iteration: 11 || Loss: 5.988711644786586
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.282906
Epoch 305 loss:5.988711644786586
MSE loss S0.19503261771959185
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.282906
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.95675705288783
Iteration: 2 || Loss: 23.956484459009197
Iteration: 3 || Loss: 23.956160560233076
Iteration: 4 || Loss: 23.955985704266535
Iteration: 5 || Loss: 23.955698567835167
Iteration: 6 || Loss: 23.955698567835167
saving ADAM checkpoint...
Sum of params:-109.28297
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.955698567835167
Iteration: 2 || Loss: 23.699673206357613
Iteration: 3 || Loss: 23.692056950342497
Iteration: 4 || Loss: 23.687450953726877
Iteration: 5 || Loss: 23.491969914909486
Iteration: 6 || Loss: 23.45273989460803
Iteration: 7 || Loss: 23.436323711271502
Iteration: 8 || Loss: 23.408505475395103
Iteration: 9 || Loss: 23.35672689766439
Iteration: 10 || Loss: 23.34969126594968
Iteration: 11 || Loss: 23.33567162611213
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.26265
Epoch 305 loss:23.33567162611213
MSE loss S0.3978349540162072
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.26265
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.62451981493174
Iteration: 2 || Loss: 66.62431870234394
Iteration: 3 || Loss: 66.62409390840557
Iteration: 4 || Loss: 66.6238841661665
Iteration: 5 || Loss: 66.62369764280693
Iteration: 6 || Loss: 66.62369764280693
saving ADAM checkpoint...
Sum of params:-109.26269
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.62369764280693
Iteration: 2 || Loss: 66.62253325895892
Iteration: 3 || Loss: 66.59189744912763
Iteration: 4 || Loss: 66.48839501344072
Iteration: 5 || Loss: 66.4414687705866
Iteration: 6 || Loss: 66.35073408477598
Iteration: 7 || Loss: 66.30045809042902
Iteration: 8 || Loss: 66.28283540170708
Iteration: 9 || Loss: 66.27414616102615
Iteration: 10 || Loss: 66.25942809313658
Iteration: 11 || Loss: 66.24745369345337
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.29758
Epoch 305 loss:66.24745369345337
MSE loss S0.9644817537066634
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.664878196765752
MSE loss S0.3483007044697618
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.53576940896214
MSE loss S1.8612360284820713
waveform batch: 2/2
Test loss - extrapolation:48.304358089983104
MSE loss S1.096613669308367
Epoch 305 mean train loss:3.295580584977658
Epoch 305 mean test loss - interpolation:3.9441463661276255
Epoch 305 mean test loss - extrapolation:12.153343958245436
Start training epoch 306
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.29758
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.372282234714539
Iteration: 2 || Loss: 6.371236832829655
Iteration: 3 || Loss: 6.370199417324011
Iteration: 4 || Loss: 6.369204483317755
Iteration: 5 || Loss: 6.368178672538775
Iteration: 6 || Loss: 6.368178672538775
saving ADAM checkpoint...
Sum of params:-109.2975
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.368178672538775
Iteration: 2 || Loss: 6.336512456761349
Iteration: 3 || Loss: 6.2566144257462
Iteration: 4 || Loss: 6.249648342786319
Iteration: 5 || Loss: 6.093262715268933
Iteration: 6 || Loss: 6.047069793047372
Iteration: 7 || Loss: 6.02686823810641
Iteration: 8 || Loss: 6.022285642301799
Iteration: 9 || Loss: 6.019523895514965
Iteration: 10 || Loss: 6.008505769662809
Iteration: 11 || Loss: 5.988821337411795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.308464
Epoch 306 loss:5.988821337411795
MSE loss S0.1952732910677858
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.308464
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.942757317858106
Iteration: 2 || Loss: 23.94244172189975
Iteration: 3 || Loss: 23.9421591535418
Iteration: 4 || Loss: 23.941901342727075
Iteration: 5 || Loss: 23.941740364282584
Iteration: 6 || Loss: 23.941740364282584
saving ADAM checkpoint...
Sum of params:-109.3085
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.941740364282584
Iteration: 2 || Loss: 23.703363606204697
Iteration: 3 || Loss: 23.68407591663793
Iteration: 4 || Loss: 23.679528346282332
Iteration: 5 || Loss: 23.50162143990965
Iteration: 6 || Loss: 23.454434268762697
Iteration: 7 || Loss: 23.432473353097354
Iteration: 8 || Loss: 23.404837626787725
Iteration: 9 || Loss: 23.35462516429236
Iteration: 10 || Loss: 23.348868242602254
Iteration: 11 || Loss: 23.33397002684007
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.28816
Epoch 306 loss:23.33397002684007
MSE loss S0.3975329786910927
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.28816
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.60648179520572
Iteration: 2 || Loss: 66.6061984536827
Iteration: 3 || Loss: 66.60596224683066
Iteration: 4 || Loss: 66.60573928654732
Iteration: 5 || Loss: 66.60554882528227
Iteration: 6 || Loss: 66.60554882528227
saving ADAM checkpoint...
Sum of params:-109.288185
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.60554882528227
Iteration: 2 || Loss: 66.6042862207994
Iteration: 3 || Loss: 66.57347977887684
Iteration: 4 || Loss: 66.46233654451419
Iteration: 5 || Loss: 66.42160409926146
Iteration: 6 || Loss: 66.3300131377427
Iteration: 7 || Loss: 66.28037524514023
Iteration: 8 || Loss: 66.26367931930304
Iteration: 9 || Loss: 66.25448868564332
Iteration: 10 || Loss: 66.24116900964547
Iteration: 11 || Loss: 66.2280207506968
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.32306
Epoch 306 loss:66.2280207506968
MSE loss S0.9648177858201814
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.663175488407518
MSE loss S0.34886357399321255
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.43151651108613
MSE loss S1.858604409373845
waveform batch: 2/2
Test loss - extrapolation:48.25286013266774
MSE loss S1.0951531270630785
Epoch 306 mean train loss:3.294855590170644
Epoch 306 mean test loss - interpolation:3.943862581401253
Epoch 306 mean test loss - extrapolation:12.140364720312823
Start training epoch 307
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.32306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3695806398320025
Iteration: 2 || Loss: 6.368620932325241
Iteration: 3 || Loss: 6.367622972062234
Iteration: 4 || Loss: 6.366685928199032
Iteration: 5 || Loss: 6.365718680117888
Iteration: 6 || Loss: 6.365718680117888
saving ADAM checkpoint...
Sum of params:-109.32297
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.365718680117888
Iteration: 2 || Loss: 6.336435564625577
Iteration: 3 || Loss: 6.259186902367008
Iteration: 4 || Loss: 6.251265907551243
Iteration: 5 || Loss: 6.093577268372773
Iteration: 6 || Loss: 6.047070586088083
Iteration: 7 || Loss: 6.026906326831582
Iteration: 8 || Loss: 6.022179036546011
Iteration: 9 || Loss: 6.019015842169141
Iteration: 10 || Loss: 6.008088261906275
Iteration: 11 || Loss: 5.987517509587426
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.334915
Epoch 307 loss:5.987517509587426
MSE loss S0.19487125976830844
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.334915
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.96814404279294
Iteration: 2 || Loss: 23.967904927998998
Iteration: 3 || Loss: 23.9676501692986
Iteration: 4 || Loss: 23.967372178595742
Iteration: 5 || Loss: 23.967186011367048
Iteration: 6 || Loss: 23.967186011367048
saving ADAM checkpoint...
Sum of params:-109.33494
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.967186011367048
Iteration: 2 || Loss: 23.734494972384717
Iteration: 3 || Loss: 23.706284846373933
Iteration: 4 || Loss: 23.6983717814455
Iteration: 5 || Loss: 23.52153047995062
Iteration: 6 || Loss: 23.46069810763042
Iteration: 7 || Loss: 23.436968903561446
Iteration: 8 || Loss: 23.408450947096032
Iteration: 9 || Loss: 23.35504311986256
Iteration: 10 || Loss: 23.349601823562526
Iteration: 11 || Loss: 23.33353443607564
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.31377
Epoch 307 loss:23.33353443607564
MSE loss S0.3977124235939367
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.31377
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.58791336307368
Iteration: 2 || Loss: 66.58760783563469
Iteration: 3 || Loss: 66.58731890920495
Iteration: 4 || Loss: 66.58704341022816
Iteration: 5 || Loss: 66.58679403810494
Iteration: 6 || Loss: 66.58679403810494
saving ADAM checkpoint...
Sum of params:-109.31381
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.58679403810494
Iteration: 2 || Loss: 66.58546158057692
Iteration: 3 || Loss: 66.55515219079926
Iteration: 4 || Loss: 66.43572355344861
Iteration: 5 || Loss: 66.39234625573391
Iteration: 6 || Loss: 66.30463871866803
Iteration: 7 || Loss: 66.2607438593885
Iteration: 8 || Loss: 66.24533736460047
Iteration: 9 || Loss: 66.23552740097317
Iteration: 10 || Loss: 66.2232849355333
Iteration: 11 || Loss: 66.20906220361356
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.34856
Epoch 307 loss:66.20906220361356
MSE loss S0.9658231717957001
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.66172920284233
MSE loss S0.34955203439108024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.34315765864194
MSE loss S1.8565956747353987
waveform batch: 2/2
Test loss - extrapolation:48.207352438600154
MSE loss S1.0936370519736787
Epoch 307 mean train loss:3.2941418672164353
Epoch 307 mean test loss - interpolation:3.9436215338070553
Epoch 307 mean test loss - extrapolation:12.129209174770175
Start training epoch 308
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.34856
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.366336690550154
Iteration: 2 || Loss: 6.3653916757660305
Iteration: 3 || Loss: 6.364500364884337
Iteration: 4 || Loss: 6.363602613254576
Iteration: 5 || Loss: 6.362628894858435
Iteration: 6 || Loss: 6.362628894858435
saving ADAM checkpoint...
Sum of params:-109.34845
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.362628894858435
Iteration: 2 || Loss: 6.3364208853061
Iteration: 3 || Loss: 6.2618887786769974
Iteration: 4 || Loss: 6.2521192374302315
Iteration: 5 || Loss: 6.093472487049104
Iteration: 6 || Loss: 6.043872458600359
Iteration: 7 || Loss: 6.026751592415095
Iteration: 8 || Loss: 6.021945917408061
Iteration: 9 || Loss: 6.01775518980498
Iteration: 10 || Loss: 6.007869378390175
Iteration: 11 || Loss: 5.987142649665766
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.36058
Epoch 308 loss:5.987142649665766
MSE loss S0.19465255541886667
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.36058
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.96339535219445
Iteration: 2 || Loss: 23.963039716075475
Iteration: 3 || Loss: 23.96276968120757
Iteration: 4 || Loss: 23.962562447111008
Iteration: 5 || Loss: 23.96230104789615
Iteration: 6 || Loss: 23.96230104789615
saving ADAM checkpoint...
Sum of params:-109.360596
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.96230104789615
Iteration: 2 || Loss: 23.762805042066198
Iteration: 3 || Loss: 23.71382944272217
Iteration: 4 || Loss: 23.708462187094735
Iteration: 5 || Loss: 23.535477891906538
Iteration: 6 || Loss: 23.459421521705604
Iteration: 7 || Loss: 23.435263398902958
Iteration: 8 || Loss: 23.407287249241097
Iteration: 9 || Loss: 23.353160310457792
Iteration: 10 || Loss: 23.347572236958175
Iteration: 11 || Loss: 23.331916910490996
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.33981
Epoch 308 loss:23.331916910490996
MSE loss S0.39801637143036933
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.33981
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.56169170153045
Iteration: 2 || Loss: 66.5614245849842
Iteration: 3 || Loss: 66.56113744615864
Iteration: 4 || Loss: 66.5609065509788
Iteration: 5 || Loss: 66.56068189755842
Iteration: 6 || Loss: 66.56068189755842
saving ADAM checkpoint...
Sum of params:-109.33986
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.56068189755842
Iteration: 2 || Loss: 66.5595289860534
Iteration: 3 || Loss: 66.52930811219153
Iteration: 4 || Loss: 66.4143988839396
Iteration: 5 || Loss: 66.3811096610234
Iteration: 6 || Loss: 66.28992846715673
Iteration: 7 || Loss: 66.24233427429446
Iteration: 8 || Loss: 66.2264413294729
Iteration: 9 || Loss: 66.21688587189452
Iteration: 10 || Loss: 66.20405084900753
Iteration: 11 || Loss: 66.19053525465858
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.373665
Epoch 308 loss:66.19053525465858
MSE loss S0.9646643120510132
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.657108793273164
MSE loss S0.34905903414507017
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.21235328052393
MSE loss S1.853669620621921
waveform batch: 2/2
Test loss - extrapolation:48.14671432977893
MSE loss S1.0922227905798492
Epoch 308 mean train loss:3.29343430395915
Epoch 308 mean test loss - interpolation:3.942851465545527
Epoch 308 mean test loss - extrapolation:12.113255634191907
Start training epoch 309
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.373665
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3661579030520175
Iteration: 2 || Loss: 6.365137667721359
Iteration: 3 || Loss: 6.364200891215028
Iteration: 4 || Loss: 6.363264546169168
Iteration: 5 || Loss: 6.362327397637297
Iteration: 6 || Loss: 6.362327397637297
saving ADAM checkpoint...
Sum of params:-109.37359
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.362327397637297
Iteration: 2 || Loss: 6.334301463796075
Iteration: 3 || Loss: 6.25866590885555
Iteration: 4 || Loss: 6.250394216163406
Iteration: 5 || Loss: 6.092845625717805
Iteration: 6 || Loss: 6.045871687288555
Iteration: 7 || Loss: 6.026352291261183
Iteration: 8 || Loss: 6.021561937969548
Iteration: 9 || Loss: 6.017976349313411
Iteration: 10 || Loss: 6.007349888831934
Iteration: 11 || Loss: 5.986890265910383
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.38529
Epoch 309 loss:5.986890265910383
MSE loss S0.19473443692962625
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.38529
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.95283443635084
Iteration: 2 || Loss: 23.952560001478474
Iteration: 3 || Loss: 23.952188520666233
Iteration: 4 || Loss: 23.952028228326366
Iteration: 5 || Loss: 23.95175330290445
Iteration: 6 || Loss: 23.95175330290445
saving ADAM checkpoint...
Sum of params:-109.38534
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.95175330290445
Iteration: 2 || Loss: 23.72555411278872
Iteration: 3 || Loss: 23.695398480898945
Iteration: 4 || Loss: 23.518223434510013
Iteration: 5 || Loss: 23.503324061740635
Iteration: 6 || Loss: 23.45567415155235
Iteration: 7 || Loss: 23.43222482339351
Iteration: 8 || Loss: 23.40504825060044
Iteration: 9 || Loss: 23.355953186232405
Iteration: 10 || Loss: 23.34658763210309
Iteration: 11 || Loss: 23.33056770853746
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.36469
Epoch 309 loss:23.33056770853746
MSE loss S0.3983252399457886
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.36469
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.54601413420603
Iteration: 2 || Loss: 66.54574034531022
Iteration: 3 || Loss: 66.54540494242211
Iteration: 4 || Loss: 66.54519038772067
Iteration: 5 || Loss: 66.54488291887607
Iteration: 6 || Loss: 66.54488291887607
saving ADAM checkpoint...
Sum of params:-109.364746
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.54488291887607
Iteration: 2 || Loss: 66.54343570687914
Iteration: 3 || Loss: 66.52484560336714
Iteration: 4 || Loss: 66.36517980451315
Iteration: 5 || Loss: 66.33036169340305
Iteration: 6 || Loss: 66.25852809111562
Iteration: 7 || Loss: 66.22318610759002
Iteration: 8 || Loss: 66.2075559771546
Iteration: 9 || Loss: 66.19844634496114
Iteration: 10 || Loss: 66.18480936702011
Iteration: 11 || Loss: 66.17228417668521
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.39845
Epoch 309 loss:66.17228417668521
MSE loss S0.9664796548587378
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.656157929171634
MSE loss S0.34987784004480094
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.13757070675767
MSE loss S1.8525291425784065
waveform batch: 2/2
Test loss - extrapolation:48.11311627368341
MSE loss S1.0913414780078998
Epoch 309 mean train loss:3.2927497293494152
Epoch 309 mean test loss - interpolation:3.9426929881952724
Epoch 309 mean test loss - extrapolation:12.104223915036757
Start training epoch 310
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.39845
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.362115752172243
Iteration: 2 || Loss: 6.361115444918393
Iteration: 3 || Loss: 6.360227857250245
Iteration: 4 || Loss: 6.359349264744092
Iteration: 5 || Loss: 6.358421387150437
Iteration: 6 || Loss: 6.358421387150437
saving ADAM checkpoint...
Sum of params:-109.39837
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.358421387150437
Iteration: 2 || Loss: 6.332758610142775
Iteration: 3 || Loss: 6.260899715203507
Iteration: 4 || Loss: 6.250367458790602
Iteration: 5 || Loss: 6.09247925121224
Iteration: 6 || Loss: 6.041533226081053
Iteration: 7 || Loss: 6.025925706629774
Iteration: 8 || Loss: 6.021166055362325
Iteration: 9 || Loss: 6.017346688277766
Iteration: 10 || Loss: 6.007292566574872
Iteration: 11 || Loss: 5.986553899787875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.4104
Epoch 310 loss:5.986553899787875
MSE loss S0.19443123383868108
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.4104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.949907896919633
Iteration: 2 || Loss: 23.949604105601423
Iteration: 3 || Loss: 23.949315871351324
Iteration: 4 || Loss: 23.9489567244054
Iteration: 5 || Loss: 23.94878068436559
Iteration: 6 || Loss: 23.94878068436559
saving ADAM checkpoint...
Sum of params:-109.41043
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.94878068436559
Iteration: 2 || Loss: 23.77726277618646
Iteration: 3 || Loss: 23.70971573265024
Iteration: 4 || Loss: 23.70457418982443
Iteration: 5 || Loss: 23.532270914822586
Iteration: 6 || Loss: 23.454015653608717
Iteration: 7 || Loss: 23.430354534259575
Iteration: 8 || Loss: 23.402496075219144
Iteration: 9 || Loss: 23.348977605561426
Iteration: 10 || Loss: 23.343276784241425
Iteration: 11 || Loss: 23.328235277051057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.39013
Epoch 310 loss:23.328235277051057
MSE loss S0.3979352395165501
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.39013
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.51557150706279
Iteration: 2 || Loss: 66.51538061574244
Iteration: 3 || Loss: 66.51514503007037
Iteration: 4 || Loss: 66.51492644141862
Iteration: 5 || Loss: 66.51472139662116
Iteration: 6 || Loss: 66.51472139662116
saving ADAM checkpoint...
Sum of params:-109.39018
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.51472139662116
Iteration: 2 || Loss: 66.5135753859462
Iteration: 3 || Loss: 66.48475453516444
Iteration: 4 || Loss: 66.37620018879865
Iteration: 5 || Loss: 66.34464470599059
Iteration: 6 || Loss: 66.25504300347585
Iteration: 7 || Loss: 66.20619904221572
Iteration: 8 || Loss: 66.18937691611606
Iteration: 9 || Loss: 66.18050054761736
Iteration: 10 || Loss: 66.16631264465087
Iteration: 11 || Loss: 66.15453587928381
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.42312
Epoch 310 loss:66.15453587928381
MSE loss S0.963743299702203
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.649171251934867
MSE loss S0.34845216462001616
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.98475408400455
MSE loss S1.8491010396834242
waveform batch: 2/2
Test loss - extrapolation:48.040783942144834
MSE loss S1.0898883183263333
Epoch 310 mean train loss:3.2920456915904395
Epoch 310 mean test loss - interpolation:3.9415285419891446
Epoch 310 mean test loss - extrapolation:12.085461502179115
Start training epoch 311
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.42312
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.363102204234039
Iteration: 2 || Loss: 6.3620916591109635
Iteration: 3 || Loss: 6.361112302239061
Iteration: 4 || Loss: 6.360119155636589
Iteration: 5 || Loss: 6.359163908799058
Iteration: 6 || Loss: 6.359163908799058
saving ADAM checkpoint...
Sum of params:-109.42302
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.359163908799058
Iteration: 2 || Loss: 6.328140134560866
Iteration: 3 || Loss: 6.253398473357578
Iteration: 4 || Loss: 6.246004411781991
Iteration: 5 || Loss: 6.091292717784724
Iteration: 6 || Loss: 6.045669396378141
Iteration: 7 || Loss: 6.0253381338865495
Iteration: 8 || Loss: 6.020713194018155
Iteration: 9 || Loss: 6.017561365358491
Iteration: 10 || Loss: 6.0069495058958555
Iteration: 11 || Loss: 5.987296843641277
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.43419
Epoch 311 loss:5.987296843641277
MSE loss S0.19510963114654156
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.43419
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.937439896705353
Iteration: 2 || Loss: 23.937142104002934
Iteration: 3 || Loss: 23.936809748967672
Iteration: 4 || Loss: 23.93641888025255
Iteration: 5 || Loss: 23.936234581126165
Iteration: 6 || Loss: 23.936234581126165
saving ADAM checkpoint...
Sum of params:-109.43423
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.936234581126165
Iteration: 2 || Loss: 23.846974607786372
Iteration: 3 || Loss: 23.67829101567906
Iteration: 4 || Loss: 23.5900043735136
Iteration: 5 || Loss: 23.47933630807962
Iteration: 6 || Loss: 23.440428074561424
Iteration: 7 || Loss: 23.423043818169486
Iteration: 8 || Loss: 23.395327204053178
Iteration: 9 || Loss: 23.3449160278229
Iteration: 10 || Loss: 23.3368059275323
Iteration: 11 || Loss: 23.32537096145336
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.41481
Epoch 311 loss:23.32537096145336
MSE loss S0.3981762629901923
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.41481
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.49787063578889
Iteration: 2 || Loss: 66.49763985029705
Iteration: 3 || Loss: 66.4974514233258
Iteration: 4 || Loss: 66.49728732646125
Iteration: 5 || Loss: 66.49712279232615
Iteration: 6 || Loss: 66.49712279232615
saving ADAM checkpoint...
Sum of params:-109.414825
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.49712279232615
Iteration: 2 || Loss: 66.49582722734465
Iteration: 3 || Loss: 66.46352105554506
Iteration: 4 || Loss: 66.38670671752988
Iteration: 5 || Loss: 66.3447566911104
Iteration: 6 || Loss: 66.24880425959667
Iteration: 7 || Loss: 66.19135558766293
Iteration: 8 || Loss: 66.17026662882898
Iteration: 9 || Loss: 66.16294413329601
Iteration: 10 || Loss: 66.14629526812462
Iteration: 11 || Loss: 66.13647155294856
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.44802
Epoch 311 loss:66.13647155294856
MSE loss S0.9640253693617687
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.64559943483122
MSE loss S0.34837716488485804
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.88128851205377
MSE loss S1.8472797914951369
waveform batch: 2/2
Test loss - extrapolation:47.995101786024
MSE loss S1.089165395446951
Epoch 311 mean train loss:3.2913496330359724
Epoch 311 mean test loss - interpolation:3.9409332391385363
Epoch 311 mean test loss - extrapolation:12.073032524839816
Start training epoch 312
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.44802
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.361402087175204
Iteration: 2 || Loss: 6.36034675781176
Iteration: 3 || Loss: 6.359350684275807
Iteration: 4 || Loss: 6.358243665790469
Iteration: 5 || Loss: 6.357295021887621
Iteration: 6 || Loss: 6.357295021887621
saving ADAM checkpoint...
Sum of params:-109.447945
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.357295021887621
Iteration: 2 || Loss: 6.325117826033408
Iteration: 3 || Loss: 6.251071198951902
Iteration: 4 || Loss: 6.243549013780511
Iteration: 5 || Loss: 6.090486080645725
Iteration: 6 || Loss: 6.043024415617528
Iteration: 7 || Loss: 6.02508289681446
Iteration: 8 || Loss: 6.0205907036465245
Iteration: 9 || Loss: 6.017547581113833
Iteration: 10 || Loss: 6.007513559800954
Iteration: 11 || Loss: 5.987957039492269
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.4583
Epoch 312 loss:5.987957039492269
MSE loss S0.1956028192008523
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.4583
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.90859728983558
Iteration: 2 || Loss: 23.908298579362693
Iteration: 3 || Loss: 23.90794877516686
Iteration: 4 || Loss: 23.907680425178803
Iteration: 5 || Loss: 23.907517943470257
Iteration: 6 || Loss: 23.907517943470257
saving ADAM checkpoint...
Sum of params:-109.45836
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.907517943470257
Iteration: 2 || Loss: 23.742256683138887
Iteration: 3 || Loss: 23.674333283322724
Iteration: 4 || Loss: 23.667924106169213
Iteration: 5 || Loss: 23.508054947420753
Iteration: 6 || Loss: 23.4386011178866
Iteration: 7 || Loss: 23.416213679019524
Iteration: 8 || Loss: 23.388541385666233
Iteration: 9 || Loss: 23.34257014229567
Iteration: 10 || Loss: 23.33680384225572
Iteration: 11 || Loss: 23.3231853555664
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.43916
Epoch 312 loss:23.3231853555664
MSE loss S0.3968990237775265
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.43916
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.47642047497357
Iteration: 2 || Loss: 66.47627402500775
Iteration: 3 || Loss: 66.47604808328106
Iteration: 4 || Loss: 66.47592359430709
Iteration: 5 || Loss: 66.47572581104171
Iteration: 6 || Loss: 66.47572581104171
saving ADAM checkpoint...
Sum of params:-109.439186
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.47572581104171
Iteration: 2 || Loss: 66.47455268456223
Iteration: 3 || Loss: 66.44061375924954
Iteration: 4 || Loss: 66.36347717439067
Iteration: 5 || Loss: 66.3238951986252
Iteration: 6 || Loss: 66.22807258166065
Iteration: 7 || Loss: 66.17118786402088
Iteration: 8 || Loss: 66.15180858228919
Iteration: 9 || Loss: 66.14416744379322
Iteration: 10 || Loss: 66.12830886828576
Iteration: 11 || Loss: 66.11853292902111
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.47235
Epoch 312 loss:66.11853292902111
MSE loss S0.9635293100341631
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.642563362706056
MSE loss S0.34824791274859673
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.77723091925525
MSE loss S1.8449174674220918
waveform batch: 2/2
Test loss - extrapolation:47.94588426659114
MSE loss S1.0878564039137024
Epoch 312 mean train loss:3.2906784594510268
Epoch 312 mean test loss - interpolation:3.940427227117676
Epoch 312 mean test loss - extrapolation:12.060259598820531
Start training epoch 313
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.47235
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.359593915483834
Iteration: 2 || Loss: 6.358568417567424
Iteration: 3 || Loss: 6.357526781784726
Iteration: 4 || Loss: 6.356484234392631
Iteration: 5 || Loss: 6.3554541623180665
Iteration: 6 || Loss: 6.3554541623180665
saving ADAM checkpoint...
Sum of params:-109.47228
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3554541623180665
Iteration: 2 || Loss: 6.322154492524308
Iteration: 3 || Loss: 6.249523090578693
Iteration: 4 || Loss: 6.24256330444765
Iteration: 5 || Loss: 6.090344500828406
Iteration: 6 || Loss: 6.044037257974525
Iteration: 7 || Loss: 6.024880609986873
Iteration: 8 || Loss: 6.020425444931278
Iteration: 9 || Loss: 6.017782436389103
Iteration: 10 || Loss: 6.007427453351703
Iteration: 11 || Loss: 5.987930973457194
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.48274
Epoch 313 loss:5.987930973457194
MSE loss S0.19549993274556168
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.48274
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.9002976901611
Iteration: 2 || Loss: 23.899961664936658
Iteration: 3 || Loss: 23.899632457800546
Iteration: 4 || Loss: 23.899406539400445
Iteration: 5 || Loss: 23.899078928546025
Iteration: 6 || Loss: 23.899078928546025
saving ADAM checkpoint...
Sum of params:-109.48282
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.899078928546025
Iteration: 2 || Loss: 23.800734796605724
Iteration: 3 || Loss: 23.65662454017508
Iteration: 4 || Loss: 23.652040532166588
Iteration: 5 || Loss: 23.474583651314727
Iteration: 6 || Loss: 23.433328562800376
Iteration: 7 || Loss: 23.414039891382902
Iteration: 8 || Loss: 23.38667511304438
Iteration: 9 || Loss: 23.340451937215395
Iteration: 10 || Loss: 23.33267235046768
Iteration: 11 || Loss: 23.32118228427069
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.46399
Epoch 313 loss:23.32118228427069
MSE loss S0.3973754693987559
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.46399
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.45314715240966
Iteration: 2 || Loss: 66.45292233106655
Iteration: 3 || Loss: 66.45279810772709
Iteration: 4 || Loss: 66.45263580298598
Iteration: 5 || Loss: 66.45243707129578
Iteration: 6 || Loss: 66.45243707129578
saving ADAM checkpoint...
Sum of params:-109.464005
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.45243707129578
Iteration: 2 || Loss: 66.45099184637074
Iteration: 3 || Loss: 66.41011792941715
Iteration: 4 || Loss: 66.36890607338663
Iteration: 5 || Loss: 66.32788177484066
Iteration: 6 || Loss: 66.22411257462953
Iteration: 7 || Loss: 66.15732191151312
Iteration: 8 || Loss: 66.13348354906347
Iteration: 9 || Loss: 66.12676125021814
Iteration: 10 || Loss: 66.11051870492379
Iteration: 11 || Loss: 66.10023435520074
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.49695
Epoch 313 loss:66.10023435520074
MSE loss S0.9647335659435916
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.64031024797586
MSE loss S0.34920569650370215
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.67804922543628
MSE loss S1.842743857876724
waveform batch: 2/2
Test loss - extrapolation:47.89487567778689
MSE loss S1.0867277712777317
Epoch 313 mean train loss:3.2899775038940904
Epoch 313 mean test loss - interpolation:3.940051707995977
Epoch 313 mean test loss - extrapolation:12.047743741935264
Start training epoch 314
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.49695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.353751545736952
Iteration: 2 || Loss: 6.352827840643255
Iteration: 3 || Loss: 6.351845707099673
Iteration: 4 || Loss: 6.350890482260251
Iteration: 5 || Loss: 6.3499246654171015
Iteration: 6 || Loss: 6.3499246654171015
saving ADAM checkpoint...
Sum of params:-109.49686
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3499246654171015
Iteration: 2 || Loss: 6.321318180179769
Iteration: 3 || Loss: 6.252259041945798
Iteration: 4 || Loss: 6.243235001202145
Iteration: 5 || Loss: 6.09068143104116
Iteration: 6 || Loss: 6.0425465318821265
Iteration: 7 || Loss: 6.025067017952469
Iteration: 8 || Loss: 6.02042275237998
Iteration: 9 || Loss: 6.016829940720613
Iteration: 10 || Loss: 6.006973855706488
Iteration: 11 || Loss: 5.986551507168727
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.507965
Epoch 314 loss:5.986551507168727
MSE loss S0.19510042211130113
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.507965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.914914029421052
Iteration: 2 || Loss: 23.91471266699091
Iteration: 3 || Loss: 23.914290473366716
Iteration: 4 || Loss: 23.913976033351332
Iteration: 5 || Loss: 23.913792208408793
Iteration: 6 || Loss: 23.913792208408793
saving ADAM checkpoint...
Sum of params:-109.50801
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.913792208408793
Iteration: 2 || Loss: 23.76206489343053
Iteration: 3 || Loss: 23.677597124976515
Iteration: 4 || Loss: 23.531120731413942
Iteration: 5 || Loss: 23.51027295660351
Iteration: 6 || Loss: 23.440336864478496
Iteration: 7 || Loss: 23.41761591286961
Iteration: 8 || Loss: 23.389593541072003
Iteration: 9 || Loss: 23.341104208171473
Iteration: 10 || Loss: 23.335675917011923
Iteration: 11 || Loss: 23.320485464368947
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.48924
Epoch 314 loss:23.320485464368947
MSE loss S0.3973765301299542
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.48924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.43354627011057
Iteration: 2 || Loss: 66.43335343124497
Iteration: 3 || Loss: 66.433166353687
Iteration: 4 || Loss: 66.43303214523335
Iteration: 5 || Loss: 66.43281808012891
Iteration: 6 || Loss: 66.43281808012891
saving ADAM checkpoint...
Sum of params:-109.48927
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.43281808012891
Iteration: 2 || Loss: 66.4316697662888
Iteration: 3 || Loss: 66.40378370876591
Iteration: 4 || Loss: 66.30746704022607
Iteration: 5 || Loss: 66.2765624099316
Iteration: 6 || Loss: 66.18808629570465
Iteration: 7 || Loss: 66.13545232630273
Iteration: 8 || Loss: 66.11629126261484
Iteration: 9 || Loss: 66.10870827690788
Iteration: 10 || Loss: 66.09268359034472
Iteration: 11 || Loss: 66.08315463897699
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.521225
Epoch 314 loss:66.08315463897699
MSE loss S0.9632886476834633
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.636070831922535
MSE loss S0.34834664719908487
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.56896984678069
MSE loss S1.8404239460058114
waveform batch: 2/2
Test loss - extrapolation:47.8424389910301
MSE loss S1.0852138779338718
Epoch 314 mean train loss:3.2893169520867125
Epoch 314 mean test loss - interpolation:3.939345138653756
Epoch 314 mean test loss - extrapolation:12.034284069817566
Start training epoch 315
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.521225
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.35471783252631
Iteration: 2 || Loss: 6.353631080778227
Iteration: 3 || Loss: 6.352646335365309
Iteration: 4 || Loss: 6.351674735889
Iteration: 5 || Loss: 6.350623764866853
Iteration: 6 || Loss: 6.350623764866853
saving ADAM checkpoint...
Sum of params:-109.52115
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.350623764866853
Iteration: 2 || Loss: 6.3179746474688185
Iteration: 3 || Loss: 6.247202289695896
Iteration: 4 || Loss: 6.240097789584722
Iteration: 5 || Loss: 6.0897384713427325
Iteration: 6 || Loss: 6.0431619579505265
Iteration: 7 || Loss: 6.024524133851698
Iteration: 8 || Loss: 6.020091981125833
Iteration: 9 || Loss: 6.017290648125851
Iteration: 10 || Loss: 6.0069006691720315
Iteration: 11 || Loss: 5.9873113709438215
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.53165
Epoch 315 loss:5.9873113709438215
MSE loss S0.1958999410994552
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.53165
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.90809178621118
Iteration: 2 || Loss: 23.907842237835645
Iteration: 3 || Loss: 23.90755657674707
Iteration: 4 || Loss: 23.90725050000117
Iteration: 5 || Loss: 23.907003291078286
Iteration: 6 || Loss: 23.907003291078286
saving ADAM checkpoint...
Sum of params:-109.53171
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.907003291078286
Iteration: 2 || Loss: 23.78256060831573
Iteration: 3 || Loss: 23.665042384191242
Iteration: 4 || Loss: 23.650293717903928
Iteration: 5 || Loss: 23.490922345301524
Iteration: 6 || Loss: 23.433109480643918
Iteration: 7 || Loss: 23.41146897003168
Iteration: 8 || Loss: 23.38390632031398
Iteration: 9 || Loss: 23.337111589155665
Iteration: 10 || Loss: 23.33101543480042
Iteration: 11 || Loss: 23.31813783483488
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.51345
Epoch 315 loss:23.31813783483488
MSE loss S0.3967817631949168
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.51345
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.40896983150843
Iteration: 2 || Loss: 66.40883468445172
Iteration: 3 || Loss: 66.40870228280134
Iteration: 4 || Loss: 66.40853234005797
Iteration: 5 || Loss: 66.40836423190527
Iteration: 6 || Loss: 66.40836423190527
saving ADAM checkpoint...
Sum of params:-109.51346
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.40836423190527
Iteration: 2 || Loss: 66.40681893359499
Iteration: 3 || Loss: 66.37421653008272
Iteration: 4 || Loss: 66.31238396295178
Iteration: 5 || Loss: 66.28001697257842
Iteration: 6 || Loss: 66.1837537609419
Iteration: 7 || Loss: 66.12121147116571
Iteration: 8 || Loss: 66.09848396454778
Iteration: 9 || Loss: 66.0915842404647
Iteration: 10 || Loss: 66.07550189048541
Iteration: 11 || Loss: 66.06550414105158
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.54504
Epoch 315 loss:66.06550414105158
MSE loss S0.9643888474724579
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.6342442088804
MSE loss S0.3492399927141242
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.47438745871715
MSE loss S1.8382738606324096
waveform batch: 2/2
Test loss - extrapolation:47.794399156967096
MSE loss S1.0840905913637806
Epoch 315 mean train loss:3.288653563683803
Epoch 315 mean test loss - interpolation:3.939040701480067
Epoch 315 mean test loss - extrapolation:12.022398884640353
Start training epoch 316
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.54504
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.350004960900064
Iteration: 2 || Loss: 6.349129983019567
Iteration: 3 || Loss: 6.348122285326572
Iteration: 4 || Loss: 6.347192613914883
Iteration: 5 || Loss: 6.346220075235704
Iteration: 6 || Loss: 6.346220075235704
saving ADAM checkpoint...
Sum of params:-109.54497
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.346220075235704
Iteration: 2 || Loss: 6.317758685740877
Iteration: 3 || Loss: 6.250082081439359
Iteration: 4 || Loss: 6.241171562792247
Iteration: 5 || Loss: 6.089990248451505
Iteration: 6 || Loss: 6.042067317414875
Iteration: 7 || Loss: 6.024596754198935
Iteration: 8 || Loss: 6.019881612706147
Iteration: 9 || Loss: 6.016022357421613
Iteration: 10 || Loss: 6.006360842262712
Iteration: 11 || Loss: 5.986017473907516
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.5562
Epoch 316 loss:5.986017473907516
MSE loss S0.19537583111267487
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.5562
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.92420212834015
Iteration: 2 || Loss: 23.924011068999743
Iteration: 3 || Loss: 23.923765928729672
Iteration: 4 || Loss: 23.923487877229928
Iteration: 5 || Loss: 23.923238662918422
Iteration: 6 || Loss: 23.923238662918422
saving ADAM checkpoint...
Sum of params:-109.55624
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.923238662918422
Iteration: 2 || Loss: 23.801546322537035
Iteration: 3 || Loss: 23.673840374471425
Iteration: 4 || Loss: 23.661987506595015
Iteration: 5 || Loss: 23.495887258270702
Iteration: 6 || Loss: 23.437647801330122
Iteration: 7 || Loss: 23.41558670322661
Iteration: 8 || Loss: 23.38737000844877
Iteration: 9 || Loss: 23.33661582698124
Iteration: 10 || Loss: 23.330589178935757
Iteration: 11 || Loss: 23.31767380325579
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.53771
Epoch 316 loss:23.31767380325579
MSE loss S0.39711484340356284
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.53771
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.39409995652349
Iteration: 2 || Loss: 66.39395336278025
Iteration: 3 || Loss: 66.3937961548404
Iteration: 4 || Loss: 66.39360116990565
Iteration: 5 || Loss: 66.3934356569083
Iteration: 6 || Loss: 66.3934356569083
saving ADAM checkpoint...
Sum of params:-109.537735
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.3934356569083
Iteration: 2 || Loss: 66.39231052552086
Iteration: 3 || Loss: 66.36057431256823
Iteration: 4 || Loss: 66.28750393361973
Iteration: 5 || Loss: 66.25455598253004
Iteration: 6 || Loss: 66.16045439970512
Iteration: 7 || Loss: 66.10245195353075
Iteration: 8 || Loss: 66.0817413655166
Iteration: 9 || Loss: 66.07449145971937
Iteration: 10 || Loss: 66.05813286089307
Iteration: 11 || Loss: 66.04881643535404
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.56895
Epoch 316 loss:66.04881643535404
MSE loss S0.9634113301457037
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.630536970270967
MSE loss S0.34862687337489795
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.37562573186375
MSE loss S1.8361924858226955
waveform batch: 2/2
Test loss - extrapolation:47.7464416873506
MSE loss S1.0827715036335033
Epoch 316 mean train loss:3.2880175073281843
Epoch 316 mean test loss - interpolation:3.9384228283784943
Epoch 316 mean test loss - extrapolation:12.010172284934528
Start training epoch 317
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.56895
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.350979728856968
Iteration: 2 || Loss: 6.349913332336943
Iteration: 3 || Loss: 6.348968039309424
Iteration: 4 || Loss: 6.347908722412474
Iteration: 5 || Loss: 6.346964027490845
Iteration: 6 || Loss: 6.346964027490845
saving ADAM checkpoint...
Sum of params:-109.568855
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.346964027490845
Iteration: 2 || Loss: 6.315486364688796
Iteration: 3 || Loss: 6.246482330367504
Iteration: 4 || Loss: 6.238929324977823
Iteration: 5 || Loss: 6.089100150141815
Iteration: 6 || Loss: 6.041881078812444
Iteration: 7 || Loss: 6.024125243839528
Iteration: 8 || Loss: 6.019640777823321
Iteration: 9 || Loss: 6.016566656909128
Iteration: 10 || Loss: 6.0064881910837595
Iteration: 11 || Loss: 5.986811876637491
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.57911
Epoch 317 loss:5.986811876637491
MSE loss S0.19556378971206437
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.57911
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.889340786404798
Iteration: 2 || Loss: 23.888988496280863
Iteration: 3 || Loss: 23.888741124228336
Iteration: 4 || Loss: 23.888503659982295
Iteration: 5 || Loss: 23.888064535436193
Iteration: 6 || Loss: 23.888064535436193
saving ADAM checkpoint...
Sum of params:-109.57916
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.888064535436193
Iteration: 2 || Loss: 23.786760453149498
Iteration: 3 || Loss: 23.647728526921405
Iteration: 4 || Loss: 23.643686997653063
Iteration: 5 || Loss: 23.469393488364993
Iteration: 6 || Loss: 23.427170106480563
Iteration: 7 || Loss: 23.407767138965553
Iteration: 8 || Loss: 23.380208623172614
Iteration: 9 || Loss: 23.33350304766309
Iteration: 10 || Loss: 23.32538486572389
Iteration: 11 || Loss: 23.31486639198571
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.56138
Epoch 317 loss:23.31486639198571
MSE loss S0.39770328207296346
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.56138
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.3728654898431
Iteration: 2 || Loss: 66.3727085662
Iteration: 3 || Loss: 66.37258211008701
Iteration: 4 || Loss: 66.3724544212726
Iteration: 5 || Loss: 66.37227624711934
Iteration: 6 || Loss: 66.37227624711934
saving ADAM checkpoint...
Sum of params:-109.5614
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.37227624711934
Iteration: 2 || Loss: 66.37027883662192
Iteration: 3 || Loss: 66.33837082011372
Iteration: 4 || Loss: 66.28470811898896
Iteration: 5 || Loss: 66.25208734708757
Iteration: 6 || Loss: 66.15479647127646
Iteration: 7 || Loss: 66.08961780765847
Iteration: 8 || Loss: 66.0640596947752
Iteration: 9 || Loss: 66.05739877506568
Iteration: 10 || Loss: 66.04280684217278
Iteration: 11 || Loss: 66.03138476152813
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.592285
Epoch 317 loss:66.03138476152813
MSE loss S0.9649848865820525
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.629676247968504
MSE loss S0.35012033553985816
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.28331908854305
MSE loss S1.8338251934778698
waveform batch: 2/2
Test loss - extrapolation:47.69479675317633
MSE loss S1.0814843026533605
Epoch 317 mean train loss:3.287347001039701
Epoch 317 mean test loss - interpolation:3.9382793746614175
Epoch 317 mean test loss - extrapolation:11.998176320143282
Start training epoch 318
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.592285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.343671986553025
Iteration: 2 || Loss: 6.342797597261824
Iteration: 3 || Loss: 6.341947969131022
Iteration: 4 || Loss: 6.340969293935009
Iteration: 5 || Loss: 6.34016407855049
Iteration: 6 || Loss: 6.34016407855049
saving ADAM checkpoint...
Sum of params:-109.5922
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.34016407855049
Iteration: 2 || Loss: 6.315955699883054
Iteration: 3 || Loss: 6.253313676453929
Iteration: 4 || Loss: 6.2419295688539185
Iteration: 5 || Loss: 6.090117970526283
Iteration: 6 || Loss: 6.0424796616012175
Iteration: 7 || Loss: 6.0241778563646795
Iteration: 8 || Loss: 6.019259229893935
Iteration: 9 || Loss: 6.014705006725324
Iteration: 10 || Loss: 6.004513955624761
Iteration: 11 || Loss: 5.9835735555127405
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.60448
Epoch 318 loss:5.9835735555127405
MSE loss S0.19423654713432878
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.60448
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.940125636063918
Iteration: 2 || Loss: 23.939927062729673
Iteration: 3 || Loss: 23.93961433982161
Iteration: 4 || Loss: 23.939214131945647
Iteration: 5 || Loss: 23.939011193035157
Iteration: 6 || Loss: 23.939011193035157
saving ADAM checkpoint...
Sum of params:-109.604515
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.939011193035157
Iteration: 2 || Loss: 23.836202112033913
Iteration: 3 || Loss: 23.67885840729748
Iteration: 4 || Loss: 23.674287269513417
Iteration: 5 || Loss: 23.490502900197576
Iteration: 6 || Loss: 23.44396464186892
Iteration: 7 || Loss: 23.422519398185667
Iteration: 8 || Loss: 23.393138342776865
Iteration: 9 || Loss: 23.33531976246442
Iteration: 10 || Loss: 23.32794460455252
Iteration: 11 || Loss: 23.316109850170342
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.586426
Epoch 318 loss:23.316109850170342
MSE loss S0.3978384645466273
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.586426
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.35674585630736
Iteration: 2 || Loss: 66.35656367483297
Iteration: 3 || Loss: 66.35638678670057
Iteration: 4 || Loss: 66.35624655544251
Iteration: 5 || Loss: 66.35610608213399
Iteration: 6 || Loss: 66.35610608213399
saving ADAM checkpoint...
Sum of params:-109.58645
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.35610608213399
Iteration: 2 || Loss: 66.3549743688983
Iteration: 3 || Loss: 66.32402224190895
Iteration: 4 || Loss: 66.25099462688948
Iteration: 5 || Loss: 66.22449184271969
Iteration: 6 || Loss: 66.13048495775529
Iteration: 7 || Loss: 66.07055786487824
Iteration: 8 || Loss: 66.04880455916295
Iteration: 9 || Loss: 66.04163472778762
Iteration: 10 || Loss: 66.02512055591413
Iteration: 11 || Loss: 66.01554321524841
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.616196
Epoch 318 loss:66.01554321524841
MSE loss S0.9633962539948504
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.625059007139217
MSE loss S0.34884467412349024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.18238449382481
MSE loss S1.8320070577427938
waveform batch: 2/2
Test loss - extrapolation:47.64816468969764
MSE loss S1.08013117076113
Epoch 318 mean train loss:3.2867319524459138
Epoch 318 mean test loss - interpolation:3.937509834523203
Epoch 318 mean test loss - extrapolation:11.985879098626873
Start training epoch 319
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.616196
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.347528820256815
Iteration: 2 || Loss: 6.3465355525092715
Iteration: 3 || Loss: 6.34553195510132
Iteration: 4 || Loss: 6.3445546703975
Iteration: 5 || Loss: 6.343584523581883
Iteration: 6 || Loss: 6.343584523581883
saving ADAM checkpoint...
Sum of params:-109.616135
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.343584523581883
Iteration: 2 || Loss: 6.312865816333068
Iteration: 3 || Loss: 6.24611859135458
Iteration: 4 || Loss: 6.2379523410266415
Iteration: 5 || Loss: 6.088160659204359
Iteration: 6 || Loss: 6.040972003969694
Iteration: 7 || Loss: 6.023467015490157
Iteration: 8 || Loss: 6.0187794938385215
Iteration: 9 || Loss: 6.015364544023289
Iteration: 10 || Loss: 6.005461580253608
Iteration: 11 || Loss: 5.985484379000336
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.62676
Epoch 319 loss:5.985484379000336
MSE loss S0.19556295653068234
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.62676
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.907744084714338
Iteration: 2 || Loss: 23.907472910276013
Iteration: 3 || Loss: 23.907159706469503
Iteration: 4 || Loss: 23.90681968839907
Iteration: 5 || Loss: 23.906519764259993
Iteration: 6 || Loss: 23.906519764259993
saving ADAM checkpoint...
Sum of params:-109.62683
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.906519764259993
Iteration: 2 || Loss: 23.822138593216312
Iteration: 3 || Loss: 23.655316652887116
Iteration: 4 || Loss: 23.644256751578713
Iteration: 5 || Loss: 23.460327473457674
Iteration: 6 || Loss: 23.42280086529522
Iteration: 7 || Loss: 23.40696265974622
Iteration: 8 || Loss: 23.37918597331624
Iteration: 9 || Loss: 23.330287959193967
Iteration: 10 || Loss: 23.32112549157658
Iteration: 11 || Loss: 23.31221288698865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.608734
Epoch 319 loss:23.31221288698865
MSE loss S0.3975147276591671
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.608734
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.3369054821597
Iteration: 2 || Loss: 66.33678427554996
Iteration: 3 || Loss: 66.33662345284617
Iteration: 4 || Loss: 66.33650231825908
Iteration: 5 || Loss: 66.33635949956988
Iteration: 6 || Loss: 66.33635949956988
saving ADAM checkpoint...
Sum of params:-109.608765
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.33635949956988
Iteration: 2 || Loss: 66.33445524897711
Iteration: 3 || Loss: 66.29356355830967
Iteration: 4 || Loss: 66.16310241337922
Iteration: 5 || Loss: 66.12393630193353
Iteration: 6 || Loss: 66.06983994499484
Iteration: 7 || Loss: 66.04587109202691
Iteration: 8 || Loss: 66.0311007167883
Iteration: 9 || Loss: 66.02305032138426
Iteration: 10 || Loss: 66.0081863737341
Iteration: 11 || Loss: 65.99861620369352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.63908
Epoch 319 loss:65.99861620369352
MSE loss S0.9631839327009495
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.625273802807868
MSE loss S0.34956435726345614
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.10520705098017
MSE loss S1.8288090566585027
waveform batch: 2/2
Test loss - extrapolation:47.59856790262213
MSE loss S1.0779735569201678
Epoch 319 mean train loss:3.286079774816638
Epoch 319 mean test loss - interpolation:3.9375456338013115
Epoch 319 mean test loss - extrapolation:11.97531457946686
Start training epoch 320
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.63908
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.345831544414009
Iteration: 2 || Loss: 6.344894253665557
Iteration: 3 || Loss: 6.344004809860896
Iteration: 4 || Loss: 6.343093276573597
Iteration: 5 || Loss: 6.342173348734041
Iteration: 6 || Loss: 6.342173348734041
saving ADAM checkpoint...
Sum of params:-109.63899
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.342173348734041
Iteration: 2 || Loss: 6.316624439964083
Iteration: 3 || Loss: 6.251979751032584
Iteration: 4 || Loss: 6.242136108924833
Iteration: 5 || Loss: 6.089293483008996
Iteration: 6 || Loss: 6.041984256277523
Iteration: 7 || Loss: 6.0233975871314955
Iteration: 8 || Loss: 6.0184085731644155
Iteration: 9 || Loss: 6.014207402568132
Iteration: 10 || Loss: 6.00376942138657
Iteration: 11 || Loss: 5.9828187553026435
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.65133
Epoch 320 loss:5.9828187553026435
MSE loss S0.19429422415307843
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.65133
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.942111241767954
Iteration: 2 || Loss: 23.941785950765293
Iteration: 3 || Loss: 23.941429488068447
Iteration: 4 || Loss: 23.94113934663626
Iteration: 5 || Loss: 23.940876419576973
Iteration: 6 || Loss: 23.940876419576973
saving ADAM checkpoint...
Sum of params:-109.65138
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.940876419576973
Iteration: 2 || Loss: 23.857256921170006
Iteration: 3 || Loss: 23.67553108461047
Iteration: 4 || Loss: 23.671339395471495
Iteration: 5 || Loss: 23.474851520097896
Iteration: 6 || Loss: 23.435170789257384
Iteration: 7 || Loss: 23.418926732433217
Iteration: 8 || Loss: 23.390003717592773
Iteration: 9 || Loss: 23.331816659049803
Iteration: 10 || Loss: 23.32196267291014
Iteration: 11 || Loss: 23.312964909660987
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.63345
Epoch 320 loss:23.312964909660987
MSE loss S0.3980764428739042
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.63345
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.31951734383149
Iteration: 2 || Loss: 66.31930793228554
Iteration: 3 || Loss: 66.3191681710679
Iteration: 4 || Loss: 66.31902634398983
Iteration: 5 || Loss: 66.31882474687606
Iteration: 6 || Loss: 66.31882474687606
saving ADAM checkpoint...
Sum of params:-109.633446
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.31882474687606
Iteration: 2 || Loss: 66.31763718425005
Iteration: 3 || Loss: 66.28056500330258
Iteration: 4 || Loss: 66.23956641752532
Iteration: 5 || Loss: 66.20620593380909
Iteration: 6 || Loss: 66.10613079613158
Iteration: 7 || Loss: 66.0410928499521
Iteration: 8 || Loss: 66.01581318618913
Iteration: 9 || Loss: 66.00898114390247
Iteration: 10 || Loss: 65.99395541764665
Iteration: 11 || Loss: 65.9826489675733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.662674
Epoch 320 loss:65.9826489675733
MSE loss S0.9641610598322683
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.620910878979934
MSE loss S0.349843508794197
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.99794564901227
MSE loss S1.8276104884990056
waveform batch: 2/2
Test loss - extrapolation:47.54938616796688
MSE loss S1.077498431045322
Epoch 320 mean train loss:3.285463194225411
Epoch 320 mean test loss - interpolation:3.9368184798299892
Epoch 320 mean test loss - extrapolation:11.96227765141493
Start training epoch 321
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.662674
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.342102719352836
Iteration: 2 || Loss: 6.341198178488238
Iteration: 3 || Loss: 6.340268212017168
Iteration: 4 || Loss: 6.339370008793342
Iteration: 5 || Loss: 6.33854058698156
Iteration: 6 || Loss: 6.33854058698156
saving ADAM checkpoint...
Sum of params:-109.662605
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.33854058698156
Iteration: 2 || Loss: 6.313283804747279
Iteration: 3 || Loss: 6.251286618988913
Iteration: 4 || Loss: 6.239896459517694
Iteration: 5 || Loss: 6.088233251446513
Iteration: 6 || Loss: 6.041149956481372
Iteration: 7 || Loss: 6.022959964286989
Iteration: 8 || Loss: 6.018062374542926
Iteration: 9 || Loss: 6.014178200367019
Iteration: 10 || Loss: 6.003477929264774
Iteration: 11 || Loss: 5.9827850459214655
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.674194
Epoch 321 loss:5.9827850459214655
MSE loss S0.19419456380724748
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.674194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.915539082643882
Iteration: 2 || Loss: 23.915246569993066
Iteration: 3 || Loss: 23.914987265117933
Iteration: 4 || Loss: 23.914669997246545
Iteration: 5 || Loss: 23.91437635807916
Iteration: 6 || Loss: 23.91437635807916
saving ADAM checkpoint...
Sum of params:-109.67426
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.91437635807916
Iteration: 2 || Loss: 23.83958298372155
Iteration: 3 || Loss: 23.6574001788416
Iteration: 4 || Loss: 23.592453924775203
Iteration: 5 || Loss: 23.465481942323173
Iteration: 6 || Loss: 23.425761011342274
Iteration: 7 || Loss: 23.41268850689862
Iteration: 8 || Loss: 23.384710022915183
Iteration: 9 || Loss: 23.32902220110398
Iteration: 10 || Loss: 23.31904120517443
Iteration: 11 || Loss: 23.310815907788182
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.65641
Epoch 321 loss:23.310815907788182
MSE loss S0.39775333713487365
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.65641
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.30441699248647
Iteration: 2 || Loss: 66.30430163125885
Iteration: 3 || Loss: 66.30413935042036
Iteration: 4 || Loss: 66.3040259763006
Iteration: 5 || Loss: 66.30381348547327
Iteration: 6 || Loss: 66.30381348547327
saving ADAM checkpoint...
Sum of params:-109.656425
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.30381348547327
Iteration: 2 || Loss: 66.30151103962206
Iteration: 3 || Loss: 66.26876824276417
Iteration: 4 || Loss: 66.22637115016285
Iteration: 5 || Loss: 66.1677946479443
Iteration: 6 || Loss: 66.08056353744703
Iteration: 7 || Loss: 66.02452198779483
Iteration: 8 || Loss: 65.99929461484055
Iteration: 9 || Loss: 65.99234456560977
Iteration: 10 || Loss: 65.97877734780845
Iteration: 11 || Loss: 65.9667044725
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.68545
Epoch 321 loss:65.9667044725
MSE loss S0.9644440201605113
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.619479448431683
MSE loss S0.35027170367122507
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.91220949112616
MSE loss S1.825503842726127
waveform batch: 2/2
Test loss - extrapolation:47.50207304899551
MSE loss S1.076286341702408
Epoch 321 mean train loss:3.28483811814516
Epoch 321 mean test loss - interpolation:3.936579908071947
Epoch 321 mean test loss - extrapolation:11.951190211676808
Start training epoch 322
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.68545
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.340561300438489
Iteration: 2 || Loss: 6.33968947776189
Iteration: 3 || Loss: 6.338832054649891
Iteration: 4 || Loss: 6.337999045267148
Iteration: 5 || Loss: 6.337132522071863
Iteration: 6 || Loss: 6.337132522071863
saving ADAM checkpoint...
Sum of params:-109.68538
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.337132522071863
Iteration: 2 || Loss: 6.314423677667423
Iteration: 3 || Loss: 6.255554298195925
Iteration: 4 || Loss: 6.241897351170815
Iteration: 5 || Loss: 6.088428290502777
Iteration: 6 || Loss: 6.041872780940652
Iteration: 7 || Loss: 6.022333375737749
Iteration: 8 || Loss: 6.017315066675433
Iteration: 9 || Loss: 6.012343481231417
Iteration: 10 || Loss: 6.001862443006575
Iteration: 11 || Loss: 5.981089647579927
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.69777
Epoch 322 loss:5.981089647579927
MSE loss S0.19348394708735875
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.69777
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.93267916691303
Iteration: 2 || Loss: 23.932291318187616
Iteration: 3 || Loss: 23.932031659483222
Iteration: 4 || Loss: 23.931605721515687
Iteration: 5 || Loss: 23.931255813724057
Iteration: 6 || Loss: 23.931255813724057
saving ADAM checkpoint...
Sum of params:-109.697815
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.931255813724057
Iteration: 2 || Loss: 23.863145493009984
Iteration: 3 || Loss: 23.665074894296666
Iteration: 4 || Loss: 23.59433077588856
Iteration: 5 || Loss: 23.47217720112606
Iteration: 6 || Loss: 23.43013008779112
Iteration: 7 || Loss: 23.417051505174268
Iteration: 8 || Loss: 23.38883773881678
Iteration: 9 || Loss: 23.329026138809255
Iteration: 10 || Loss: 23.320589739355427
Iteration: 11 || Loss: 23.310718755393932
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.68017
Epoch 322 loss:23.310718755393932
MSE loss S0.39655355259904934
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.68017
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.29280489847115
Iteration: 2 || Loss: 66.29259724208913
Iteration: 3 || Loss: 66.29245162101277
Iteration: 4 || Loss: 66.29226624893978
Iteration: 5 || Loss: 66.29212344716844
Iteration: 6 || Loss: 66.29212344716844
saving ADAM checkpoint...
Sum of params:-109.68017
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.29212344716844
Iteration: 2 || Loss: 66.2908608424339
Iteration: 3 || Loss: 66.25941935764031
Iteration: 4 || Loss: 66.19617605627782
Iteration: 5 || Loss: 66.1394788768533
Iteration: 6 || Loss: 66.05526799277068
Iteration: 7 || Loss: 66.00500798997027
Iteration: 8 || Loss: 65.98406344931466
Iteration: 9 || Loss: 65.97691214823357
Iteration: 10 || Loss: 65.96099804269306
Iteration: 11 || Loss: 65.95128612621949
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.70914
Epoch 322 loss:65.95128612621949
MSE loss S0.96239884116793
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.614476952807816
MSE loss S0.34874970670281347
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.81590078114051
MSE loss S1.8234859050324665
waveform batch: 2/2
Test loss - extrapolation:47.45522173058679
MSE loss S1.074910339642191
Epoch 322 mean train loss:3.2842446389377016
Epoch 322 mean test loss - interpolation:3.9357461588013027
Epoch 322 mean test loss - extrapolation:11.939260209310609
Start training epoch 323
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.70914
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.34583095768695
Iteration: 2 || Loss: 6.344856599408619
Iteration: 3 || Loss: 6.343912896496276
Iteration: 4 || Loss: 6.342941409406933
Iteration: 5 || Loss: 6.341959265546135
Iteration: 6 || Loss: 6.341959265546135
saving ADAM checkpoint...
Sum of params:-109.70907
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.341959265546135
Iteration: 2 || Loss: 6.312200099234498
Iteration: 3 || Loss: 6.247580664003573
Iteration: 4 || Loss: 6.237835049525204
Iteration: 5 || Loss: 6.086641860578779
Iteration: 6 || Loss: 6.039709109488972
Iteration: 7 || Loss: 6.021802922011098
Iteration: 8 || Loss: 6.0171138442852525
Iteration: 9 || Loss: 6.013215230779007
Iteration: 10 || Loss: 6.003811966879478
Iteration: 11 || Loss: 5.98352778907426
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.71988
Epoch 323 loss:5.98352778907426
MSE loss S0.19518078628689572
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.71988
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.907625364288037
Iteration: 2 || Loss: 23.907231600099394
Iteration: 3 || Loss: 23.90692898025144
Iteration: 4 || Loss: 23.906664244324183
Iteration: 5 || Loss: 23.906279889693792
Iteration: 6 || Loss: 23.906279889693792
saving ADAM checkpoint...
Sum of params:-109.71993
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.906279889693792
Iteration: 2 || Loss: 23.851913048836348
Iteration: 3 || Loss: 23.653128183880263
Iteration: 4 || Loss: 23.64654975579473
Iteration: 5 || Loss: 23.451269989823903
Iteration: 6 || Loss: 23.411676975142047
Iteration: 7 || Loss: 23.400541390988497
Iteration: 8 || Loss: 23.373061455514392
Iteration: 9 || Loss: 23.32425124328809
Iteration: 10 || Loss: 23.31536685521108
Iteration: 11 || Loss: 23.306986580400896
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.702415
Epoch 323 loss:23.306986580400896
MSE loss S0.3951735710044367
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.702415
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.26893928930517
Iteration: 2 || Loss: 66.26869348496515
Iteration: 3 || Loss: 66.26861225160278
Iteration: 4 || Loss: 66.26847396863717
Iteration: 5 || Loss: 66.26829324936331
Iteration: 6 || Loss: 66.26829324936331
saving ADAM checkpoint...
Sum of params:-109.702415
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.26829324936331
Iteration: 2 || Loss: 66.2666704320262
Iteration: 3 || Loss: 66.23383209483904
Iteration: 4 || Loss: 66.18527243394708
Iteration: 5 || Loss: 66.11298388926802
Iteration: 6 || Loss: 66.0355589392284
Iteration: 7 || Loss: 65.98844049732725
Iteration: 8 || Loss: 65.96668893229705
Iteration: 9 || Loss: 65.959796527067
Iteration: 10 || Loss: 65.94543100770136
Iteration: 11 || Loss: 65.93468943419916
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.73173
Epoch 323 loss:65.93468943419916
MSE loss S0.9632903298024537
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.61325737734655
MSE loss S0.3496382531936483
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.72776416647346
MSE loss S1.8213458887718177
waveform batch: 2/2
Test loss - extrapolation:47.40851720516413
MSE loss S1.073895931865705
Epoch 323 mean train loss:3.2836277173680797
Epoch 323 mean test loss - interpolation:3.9355428962244248
Epoch 323 mean test loss - extrapolation:11.928023447636468
Start training epoch 324
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.73173
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.341677092397034
Iteration: 2 || Loss: 6.340820793580316
Iteration: 3 || Loss: 6.339903421644112
Iteration: 4 || Loss: 6.338982638632368
Iteration: 5 || Loss: 6.3381406979476065
Iteration: 6 || Loss: 6.3381406979476065
saving ADAM checkpoint...
Sum of params:-109.73166
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3381406979476065
Iteration: 2 || Loss: 6.312559310892263
Iteration: 3 || Loss: 6.252502191992399
Iteration: 4 || Loss: 6.240008591798526
Iteration: 5 || Loss: 6.0872999730345825
Iteration: 6 || Loss: 6.040392218782442
Iteration: 7 || Loss: 6.02169068841201
Iteration: 8 || Loss: 6.016832014674285
Iteration: 9 || Loss: 6.012648651851688
Iteration: 10 || Loss: 6.002733298357628
Iteration: 11 || Loss: 5.981740095590738
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.74336
Epoch 324 loss:5.981740095590738
MSE loss S0.19412812939107438
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.74336
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.91134993526048
Iteration: 2 || Loss: 23.910908593008912
Iteration: 3 || Loss: 23.910568704524398
Iteration: 4 || Loss: 23.910276095172705
Iteration: 5 || Loss: 23.909987319029657
Iteration: 6 || Loss: 23.909987319029657
saving ADAM checkpoint...
Sum of params:-109.7434
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.909987319029657
Iteration: 2 || Loss: 23.856613174207318
Iteration: 3 || Loss: 23.652094208676072
Iteration: 4 || Loss: 23.462533385411064
Iteration: 5 || Loss: 23.45895953028401
Iteration: 6 || Loss: 23.418818794529113
Iteration: 7 || Loss: 23.406741621655087
Iteration: 8 || Loss: 23.378643006767327
Iteration: 9 || Loss: 23.324171819679666
Iteration: 10 || Loss: 23.315285890983127
Iteration: 11 || Loss: 23.306693484074547
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.72625
Epoch 324 loss:23.306693484074547
MSE loss S0.39528675924361634
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.72625
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.25255737309121
Iteration: 2 || Loss: 66.25241383801848
Iteration: 3 || Loss: 66.25222750673879
Iteration: 4 || Loss: 66.25207150343395
Iteration: 5 || Loss: 66.2519362174601
Iteration: 6 || Loss: 66.2519362174601
saving ADAM checkpoint...
Sum of params:-109.72627
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.2519362174601
Iteration: 2 || Loss: 66.25058816725848
Iteration: 3 || Loss: 66.21636260298745
Iteration: 4 || Loss: 66.16923803167133
Iteration: 5 || Loss: 66.105698572609
Iteration: 6 || Loss: 66.02353303590507
Iteration: 7 || Loss: 65.97317132646715
Iteration: 8 || Loss: 65.95113849709092
Iteration: 9 || Loss: 65.9442914715944
Iteration: 10 || Loss: 65.92944273417518
Iteration: 11 || Loss: 65.9189424060122
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.755104
Epoch 324 loss:65.9189424060122
MSE loss S0.9629016520243501
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.610156719588286
MSE loss S0.34946838090967486
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.63555107099907
MSE loss S1.8193614634447481
waveform batch: 2/2
Test loss - extrapolation:47.360778754342135
MSE loss S1.0725520943097278
Epoch 324 mean train loss:3.2830129650233615
Epoch 324 mean test loss - interpolation:3.935026119931381
Epoch 324 mean test loss - extrapolation:11.916360818778434
Start training epoch 325
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.755104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.341207713993366
Iteration: 2 || Loss: 6.340237857974551
Iteration: 3 || Loss: 6.3393016237426805
Iteration: 4 || Loss: 6.338374029675468
Iteration: 5 || Loss: 6.337485631323602
Iteration: 6 || Loss: 6.337485631323602
saving ADAM checkpoint...
Sum of params:-109.755035
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.337485631323602
Iteration: 2 || Loss: 6.310984505848799
Iteration: 3 || Loss: 6.251120710493497
Iteration: 4 || Loss: 6.239027497143234
Iteration: 5 || Loss: 6.0865892412407945
Iteration: 6 || Loss: 6.039956339041059
Iteration: 7 || Loss: 6.021470825786233
Iteration: 8 || Loss: 6.016653805206763
Iteration: 9 || Loss: 6.012770611324132
Iteration: 10 || Loss: 6.002646381322781
Iteration: 11 || Loss: 5.981712358984812
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.76661
Epoch 325 loss:5.981712358984812
MSE loss S0.1944232428804648
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.76661
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.911182378623618
Iteration: 2 || Loss: 23.910819013949915
Iteration: 3 || Loss: 23.910404835677415
Iteration: 4 || Loss: 23.91017651803167
Iteration: 5 || Loss: 23.909872160661642
Iteration: 6 || Loss: 23.909872160661642
saving ADAM checkpoint...
Sum of params:-109.76666
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.909872160661642
Iteration: 2 || Loss: 23.84736125684687
Iteration: 3 || Loss: 23.654593441633462
Iteration: 4 || Loss: 23.64999679325814
Iteration: 5 || Loss: 23.45556008855379
Iteration: 6 || Loss: 23.415034693408735
Iteration: 7 || Loss: 23.40320253420873
Iteration: 8 || Loss: 23.37535750317032
Iteration: 9 || Loss: 23.32216758848605
Iteration: 10 || Loss: 23.314307312859533
Iteration: 11 || Loss: 23.30516303165516
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.74954
Epoch 325 loss:23.30516303165516
MSE loss S0.395933043510162
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.74954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.2370166383264
Iteration: 2 || Loss: 66.23691010868225
Iteration: 3 || Loss: 66.23672313535877
Iteration: 4 || Loss: 66.23661551821601
Iteration: 5 || Loss: 66.23643406883055
Iteration: 6 || Loss: 66.23643406883055
saving ADAM checkpoint...
Sum of params:-109.74955
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.23643406883055
Iteration: 2 || Loss: 66.23457072934042
Iteration: 3 || Loss: 66.2014936289778
Iteration: 4 || Loss: 66.15579563751572
Iteration: 5 || Loss: 66.09380716114272
Iteration: 6 || Loss: 66.01080499740434
Iteration: 7 || Loss: 65.95857851634756
Iteration: 8 || Loss: 65.93530636984076
Iteration: 9 || Loss: 65.92836698766699
Iteration: 10 || Loss: 65.91431001667254
Iteration: 11 || Loss: 65.90324844605684
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.777794
Epoch 325 loss:65.90324844605684
MSE loss S0.963252698107693
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.60793866244019
MSE loss S0.34980695630216696
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.54666273385199
MSE loss S1.8174929595667206
waveform batch: 2/2
Test loss - extrapolation:47.31419542222764
MSE loss S1.0714768516490163
Epoch 325 mean train loss:3.2824180633343727
Epoch 325 mean test loss - interpolation:3.9346564437400318
Epoch 325 mean test loss - extrapolation:11.905071513006638
Start training epoch 326
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.777794
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.339097036332498
Iteration: 2 || Loss: 6.3381855721522475
Iteration: 3 || Loss: 6.337281092569914
Iteration: 4 || Loss: 6.336347282744214
Iteration: 5 || Loss: 6.3355162148109345
Iteration: 6 || Loss: 6.3355162148109345
saving ADAM checkpoint...
Sum of params:-109.77775
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3355162148109345
Iteration: 2 || Loss: 6.310675700497829
Iteration: 3 || Loss: 6.252898364265292
Iteration: 4 || Loss: 6.239408951432418
Iteration: 5 || Loss: 6.08655522096474
Iteration: 6 || Loss: 6.040086880141246
Iteration: 7 || Loss: 6.021060135945196
Iteration: 8 || Loss: 6.016090516478012
Iteration: 9 || Loss: 6.011608246453319
Iteration: 10 || Loss: 6.001726062211366
Iteration: 11 || Loss: 5.9807642801961585
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.789536
Epoch 326 loss:5.9807642801961585
MSE loss S0.1940076862768493
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.789536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.911667261460874
Iteration: 2 || Loss: 23.911328309782558
Iteration: 3 || Loss: 23.91098454100468
Iteration: 4 || Loss: 23.910657572597103
Iteration: 5 || Loss: 23.910490521194927
Iteration: 6 || Loss: 23.910490521194927
saving ADAM checkpoint...
Sum of params:-109.78957
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.910490521194927
Iteration: 2 || Loss: 23.853538102211875
Iteration: 3 || Loss: 23.651826302749555
Iteration: 4 || Loss: 23.624229926745425
Iteration: 5 || Loss: 23.45807344190844
Iteration: 6 || Loss: 23.417354002364114
Iteration: 7 || Loss: 23.405181046443094
Iteration: 8 || Loss: 23.377033162563187
Iteration: 9 || Loss: 23.32109362646277
Iteration: 10 || Loss: 23.312923849690602
Iteration: 11 || Loss: 23.30423566115194
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.77286
Epoch 326 loss:23.30423566115194
MSE loss S0.39521965499070555
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.77286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.21996753833787
Iteration: 2 || Loss: 66.21978373535627
Iteration: 3 || Loss: 66.21962936246742
Iteration: 4 || Loss: 66.21945763100538
Iteration: 5 || Loss: 66.21933553343126
Iteration: 6 || Loss: 66.21933553343126
saving ADAM checkpoint...
Sum of params:-109.77287
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.21933553343126
Iteration: 2 || Loss: 66.2179348421812
Iteration: 3 || Loss: 66.18568327618404
Iteration: 4 || Loss: 66.13463400415621
Iteration: 5 || Loss: 66.07061307639616
Iteration: 6 || Loss: 65.99054816717775
Iteration: 7 || Loss: 65.94183842993537
Iteration: 8 || Loss: 65.91998871150373
Iteration: 9 || Loss: 65.91307153389761
Iteration: 10 || Loss: 65.89841141135528
Iteration: 11 || Loss: 65.88786908961474
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.80079
Epoch 326 loss:65.88786908961474
MSE loss S0.9625797185546519
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.604887515515596
MSE loss S0.3494539705049001
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.45760737434433
MSE loss S1.8154678728862652
waveform batch: 2/2
Test loss - extrapolation:47.26771452661558
MSE loss S1.070130860684374
Epoch 326 mean train loss:3.281823070033201
Epoch 326 mean test loss - interpolation:3.934147919252599
Epoch 326 mean test loss - extrapolation:11.893776825079991
Start training epoch 327
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.80079
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.339760487816685
Iteration: 2 || Loss: 6.3388616018158475
Iteration: 3 || Loss: 6.33794438571392
Iteration: 4 || Loss: 6.337004234960673
Iteration: 5 || Loss: 6.336084227560666
Iteration: 6 || Loss: 6.336084227560666
saving ADAM checkpoint...
Sum of params:-109.80072
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.336084227560666
Iteration: 2 || Loss: 6.309684110865167
Iteration: 3 || Loss: 6.251078847945449
Iteration: 4 || Loss: 6.238422939300241
Iteration: 5 || Loss: 6.085744197517277
Iteration: 6 || Loss: 6.039376317679451
Iteration: 7 || Loss: 6.020755980033059
Iteration: 8 || Loss: 6.015883750446628
Iteration: 9 || Loss: 6.011807530594168
Iteration: 10 || Loss: 6.001884417229817
Iteration: 11 || Loss: 5.981101886847956
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.81212
Epoch 327 loss:5.981101886847956
MSE loss S0.19429649011322958
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.81212
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.899816217545418
Iteration: 2 || Loss: 23.899485482701202
Iteration: 3 || Loss: 23.899156646992438
Iteration: 4 || Loss: 23.898756812675735
Iteration: 5 || Loss: 23.89852740355037
Iteration: 6 || Loss: 23.89852740355037
saving ADAM checkpoint...
Sum of params:-109.81218
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.89852740355037
Iteration: 2 || Loss: 23.852108051206297
Iteration: 3 || Loss: 23.64375991014542
Iteration: 4 || Loss: 23.572501408117553
Iteration: 5 || Loss: 23.451558140802526
Iteration: 6 || Loss: 23.412484634379823
Iteration: 7 || Loss: 23.40044051641305
Iteration: 8 || Loss: 23.372527521271802
Iteration: 9 || Loss: 23.318586010594657
Iteration: 10 || Loss: 23.309644254952115
Iteration: 11 || Loss: 23.302106270750745
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.79567
Epoch 327 loss:23.302106270750745
MSE loss S0.3953178366022876
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.79567
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.19875851586707
Iteration: 2 || Loss: 66.19863587533882
Iteration: 3 || Loss: 66.19850697934724
Iteration: 4 || Loss: 66.19840593968433
Iteration: 5 || Loss: 66.19823513256699
Iteration: 6 || Loss: 66.19823513256699
saving ADAM checkpoint...
Sum of params:-109.79571
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.19823513256699
Iteration: 2 || Loss: 66.19628735078194
Iteration: 3 || Loss: 66.16995768774696
Iteration: 4 || Loss: 66.11207252520306
Iteration: 5 || Loss: 66.04249468299848
Iteration: 6 || Loss: 65.96955017601395
Iteration: 7 || Loss: 65.92602129973695
Iteration: 8 || Loss: 65.90418098884808
Iteration: 9 || Loss: 65.89711085063142
Iteration: 10 || Loss: 65.8839459101793
Iteration: 11 || Loss: 65.87251555490981
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.823135
Epoch 327 loss:65.87251555490981
MSE loss S0.9633871615157292
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.603889110934816
MSE loss S0.3501885424661074
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.37444753327664
MSE loss S1.8136127697628381
waveform batch: 2/2
Test loss - extrapolation:47.22376089335599
MSE loss S1.0690536923278715
Epoch 327 mean train loss:3.281231852155466
Epoch 327 mean test loss - interpolation:3.933981518489136
Epoch 327 mean test loss - extrapolation:11.883184035552718
Start training epoch 328
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.823135
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.336896302196463
Iteration: 2 || Loss: 6.335975464278816
Iteration: 3 || Loss: 6.335124005224171
Iteration: 4 || Loss: 6.334248051153828
Iteration: 5 || Loss: 6.33339055096164
Iteration: 6 || Loss: 6.33339055096164
saving ADAM checkpoint...
Sum of params:-109.82307
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.33339055096164
Iteration: 2 || Loss: 6.309885118075602
Iteration: 3 || Loss: 6.255595347215373
Iteration: 4 || Loss: 6.240422898685374
Iteration: 5 || Loss: 6.086046341138903
Iteration: 6 || Loss: 6.040346162969355
Iteration: 7 || Loss: 6.020275805981297
Iteration: 8 || Loss: 6.015258385270871
Iteration: 9 || Loss: 6.0104550367554985
Iteration: 10 || Loss: 6.000135662740746
Iteration: 11 || Loss: 5.979194196513632
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.835304
Epoch 328 loss:5.979194196513632
MSE loss S0.19351861994542277
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.835304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.920408920349015
Iteration: 2 || Loss: 23.920073734699
Iteration: 3 || Loss: 23.91979597073839
Iteration: 4 || Loss: 23.919371532895504
Iteration: 5 || Loss: 23.919094834339067
Iteration: 6 || Loss: 23.919094834339067
saving ADAM checkpoint...
Sum of params:-109.83537
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.919094834339067
Iteration: 2 || Loss: 23.87369417892032
Iteration: 3 || Loss: 23.654255286087697
Iteration: 4 || Loss: 23.641004531756963
Iteration: 5 || Loss: 23.461458721554397
Iteration: 6 || Loss: 23.421103192719514
Iteration: 7 || Loss: 23.408165097365924
Iteration: 8 || Loss: 23.379369578703272
Iteration: 9 || Loss: 23.31857329215482
Iteration: 10 || Loss: 23.310030690222998
Iteration: 11 || Loss: 23.302167095137236
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.819
Epoch 328 loss:23.302167095137236
MSE loss S0.3950268681235807
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.819
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.18254235245948
Iteration: 2 || Loss: 66.18237000273784
Iteration: 3 || Loss: 66.18224159378195
Iteration: 4 || Loss: 66.18208575824644
Iteration: 5 || Loss: 66.1819806033884
Iteration: 6 || Loss: 66.1819806033884
saving ADAM checkpoint...
Sum of params:-109.819046
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.1819806033884
Iteration: 2 || Loss: 66.18056073800737
Iteration: 3 || Loss: 66.14694219467184
Iteration: 4 || Loss: 66.10512418462889
Iteration: 5 || Loss: 66.0449297618034
Iteration: 6 || Loss: 65.96355251675988
Iteration: 7 || Loss: 65.91294502845322
Iteration: 8 || Loss: 65.88979288193299
Iteration: 9 || Loss: 65.88284890982247
Iteration: 10 || Loss: 65.86901970416986
Iteration: 11 || Loss: 65.85768266463377
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.84577
Epoch 328 loss:65.85768266463377
MSE loss S0.9627364378883951
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.600372086405056
MSE loss S0.3498250682911538
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.28387923246562
MSE loss S1.811622417729879
waveform batch: 2/2
Test loss - extrapolation:47.17558852360989
MSE loss S1.067692446080343
Epoch 328 mean train loss:3.2806566881477464
Epoch 328 mean test loss - interpolation:3.933395347734176
Epoch 328 mean test loss - extrapolation:11.871622313006293
Start training epoch 329
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.84577
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3370697990484075
Iteration: 2 || Loss: 6.336245651617684
Iteration: 3 || Loss: 6.335287286958886
Iteration: 4 || Loss: 6.33444730809187
Iteration: 5 || Loss: 6.33349737129776
Iteration: 6 || Loss: 6.33349737129776
saving ADAM checkpoint...
Sum of params:-109.8457
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.33349737129776
Iteration: 2 || Loss: 6.308955172814587
Iteration: 3 || Loss: 6.253264354562056
Iteration: 4 || Loss: 6.238893041359919
Iteration: 5 || Loss: 6.085015686205888
Iteration: 6 || Loss: 6.039403310442758
Iteration: 7 || Loss: 6.019881200805471
Iteration: 8 || Loss: 6.014906055032088
Iteration: 9 || Loss: 6.010378945105414
Iteration: 10 || Loss: 6.0001594975330965
Iteration: 11 || Loss: 5.979453093459
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.85744
Epoch 329 loss:5.979453093459
MSE loss S0.1937053790469856
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.85744
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.904339380683624
Iteration: 2 || Loss: 23.903981000205867
Iteration: 3 || Loss: 23.903744425491784
Iteration: 4 || Loss: 23.90330045348719
Iteration: 5 || Loss: 23.902891122016243
Iteration: 6 || Loss: 23.902891122016243
saving ADAM checkpoint...
Sum of params:-109.8575
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.902891122016243
Iteration: 2 || Loss: 23.864031364661546
Iteration: 3 || Loss: 23.64448465437197
Iteration: 4 || Loss: 23.599118764706176
Iteration: 5 || Loss: 23.454370608152384
Iteration: 6 || Loss: 23.414990806575958
Iteration: 7 || Loss: 23.402308626190543
Iteration: 8 || Loss: 23.37413696724202
Iteration: 9 || Loss: 23.316018693361166
Iteration: 10 || Loss: 23.30759857659505
Iteration: 11 || Loss: 23.300214910400822
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.8414
Epoch 329 loss:23.300214910400822
MSE loss S0.39527670740806525
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.8414
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.16559189807543
Iteration: 2 || Loss: 66.16543080882185
Iteration: 3 || Loss: 66.16527305526714
Iteration: 4 || Loss: 66.16510761627343
Iteration: 5 || Loss: 66.16500962720944
Iteration: 6 || Loss: 66.16500962720944
saving ADAM checkpoint...
Sum of params:-109.84143
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.16500962720944
Iteration: 2 || Loss: 66.16312322907199
Iteration: 3 || Loss: 66.13545234539369
Iteration: 4 || Loss: 66.08420639484159
Iteration: 5 || Loss: 66.02233637201817
Iteration: 6 || Loss: 65.94561006770189
Iteration: 7 || Loss: 65.89802329196665
Iteration: 8 || Loss: 65.87459877413349
Iteration: 9 || Loss: 65.86753449770247
Iteration: 10 || Loss: 65.85484587835322
Iteration: 11 || Loss: 65.8429678200022
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.86766
Epoch 329 loss:65.8429678200022
MSE loss S0.9632715244599621
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.59896869743139
MSE loss S0.3502837552872198
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.20137271114578
MSE loss S1.8099035042577694
waveform batch: 2/2
Test loss - extrapolation:47.13214920287388
MSE loss S1.0666311791939818
Epoch 329 mean train loss:3.280090890478001
Epoch 329 mean test loss - interpolation:3.933161449571898
Epoch 329 mean test loss - extrapolation:11.861126826168304
Start training epoch 330
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.86766
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.335165796051052
Iteration: 2 || Loss: 6.334200004611146
Iteration: 3 || Loss: 6.33343827108011
Iteration: 4 || Loss: 6.332568874225705
Iteration: 5 || Loss: 6.331720450736067
Iteration: 6 || Loss: 6.331720450736067
saving ADAM checkpoint...
Sum of params:-109.867615
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.331720450736067
Iteration: 2 || Loss: 6.308878699062927
Iteration: 3 || Loss: 6.25642570297874
Iteration: 4 || Loss: 6.240212087357854
Iteration: 5 || Loss: 6.084957489532942
Iteration: 6 || Loss: 6.040047521961867
Iteration: 7 || Loss: 6.019305940121866
Iteration: 8 || Loss: 6.014203496438194
Iteration: 9 || Loss: 6.009048077371475
Iteration: 10 || Loss: 5.998623407096437
Iteration: 11 || Loss: 5.97790170856408
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.88002
Epoch 330 loss:5.97790170856408
MSE loss S0.19318811475362618
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.88002
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.924243834977233
Iteration: 2 || Loss: 23.92387923910322
Iteration: 3 || Loss: 23.923571719982817
Iteration: 4 || Loss: 23.92315654058203
Iteration: 5 || Loss: 23.922766079639054
Iteration: 6 || Loss: 23.922766079639054
saving ADAM checkpoint...
Sum of params:-109.88008
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.922766079639054
Iteration: 2 || Loss: 23.882402708617175
Iteration: 3 || Loss: 23.655581697031025
Iteration: 4 || Loss: 23.648927756298374
Iteration: 5 || Loss: 23.46169197363879
Iteration: 6 || Loss: 23.42122358751794
Iteration: 7 || Loss: 23.407947617436935
Iteration: 8 || Loss: 23.378829359865797
Iteration: 9 || Loss: 23.315703209469863
Iteration: 10 || Loss: 23.30777603554509
Iteration: 11 || Loss: 23.300078981995117
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.86406
Epoch 330 loss:23.300078981995117
MSE loss S0.39466284671084373
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.86406
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.15204820711091
Iteration: 2 || Loss: 66.15188793880482
Iteration: 3 || Loss: 66.15174110317417
Iteration: 4 || Loss: 66.15159748031652
Iteration: 5 || Loss: 66.15139868643439
Iteration: 6 || Loss: 66.15139868643439
saving ADAM checkpoint...
Sum of params:-109.86409
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.15139868643439
Iteration: 2 || Loss: 66.15027110155934
Iteration: 3 || Loss: 66.11138738639208
Iteration: 4 || Loss: 66.08039039443405
Iteration: 5 || Loss: 66.02125725638825
Iteration: 6 || Loss: 65.93708034682602
Iteration: 7 || Loss: 65.88431368817035
Iteration: 8 || Loss: 65.86057494965706
Iteration: 9 || Loss: 65.85366689998223
Iteration: 10 || Loss: 65.83985662035727
Iteration: 11 || Loss: 65.8285599960981
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.89007
Epoch 330 loss:65.8285599960981
MSE loss S0.9624101534818321
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.595142769985777
MSE loss S0.3496964169380175
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.11385485615979
MSE loss S1.8079702561927444
waveform batch: 2/2
Test loss - extrapolation:47.08525709179689
MSE loss S1.0653430970093642
Epoch 330 mean train loss:3.2795358857468035
Epoch 330 mean test loss - interpolation:3.9325237949976297
Epoch 330 mean test loss - extrapolation:11.849925995663057
Start training epoch 331
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.89007
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.336514684681559
Iteration: 2 || Loss: 6.335613027038763
Iteration: 3 || Loss: 6.334745564006445
Iteration: 4 || Loss: 6.333837993221535
Iteration: 5 || Loss: 6.332959506845436
Iteration: 6 || Loss: 6.332959506845436
saving ADAM checkpoint...
Sum of params:-109.889984
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.332959506845436
Iteration: 2 || Loss: 6.308243153064057
Iteration: 3 || Loss: 6.253291621478237
Iteration: 4 || Loss: 6.238152144489309
Iteration: 5 || Loss: 6.083919370295388
Iteration: 6 || Loss: 6.038608747585563
Iteration: 7 || Loss: 6.018921031352412
Iteration: 8 || Loss: 6.0139703692735775
Iteration: 9 || Loss: 6.009392062500802
Iteration: 10 || Loss: 5.99930845701037
Iteration: 11 || Loss: 5.978663313201534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.90166
Epoch 331 loss:5.978663313201534
MSE loss S0.19380537026744474
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.90166
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.90629992501354
Iteration: 2 || Loss: 23.906026101903013
Iteration: 3 || Loss: 23.9055648729537
Iteration: 4 || Loss: 23.905301482344125
Iteration: 5 || Loss: 23.904908129777308
Iteration: 6 || Loss: 23.904908129777308
saving ADAM checkpoint...
Sum of params:-109.9017
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.904908129777308
Iteration: 2 || Loss: 23.864886980381048
Iteration: 3 || Loss: 23.645154244965372
Iteration: 4 || Loss: 23.636771312777388
Iteration: 5 || Loss: 23.451266567748288
Iteration: 6 || Loss: 23.411857118060155
Iteration: 7 || Loss: 23.399438494883917
Iteration: 8 || Loss: 23.371222670478986
Iteration: 9 || Loss: 23.312983500916065
Iteration: 10 || Loss: 23.30511770641576
Iteration: 11 || Loss: 23.297918217109704
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.88592
Epoch 331 loss:23.297918217109704
MSE loss S0.3946055493777685
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.88592
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.13426089732123
Iteration: 2 || Loss: 66.134074934585
Iteration: 3 || Loss: 66.13388475925991
Iteration: 4 || Loss: 66.13375286160519
Iteration: 5 || Loss: 66.13363965199393
Iteration: 6 || Loss: 66.13363965199393
saving ADAM checkpoint...
Sum of params:-109.88596
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.13363965199393
Iteration: 2 || Loss: 66.1320208226101
Iteration: 3 || Loss: 66.09574112847339
Iteration: 4 || Loss: 66.06676392166625
Iteration: 5 || Loss: 66.00592069441181
Iteration: 6 || Loss: 65.9238571362318
Iteration: 7 || Loss: 65.8707297021655
Iteration: 8 || Loss: 65.84550835456048
Iteration: 9 || Loss: 65.83843857083203
Iteration: 10 || Loss: 65.82625769442905
Iteration: 11 || Loss: 65.81413234915155
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.91156
Epoch 331 loss:65.81413234915155
MSE loss S0.9631414872985888
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.593714002316993
MSE loss S0.3502912059271148
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:95.03028988531501
MSE loss S1.806276116904629
waveform batch: 2/2
Test loss - extrapolation:47.041400656572236
MSE loss S1.064334544394195
Epoch 331 mean train loss:3.278990133774579
Epoch 331 mean test loss - interpolation:3.9322856670528323
Epoch 331 mean test loss - extrapolation:11.83930754515727
Start training epoch 332
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.91156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.333236055605154
Iteration: 2 || Loss: 6.3323551965151115
Iteration: 3 || Loss: 6.3315021380131515
Iteration: 4 || Loss: 6.330691762052376
Iteration: 5 || Loss: 6.3297963086234725
Iteration: 6 || Loss: 6.3297963086234725
saving ADAM checkpoint...
Sum of params:-109.91149
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3297963086234725
Iteration: 2 || Loss: 6.307584687453306
Iteration: 3 || Loss: 6.256812588986724
Iteration: 4 || Loss: 6.239456525015605
Iteration: 5 || Loss: 6.0838347397258765
Iteration: 6 || Loss: 6.039566128372458
Iteration: 7 || Loss: 6.018295471156526
Iteration: 8 || Loss: 6.013181478068181
Iteration: 9 || Loss: 6.008069541982361
Iteration: 10 || Loss: 5.997500595914005
Iteration: 11 || Loss: 5.976888285619328
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.923874
Epoch 332 loss:5.976888285619328
MSE loss S0.1928948952125546
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-109.923874
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.91319815842871
Iteration: 2 || Loss: 23.912859069899632
Iteration: 3 || Loss: 23.912486050814476
Iteration: 4 || Loss: 23.912038919836817
Iteration: 5 || Loss: 23.91177836048809
Iteration: 6 || Loss: 23.91177836048809
saving ADAM checkpoint...
Sum of params:-109.92393
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.91177836048809
Iteration: 2 || Loss: 23.874633242813797
Iteration: 3 || Loss: 23.645613516617555
Iteration: 4 || Loss: 23.589908921088995
Iteration: 5 || Loss: 23.45882409576041
Iteration: 6 || Loss: 23.418902755045806
Iteration: 7 || Loss: 23.405437141963258
Iteration: 8 || Loss: 23.376442161155776
Iteration: 9 || Loss: 23.31259595698453
Iteration: 10 || Loss: 23.304985776082326
Iteration: 11 || Loss: 23.297590985431018
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.90847
Epoch 332 loss:23.297590985431018
MSE loss S0.3948631276966583
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-109.90847
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.12120740279457
Iteration: 2 || Loss: 66.12106858321471
Iteration: 3 || Loss: 66.12093893859605
Iteration: 4 || Loss: 66.12078486560227
Iteration: 5 || Loss: 66.12061710867475
Iteration: 6 || Loss: 66.12061710867475
saving ADAM checkpoint...
Sum of params:-109.90851
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.12061710867475
Iteration: 2 || Loss: 66.11911614917146
Iteration: 3 || Loss: 66.08753584315691
Iteration: 4 || Loss: 66.04599134264566
Iteration: 5 || Loss: 65.98853116519889
Iteration: 6 || Loss: 65.90751422855726
Iteration: 7 || Loss: 65.8561660939386
Iteration: 8 || Loss: 65.83180216086677
Iteration: 9 || Loss: 65.8247565835813
Iteration: 10 || Loss: 65.81193147189381
Iteration: 11 || Loss: 65.80002137452962
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-109.93367
Epoch 332 loss:65.80002137452962
MSE loss S0.9626348225149892
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:23.5910161801494
MSE loss S0.35001884205080747
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:94.9499399989446
MSE loss S1.8044219010671279
waveform batch: 2/2
Test loss - extrapolation:46.99775558042677
MSE loss S1.0630632272431346
Epoch 332 mean train loss:3.278431056744137
Epoch 332 mean test loss - interpolation:3.9318360300249
Epoch 332 mean test loss - extrapolation:11.828974631614281
Start training epoch 333
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-109.93367
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.334271793303004
Iteration: 2 || Loss: 6.333451632535302
Iteration: 3 || Loss: 6.332544920293658
Iteration: 4 || Loss: 6.331689721772065
Iteration: 5 || Loss: 6.330898892074309
Iteration: 6 || Loss: 6.330898892074309
saving ADAM checkpoint...
Sum of params:-109.93359
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.330898892074309
Iteration: 2 || Loss: 6.307524975014209
Iteration: 3 || Loss: 6.255875167000417
Iteration: 4 || Loss: 6.238850335497149
Iteration: 5 || Loss: 6.083088009940586
Iteration: 6 || Loss: 6.0387678400596085
Iteration: 7 || Loss: 6.017827902973784
Iteration: 8 || Loss: 6.012845596373662
Iteration: 9 || Loss: 6.008047523839837
