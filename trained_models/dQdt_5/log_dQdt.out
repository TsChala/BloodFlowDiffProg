Size of 1D data matrix:(300, 110, 47, 4)
Shape: [timesteps,spatial locations,waveforms,variables]
Number of parameters in neural network: 2330
optimizer 1 is ADAM
optimizer 2 is BFGS optimizer
ODE Time integrator selected:RK4
Fresh training initialized
Batch size:10
Start training epoch 1
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-0.55454886
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 500.4922932645069
Iteration: 2 || Loss: 489.8452480567436
Iteration: 3 || Loss: 487.2268559798006
Iteration: 4 || Loss: 485.58372507144645
Iteration: 5 || Loss: 484.01047960548
Iteration: 6 || Loss: 484.01047960548
saving ADAM checkpoint...
Sum of params:12.062765
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 484.01047960548
Iteration: 2 || Loss: 482.0020465065355
Iteration: 3 || Loss: 480.48645981828537
Iteration: 4 || Loss: 477.90198247257945
Iteration: 5 || Loss: 474.89852649990183
Iteration: 6 || Loss: 473.9746957104378
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:6.778488
Epoch 1 loss:473.9746957104378
waveform batch: 2/3
Using ADAM optimizer
Sum of params:6.778488
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2143.1299867163825
Iteration: 2 || Loss: 2114.731644938598
Iteration: 3 || Loss: 2094.7009942547957
Iteration: 4 || Loss: 2071.8955241696026
Iteration: 5 || Loss: 2069.7745502730595
Iteration: 6 || Loss: 2069.7745502730595
saving ADAM checkpoint...
Sum of params:7.120581
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2069.7745502730595
Iteration: 2 || Loss: 2043.8187377870777
Iteration: 3 || Loss: 1993.2110858482893
Iteration: 4 || Loss: 1950.6162920249024
Iteration: 5 || Loss: 1934.9873457294289
Iteration: 6 || Loss: 1886.91802581493
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:12.168191
Epoch 1 loss:1886.91802581493
waveform batch: 3/3
Using ADAM optimizer
Sum of params:12.168191
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7406.981593864554
Iteration: 2 || Loss: 7409.438048883796
Iteration: 3 || Loss: 7209.738308225585
Iteration: 4 || Loss: 6800.674846977156
Iteration: 5 || Loss: 6688.751294629431
Iteration: 6 || Loss: 6688.751294629431
saving ADAM checkpoint...
Sum of params:7.9257555
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6688.751294629431
Iteration: 2 || Loss: 6555.986267582528
Iteration: 3 || Loss: 6526.992883264745
Iteration: 4 || Loss: 6427.00735141306
Iteration: 5 || Loss: 6262.228742434133
Iteration: 6 || Loss: 5789.725712958795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:18.28593
Epoch 1 loss:5789.725712958795
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:1742.4453305958812
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3964.1728423930185
waveform batch: 2/2
Test loss - extrapolation:2747.3909406793764
Epoch 1 mean train loss:281.0558080856608
Epoch 1 mean test loss - interpolation:290.4075550993135
Epoch 1 mean test loss - extrapolation:559.2969819226996
Start training epoch 2
waveform batch: 1/3
Using ADAM optimizer
Sum of params:18.28593
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 526.3464498264843
Iteration: 2 || Loss: 386.1164198081075
Iteration: 3 || Loss: 413.55549947698165
Iteration: 4 || Loss: 422.969138347265
Iteration: 5 || Loss: 405.7829986886258
Iteration: 6 || Loss: 386.1164198081075
saving ADAM checkpoint...
Sum of params:22.315933
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 386.1164198081075
Iteration: 2 || Loss: 320.57199783556496
Iteration: 3 || Loss: 314.3461546910946
Iteration: 4 || Loss: 298.5894254408648
Iteration: 5 || Loss: 244.39268761932388
Iteration: 6 || Loss: 233.95982595429504
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:28.123278
Epoch 2 loss:233.95982595429504
waveform batch: 2/3
Using ADAM optimizer
Sum of params:28.123278
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1299.2086863095378
Iteration: 2 || Loss: 1800.4416592749105
Iteration: 3 || Loss: 1215.2424941473987
Iteration: 4 || Loss: 1258.2282928794264
Iteration: 5 || Loss: 1372.2952149475464
Iteration: 6 || Loss: 1215.2424941473987
saving ADAM checkpoint...
Sum of params:28.034521
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1215.2424941473987
Iteration: 2 || Loss: 1199.3900954567553
Iteration: 3 || Loss: 1183.0854392011674
Iteration: 4 || Loss: 1143.7848766046184
Iteration: 5 || Loss: 1110.7284042950664
Iteration: 6 || Loss: 1096.8780042150408
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:29.070478
Epoch 2 loss:1096.8780042150408
waveform batch: 3/3
Using ADAM optimizer
Sum of params:29.070478
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5061.532339562911
Iteration: 2 || Loss: 5704.929091133836
Iteration: 3 || Loss: 4728.440149639099
Iteration: 4 || Loss: 4626.365963102264
Iteration: 5 || Loss: 4637.37303435204
Iteration: 6 || Loss: 4626.365963102264
saving ADAM checkpoint...
Sum of params:25.503231
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4626.365963102264
Iteration: 2 || Loss: 4501.561628729146
Iteration: 3 || Loss: 4247.547024146827
Iteration: 4 || Loss: 3653.2585728364857
Iteration: 5 || Loss: 3539.576432903144
Iteration: 6 || Loss: 2930.700570736092
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:37.308346
Epoch 2 loss:2930.700570736092
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:773.0357272081621
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:2139.129630608381
waveform batch: 2/2
Test loss - extrapolation:1579.3830882666916
Epoch 2 mean train loss:146.94960003122165
Epoch 2 mean test loss - interpolation:128.83928786802701
Epoch 2 mean test loss - extrapolation:309.87605990625605
Start training epoch 3
Changing learning rate to:0.001
waveform batch: 1/3
Using ADAM optimizer
Sum of params:37.308346
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 100.74650704107484
Iteration: 2 || Loss: 99.58229702952072
Iteration: 3 || Loss: 94.13707782247562
Iteration: 4 || Loss: 86.97334490904545
Iteration: 5 || Loss: 82.470672326056
Iteration: 6 || Loss: 82.470672326056
saving ADAM checkpoint...
Sum of params:37.663803
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 82.470672326056
Iteration: 2 || Loss: 81.44513556444169
Iteration: 3 || Loss: 66.3265366368903
Iteration: 4 || Loss: 65.51754023964862
Iteration: 5 || Loss: 60.991684676860714
Iteration: 6 || Loss: 54.00666588395658
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:41.503433
Epoch 3 loss:54.00666588395658
waveform batch: 2/3
Using ADAM optimizer
Sum of params:41.503433
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 394.67669463579904
Iteration: 2 || Loss: 397.4225165564757
Iteration: 3 || Loss: 385.85775493616654
Iteration: 4 || Loss: 380.45351949817496
Iteration: 5 || Loss: 378.79482711005465
Iteration: 6 || Loss: 378.79482711005465
saving ADAM checkpoint...
Sum of params:41.885784
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 378.79482711005465
Iteration: 2 || Loss: 376.02460695613945
Iteration: 3 || Loss: 324.961611705096
Iteration: 4 || Loss: 316.63742867582414
Iteration: 5 || Loss: 280.3610977133887
Iteration: 6 || Loss: 190.65143357485508
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:21.540138
Epoch 3 loss:190.65143357485508
waveform batch: 3/3
Using ADAM optimizer
Sum of params:21.540138
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1822.1097569370384
Iteration: 2 || Loss: 1809.6643344404467
Iteration: 3 || Loss: 1793.1312140520452
Iteration: 4 || Loss: 1778.856713468927
Iteration: 5 || Loss: 1766.8678726079086
Iteration: 6 || Loss: 1766.8678726079086
saving ADAM checkpoint...
Sum of params:20.2888
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1766.8678726079086
Iteration: 2 || Loss: 1764.2778992878768
Iteration: 3 || Loss: 1589.0697065673817
Iteration: 4 || Loss: 1338.5158939731414
Iteration: 5 || Loss: 1269.8333151288332
Iteration: 6 || Loss: 1161.0705766433591
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:36.15944
Epoch 3 loss:1161.0705766433591
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:258.8056639722494
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:1012.3460536808474
waveform batch: 2/2
Test loss - extrapolation:827.1236642890133
Epoch 3 mean train loss:48.47340262421279
Epoch 3 mean test loss - interpolation:43.13427732870824
Epoch 3 mean test loss - extrapolation:153.28914316415504
Start training epoch 4
waveform batch: 1/3
Using ADAM optimizer
Sum of params:36.15944
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 54.54342997730304
Iteration: 2 || Loss: 57.13312578748544
Iteration: 3 || Loss: 52.10393046365769
Iteration: 4 || Loss: 52.56272638817507
Iteration: 5 || Loss: 52.4547999943218
Iteration: 6 || Loss: 52.10393046365769
saving ADAM checkpoint...
Sum of params:35.927605
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 52.10393046365769
Iteration: 2 || Loss: 51.96244913439784
Iteration: 3 || Loss: 41.79869039477359
Iteration: 4 || Loss: 39.97803734034732
Iteration: 5 || Loss: 39.55494547547119
Iteration: 6 || Loss: 37.476336013318026
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:35.681686
Epoch 4 loss:37.476336013318026
waveform batch: 2/3
Using ADAM optimizer
Sum of params:35.681686
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 149.25836355713486
Iteration: 2 || Loss: 144.35567946365583
Iteration: 3 || Loss: 144.1001764566353
Iteration: 4 || Loss: 141.05475534841668
Iteration: 5 || Loss: 137.83770408270885
Iteration: 6 || Loss: 137.83770408270885
saving ADAM checkpoint...
Sum of params:36.26896
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 137.83770408270885
Iteration: 2 || Loss: 137.5169540924403
Iteration: 3 || Loss: 129.0460184777647
Iteration: 4 || Loss: 124.75273500162548
Iteration: 5 || Loss: 105.05570334065732
Iteration: 6 || Loss: 74.33772261115352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:37.32778
Epoch 4 loss:74.33772261115352
waveform batch: 3/3
Using ADAM optimizer
Sum of params:37.32778
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 976.2377306060183
Iteration: 2 || Loss: 965.4626144676729
Iteration: 3 || Loss: 955.8972074485412
Iteration: 4 || Loss: 943.2943143334676
Iteration: 5 || Loss: 934.5321184793461
Iteration: 6 || Loss: 934.5321184793461
saving ADAM checkpoint...
Sum of params:38.63653
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 934.5321184793461
Iteration: 2 || Loss: 932.5828416742952
Iteration: 3 || Loss: 813.2842049508894
Iteration: 4 || Loss: 805.6446129461176
Iteration: 5 || Loss: 763.7798381850187
Iteration: 6 || Loss: 728.2098214509524
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:42.269485
Epoch 4 loss:728.2098214509524
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:158.20912306175364
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:774.2933235572406
waveform batch: 2/2
Test loss - extrapolation:549.3007249243549
Epoch 4 mean train loss:28.966340692255997
Epoch 4 mean test loss - interpolation:26.36818717695894
Epoch 4 mean test loss - extrapolation:110.29950404013296
Start training epoch 5
waveform batch: 1/3
Using ADAM optimizer
Sum of params:42.269485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 114.47106146117794
Iteration: 2 || Loss: 58.33066383864152
Iteration: 3 || Loss: 51.115356104819575
Iteration: 4 || Loss: 66.70424739240285
Iteration: 5 || Loss: 74.49951564054511
Iteration: 6 || Loss: 51.115356104819575
saving ADAM checkpoint...
Sum of params:42.222637
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 51.115356104819575
Iteration: 2 || Loss: 49.202132964187356
Iteration: 3 || Loss: 48.95470241383626
Iteration: 4 || Loss: 36.677868424809766
Iteration: 5 || Loss: 31.63563464578496
Iteration: 6 || Loss: 27.09017920203281
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:41.67656
Epoch 5 loss:27.09017920203281
waveform batch: 2/3
Using ADAM optimizer
Sum of params:41.67656
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 59.932339061998384
Iteration: 2 || Loss: 77.00062268665455
Iteration: 3 || Loss: 60.03507978843261
Iteration: 4 || Loss: 61.87247394741598
Iteration: 5 || Loss: 67.69848398246249
Iteration: 6 || Loss: 59.932339061998384
saving ADAM checkpoint...
Sum of params:41.67656
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 59.932339061998384
Iteration: 2 || Loss: 59.237432092186474
Iteration: 3 || Loss: 58.91703208853258
Iteration: 4 || Loss: 57.03908317963309
Iteration: 5 || Loss: 56.935703676781145
Iteration: 6 || Loss: 55.980166454160226
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:41.772644
Epoch 5 loss:55.980166454160226
waveform batch: 3/3
Using ADAM optimizer
Sum of params:41.772644
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 774.5850480066765
Iteration: 2 || Loss: 770.6666470352166
Iteration: 3 || Loss: 759.1542466558943
Iteration: 4 || Loss: 749.8668525664564
Iteration: 5 || Loss: 745.5858545440989
Iteration: 6 || Loss: 745.5858545440989
saving ADAM checkpoint...
Sum of params:43.035423
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 745.5858545440989
Iteration: 2 || Loss: 742.270601484808
Iteration: 3 || Loss: 670.9411482198234
Iteration: 4 || Loss: 633.8655452695388
Iteration: 5 || Loss: 622.5341826142279
Iteration: 6 || Loss: 614.8603020285661
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:46.041016
Epoch 5 loss:614.8603020285661
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:144.89147361197706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:657.3652626010122
waveform batch: 2/2
Test loss - extrapolation:434.6369992797343
Epoch 5 mean train loss:24.066574058095142
Epoch 5 mean test loss - interpolation:24.14857893532951
Epoch 5 mean test loss - extrapolation:91.00018849006221
Start training epoch 6
Changing learning rate to:0.0001
waveform batch: 1/3
Using ADAM optimizer
Sum of params:46.041016
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 95.78244474103371
Iteration: 2 || Loss: 90.33221668348158
Iteration: 3 || Loss: 85.28998152783056
Iteration: 4 || Loss: 80.65982734760111
Iteration: 5 || Loss: 76.44348880646305
Iteration: 6 || Loss: 76.44348880646305
saving ADAM checkpoint...
Sum of params:45.997124
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 76.44348880646305
Iteration: 2 || Loss: 55.843823659131864
Iteration: 3 || Loss: 54.72624449095368
Iteration: 4 || Loss: 38.57014687841404
Iteration: 5 || Loss: 35.568447190087554
Iteration: 6 || Loss: 27.245642500003243
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:45.269455
Epoch 6 loss:27.245642500003243
waveform batch: 2/3
Using ADAM optimizer
Sum of params:45.269455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 62.57590283727336
Iteration: 2 || Loss: 62.13531364551013
Iteration: 3 || Loss: 62.165561034063124
Iteration: 4 || Loss: 62.189375589487156
Iteration: 5 || Loss: 62.0564828480825
Iteration: 6 || Loss: 62.0564828480825
saving ADAM checkpoint...
Sum of params:45.34084
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 62.0564828480825
Iteration: 2 || Loss: 61.90383684276869
Iteration: 3 || Loss: 55.41366767086672
Iteration: 4 || Loss: 54.89863211501469
Iteration: 5 || Loss: 53.46033515664118
Iteration: 6 || Loss: 53.28758260602693
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:45.581482
Epoch 6 loss:53.28758260602693
waveform batch: 3/3
Using ADAM optimizer
Sum of params:45.581482
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 671.7808349655498
Iteration: 2 || Loss: 669.4269701883362
Iteration: 3 || Loss: 667.5114805771132
Iteration: 4 || Loss: 666.018172156583
Iteration: 5 || Loss: 664.9152360439348
Iteration: 6 || Loss: 664.9152360439348
saving ADAM checkpoint...
Sum of params:45.732437
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 664.9152360439348
Iteration: 2 || Loss: 664.3581502047705
Iteration: 3 || Loss: 554.7125140135972
Iteration: 4 || Loss: 544.3245688538403
Iteration: 5 || Loss: 530.324322606513
Iteration: 6 || Loss: 496.2091961761726
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:48.440845
Epoch 6 loss:496.2091961761726
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:90.53521885799222
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:532.7240419002866
waveform batch: 2/2
Test loss - extrapolation:404.76575809924043
Epoch 6 mean train loss:19.887669699386304
Epoch 6 mean test loss - interpolation:15.089203142998704
Epoch 6 mean test loss - extrapolation:78.12414999996058
Start training epoch 7
waveform batch: 1/3
Using ADAM optimizer
Sum of params:48.440845
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.69515389386792
Iteration: 2 || Loss: 32.180164448766305
Iteration: 3 || Loss: 31.205612869452974
Iteration: 4 || Loss: 30.733614950823622
Iteration: 5 || Loss: 30.66754815477046
Iteration: 6 || Loss: 30.66754815477046
saving ADAM checkpoint...
Sum of params:48.422043
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.66754815477046
Iteration: 2 || Loss: 30.644949615527306
Iteration: 3 || Loss: 25.697606165814474
Iteration: 4 || Loss: 23.66764904827152
Iteration: 5 || Loss: 23.390410611798014
Iteration: 6 || Loss: 21.907583035349546
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:48.03848
Epoch 7 loss:21.907583035349546
waveform batch: 2/3
Using ADAM optimizer
Sum of params:48.03848
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 57.49864605241485
Iteration: 2 || Loss: 56.59090872308492
Iteration: 3 || Loss: 56.43394747949087
Iteration: 4 || Loss: 56.62250859456395
Iteration: 5 || Loss: 56.671516834035636
Iteration: 6 || Loss: 56.43394747949087
saving ADAM checkpoint...
Sum of params:48.049084
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.43394747949087
Iteration: 2 || Loss: 56.39582005351099
Iteration: 3 || Loss: 51.72193808743389
Iteration: 4 || Loss: 51.42022100466947
Iteration: 5 || Loss: 50.94501127494535
Iteration: 6 || Loss: 50.394668943011695
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:48.05896
Epoch 7 loss:50.394668943011695
waveform batch: 3/3
Using ADAM optimizer
Sum of params:48.05896
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 550.379189120633
Iteration: 2 || Loss: 548.3238090382108
Iteration: 3 || Loss: 546.7028062617879
Iteration: 4 || Loss: 545.5030129907142
Iteration: 5 || Loss: 544.6842482595331
Iteration: 6 || Loss: 544.6842482595331
saving ADAM checkpoint...
Sum of params:47.935074
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 544.6842482595331
Iteration: 2 || Loss: 544.3540789908681
Iteration: 3 || Loss: 483.55544090258235
Iteration: 4 || Loss: 466.0088541081346
Iteration: 5 || Loss: 463.18936037160097
Iteration: 6 || Loss: 429.42819924697534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:50.396667
Epoch 7 loss:429.42819924697534
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:82.55368408392138
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:479.8022108660148
waveform batch: 2/2
Test loss - extrapolation:351.1358809083567
Epoch 7 mean train loss:17.301050042252985
Epoch 7 mean test loss - interpolation:13.75894734732023
Epoch 7 mean test loss - extrapolation:69.24484098119763
Start training epoch 8
waveform batch: 1/3
Using ADAM optimizer
Sum of params:50.396667
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.34727012346664
Iteration: 2 || Loss: 32.87205430908009
Iteration: 3 || Loss: 30.91087332393698
Iteration: 4 || Loss: 29.457215459544187
Iteration: 5 || Loss: 28.487684628613728
Iteration: 6 || Loss: 28.487684628613728
saving ADAM checkpoint...
Sum of params:50.400757
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.487684628613728
Iteration: 2 || Loss: 27.8592266282301
Iteration: 3 || Loss: 27.727815608609486
Iteration: 4 || Loss: 21.563354903891515
Iteration: 5 || Loss: 21.09361428839429
Iteration: 6 || Loss: 20.722332849860827
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:49.903404
Epoch 8 loss:20.722332849860827
waveform batch: 2/3
Using ADAM optimizer
Sum of params:49.903404
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 53.621640828387285
Iteration: 2 || Loss: 52.77554199511564
Iteration: 3 || Loss: 52.5805161309836
Iteration: 4 || Loss: 52.729817383546575
Iteration: 5 || Loss: 52.79962405633889
Iteration: 6 || Loss: 52.5805161309836
saving ADAM checkpoint...
Sum of params:49.9272
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 52.5805161309836
Iteration: 2 || Loss: 52.55693167794323
Iteration: 3 || Loss: 49.086742880226296
Iteration: 4 || Loss: 48.920758056448605
Iteration: 5 || Loss: 47.09963294555377
Iteration: 6 || Loss: 46.722662109178685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:49.953697
Epoch 8 loss:46.722662109178685
waveform batch: 3/3
Using ADAM optimizer
Sum of params:49.953697
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 471.1963850746816
Iteration: 2 || Loss: 469.87751712021566
Iteration: 3 || Loss: 468.94109039542514
Iteration: 4 || Loss: 468.3423238823278
Iteration: 5 || Loss: 467.96429393184275
Iteration: 6 || Loss: 467.96429393184275
saving ADAM checkpoint...
Sum of params:49.958015
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 467.96429393184275
Iteration: 2 || Loss: 467.70612239071545
Iteration: 3 || Loss: 418.8882868657232
Iteration: 4 || Loss: 404.56433433066746
Iteration: 5 || Loss: 401.9078515782313
Iteration: 6 || Loss: 392.8668818220673
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:51.252327
Epoch 8 loss:392.8668818220673
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:77.50412237986679
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:462.8755692131155
waveform batch: 2/2
Test loss - extrapolation:333.28339200645894
Epoch 8 mean train loss:15.872823337279543
Epoch 8 mean test loss - interpolation:12.917353729977798
Epoch 8 mean test loss - extrapolation:66.3465801016312
Start training epoch 9
Changing learning rate to:1.0e-5
waveform batch: 1/3
Using ADAM optimizer
Sum of params:51.252327
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.823783509189404
Iteration: 2 || Loss: 39.457482950009805
Iteration: 3 || Loss: 39.09638727000967
Iteration: 4 || Loss: 38.74033804780962
Iteration: 5 || Loss: 38.389354212143466
Iteration: 6 || Loss: 38.389354212143466
saving ADAM checkpoint...
Sum of params:51.25444
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.389354212143466
Iteration: 2 || Loss: 26.29357553317947
Iteration: 3 || Loss: 25.346259823082683
Iteration: 4 || Loss: 20.067653368767278
Iteration: 5 || Loss: 19.49618012663995
Iteration: 6 || Loss: 19.48574045966906
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:50.659016
Epoch 9 loss:19.48574045966906
waveform batch: 2/3
Using ADAM optimizer
Sum of params:50.659016
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.27757739439712
Iteration: 2 || Loss: 49.21673327901052
Iteration: 3 || Loss: 49.161501354654476
Iteration: 4 || Loss: 49.11190055678028
Iteration: 5 || Loss: 49.067946473179575
Iteration: 6 || Loss: 49.067946473179575
saving ADAM checkpoint...
Sum of params:50.663437
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.067946473179575
Iteration: 2 || Loss: 48.94959095045182
Iteration: 3 || Loss: 47.93204980244721
Iteration: 4 || Loss: 45.67407965991802
Iteration: 5 || Loss: 45.61573270417346
Iteration: 6 || Loss: 44.40713543925449
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:50.50823
Epoch 9 loss:44.40713543925449
waveform batch: 3/3
Using ADAM optimizer
Sum of params:50.50823
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 448.06956570762503
Iteration: 2 || Loss: 447.93760960482876
Iteration: 3 || Loss: 447.8096556513053
Iteration: 4 || Loss: 447.68569875141685
Iteration: 5 || Loss: 447.5657422424938
Iteration: 6 || Loss: 447.5657422424938
saving ADAM checkpoint...
Sum of params:50.508133
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 447.5657422424938
Iteration: 2 || Loss: 446.645574558274
Iteration: 3 || Loss: 391.8275394652162
Iteration: 4 || Loss: 382.0107007063363
Iteration: 5 || Loss: 378.97935907640283
Iteration: 6 || Loss: 369.2177497673277
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:52.8301
Epoch 9 loss:369.2177497673277
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:87.85137956478847
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:440.4042333125781
waveform batch: 2/2
Test loss - extrapolation:291.3211435440667
Epoch 9 mean train loss:14.934849160905214
Epoch 9 mean test loss - interpolation:14.641896594131412
Epoch 9 mean test loss - extrapolation:60.97711473805373
Start training epoch 10
waveform batch: 1/3
Using ADAM optimizer
Sum of params:52.8301
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.41318705544559
Iteration: 2 || Loss: 40.22768072327842
Iteration: 3 || Loss: 40.04549265026459
Iteration: 4 || Loss: 39.86656396217819
Iteration: 5 || Loss: 39.6909822948699
Iteration: 6 || Loss: 39.6909822948699
saving ADAM checkpoint...
Sum of params:52.831413
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.6909822948699
Iteration: 2 || Loss: 35.05500970643563
Iteration: 3 || Loss: 34.78754132713942
Iteration: 4 || Loss: 23.79021016991486
Iteration: 5 || Loss: 21.263683172101935
Iteration: 6 || Loss: 20.885538230122293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:51.86419
Epoch 10 loss:20.885538230122293
waveform batch: 2/3
Using ADAM optimizer
Sum of params:51.86419
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 52.772394751162906
Iteration: 2 || Loss: 52.75404687764174
Iteration: 3 || Loss: 52.739491827527
Iteration: 4 || Loss: 52.72805211556251
Iteration: 5 || Loss: 52.718292995707905
Iteration: 6 || Loss: 52.718292995707905
saving ADAM checkpoint...
Sum of params:51.869144
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 52.718292995707905
Iteration: 2 || Loss: 52.69155213245259
Iteration: 3 || Loss: 43.11669530166528
Iteration: 4 || Loss: 42.702957189752915
Iteration: 5 || Loss: 41.926669116346325
Iteration: 6 || Loss: 41.71370488482729
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:52.09826
Epoch 10 loss:41.71370488482729
waveform batch: 3/3
Using ADAM optimizer
Sum of params:52.09826
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 387.59746191277685
Iteration: 2 || Loss: 387.4746730453873
Iteration: 3 || Loss: 387.35487592427273
Iteration: 4 || Loss: 387.23803809458184
Iteration: 5 || Loss: 387.12418563428974
Iteration: 6 || Loss: 387.12418563428974
saving ADAM checkpoint...
Sum of params:52.105934
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 387.12418563428974
Iteration: 2 || Loss: 386.07875499871216
Iteration: 3 || Loss: 344.99215819679773
Iteration: 4 || Loss: 334.87600255798685
Iteration: 5 || Loss: 331.689468143309
Iteration: 6 || Loss: 327.1975313406813
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:54.20244
Epoch 10 loss:327.1975313406813
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:81.02583630277826
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:404.7915080518305
waveform batch: 2/2
Test loss - extrapolation:259.6447068321349
Epoch 10 mean train loss:13.441268084676926
Epoch 10 mean test loss - interpolation:13.504306050463043
Epoch 10 mean test loss - extrapolation:55.36968457366378
Start training epoch 11
waveform batch: 1/3
Using ADAM optimizer
Sum of params:54.20244
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.13041069866827
Iteration: 2 || Loss: 40.87932612277029
Iteration: 3 || Loss: 40.63165638891971
Iteration: 4 || Loss: 40.38731882257219
Iteration: 5 || Loss: 40.14631306177699
Iteration: 6 || Loss: 40.14631306177699
saving ADAM checkpoint...
Sum of params:54.204224
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.14631306177699
Iteration: 2 || Loss: 31.176634998489643
Iteration: 3 || Loss: 30.74638561788354
Iteration: 4 || Loss: 23.094156359002366
Iteration: 5 || Loss: 21.28432982260617
Iteration: 6 || Loss: 18.767750526378517
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:53.459778
Epoch 11 loss:18.767750526378517
waveform batch: 2/3
Using ADAM optimizer
Sum of params:53.459778
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.91274133898804
Iteration: 2 || Loss: 50.79864335067529
Iteration: 3 || Loss: 50.688062028336844
Iteration: 4 || Loss: 50.58095707860714
Iteration: 5 || Loss: 50.47741246402548
Iteration: 6 || Loss: 50.47741246402548
saving ADAM checkpoint...
Sum of params:53.454407
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.47741246402548
Iteration: 2 || Loss: 49.162126494942385
Iteration: 3 || Loss: 48.74710847580175
Iteration: 4 || Loss: 40.99248667465198
Iteration: 5 || Loss: 40.751020641443915
Iteration: 6 || Loss: 39.96923838039136
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:53.605736
Epoch 11 loss:39.96923838039136
waveform batch: 3/3
Using ADAM optimizer
Sum of params:53.605736
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 344.8598043832388
Iteration: 2 || Loss: 344.76085203500884
Iteration: 3 || Loss: 344.6639915992526
Iteration: 4 || Loss: 344.5692719481497
Iteration: 5 || Loss: 344.47666019867654
Iteration: 6 || Loss: 344.47666019867654
saving ADAM checkpoint...
Sum of params:53.60565
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 344.47666019867654
Iteration: 2 || Loss: 343.65273548944197
Iteration: 3 || Loss: 316.71763651411106
Iteration: 4 || Loss: 309.5288063376393
Iteration: 5 || Loss: 305.29684502631386
Iteration: 6 || Loss: 299.448241156255
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:56.565956
Epoch 11 loss:299.448241156255
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:79.7163099851235
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:365.2719694032483
waveform batch: 2/2
Test loss - extrapolation:231.41848710137248
Epoch 11 mean train loss:12.351214829759478
Epoch 11 mean test loss - interpolation:13.28605166418725
Epoch 11 mean test loss - extrapolation:49.724204708718396
Start training epoch 12
Changing learning rate to:1.0000000000000002e-6
waveform batch: 1/3
Using ADAM optimizer
Sum of params:56.565956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.74593513799779
Iteration: 2 || Loss: 35.73122118306978
Iteration: 3 || Loss: 35.71653992697432
Iteration: 4 || Loss: 35.70188855191221
Iteration: 5 || Loss: 35.68727463031136
Iteration: 6 || Loss: 35.68727463031136
saving ADAM checkpoint...
Sum of params:56.56619
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.68727463031136
Iteration: 2 || Loss: 32.66040711870868
Iteration: 3 || Loss: 32.48004278703682
Iteration: 4 || Loss: 22.343318928177
Iteration: 5 || Loss: 19.94745055624157
Iteration: 6 || Loss: 19.4479115174848
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:55.81382
Epoch 12 loss:19.4479115174848
waveform batch: 2/3
Using ADAM optimizer
Sum of params:55.81382
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.4038424323907
Iteration: 2 || Loss: 50.40181664920769
Iteration: 3 || Loss: 50.39983985590431
Iteration: 4 || Loss: 50.39791384999379
Iteration: 5 || Loss: 50.39603773015269
Iteration: 6 || Loss: 50.39603773015269
saving ADAM checkpoint...
Sum of params:55.814285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.39603773015269
Iteration: 2 || Loss: 50.38042224152214
Iteration: 3 || Loss: 42.76252118836553
Iteration: 4 || Loss: 42.59558706412617
Iteration: 5 || Loss: 38.307870620597015
Iteration: 6 || Loss: 37.900773601194025
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:56.20607
Epoch 12 loss:37.900773601194025
waveform batch: 3/3
Using ADAM optimizer
Sum of params:56.20607
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 298.2689394621535
Iteration: 2 || Loss: 298.26077622327267
Iteration: 3 || Loss: 298.25264720947166
Iteration: 4 || Loss: 298.24452157759004
Iteration: 5 || Loss: 298.236434473237
Iteration: 6 || Loss: 298.236434473237
saving ADAM checkpoint...
Sum of params:56.206917
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 298.236434473237
Iteration: 2 || Loss: 297.64434142963364
Iteration: 3 || Loss: 275.0285494691568
Iteration: 4 || Loss: 271.2066622527047
Iteration: 5 || Loss: 250.38774096448785
Iteration: 6 || Loss: 243.92379105828005
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:60.080322
Epoch 12 loss:243.92379105828005
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:72.02554291608344
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:312.6437921259427
waveform batch: 2/2
Test loss - extrapolation:190.7123157397015
Epoch 12 mean train loss:10.388706075067548
Epoch 12 mean test loss - interpolation:12.004257152680573
Epoch 12 mean test loss - extrapolation:41.94634232213701
Start training epoch 13
waveform batch: 1/3
Using ADAM optimizer
Sum of params:60.080322
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.767208892734395
Iteration: 2 || Loss: 37.73735245671841
Iteration: 3 || Loss: 37.70751455249777
Iteration: 4 || Loss: 37.67773973612142
Iteration: 5 || Loss: 37.64801165227878
Iteration: 6 || Loss: 37.64801165227878
saving ADAM checkpoint...
Sum of params:60.080315
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.64801165227878
Iteration: 2 || Loss: 27.731053629881576
Iteration: 3 || Loss: 27.254869161338327
Iteration: 4 || Loss: 19.29001628757141
Iteration: 5 || Loss: 17.998095135086725
Iteration: 6 || Loss: 17.931419430693428
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:59.438904
Epoch 13 loss:17.931419430693428
waveform batch: 2/3
Using ADAM optimizer
Sum of params:59.438904
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 47.83326865612228
Iteration: 2 || Loss: 47.82837780880588
Iteration: 3 || Loss: 47.823557710561026
Iteration: 4 || Loss: 47.81879067049736
Iteration: 5 || Loss: 47.81411988024946
Iteration: 6 || Loss: 47.81411988024946
saving ADAM checkpoint...
Sum of params:59.439335
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 47.81411988024946
Iteration: 2 || Loss: 47.69546830273934
Iteration: 3 || Loss: 44.87126972758138
Iteration: 4 || Loss: 43.36237341201444
Iteration: 5 || Loss: 42.57381023322139
Iteration: 6 || Loss: 41.838141415427
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:59.40722
Epoch 13 loss:41.838141415427
waveform batch: 3/3
Using ADAM optimizer
Sum of params:59.40722
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 252.47999149805426
Iteration: 2 || Loss: 252.468043989448
Iteration: 3 || Loss: 252.45613073439142
Iteration: 4 || Loss: 252.4442917425082
Iteration: 5 || Loss: 252.43249545576185
Iteration: 6 || Loss: 252.43249545576185
saving ADAM checkpoint...
Sum of params:59.407658
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 252.43249545576185
Iteration: 2 || Loss: 251.407909070727
Iteration: 3 || Loss: 224.49215913590612
Iteration: 4 || Loss: 220.10619449018182
Iteration: 5 || Loss: 219.1220748710038
Iteration: 6 || Loss: 214.15011078569194
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:60.50858
Epoch 13 loss:214.15011078569194
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:55.124297934225616
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:287.04841155060717
waveform batch: 2/2
Test loss - extrapolation:185.28332306257892
Epoch 13 mean train loss:9.445505918338357
Epoch 13 mean test loss - interpolation:9.187382989037603
Epoch 13 mean test loss - extrapolation:39.360977884432174
Start training epoch 14
waveform batch: 1/3
Using ADAM optimizer
Sum of params:60.50858
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.439559747931916
Iteration: 2 || Loss: 21.433397572078405
Iteration: 3 || Loss: 21.427271496793647
Iteration: 4 || Loss: 21.42118095387446
Iteration: 5 || Loss: 21.415137638012297
Iteration: 6 || Loss: 21.415137638012297
saving ADAM checkpoint...
Sum of params:60.50891
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.415137638012297
Iteration: 2 || Loss: 21.00025508601454
Iteration: 3 || Loss: 20.823191113675676
Iteration: 4 || Loss: 16.82320033116078
Iteration: 5 || Loss: 16.588997700712984
Iteration: 6 || Loss: 15.162857396932129
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:59.90299
Epoch 14 loss:15.162857396932129
waveform batch: 2/3
Using ADAM optimizer
Sum of params:59.90299
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.845685425021955
Iteration: 2 || Loss: 41.84226585277856
Iteration: 3 || Loss: 41.83890112927823
Iteration: 4 || Loss: 41.83558869724751
Iteration: 5 || Loss: 41.832367813183076
Iteration: 6 || Loss: 41.832367813183076
saving ADAM checkpoint...
Sum of params:59.90339
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.832367813183076
Iteration: 2 || Loss: 41.749899089918806
Iteration: 3 || Loss: 40.674120753303896
Iteration: 4 || Loss: 39.52169756310369
Iteration: 5 || Loss: 39.4222599333696
Iteration: 6 || Loss: 36.551678328608986
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:59.53921
Epoch 14 loss:36.551678328608986
waveform batch: 3/3
Using ADAM optimizer
Sum of params:59.53921
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 235.82670614353034
Iteration: 2 || Loss: 235.82251351703624
Iteration: 3 || Loss: 235.81835773469513
Iteration: 4 || Loss: 235.81421964944158
Iteration: 5 || Loss: 235.81010338735604
Iteration: 6 || Loss: 235.81010338735604
saving ADAM checkpoint...
Sum of params:59.53914
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 235.81010338735604
Iteration: 2 || Loss: 235.70555229324486
Iteration: 3 || Loss: 213.94705855639123
Iteration: 4 || Loss: 209.53257404640044
Iteration: 5 || Loss: 208.46764314647112
Iteration: 6 || Loss: 200.7656707838814
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:60.957855
Epoch 14 loss:200.7656707838814
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:45.896494733110615
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:278.9128741084153
waveform batch: 2/2
Test loss - extrapolation:185.3748648572565
Epoch 14 mean train loss:8.706214017566294
Epoch 14 mean test loss - interpolation:7.649415788851769
Epoch 14 mean test loss - extrapolation:38.69064491380598
Start training epoch 15
Changing learning rate to:1.0000000000000002e-7
waveform batch: 1/3
Using ADAM optimizer
Sum of params:60.957855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.69556656640487
Iteration: 2 || Loss: 17.69389174003873
Iteration: 3 || Loss: 17.69221705072475
Iteration: 4 || Loss: 17.69054394351809
Iteration: 5 || Loss: 17.688870006878847
Iteration: 6 || Loss: 17.688870006878847
saving ADAM checkpoint...
Sum of params:60.957848
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.688870006878847
Iteration: 2 || Loss: 14.956175820994233
Iteration: 3 || Loss: 14.800201639995466
Iteration: 4 || Loss: 13.125968536515698
Iteration: 5 || Loss: 13.067937200965055
Iteration: 6 || Loss: 13.055695931335189
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:60.61405
Epoch 15 loss:13.055695931335189
waveform batch: 2/3
Using ADAM optimizer
Sum of params:60.61405
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.88622043668772
Iteration: 2 || Loss: 37.885296242615034
Iteration: 3 || Loss: 37.884366163982854
Iteration: 4 || Loss: 37.883436800936494
Iteration: 5 || Loss: 37.88251407614708
Iteration: 6 || Loss: 37.88251407614708
saving ADAM checkpoint...
Sum of params:60.61408
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.88251407614708
Iteration: 2 || Loss: 37.204269352291256
Iteration: 3 || Loss: 37.04126617113411
Iteration: 4 || Loss: 35.933065705817505
Iteration: 5 || Loss: 35.85720318721007
Iteration: 6 || Loss: 34.984428853496276
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:60.741302
Epoch 15 loss:34.984428853496276
waveform batch: 3/3
Using ADAM optimizer
Sum of params:60.741302
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 217.33441790555042
Iteration: 2 || Loss: 217.33383711740888
Iteration: 3 || Loss: 217.3332658704463
Iteration: 4 || Loss: 217.33268280563155
Iteration: 5 || Loss: 217.33212209142593
Iteration: 6 || Loss: 217.33212209142593
saving ADAM checkpoint...
Sum of params:60.741375
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 217.33212209142593
Iteration: 2 || Loss: 217.1360469989542
Iteration: 3 || Loss: 200.485628380325
Iteration: 4 || Loss: 197.2120528569643
Iteration: 5 || Loss: 195.41955116097893
Iteration: 6 || Loss: 189.3241271664197
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.182312
Epoch 15 loss:189.3241271664197
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:48.40699856438844
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:267.4380438650901
waveform batch: 2/2
Test loss - extrapolation:170.96406969691412
Epoch 15 mean train loss:8.184974205215557
Epoch 15 mean test loss - interpolation:8.06783309406474
Epoch 15 mean test loss - extrapolation:36.53350946350035
Start training epoch 16
waveform batch: 1/3
Using ADAM optimizer
Sum of params:62.182312
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.372721052196223
Iteration: 2 || Loss: 21.370323997728068
Iteration: 3 || Loss: 21.367936495006365
Iteration: 4 || Loss: 21.36554551999945
Iteration: 5 || Loss: 21.363154505291625
Iteration: 6 || Loss: 21.363154505291625
saving ADAM checkpoint...
Sum of params:62.182316
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.363154505291625
Iteration: 2 || Loss: 15.449061346501217
Iteration: 3 || Loss: 15.184087428543657
Iteration: 4 || Loss: 12.86718149097908
Iteration: 5 || Loss: 12.799389621705837
Iteration: 6 || Loss: 12.79212128430351
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:61.50464
Epoch 16 loss:12.79212128430351
waveform batch: 2/3
Using ADAM optimizer
Sum of params:61.50464
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.38455295693625
Iteration: 2 || Loss: 37.38377178020898
Iteration: 3 || Loss: 37.382992484120344
Iteration: 4 || Loss: 37.38221777020857
Iteration: 5 || Loss: 37.3814465255008
Iteration: 6 || Loss: 37.3814465255008
saving ADAM checkpoint...
Sum of params:61.504658
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.3814465255008
Iteration: 2 || Loss: 36.885059200339015
Iteration: 3 || Loss: 36.67224118456304
Iteration: 4 || Loss: 34.83818190934765
Iteration: 5 || Loss: 34.71915887457115
Iteration: 6 || Loss: 34.197077193721626
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:61.720135
Epoch 16 loss:34.197077193721626
waveform batch: 3/3
Using ADAM optimizer
Sum of params:61.720135
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 201.10482730425744
Iteration: 2 || Loss: 201.10429494124986
Iteration: 3 || Loss: 201.10376070465054
Iteration: 4 || Loss: 201.1032241949889
Iteration: 5 || Loss: 201.10267648414117
Iteration: 6 || Loss: 201.10267648414117
saving ADAM checkpoint...
Sum of params:61.72019
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 201.10267648414117
Iteration: 2 || Loss: 200.9430785981963
Iteration: 3 || Loss: 188.65360355152342
Iteration: 4 || Loss: 186.9957807192671
Iteration: 5 || Loss: 183.57894664191903
Iteration: 6 || Loss: 179.22037992018403
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.926952
Epoch 16 loss:179.22037992018403
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:45.54967319246871
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:254.01924963916767
waveform batch: 2/2
Test loss - extrapolation:162.7599271175903
Epoch 16 mean train loss:7.80033028959342
Epoch 16 mean test loss - interpolation:7.591612198744785
Epoch 16 mean test loss - extrapolation:34.73159806306317
Start training epoch 17
waveform batch: 1/3
Using ADAM optimizer
Sum of params:62.926952
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 16.94896934938744
Iteration: 2 || Loss: 16.947611478709874
Iteration: 3 || Loss: 16.9462637575217
Iteration: 4 || Loss: 16.94491293315145
Iteration: 5 || Loss: 16.943562525779967
Iteration: 6 || Loss: 16.943562525779967
saving ADAM checkpoint...
Sum of params:62.926964
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 16.943562525779967
Iteration: 2 || Loss: 14.99212747553744
Iteration: 3 || Loss: 14.885082942354847
Iteration: 4 || Loss: 12.543212335594232
Iteration: 5 || Loss: 12.486355960294148
Iteration: 6 || Loss: 12.463752422754538
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:61.853775
Epoch 17 loss:12.463752422754538
waveform batch: 2/3
Using ADAM optimizer
Sum of params:61.853775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.27965348982585
Iteration: 2 || Loss: 37.27869309616981
Iteration: 3 || Loss: 37.27772801702014
Iteration: 4 || Loss: 37.276766365027086
Iteration: 5 || Loss: 37.27580053320736
Iteration: 6 || Loss: 37.27580053320736
saving ADAM checkpoint...
Sum of params:61.853786
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.27580053320736
Iteration: 2 || Loss: 36.43277187336723
Iteration: 3 || Loss: 36.26032553797105
Iteration: 4 || Loss: 33.891785245101815
Iteration: 5 || Loss: 33.86552542088888
Iteration: 6 || Loss: 32.45020791072645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.213356
Epoch 17 loss:32.45020791072645
waveform batch: 3/3
Using ADAM optimizer
Sum of params:62.213356
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 194.7697497407614
Iteration: 2 || Loss: 194.76885341268732
Iteration: 3 || Loss: 194.7679675083773
Iteration: 4 || Loss: 194.76707830821184
Iteration: 5 || Loss: 194.7661685023072
Iteration: 6 || Loss: 194.7661685023072
saving ADAM checkpoint...
Sum of params:62.21334
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 194.7661685023072
Iteration: 2 || Loss: 194.17604042075845
Iteration: 3 || Loss: 183.6577390899539
Iteration: 4 || Loss: 182.37889889190015
Iteration: 5 || Loss: 180.8253386362967
Iteration: 6 || Loss: 178.77134948832446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.8689
Epoch 17 loss:178.77134948832446
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.97179884445776
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:257.33434662451435
waveform batch: 2/2
Test loss - extrapolation:177.45774368739797
Epoch 17 mean train loss:7.713286545579498
Epoch 17 mean test loss - interpolation:5.995299807409626
Epoch 17 mean test loss - extrapolation:36.23267419265936
Start training epoch 18
waveform batch: 1/3
Using ADAM optimizer
Sum of params:62.8689
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.982710101507356
Iteration: 2 || Loss: 11.982164380318117
Iteration: 3 || Loss: 11.981617851909084
Iteration: 4 || Loss: 11.98107277695655
Iteration: 5 || Loss: 11.980527676661403
Iteration: 6 || Loss: 11.980527676661403
saving ADAM checkpoint...
Sum of params:62.8689
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.980527676661403
Iteration: 2 || Loss: 11.732541073166532
Iteration: 3 || Loss: 11.689301123600778
Iteration: 4 || Loss: 11.316070469338937
Iteration: 5 || Loss: 11.251826306291983
Iteration: 6 || Loss: 11.230116603020475
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.50792
Epoch 18 loss:11.230116603020475
waveform batch: 2/3
Using ADAM optimizer
Sum of params:62.50792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.355563885863454
Iteration: 2 || Loss: 32.354859228218636
Iteration: 3 || Loss: 32.35414891369984
Iteration: 4 || Loss: 32.353444485608456
Iteration: 5 || Loss: 32.35273642190538
Iteration: 6 || Loss: 32.35273642190538
saving ADAM checkpoint...
Sum of params:62.507954
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.35273642190538
Iteration: 2 || Loss: 31.89459463315316
Iteration: 3 || Loss: 31.829138312147613
Iteration: 4 || Loss: 31.501266525212856
Iteration: 5 || Loss: 31.129779100277617
Iteration: 6 || Loss: 30.604569288893355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.679173
Epoch 18 loss:30.604569288893355
waveform batch: 3/3
Using ADAM optimizer
Sum of params:62.679173
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 189.0780543196279
Iteration: 2 || Loss: 189.07705766153944
Iteration: 3 || Loss: 189.07605677267324
Iteration: 4 || Loss: 189.0750661546798
Iteration: 5 || Loss: 189.0740665178975
Iteration: 6 || Loss: 189.0740665178975
saving ADAM checkpoint...
Sum of params:62.67917
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 189.0740665178975
Iteration: 2 || Loss: 188.1838676764645
Iteration: 3 || Loss: 181.7756902802071
Iteration: 4 || Loss: 180.41208700294467
Iteration: 5 || Loss: 168.32057816540078
Iteration: 6 || Loss: 167.8631641310218
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:63.54572
Epoch 18 loss:167.8631641310218
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:40.01939662543212
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:241.14423571873832
waveform batch: 2/2
Test loss - extrapolation:157.07421033249477
Epoch 18 mean train loss:7.2309603456184695
Epoch 18 mean test loss - interpolation:6.66989943757202
Epoch 18 mean test loss - extrapolation:33.18487050426942
Start training epoch 19
waveform batch: 1/3
Using ADAM optimizer
Sum of params:63.54572
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.623472785052503
Iteration: 2 || Loss: 13.622488749154472
Iteration: 3 || Loss: 13.621509634385795
Iteration: 4 || Loss: 13.620526076827662
Iteration: 5 || Loss: 13.61954380025703
Iteration: 6 || Loss: 13.61954380025703
saving ADAM checkpoint...
Sum of params:63.545723
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.61954380025703
Iteration: 2 || Loss: 12.63741236439985
Iteration: 3 || Loss: 12.550448841457834
Iteration: 4 || Loss: 11.791078680830342
Iteration: 5 || Loss: 11.429038089835375
Iteration: 6 || Loss: 11.370026801018192
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.375355
Epoch 19 loss:11.370026801018192
waveform batch: 2/3
Using ADAM optimizer
Sum of params:62.375355
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.97323761515789
Iteration: 2 || Loss: 34.97243571668507
Iteration: 3 || Loss: 34.97163258343769
Iteration: 4 || Loss: 34.970829296418934
Iteration: 5 || Loss: 34.9700250445575
Iteration: 6 || Loss: 34.9700250445575
saving ADAM checkpoint...
Sum of params:62.375393
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.9700250445575
Iteration: 2 || Loss: 34.36147413764194
Iteration: 3 || Loss: 34.06519411664515
Iteration: 4 || Loss: 31.80634891155178
Iteration: 5 || Loss: 31.619130403185416
Iteration: 6 || Loss: 30.53620433205185
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:62.88645
Epoch 19 loss:30.53620433205185
waveform batch: 3/3
Using ADAM optimizer
Sum of params:62.88645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 173.65633277345364
Iteration: 2 || Loss: 173.65582857206272
Iteration: 3 || Loss: 173.65531532862008
Iteration: 4 || Loss: 173.6547946647008
Iteration: 5 || Loss: 173.65427715286458
Iteration: 6 || Loss: 173.65427715286458
saving ADAM checkpoint...
Sum of params:62.886387
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 173.65427715286458
Iteration: 2 || Loss: 173.50112486564
Iteration: 3 || Loss: 167.18587354470986
Iteration: 4 || Loss: 166.17496427477005
Iteration: 5 || Loss: 161.39724712291186
Iteration: 6 || Loss: 159.3677081179332
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:63.87464
Epoch 19 loss:159.3677081179332
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:38.77416711232839
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:232.52219066045672
waveform batch: 2/2
Test loss - extrapolation:149.77790705576373
Epoch 19 mean train loss:6.940480663827698
Epoch 19 mean test loss - interpolation:6.462361185388065
Epoch 19 mean test loss - extrapolation:31.8583414763517
Start training epoch 20
waveform batch: 1/3
Using ADAM optimizer
Sum of params:63.87464
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.80788029501153
Iteration: 2 || Loss: 13.806632406637059
Iteration: 3 || Loss: 13.805386551776536
Iteration: 4 || Loss: 13.804138620976476
Iteration: 5 || Loss: 13.802892822567564
Iteration: 6 || Loss: 13.802892822567564
saving ADAM checkpoint...
Sum of params:63.87464
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.802892822567564
Iteration: 2 || Loss: 12.236703384275744
Iteration: 3 || Loss: 12.156532310962419
Iteration: 4 || Loss: 11.173778794132934
Iteration: 5 || Loss: 11.128826195274678
Iteration: 6 || Loss: 11.081222268856338
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:63.077477
Epoch 20 loss:11.081222268856338
waveform batch: 2/3
Using ADAM optimizer
Sum of params:63.077477
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.40547144773445
Iteration: 2 || Loss: 33.40462601747982
Iteration: 3 || Loss: 33.403780728321614
Iteration: 4 || Loss: 33.40293678970961
Iteration: 5 || Loss: 33.40208344533118
Iteration: 6 || Loss: 33.40208344533118
saving ADAM checkpoint...
Sum of params:63.07751
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.40208344533118
Iteration: 2 || Loss: 32.734995027305644
Iteration: 3 || Loss: 32.507852946399446
Iteration: 4 || Loss: 31.31312282313039
Iteration: 5 || Loss: 31.120880472490725
Iteration: 6 || Loss: 30.029817863482084
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:63.613804
Epoch 20 loss:30.029817863482084
waveform batch: 3/3
Using ADAM optimizer
Sum of params:63.613804
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 164.24024898302815
Iteration: 2 || Loss: 164.2398421884299
Iteration: 3 || Loss: 164.2394284221272
Iteration: 4 || Loss: 164.2390224821484
Iteration: 5 || Loss: 164.23860427230997
Iteration: 6 || Loss: 164.23860427230997
saving ADAM checkpoint...
Sum of params:63.6138
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 164.23860427230997
Iteration: 2 || Loss: 164.1457705293916
Iteration: 3 || Loss: 159.08428915268095
Iteration: 4 || Loss: 157.17486630760598
Iteration: 5 || Loss: 152.630809471115
Iteration: 6 || Loss: 151.52578430965787
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:64.37273
Epoch 20 loss:151.52578430965787
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.20807514169466
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:224.1326347065078
waveform batch: 2/2
Test loss - extrapolation:145.33155504058382
Epoch 20 mean train loss:6.642649118689527
Epoch 20 mean test loss - interpolation:5.868012523615776
Epoch 20 mean test loss - extrapolation:30.7886824789243
Start training epoch 21
waveform batch: 1/3
Using ADAM optimizer
Sum of params:64.37273
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.372695661858746
Iteration: 2 || Loss: 11.372431958582036
Iteration: 3 || Loss: 11.372165860361816
Iteration: 4 || Loss: 11.37190249876749
Iteration: 5 || Loss: 11.37163721486876
Iteration: 6 || Loss: 11.37163721486876
saving ADAM checkpoint...
Sum of params:64.372734
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.37163721486876
Iteration: 2 || Loss: 11.303526610168568
Iteration: 3 || Loss: 11.131311106612545
Iteration: 4 || Loss: 10.78240498699562
Iteration: 5 || Loss: 10.701196164072698
Iteration: 6 || Loss: 10.027030881733827
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:64.336296
Epoch 21 loss:10.027030881733827
waveform batch: 2/3
Using ADAM optimizer
Sum of params:64.336296
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.0062057149758
Iteration: 2 || Loss: 33.0048313051816
Iteration: 3 || Loss: 33.003459188007746
Iteration: 4 || Loss: 33.00208646112061
Iteration: 5 || Loss: 33.00071297093965
Iteration: 6 || Loss: 33.00071297093965
saving ADAM checkpoint...
Sum of params:64.33625
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.00071297093965
Iteration: 2 || Loss: 30.981056444340037
Iteration: 3 || Loss: 30.771955371791087
Iteration: 4 || Loss: 29.92171238034086
Iteration: 5 || Loss: 29.802379430779258
Iteration: 6 || Loss: 28.978436010624783
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:64.39295
Epoch 21 loss:28.978436010624783
waveform batch: 3/3
Using ADAM optimizer
Sum of params:64.39295
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 155.81161640265552
Iteration: 2 || Loss: 155.81098472650064
Iteration: 3 || Loss: 155.81035444568082
Iteration: 4 || Loss: 155.80971568506496
Iteration: 5 || Loss: 155.8090866786137
Iteration: 6 || Loss: 155.8090866786137
saving ADAM checkpoint...
Sum of params:64.392944
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 155.8090866786137
Iteration: 2 || Loss: 155.49471020455763
Iteration: 3 || Loss: 151.8106052163034
Iteration: 4 || Loss: 151.35988516025162
Iteration: 5 || Loss: 147.13167553252933
Iteration: 6 || Loss: 144.75350893697393
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:65.352646
Epoch 21 loss:144.75350893697393
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.56985067910053
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:213.603761595818
waveform batch: 2/2
Test loss - extrapolation:136.4478850863663
Epoch 21 mean train loss:6.336516407908019
Epoch 21 mean test loss - interpolation:5.9283084465167555
Epoch 21 mean test loss - extrapolation:29.170970556848697
Start training epoch 22
waveform batch: 1/3
Using ADAM optimizer
Sum of params:65.352646
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.067856441986114
Iteration: 2 || Loss: 11.067405180675804
Iteration: 3 || Loss: 11.066955509314091
Iteration: 4 || Loss: 11.066504870465977
Iteration: 5 || Loss: 11.066055299815343
Iteration: 6 || Loss: 11.066055299815343
saving ADAM checkpoint...
Sum of params:65.352646
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.066055299815343
Iteration: 2 || Loss: 10.86225644025704
Iteration: 3 || Loss: 10.744093411840247
Iteration: 4 || Loss: 10.303475256118546
Iteration: 5 || Loss: 10.247892021501551
Iteration: 6 || Loss: 10.14366180952445
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:64.70169
Epoch 22 loss:10.14366180952445
waveform batch: 2/3
Using ADAM optimizer
Sum of params:64.70169
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.905058256278146
Iteration: 2 || Loss: 31.90423500250347
Iteration: 3 || Loss: 31.90341005464073
Iteration: 4 || Loss: 31.902594124341732
Iteration: 5 || Loss: 31.90177548529355
Iteration: 6 || Loss: 31.90177548529355
saving ADAM checkpoint...
Sum of params:64.70173
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.90177548529355
Iteration: 2 || Loss: 31.255748024328355
Iteration: 3 || Loss: 31.03268727310116
Iteration: 4 || Loss: 29.925477098929363
Iteration: 5 || Loss: 29.745197204778275
Iteration: 6 || Loss: 28.779030422481945
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:65.35603
Epoch 22 loss:28.779030422481945
waveform batch: 3/3
Using ADAM optimizer
Sum of params:65.35603
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 144.17806932647017
Iteration: 2 || Loss: 144.1775459738932
Iteration: 3 || Loss: 144.17702737654554
Iteration: 4 || Loss: 144.1765091181691
Iteration: 5 || Loss: 144.1759863774327
Iteration: 6 || Loss: 144.1759863774327
saving ADAM checkpoint...
Sum of params:65.3561
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 144.1759863774327
Iteration: 2 || Loss: 143.92421967274737
Iteration: 3 || Loss: 142.89758213094757
Iteration: 4 || Loss: 139.92057087132918
Iteration: 5 || Loss: 138.38321814270034
Iteration: 6 || Loss: 137.609784469857
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:66.01092
Epoch 22 loss:137.609784469857
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:32.473649705183604
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:206.65588791886387
waveform batch: 2/2
Test loss - extrapolation:133.04024337117255
Epoch 22 mean train loss:6.087326782822876
Epoch 22 mean test loss - interpolation:5.412274950863934
Epoch 22 mean test loss - extrapolation:28.30801094083637
Start training epoch 23
waveform batch: 1/3
Using ADAM optimizer
Sum of params:66.01092
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.25045796661908
Iteration: 2 || Loss: 10.2503231584403
Iteration: 3 || Loss: 10.25018812510208
Iteration: 4 || Loss: 10.250053966780508
Iteration: 5 || Loss: 10.249920665071368
Iteration: 6 || Loss: 10.249920665071368
saving ADAM checkpoint...
Sum of params:66.01087
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.249920665071368
Iteration: 2 || Loss: 10.235765122861736
Iteration: 3 || Loss: 10.05264815750736
Iteration: 4 || Loss: 9.910822347629674
Iteration: 5 || Loss: 9.509741495405729
Iteration: 6 || Loss: 9.058924199992733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:66.31591
Epoch 23 loss:9.058924199992733
waveform batch: 2/3
Using ADAM optimizer
Sum of params:66.31591
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.201975080453344
Iteration: 2 || Loss: 31.20099540369673
Iteration: 3 || Loss: 31.20001613505519
Iteration: 4 || Loss: 31.199037213193648
Iteration: 5 || Loss: 31.198064197828327
Iteration: 6 || Loss: 31.198064197828327
saving ADAM checkpoint...
Sum of params:66.31577
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.198064197828327
Iteration: 2 || Loss: 30.144629003275977
Iteration: 3 || Loss: 29.101007583059328
Iteration: 4 || Loss: 28.554943794622254
Iteration: 5 || Loss: 28.42539536253788
Iteration: 6 || Loss: 27.30478121851818
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:66.01524
Epoch 23 loss:27.30478121851818
waveform batch: 3/3
Using ADAM optimizer
Sum of params:66.01524
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 140.11134929457796
Iteration: 2 || Loss: 140.11083683993454
Iteration: 3 || Loss: 140.11033248797682
Iteration: 4 || Loss: 140.10982666828056
Iteration: 5 || Loss: 140.10929993490458
Iteration: 6 || Loss: 140.10929993490458
saving ADAM checkpoint...
Sum of params:66.0152
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 140.10929993490458
Iteration: 2 || Loss: 139.8902771328608
Iteration: 3 || Loss: 138.4795372709969
Iteration: 4 || Loss: 136.93924857287945
Iteration: 5 || Loss: 132.94564823330504
Iteration: 6 || Loss: 130.86006334863967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:66.69974
Epoch 23 loss:130.86006334863967
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:31.299437013180157
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:198.84236480712335
waveform batch: 2/2
Test loss - extrapolation:124.93278569620405
Epoch 23 mean train loss:5.766336854039675
Epoch 23 mean test loss - interpolation:5.216572835530026
Epoch 23 mean test loss - extrapolation:26.98126254194395
Start training epoch 24
waveform batch: 1/3
Using ADAM optimizer
Sum of params:66.69974
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.338430352996411
Iteration: 2 || Loss: 10.337528283346902
Iteration: 3 || Loss: 10.336631745967543
Iteration: 4 || Loss: 10.335731182559723
Iteration: 5 || Loss: 10.334832020918988
Iteration: 6 || Loss: 10.334832020918988
saving ADAM checkpoint...
Sum of params:66.69981
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.334832020918988
Iteration: 2 || Loss: 9.527839766571347
Iteration: 3 || Loss: 9.478167238033542
Iteration: 4 || Loss: 9.309877398618474
Iteration: 5 || Loss: 9.295794358437139
Iteration: 6 || Loss: 9.22299669806035
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:67.01672
Epoch 24 loss:9.22299669806035
waveform batch: 2/3
Using ADAM optimizer
Sum of params:67.01672
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.75595355852935
Iteration: 2 || Loss: 30.754994081756497
Iteration: 3 || Loss: 30.754035061211162
Iteration: 4 || Loss: 30.75307965705691
Iteration: 5 || Loss: 30.752124627710092
Iteration: 6 || Loss: 30.752124627710092
saving ADAM checkpoint...
Sum of params:67.01666
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.752124627710092
Iteration: 2 || Loss: 29.826774192381844
Iteration: 3 || Loss: 29.245934244997972
Iteration: 4 || Loss: 28.831352202333967
Iteration: 5 || Loss: 28.573941157973977
Iteration: 6 || Loss: 27.183646889736114
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:66.91145
Epoch 24 loss:27.183646889736114
waveform batch: 3/3
Using ADAM optimizer
Sum of params:66.91145
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 127.92527517153833
Iteration: 2 || Loss: 127.92494055780811
Iteration: 3 || Loss: 127.92460543720668
Iteration: 4 || Loss: 127.92427053330822
Iteration: 5 || Loss: 127.92394107363776
Iteration: 6 || Loss: 127.92394107363776
saving ADAM checkpoint...
Sum of params:66.91138
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 127.92394107363776
Iteration: 2 || Loss: 127.8493307925211
Iteration: 3 || Loss: 126.83641233940547
Iteration: 4 || Loss: 125.33843352978887
Iteration: 5 || Loss: 115.6248780717918
Iteration: 6 || Loss: 107.69212095581882
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:70.39194
Epoch 24 loss:107.69212095581882
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:35.136718238470834
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:151.24794207222288
waveform batch: 2/2
Test loss - extrapolation:83.91466311753504
Epoch 24 mean train loss:4.968922915297079
Epoch 24 mean test loss - interpolation:5.856119706411806
Epoch 24 mean test loss - extrapolation:19.59688376581316
Start training epoch 25
waveform batch: 1/3
Using ADAM optimizer
Sum of params:70.39194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.216410117292657
Iteration: 2 || Loss: 12.216306416608397
Iteration: 3 || Loss: 12.216204451050281
Iteration: 4 || Loss: 12.216102497624519
Iteration: 5 || Loss: 12.216001192432646
Iteration: 6 || Loss: 12.216001192432646
saving ADAM checkpoint...
Sum of params:70.391846
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.216001192432646
Iteration: 2 || Loss: 12.205701700658407
Iteration: 3 || Loss: 11.833927237306822
Iteration: 4 || Loss: 11.47726051606306
Iteration: 5 || Loss: 10.948113798310615
Iteration: 6 || Loss: 9.878302432147102
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:70.398926
Epoch 25 loss:9.878302432147102
waveform batch: 2/3
Using ADAM optimizer
Sum of params:70.398926
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.66495191634686
Iteration: 2 || Loss: 27.664651691897493
Iteration: 3 || Loss: 27.664347232023253
Iteration: 4 || Loss: 27.664046878017505
Iteration: 5 || Loss: 27.663745350355597
Iteration: 6 || Loss: 27.663745350355597
saving ADAM checkpoint...
Sum of params:70.39878
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.663745350355597
Iteration: 2 || Loss: 27.597701748371257
Iteration: 3 || Loss: 27.11557834662134
Iteration: 4 || Loss: 26.87911720887439
Iteration: 5 || Loss: 26.407952874443495
Iteration: 6 || Loss: 25.736393581220888
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:70.25542
Epoch 25 loss:25.736393581220888
waveform batch: 3/3
Using ADAM optimizer
Sum of params:70.25542
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 98.42325434135402
Iteration: 2 || Loss: 98.42260580316605
Iteration: 3 || Loss: 98.42194710583394
Iteration: 4 || Loss: 98.4212928773936
Iteration: 5 || Loss: 98.42064257324334
Iteration: 6 || Loss: 98.42064257324334
saving ADAM checkpoint...
Sum of params:70.255295
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 98.42064257324334
Iteration: 2 || Loss: 98.10222132855851
Iteration: 3 || Loss: 97.95401199497968
Iteration: 4 || Loss: 96.84013461553707
Iteration: 5 || Loss: 96.47520519065948
Iteration: 6 || Loss: 94.69283384029299
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:71.414536
Epoch 25 loss:94.69283384029299
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:26.692799340853934
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:144.8948659829597
waveform batch: 2/2
Test loss - extrapolation:83.9780763448499
Epoch 25 mean train loss:4.493363098402103
Epoch 25 mean test loss - interpolation:4.448799890142323
Epoch 25 mean test loss - extrapolation:19.072745193984133
Start training epoch 26
waveform batch: 1/3
Using ADAM optimizer
Sum of params:71.414536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.512828814056059
Iteration: 2 || Loss: 9.512673810268947
Iteration: 3 || Loss: 9.512519088400989
Iteration: 4 || Loss: 9.512366021856757
Iteration: 5 || Loss: 9.512212126247313
Iteration: 6 || Loss: 9.512212126247313
saving ADAM checkpoint...
Sum of params:71.41451
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.512212126247313
Iteration: 2 || Loss: 9.491467795456279
Iteration: 3 || Loss: 9.445973544673828
Iteration: 4 || Loss: 9.242125991082725
Iteration: 5 || Loss: 8.640806882521192
Iteration: 6 || Loss: 8.226321774381638
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:72.10564
Epoch 26 loss:8.226321774381638
waveform batch: 2/3
Using ADAM optimizer
Sum of params:72.10564
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.02322827228361
Iteration: 2 || Loss: 29.02232419262775
Iteration: 3 || Loss: 29.021422407335127
Iteration: 4 || Loss: 29.02051667698566
Iteration: 5 || Loss: 29.019606992256463
Iteration: 6 || Loss: 29.019606992256463
saving ADAM checkpoint...
Sum of params:72.105606
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.019606992256463
Iteration: 2 || Loss: 28.30716505761311
Iteration: 3 || Loss: 25.935368415290828
Iteration: 4 || Loss: 25.78192055137416
Iteration: 5 || Loss: 23.994245670036545
Iteration: 6 || Loss: 23.824461218711193
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:71.8306
Epoch 26 loss:23.824461218711193
waveform batch: 3/3
Using ADAM optimizer
Sum of params:71.8306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 94.40316678106645
Iteration: 2 || Loss: 94.40305598577864
Iteration: 3 || Loss: 94.40294209124332
Iteration: 4 || Loss: 94.40283165733483
Iteration: 5 || Loss: 94.40272551195503
Iteration: 6 || Loss: 94.40272551195503
saving ADAM checkpoint...
Sum of params:71.83055
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 94.40272551195503
Iteration: 2 || Loss: 94.37083886154784
Iteration: 3 || Loss: 94.31253159601033
Iteration: 4 || Loss: 93.17273617726796
Iteration: 5 || Loss: 92.17375460949978
Iteration: 6 || Loss: 88.4505786386339
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:72.66968
Epoch 26 loss:88.4505786386339
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:24.351787324855692
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:137.93850025465056
waveform batch: 2/2
Test loss - extrapolation:81.99981622704803
Epoch 26 mean train loss:4.1552193666112665
Epoch 26 mean test loss - interpolation:4.058631220809282
Epoch 26 mean test loss - extrapolation:18.32819304014155
Start training epoch 27
waveform batch: 1/3
Using ADAM optimizer
Sum of params:72.66968
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.220201658912666
Iteration: 2 || Loss: 9.21982490703655
Iteration: 3 || Loss: 9.219446531930542
Iteration: 4 || Loss: 9.219068445073484
Iteration: 5 || Loss: 9.218691347587875
Iteration: 6 || Loss: 9.218691347587875
saving ADAM checkpoint...
Sum of params:72.66966
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.218691347587875
Iteration: 2 || Loss: 9.118922560294084
Iteration: 3 || Loss: 8.831792743207352
Iteration: 4 || Loss: 8.54971683356911
Iteration: 5 || Loss: 8.182206928059776
Iteration: 6 || Loss: 8.167693550911755
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:72.83033
Epoch 27 loss:8.167693550911755
waveform batch: 2/3
Using ADAM optimizer
Sum of params:72.83033
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.844563003154917
Iteration: 2 || Loss: 23.844310680555168
Iteration: 3 || Loss: 23.844061071275895
Iteration: 4 || Loss: 23.843809313533995
Iteration: 5 || Loss: 23.843558799571728
Iteration: 6 || Loss: 23.843558799571728
saving ADAM checkpoint...
Sum of params:72.830376
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.843558799571728
Iteration: 2 || Loss: 23.79723306514356
Iteration: 3 || Loss: 23.235334313179145
Iteration: 4 || Loss: 23.075824552318323
Iteration: 5 || Loss: 22.457109742016222
Iteration: 6 || Loss: 22.207561202933835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:72.43688
Epoch 27 loss:22.207561202933835
waveform batch: 3/3
Using ADAM optimizer
Sum of params:72.43688
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 88.78274916530539
Iteration: 2 || Loss: 88.78261569582985
Iteration: 3 || Loss: 88.78248107001205
Iteration: 4 || Loss: 88.78234762645344
Iteration: 5 || Loss: 88.78221677916451
Iteration: 6 || Loss: 88.78221677916451
saving ADAM checkpoint...
Sum of params:72.43675
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 88.78221677916451
Iteration: 2 || Loss: 88.7707897380541
Iteration: 3 || Loss: 88.23642422200324
Iteration: 4 || Loss: 86.78357869988973
Iteration: 5 || Loss: 86.40435846513832
Iteration: 6 || Loss: 85.10619772577837
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:73.26966
Epoch 27 loss:85.10619772577837
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:22.902915005396615
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:135.0994488480046
waveform batch: 2/2
Test loss - extrapolation:80.3702662782198
Epoch 27 mean train loss:3.982119051021516
Epoch 27 mean test loss - interpolation:3.817152500899436
Epoch 27 mean test loss - extrapolation:17.955809593852035
Start training epoch 28
waveform batch: 1/3
Using ADAM optimizer
Sum of params:73.26966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.764729401812666
Iteration: 2 || Loss: 8.764610705371503
Iteration: 3 || Loss: 8.764491521484713
Iteration: 4 || Loss: 8.764371602211597
Iteration: 5 || Loss: 8.764253631226637
Iteration: 6 || Loss: 8.764253631226637
saving ADAM checkpoint...
Sum of params:73.269646
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.764253631226637
Iteration: 2 || Loss: 8.733218700721988
Iteration: 3 || Loss: 8.41288436307829
Iteration: 4 || Loss: 8.147886537440387
Iteration: 5 || Loss: 7.970422487331748
Iteration: 6 || Loss: 7.696170029808937
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:73.591965
Epoch 28 loss:7.696170029808937
waveform batch: 2/3
Using ADAM optimizer
Sum of params:73.591965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.617614664614592
Iteration: 2 || Loss: 22.61734218096607
Iteration: 3 || Loss: 22.6170711639957
Iteration: 4 || Loss: 22.616798800257726
Iteration: 5 || Loss: 22.616529461432382
Iteration: 6 || Loss: 22.616529461432382
saving ADAM checkpoint...
Sum of params:73.59185
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.616529461432382
Iteration: 2 || Loss: 22.565691192831512
Iteration: 3 || Loss: 22.344233645376164
Iteration: 4 || Loss: 22.179077590009232
Iteration: 5 || Loss: 21.661538805022918
Iteration: 6 || Loss: 21.606967510980567
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:73.43705
Epoch 28 loss:21.606967510980567
waveform batch: 3/3
Using ADAM optimizer
Sum of params:73.43705
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 84.5476187251509
Iteration: 2 || Loss: 84.54755044883055
Iteration: 3 || Loss: 84.54748331871947
Iteration: 4 || Loss: 84.54741178030824
Iteration: 5 || Loss: 84.54733909575612
Iteration: 6 || Loss: 84.54733909575612
saving ADAM checkpoint...
Sum of params:73.437004
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 84.54733909575612
Iteration: 2 || Loss: 84.5389153281149
Iteration: 3 || Loss: 84.51333122883771
Iteration: 4 || Loss: 83.64209890215194
Iteration: 5 || Loss: 80.25859448809221
Iteration: 6 || Loss: 75.76042076938496
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:74.99055
Epoch 28 loss:75.76042076938496
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:22.29378551940863
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:117.71733666525105
waveform batch: 2/2
Test loss - extrapolation:66.0078296477776
Epoch 28 mean train loss:3.622881321040499
Epoch 28 mean test loss - interpolation:3.715630919901438
Epoch 28 mean test loss - extrapolation:15.31043052608572
Start training epoch 29
waveform batch: 1/3
Using ADAM optimizer
Sum of params:74.99055
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.830076028374354
Iteration: 2 || Loss: 9.829920265631644
Iteration: 3 || Loss: 9.82976483331666
Iteration: 4 || Loss: 9.829610529018655
Iteration: 5 || Loss: 9.829457027202716
Iteration: 6 || Loss: 9.829457027202716
saving ADAM checkpoint...
Sum of params:74.99056
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.829457027202716
Iteration: 2 || Loss: 9.81751451975163
Iteration: 3 || Loss: 8.941502440621637
Iteration: 4 || Loss: 8.823252172415252
Iteration: 5 || Loss: 8.698370358087688
Iteration: 6 || Loss: 7.291197227900666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.008675
Epoch 29 loss:7.291197227900666
waveform batch: 2/3
Using ADAM optimizer
Sum of params:76.008675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.286657314453567
Iteration: 2 || Loss: 23.285784455648496
Iteration: 3 || Loss: 23.284917036632166
Iteration: 4 || Loss: 23.284044615494462
Iteration: 5 || Loss: 23.283177846041298
Iteration: 6 || Loss: 23.283177846041298
saving ADAM checkpoint...
Sum of params:76.00866
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.283177846041298
Iteration: 2 || Loss: 22.519604815707797
Iteration: 3 || Loss: 22.215522502970995
Iteration: 4 || Loss: 21.505475320837004
Iteration: 5 || Loss: 20.16652533713094
Iteration: 6 || Loss: 20.04487491444592
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:75.98327
Epoch 29 loss:20.04487491444592
waveform batch: 3/3
Using ADAM optimizer
Sum of params:75.98327
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 74.28156419741562
Iteration: 2 || Loss: 74.28128919201639
Iteration: 3 || Loss: 74.28101413256184
Iteration: 4 || Loss: 74.28074569807032
Iteration: 5 || Loss: 74.28047118018151
Iteration: 6 || Loss: 74.28047118018151
saving ADAM checkpoint...
Sum of params:75.98314
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 74.28047118018151
Iteration: 2 || Loss: 74.21712060887492
Iteration: 3 || Loss: 73.5858736668803
Iteration: 4 || Loss: 73.1989314137013
Iteration: 5 || Loss: 72.51288464361058
Iteration: 6 || Loss: 72.37374705108624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.292274
Epoch 29 loss:72.37374705108624
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:20.356204030201827
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:115.3200593362154
waveform batch: 2/2
Test loss - extrapolation:67.6010344118807
Epoch 29 mean train loss:3.4382696273597526
Epoch 29 mean test loss - interpolation:3.3927006717003043
Epoch 29 mean test loss - extrapolation:15.243424479008008
Start training epoch 30
waveform batch: 1/3
Using ADAM optimizer
Sum of params:76.292274
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.306924747073236
Iteration: 2 || Loss: 8.306104754021101
Iteration: 3 || Loss: 8.30528227247831
Iteration: 4 || Loss: 8.304463565632624
Iteration: 5 || Loss: 8.303641404469923
Iteration: 6 || Loss: 8.303641404469923
saving ADAM checkpoint...
Sum of params:76.292274
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.303641404469923
Iteration: 2 || Loss: 7.852889609977951
Iteration: 3 || Loss: 7.375167299772999
Iteration: 4 || Loss: 7.3435279554592965
Iteration: 5 || Loss: 7.239652908066059
Iteration: 6 || Loss: 6.99513516687956
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.50116
Epoch 30 loss:6.99513516687956
waveform batch: 2/3
Using ADAM optimizer
Sum of params:76.50116
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.317235359263602
Iteration: 2 || Loss: 20.31692951279529
Iteration: 3 || Loss: 20.316621092876957
Iteration: 4 || Loss: 20.31631474551612
Iteration: 5 || Loss: 20.316009050030974
Iteration: 6 || Loss: 20.316009050030974
saving ADAM checkpoint...
Sum of params:76.50127
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.316009050030974
Iteration: 2 || Loss: 20.26756128946427
Iteration: 3 || Loss: 20.050556602285294
Iteration: 4 || Loss: 19.96804630997107
Iteration: 5 || Loss: 19.616761242135535
Iteration: 6 || Loss: 19.54295470539561
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.22045
Epoch 30 loss:19.54295470539561
waveform batch: 3/3
Using ADAM optimizer
Sum of params:76.22045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.67797274001506
Iteration: 2 || Loss: 72.67775804653546
Iteration: 3 || Loss: 72.67754025329052
Iteration: 4 || Loss: 72.67732341136366
Iteration: 5 || Loss: 72.67710492445487
Iteration: 6 || Loss: 72.67710492445487
saving ADAM checkpoint...
Sum of params:76.220276
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.67710492445487
Iteration: 2 || Loss: 72.64669859555708
Iteration: 3 || Loss: 72.43880390711912
Iteration: 4 || Loss: 71.9878296755067
Iteration: 5 || Loss: 71.5846001553273
Iteration: 6 || Loss: 71.2715894041505
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.605515
Epoch 30 loss:71.2715894041505
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:19.959863853402695
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:113.66084463974364
waveform batch: 2/2
Test loss - extrapolation:66.6509862068843
Epoch 30 mean train loss:3.3727475612560576
Epoch 30 mean test loss - interpolation:3.3266439755671158
Epoch 30 mean test loss - extrapolation:15.025985903885662
Start training epoch 31
waveform batch: 1/3
Using ADAM optimizer
Sum of params:76.605515
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.5418422139704315
Iteration: 2 || Loss: 7.541349393099422
Iteration: 3 || Loss: 7.540856985705573
Iteration: 4 || Loss: 7.540365173272736
Iteration: 5 || Loss: 7.539874532693289
Iteration: 6 || Loss: 7.539874532693289
saving ADAM checkpoint...
Sum of params:76.60551
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.539874532693289
Iteration: 2 || Loss: 7.386810445671072
Iteration: 3 || Loss: 7.111093725339068
Iteration: 4 || Loss: 7.089461381902231
Iteration: 5 || Loss: 6.968940593283034
Iteration: 6 || Loss: 6.865961417171678
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.79692
Epoch 31 loss:6.865961417171678
waveform batch: 2/3
Using ADAM optimizer
Sum of params:76.79692
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.699849879833877
Iteration: 2 || Loss: 19.69974640262327
Iteration: 3 || Loss: 19.699642149163974
Iteration: 4 || Loss: 19.69953746185499
Iteration: 5 || Loss: 19.699434697356434
Iteration: 6 || Loss: 19.699434697356434
saving ADAM checkpoint...
Sum of params:76.79698
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.699434697356434
Iteration: 2 || Loss: 19.695323251738422
Iteration: 3 || Loss: 19.66318442725829
Iteration: 4 || Loss: 19.351064115380172
Iteration: 5 || Loss: 19.165116252632274
Iteration: 6 || Loss: 18.915696752144953
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.45976
Epoch 31 loss:18.915696752144953
waveform batch: 3/3
Using ADAM optimizer
Sum of params:76.45976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 72.78461279008648
Iteration: 2 || Loss: 72.7843698402014
Iteration: 3 || Loss: 72.78412604271826
Iteration: 4 || Loss: 72.78388287434797
Iteration: 5 || Loss: 72.78364162825966
Iteration: 6 || Loss: 72.78364162825966
saving ADAM checkpoint...
Sum of params:76.45963
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 72.78364162825966
Iteration: 2 || Loss: 72.74829608319158
Iteration: 3 || Loss: 72.41476022679377
Iteration: 4 || Loss: 71.19240961881717
Iteration: 5 || Loss: 70.9597562913013
Iteration: 6 || Loss: 70.48073573909511
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.78567
Epoch 31 loss:70.48073573909511
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:19.140146564256998
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:113.93196995014365
waveform batch: 2/2
Test loss - extrapolation:68.04431708318681
Epoch 31 mean train loss:3.319392893393508
Epoch 31 mean test loss - interpolation:3.190024427376166
Epoch 31 mean test loss - extrapolation:15.164690586110872
Start training epoch 32
waveform batch: 1/3
Using ADAM optimizer
Sum of params:76.78567
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.986949358970847
Iteration: 2 || Loss: 6.98673606398817
Iteration: 3 || Loss: 6.98652413896698
Iteration: 4 || Loss: 6.986311537497199
Iteration: 5 || Loss: 6.986099744282119
Iteration: 6 || Loss: 6.986099744282119
saving ADAM checkpoint...
Sum of params:76.785706
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.986099744282119
Iteration: 2 || Loss: 6.962616854778586
Iteration: 3 || Loss: 6.825653373119198
Iteration: 4 || Loss: 6.745105216461591
Iteration: 5 || Loss: 6.605761429809752
Iteration: 6 || Loss: 6.545636358273018
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.91966
Epoch 32 loss:6.545636358273018
waveform batch: 2/3
Using ADAM optimizer
Sum of params:76.91966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.24216257865213
Iteration: 2 || Loss: 19.241896724837293
Iteration: 3 || Loss: 19.241633799527033
Iteration: 4 || Loss: 19.241371986583534
Iteration: 5 || Loss: 19.241109266194236
Iteration: 6 || Loss: 19.241109266194236
saving ADAM checkpoint...
Sum of params:76.91977
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.241109266194236
Iteration: 2 || Loss: 19.19383374160573
Iteration: 3 || Loss: 19.08227958391616
Iteration: 4 || Loss: 18.899470467124903
Iteration: 5 || Loss: 18.748049963539305
Iteration: 6 || Loss: 18.664000288825783
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:76.91676
Epoch 32 loss:18.664000288825783
waveform batch: 3/3
Using ADAM optimizer
Sum of params:76.91676
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 71.30764818777536
Iteration: 2 || Loss: 71.30725822591123
Iteration: 3 || Loss: 71.3068670420786
Iteration: 4 || Loss: 71.30647896041891
Iteration: 5 || Loss: 71.30609052742474
Iteration: 6 || Loss: 71.30609052742474
saving ADAM checkpoint...
Sum of params:76.91651
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 71.30609052742474
Iteration: 2 || Loss: 71.20313120374914
Iteration: 3 || Loss: 70.84165605579864
Iteration: 4 || Loss: 70.48613754567744
Iteration: 5 || Loss: 70.25810610210618
Iteration: 6 || Loss: 68.56038632717154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:77.52594
Epoch 32 loss:68.56038632717154
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:19.991145845108218
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:106.7448762520138
waveform batch: 2/2
Test loss - extrapolation:61.90305447349024
Epoch 32 mean train loss:3.2334490680782877
Epoch 32 mean test loss - interpolation:3.3318576408513696
Epoch 32 mean test loss - extrapolation:14.053994227125337
Start training epoch 33
waveform batch: 1/3
Using ADAM optimizer
Sum of params:77.52594
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.560577106017234
Iteration: 2 || Loss: 7.55987763254314
Iteration: 3 || Loss: 7.559177008734181
Iteration: 4 || Loss: 7.558479319354007
Iteration: 5 || Loss: 7.557781948009669
Iteration: 6 || Loss: 7.557781948009669
saving ADAM checkpoint...
Sum of params:77.52591
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.557781948009669
Iteration: 2 || Loss: 7.260602536808705
Iteration: 3 || Loss: 7.161778414973709
Iteration: 4 || Loss: 7.14014779334902
Iteration: 5 || Loss: 6.742571550605882
Iteration: 6 || Loss: 6.689300254160725
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:77.68137
Epoch 33 loss:6.689300254160725
waveform batch: 2/3
Using ADAM optimizer
Sum of params:77.68137
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.231039577083276
Iteration: 2 || Loss: 19.230906783537353
Iteration: 3 || Loss: 19.23077169135168
Iteration: 4 || Loss: 19.23063734919111
Iteration: 5 || Loss: 19.230505049101478
Iteration: 6 || Loss: 19.230505049101478
saving ADAM checkpoint...
Sum of params:77.68151
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.230505049101478
Iteration: 2 || Loss: 19.220423007162378
Iteration: 3 || Loss: 19.17943607384512
Iteration: 4 || Loss: 18.766352983491938
Iteration: 5 || Loss: 18.692466589891346
Iteration: 6 || Loss: 18.33711903219259
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:77.42539
Epoch 33 loss:18.33711903219259
waveform batch: 3/3
Using ADAM optimizer
Sum of params:77.42539
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.39975769668261
Iteration: 2 || Loss: 68.3995952482705
Iteration: 3 || Loss: 68.39943233984158
Iteration: 4 || Loss: 68.39927213408737
Iteration: 5 || Loss: 68.399111512849
Iteration: 6 || Loss: 68.399111512849
saving ADAM checkpoint...
Sum of params:77.42528
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.399111512849
Iteration: 2 || Loss: 68.38352114508581
Iteration: 3 || Loss: 68.25643549087934
Iteration: 4 || Loss: 67.4589257211714
Iteration: 5 || Loss: 67.30042173926512
Iteration: 6 || Loss: 66.47649574299926
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:77.83722
Epoch 33 loss:66.47649574299926
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:18.702503002797187
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:106.0189618554397
waveform batch: 2/2
Test loss - extrapolation:62.59050159296816
Epoch 33 mean train loss:3.155272932046641
Epoch 33 mean test loss - interpolation:3.1170838337995312
Epoch 33 mean test loss - extrapolation:14.050788620700656
Start training epoch 34
waveform batch: 1/3
Using ADAM optimizer
Sum of params:77.83722
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.0276096041821345
Iteration: 2 || Loss: 7.027047625781475
Iteration: 3 || Loss: 7.026486576924129
Iteration: 4 || Loss: 7.02592807216466
Iteration: 5 || Loss: 7.025369625287131
Iteration: 6 || Loss: 7.025369625287131
saving ADAM checkpoint...
Sum of params:77.83722
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.025369625287131
Iteration: 2 || Loss: 6.824360657084507
Iteration: 3 || Loss: 6.776865439232742
Iteration: 4 || Loss: 6.756858460055601
Iteration: 5 || Loss: 6.399040814404831
Iteration: 6 || Loss: 6.377808291304866
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.04486
Epoch 34 loss:6.377808291304866
waveform batch: 2/3
Using ADAM optimizer
Sum of params:78.04486
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.782186282094965
Iteration: 2 || Loss: 18.78197525125029
Iteration: 3 || Loss: 18.781762518985666
Iteration: 4 || Loss: 18.781550865101014
Iteration: 5 || Loss: 18.7813386184483
Iteration: 6 || Loss: 18.7813386184483
saving ADAM checkpoint...
Sum of params:78.04491
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.7813386184483
Iteration: 2 || Loss: 18.749156060242434
Iteration: 3 || Loss: 18.689814156709783
Iteration: 4 || Loss: 18.304194601085428
Iteration: 5 || Loss: 18.16962319922927
Iteration: 6 || Loss: 18.13297311937763
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:77.95935
Epoch 34 loss:18.13297311937763
waveform batch: 3/3
Using ADAM optimizer
Sum of params:77.95935
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 66.69513084408601
Iteration: 2 || Loss: 66.69504832041746
Iteration: 3 || Loss: 66.69496808861412
Iteration: 4 || Loss: 66.6948873372049
Iteration: 5 || Loss: 66.69481072981897
Iteration: 6 || Loss: 66.69481072981897
saving ADAM checkpoint...
Sum of params:77.95915
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 66.69481072981897
Iteration: 2 || Loss: 66.64133359662084
Iteration: 3 || Loss: 66.6315519968497
Iteration: 4 || Loss: 66.29031278953137
Iteration: 5 || Loss: 65.20884435189613
Iteration: 6 || Loss: 63.846602041888005
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.15528
Epoch 34 loss:63.846602041888005
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:18.442076730411934
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:105.42964234022226
waveform batch: 2/2
Test loss - extrapolation:60.43597348335299
Epoch 34 mean train loss:3.046806325950707
Epoch 34 mean test loss - interpolation:3.0736794550686555
Epoch 34 mean test loss - extrapolation:13.822134651964603
Start training epoch 35
waveform batch: 1/3
Using ADAM optimizer
Sum of params:78.15528
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.584548516085292
Iteration: 2 || Loss: 10.582591691185966
Iteration: 3 || Loss: 10.580634770196507
Iteration: 4 || Loss: 10.578678818919968
Iteration: 5 || Loss: 10.576722953697226
Iteration: 6 || Loss: 10.576722953697226
saving ADAM checkpoint...
Sum of params:78.1553
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.576722953697226
Iteration: 2 || Loss: 7.697298112790766
Iteration: 3 || Loss: 7.513262550086019
Iteration: 4 || Loss: 7.3939413643084455
Iteration: 5 || Loss: 7.380891654123232
Iteration: 6 || Loss: 7.316868958487456
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:77.94144
Epoch 35 loss:7.316868958487456
waveform batch: 2/3
Using ADAM optimizer
Sum of params:77.94144
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.804342569688032
Iteration: 2 || Loss: 19.803789375552896
Iteration: 3 || Loss: 19.803238927984992
Iteration: 4 || Loss: 19.8026837841386
Iteration: 5 || Loss: 19.80213470439504
Iteration: 6 || Loss: 19.80213470439504
saving ADAM checkpoint...
Sum of params:77.94133
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.80213470439504
Iteration: 2 || Loss: 19.536138492620058
Iteration: 3 || Loss: 18.531225493340262
Iteration: 4 || Loss: 18.482003163297623
Iteration: 5 || Loss: 18.41906944236496
Iteration: 6 || Loss: 18.076443779689978
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.29126
Epoch 35 loss:18.076443779689978
waveform batch: 3/3
Using ADAM optimizer
Sum of params:78.29126
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 63.70411993304616
Iteration: 2 || Loss: 63.70394335789308
Iteration: 3 || Loss: 63.70376466516286
Iteration: 4 || Loss: 63.70358768454827
Iteration: 5 || Loss: 63.703410601754726
Iteration: 6 || Loss: 63.703410601754726
saving ADAM checkpoint...
Sum of params:78.29113
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 63.703410601754726
Iteration: 2 || Loss: 63.666115732284496
Iteration: 3 || Loss: 63.349671433617964
Iteration: 4 || Loss: 62.53841408190992
Iteration: 5 || Loss: 62.42428227677557
Iteration: 6 || Loss: 62.21462762655855
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.619576
Epoch 35 loss:62.21462762655855
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:17.99375771546742
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:100.07057558551455
waveform batch: 2/2
Test loss - extrapolation:57.96398364132827
Epoch 35 mean train loss:3.0209634608529647
Epoch 35 mean test loss - interpolation:2.99895961924457
Epoch 35 mean test loss - extrapolation:13.1695466022369
Start training epoch 36
waveform batch: 1/3
Using ADAM optimizer
Sum of params:78.619576
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.495627523919796
Iteration: 2 || Loss: 7.495006395443365
Iteration: 3 || Loss: 7.4943847876855205
Iteration: 4 || Loss: 7.493762789016478
Iteration: 5 || Loss: 7.493143115930761
Iteration: 6 || Loss: 7.493143115930761
saving ADAM checkpoint...
Sum of params:78.619576
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.493143115930761
Iteration: 2 || Loss: 7.251668886045648
Iteration: 3 || Loss: 7.171255332160357
Iteration: 4 || Loss: 7.1254754805321605
Iteration: 5 || Loss: 6.861885685585959
Iteration: 6 || Loss: 6.828793567301523
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.7382
Epoch 36 loss:6.828793567301523
waveform batch: 2/3
Using ADAM optimizer
Sum of params:78.7382
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.21398181210174
Iteration: 2 || Loss: 18.213782515821737
Iteration: 3 || Loss: 18.21358599554432
Iteration: 4 || Loss: 18.213387680024884
Iteration: 5 || Loss: 18.21319041461289
Iteration: 6 || Loss: 18.21319041461289
saving ADAM checkpoint...
Sum of params:78.73834
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.21319041461289
Iteration: 2 || Loss: 18.18856322475662
Iteration: 3 || Loss: 18.16911520522173
Iteration: 4 || Loss: 18.00841409681058
Iteration: 5 || Loss: 17.927120776120265
Iteration: 6 || Loss: 17.72643870929035
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.69162
Epoch 36 loss:17.72643870929035
waveform batch: 3/3
Using ADAM optimizer
Sum of params:78.69162
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 63.383795914781
Iteration: 2 || Loss: 63.38357619451452
Iteration: 3 || Loss: 63.383354559670316
Iteration: 4 || Loss: 63.383140852034145
Iteration: 5 || Loss: 63.38292141506297
Iteration: 6 || Loss: 63.38292141506297
saving ADAM checkpoint...
Sum of params:78.691505
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 63.38292141506297
Iteration: 2 || Loss: 63.327719197365674
Iteration: 3 || Loss: 62.9327674884366
Iteration: 4 || Loss: 62.28377245226181
Iteration: 5 || Loss: 62.13777304622272
Iteration: 6 || Loss: 61.7416882260506
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.92306
Epoch 36 loss:61.7416882260506
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:17.711617798111323
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:99.23447585154757
waveform batch: 2/2
Test loss - extrapolation:57.9800543056807
Epoch 36 mean train loss:2.975755879401465
Epoch 36 mean test loss - interpolation:2.9519362996852205
Epoch 36 mean test loss - extrapolation:13.10121084643569
Start training epoch 37
waveform batch: 1/3
Using ADAM optimizer
Sum of params:78.92306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.101108953460217
Iteration: 2 || Loss: 7.100492815628897
Iteration: 3 || Loss: 7.099877413144872
Iteration: 4 || Loss: 7.09926273175062
Iteration: 5 || Loss: 7.098647948740592
Iteration: 6 || Loss: 7.098647948740592
saving ADAM checkpoint...
Sum of params:78.92305
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.098647948740592
Iteration: 2 || Loss: 6.857467314830989
Iteration: 3 || Loss: 6.787807369945562
Iteration: 4 || Loss: 6.761483729707971
Iteration: 5 || Loss: 6.5578031173067055
Iteration: 6 || Loss: 6.523705568292328
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.04921
Epoch 37 loss:6.523705568292328
waveform batch: 2/3
Using ADAM optimizer
Sum of params:79.04921
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.865668330873067
Iteration: 2 || Loss: 17.865524074164384
Iteration: 3 || Loss: 17.86537943517231
Iteration: 4 || Loss: 17.86523633541578
Iteration: 5 || Loss: 17.865093709754262
Iteration: 6 || Loss: 17.865093709754262
saving ADAM checkpoint...
Sum of params:79.04928
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.865093709754262
Iteration: 2 || Loss: 17.853977991052282
Iteration: 3 || Loss: 17.826694927336717
Iteration: 4 || Loss: 17.621941655518324
Iteration: 5 || Loss: 17.579583949452726
Iteration: 6 || Loss: 17.402467785431725
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:78.96878
Epoch 37 loss:17.402467785431725
waveform batch: 3/3
Using ADAM optimizer
Sum of params:78.96878
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 62.831388758056995
Iteration: 2 || Loss: 62.831194087961585
Iteration: 3 || Loss: 62.8310037754041
Iteration: 4 || Loss: 62.8308086395522
Iteration: 5 || Loss: 62.83061780084749
Iteration: 6 || Loss: 62.83061780084749
saving ADAM checkpoint...
Sum of params:78.96857
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 62.83061780084749
Iteration: 2 || Loss: 62.753101864326226
Iteration: 3 || Loss: 62.45046349250093
Iteration: 4 || Loss: 61.93081507842712
Iteration: 5 || Loss: 61.7385613712688
Iteration: 6 || Loss: 61.26402032010154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.17084
Epoch 37 loss:61.26402032010154
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:17.555024668407768
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:98.2641700816835
waveform batch: 2/2
Test loss - extrapolation:57.5737142988537
Epoch 37 mean train loss:2.9375928853043307
Epoch 37 mean test loss - interpolation:2.925837444734628
Epoch 37 mean test loss - extrapolation:12.986490365044768
Start training epoch 38
waveform batch: 1/3
Using ADAM optimizer
Sum of params:79.17084
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.896994132523795
Iteration: 2 || Loss: 6.896330939207466
Iteration: 3 || Loss: 6.895669125597268
Iteration: 4 || Loss: 6.895008218616531
Iteration: 5 || Loss: 6.894348033106586
Iteration: 6 || Loss: 6.894348033106586
saving ADAM checkpoint...
Sum of params:79.170845
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.894348033106586
Iteration: 2 || Loss: 6.613736881550289
Iteration: 3 || Loss: 6.5515774537006415
Iteration: 4 || Loss: 6.527438036453611
Iteration: 5 || Loss: 6.364531374764451
Iteration: 6 || Loss: 6.297619543297441
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.298935
Epoch 38 loss:6.297619543297441
waveform batch: 2/3
Using ADAM optimizer
Sum of params:79.298935
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.574124610556293
Iteration: 2 || Loss: 17.573995690268944
Iteration: 3 || Loss: 17.573868856252297
Iteration: 4 || Loss: 17.573741805985694
Iteration: 5 || Loss: 17.57361609930006
Iteration: 6 || Loss: 17.57361609930006
saving ADAM checkpoint...
Sum of params:79.299034
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.57361609930006
Iteration: 2 || Loss: 17.566210999626627
Iteration: 3 || Loss: 17.38813994282543
Iteration: 4 || Loss: 17.332545442120658
Iteration: 5 || Loss: 17.279139614324805
Iteration: 6 || Loss: 17.131464417923883
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.18873
Epoch 38 loss:17.131464417923883
waveform batch: 3/3
Using ADAM optimizer
Sum of params:79.18873
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 62.27720379773651
Iteration: 2 || Loss: 62.27701636767478
Iteration: 3 || Loss: 62.27683318217759
Iteration: 4 || Loss: 62.276643349773195
Iteration: 5 || Loss: 62.27645765743972
Iteration: 6 || Loss: 62.27645765743972
saving ADAM checkpoint...
Sum of params:79.18855
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 62.27645765743972
Iteration: 2 || Loss: 62.21573657043694
Iteration: 3 || Loss: 61.924086679190935
Iteration: 4 || Loss: 61.37126289845407
Iteration: 5 || Loss: 61.23152222080873
Iteration: 6 || Loss: 60.741299214579826
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.36902
Epoch 38 loss:60.741299214579826
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:17.35628233232431
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.35780699318319
waveform batch: 2/2
Test loss - extrapolation:57.182507544424105
Epoch 38 mean train loss:2.9024270060621085
Epoch 38 mean test loss - interpolation:2.8927137220540513
Epoch 38 mean test loss - extrapolation:12.878359544800608
Start training epoch 39
waveform batch: 1/3
Using ADAM optimizer
Sum of params:79.36902
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.6525313086211755
Iteration: 2 || Loss: 6.651917514771234
Iteration: 3 || Loss: 6.651306420631256
Iteration: 4 || Loss: 6.650696467654745
Iteration: 5 || Loss: 6.650086749501263
Iteration: 6 || Loss: 6.650086749501263
saving ADAM checkpoint...
Sum of params:79.36902
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.650086749501263
Iteration: 2 || Loss: 6.411293338954089
Iteration: 3 || Loss: 6.352682065126142
Iteration: 4 || Loss: 6.330881556191192
Iteration: 5 || Loss: 6.155913935130896
Iteration: 6 || Loss: 6.113733976299841
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.49512
Epoch 39 loss:6.113733976299841
waveform batch: 2/3
Using ADAM optimizer
Sum of params:79.49512
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.306768067434987
Iteration: 2 || Loss: 17.30666234220106
Iteration: 3 || Loss: 17.306555350562924
Iteration: 4 || Loss: 17.306448530374315
Iteration: 5 || Loss: 17.30634306190173
Iteration: 6 || Loss: 17.30634306190173
saving ADAM checkpoint...
Sum of params:79.49515
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.30634306190173
Iteration: 2 || Loss: 17.30146440137187
Iteration: 3 || Loss: 17.12971373084299
Iteration: 4 || Loss: 17.07955784867236
Iteration: 5 || Loss: 17.022340046850978
Iteration: 6 || Loss: 16.883820818079457
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.37746
Epoch 39 loss:16.883820818079457
waveform batch: 3/3
Using ADAM optimizer
Sum of params:79.37746
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 61.71946540265905
Iteration: 2 || Loss: 61.71930116517253
Iteration: 3 || Loss: 61.71913795901001
Iteration: 4 || Loss: 61.7189752597842
Iteration: 5 || Loss: 61.71880729423647
Iteration: 6 || Loss: 61.71880729423647
saving ADAM checkpoint...
Sum of params:79.37732
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 61.71880729423647
Iteration: 2 || Loss: 61.51413890756371
Iteration: 3 || Loss: 61.383604751417735
Iteration: 4 || Loss: 60.957400306540634
Iteration: 5 || Loss: 60.74902217355457
Iteration: 6 || Loss: 60.23248026545133
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.56943
Epoch 39 loss:60.23248026545133
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:17.152574461913737
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.36369666888505
waveform batch: 2/2
Test loss - extrapolation:56.723438822728596
Epoch 39 mean train loss:2.870001208959677
Epoch 39 mean test loss - interpolation:2.858762410318956
Epoch 39 mean test loss - extrapolation:12.757261290967804
Start training epoch 40
waveform batch: 1/3
Using ADAM optimizer
Sum of params:79.56943
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.421082827752987
Iteration: 2 || Loss: 6.4205189160966
Iteration: 3 || Loss: 6.4199541658582815
Iteration: 4 || Loss: 6.4193925224510116
Iteration: 5 || Loss: 6.418831523385591
Iteration: 6 || Loss: 6.418831523385591
saving ADAM checkpoint...
Sum of params:79.56943
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.418831523385591
Iteration: 2 || Loss: 6.21585551350174
Iteration: 3 || Loss: 6.169313712435415
Iteration: 4 || Loss: 6.149297299168976
Iteration: 5 || Loss: 5.957593417551668
Iteration: 6 || Loss: 5.939069147437125
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.690994
Epoch 40 loss:5.939069147437125
waveform batch: 2/3
Using ADAM optimizer
Sum of params:79.690994
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.075835996305965
Iteration: 2 || Loss: 17.075761902692612
Iteration: 3 || Loss: 17.07568883991637
Iteration: 4 || Loss: 17.075616154227507
Iteration: 5 || Loss: 17.075544780164535
Iteration: 6 || Loss: 17.075544780164535
saving ADAM checkpoint...
Sum of params:79.69104
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.075544780164535
Iteration: 2 || Loss: 17.073927315693442
Iteration: 3 || Loss: 16.845483885930246
Iteration: 4 || Loss: 16.838895203944737
Iteration: 5 || Loss: 16.70123560139898
Iteration: 6 || Loss: 16.550912496790666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.46735
Epoch 40 loss:16.550912496790666
waveform batch: 3/3
Using ADAM optimizer
Sum of params:79.46735
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 61.47147531160078
Iteration: 2 || Loss: 61.47135834639198
Iteration: 3 || Loss: 61.47123943821471
Iteration: 4 || Loss: 61.47112464073591
Iteration: 5 || Loss: 61.471005138186534
Iteration: 6 || Loss: 61.471005138186534
saving ADAM checkpoint...
Sum of params:79.467285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 61.471005138186534
Iteration: 2 || Loss: 61.43449003234679
Iteration: 3 || Loss: 61.28474441740889
Iteration: 4 || Loss: 60.582134148922606
Iteration: 5 || Loss: 60.39854169689112
Iteration: 6 || Loss: 60.056961484579965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.71529
Epoch 40 loss:60.056961484579965
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:16.890251381241853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:96.1908483569227
waveform batch: 2/2
Test loss - extrapolation:57.14502892014686
Epoch 40 mean train loss:2.8464463147864745
Epoch 40 mean test loss - interpolation:2.8150418968736424
Epoch 40 mean test loss - extrapolation:12.77798977308913
Start training epoch 41
waveform batch: 1/3
Using ADAM optimizer
Sum of params:79.71529
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.9607494210044365
Iteration: 2 || Loss: 5.960617665906995
Iteration: 3 || Loss: 5.960486643681935
Iteration: 4 || Loss: 5.960356338077758
Iteration: 5 || Loss: 5.960225208475873
Iteration: 6 || Loss: 5.960225208475873
saving ADAM checkpoint...
Sum of params:79.715294
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.960225208475873
Iteration: 2 || Loss: 5.952462821792044
Iteration: 3 || Loss: 5.891787471216378
Iteration: 4 || Loss: 5.744739547620738
Iteration: 5 || Loss: 5.666806127794239
Iteration: 6 || Loss: 5.418336243345399
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.02542
Epoch 41 loss:5.418336243345399
waveform batch: 2/3
Using ADAM optimizer
Sum of params:80.02542
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.128044801318765
Iteration: 2 || Loss: 17.12789593657841
Iteration: 3 || Loss: 17.127748904593435
Iteration: 4 || Loss: 17.127601675869993
Iteration: 5 || Loss: 17.12745596528292
Iteration: 6 || Loss: 17.12745596528292
saving ADAM checkpoint...
Sum of params:80.02542
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.12745596528292
Iteration: 2 || Loss: 17.114841098713086
Iteration: 3 || Loss: 17.04911789476225
Iteration: 4 || Loss: 16.619475562810543
Iteration: 5 || Loss: 16.408595270823287
Iteration: 6 || Loss: 16.29366005497251
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.88408
Epoch 41 loss:16.29366005497251
waveform batch: 3/3
Using ADAM optimizer
Sum of params:79.88408
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 61.03788255674042
Iteration: 2 || Loss: 61.03759346479574
Iteration: 3 || Loss: 61.03730634077166
Iteration: 4 || Loss: 61.03701568892261
Iteration: 5 || Loss: 61.03672810271602
Iteration: 6 || Loss: 61.03672810271602
saving ADAM checkpoint...
Sum of params:79.88383
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 61.03672810271602
Iteration: 2 || Loss: 60.97924841278509
Iteration: 3 || Loss: 60.754903776108534
Iteration: 4 || Loss: 60.49746976293461
Iteration: 5 || Loss: 60.3789439966317
Iteration: 6 || Loss: 58.97860017684085
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.48609
Epoch 41 loss:58.97860017684085
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:18.560368981997506
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:97.40621627854075
waveform batch: 2/2
Test loss - extrapolation:51.98561683754498
Epoch 41 mean train loss:2.782434361212371
Epoch 41 mean test loss - interpolation:3.0933948303329175
Epoch 41 mean test loss - extrapolation:12.449319426340479
Start training epoch 42
waveform batch: 1/3
Using ADAM optimizer
Sum of params:79.48609
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.550362549992107
Iteration: 2 || Loss: 10.548323826576258
Iteration: 3 || Loss: 10.54628742093008
Iteration: 4 || Loss: 10.544251132373724
Iteration: 5 || Loss: 10.542212621152705
Iteration: 6 || Loss: 10.542212621152705
saving ADAM checkpoint...
Sum of params:79.48609
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.542212621152705
Iteration: 2 || Loss: 7.7758758494621585
Iteration: 3 || Loss: 7.537272909762459
Iteration: 4 || Loss: 7.248202345107729
Iteration: 5 || Loss: 7.216296962498223
Iteration: 6 || Loss: 7.202487150557672
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.707214
Epoch 42 loss:7.202487150557672
waveform batch: 2/3
Using ADAM optimizer
Sum of params:79.707214
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.22008790087642
Iteration: 2 || Loss: 18.21928445238548
Iteration: 3 || Loss: 18.218486169715575
Iteration: 4 || Loss: 18.21768721991179
Iteration: 5 || Loss: 18.216887928871756
Iteration: 6 || Loss: 18.216887928871756
saving ADAM checkpoint...
Sum of params:79.70731
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.216887928871756
Iteration: 2 || Loss: 17.7259316504161
Iteration: 3 || Loss: 17.636965682552127
Iteration: 4 || Loss: 17.4401098867042
Iteration: 5 || Loss: 17.154809363643736
Iteration: 6 || Loss: 17.13510901896186
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:79.80967
Epoch 42 loss:17.13510901896186
waveform batch: 3/3
Using ADAM optimizer
Sum of params:79.80967
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 57.40886226411442
Iteration: 2 || Loss: 57.40867401005318
Iteration: 3 || Loss: 57.40849184068667
Iteration: 4 || Loss: 57.408303950843425
Iteration: 5 || Loss: 57.4081164517258
Iteration: 6 || Loss: 57.4081164517258
saving ADAM checkpoint...
Sum of params:79.80959
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 57.4081164517258
Iteration: 2 || Loss: 57.08186334716337
Iteration: 3 || Loss: 56.985769370618925
Iteration: 4 || Loss: 56.371615031898436
Iteration: 5 || Loss: 56.23511023392772
Iteration: 6 || Loss: 56.1056082941938
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.03796
Epoch 42 loss:56.1056082941938
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:16.792881866841185
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:91.43879477581217
waveform batch: 2/2
Test loss - extrapolation:50.78933918325158
Epoch 42 mean train loss:2.773903602197011
Epoch 42 mean test loss - interpolation:2.798813644473531
Epoch 42 mean test loss - extrapolation:11.852344496588644
Start training epoch 43
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.03796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.335720225012006
Iteration: 2 || Loss: 8.33435581530756
Iteration: 3 || Loss: 8.332988083285471
Iteration: 4 || Loss: 8.33162484876307
Iteration: 5 || Loss: 8.330262252738889
Iteration: 6 || Loss: 8.330262252738889
saving ADAM checkpoint...
Sum of params:80.037964
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.330262252738889
Iteration: 2 || Loss: 7.162538689335306
Iteration: 3 || Loss: 7.044956949172151
Iteration: 4 || Loss: 6.906633602375617
Iteration: 5 || Loss: 6.759123717673542
Iteration: 6 || Loss: 6.557840755684881
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.146126
Epoch 43 loss:6.557840755684881
waveform batch: 2/3
Using ADAM optimizer
Sum of params:80.146126
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.27963234394504
Iteration: 2 || Loss: 17.279248572905644
Iteration: 3 || Loss: 17.278865315762275
Iteration: 4 || Loss: 17.278484432071078
Iteration: 5 || Loss: 17.278101569226425
Iteration: 6 || Loss: 17.278101569226425
saving ADAM checkpoint...
Sum of params:80.14616
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.278101569226425
Iteration: 2 || Loss: 17.184696520878262
Iteration: 3 || Loss: 17.14286700365735
Iteration: 4 || Loss: 16.993444403532685
Iteration: 5 || Loss: 16.810865905829406
Iteration: 6 || Loss: 16.774162455436247
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.24739
Epoch 43 loss:16.774162455436247
waveform batch: 3/3
Using ADAM optimizer
Sum of params:80.24739
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 56.73721460164668
Iteration: 2 || Loss: 56.737021424201586
Iteration: 3 || Loss: 56.736826544393644
Iteration: 4 || Loss: 56.73663389758769
Iteration: 5 || Loss: 56.736440585858546
Iteration: 6 || Loss: 56.736440585858546
saving ADAM checkpoint...
Sum of params:80.24724
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.736440585858546
Iteration: 2 || Loss: 56.40243606239116
Iteration: 3 || Loss: 56.19697486313788
Iteration: 4 || Loss: 55.95175689245884
Iteration: 5 || Loss: 55.85712164487744
Iteration: 6 || Loss: 55.400986180096154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.56024
Epoch 43 loss:55.400986180096154
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:16.4182133509154
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:90.10858449642005
waveform batch: 2/2
Test loss - extrapolation:51.02765876142598
Epoch 43 mean train loss:2.7149306686626646
Epoch 43 mean test loss - interpolation:2.7363688918192337
Epoch 43 mean test loss - extrapolation:11.761353604820505
Start training epoch 44
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.56024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.842697544597542
Iteration: 2 || Loss: 7.841444912976502
Iteration: 3 || Loss: 7.8401903322693896
Iteration: 4 || Loss: 7.8389369729858585
Iteration: 5 || Loss: 7.8376844023023375
Iteration: 6 || Loss: 7.8376844023023375
saving ADAM checkpoint...
Sum of params:80.56022
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.8376844023023375
Iteration: 2 || Loss: 6.789987381982593
Iteration: 3 || Loss: 6.691829503915735
Iteration: 4 || Loss: 6.612635458803024
Iteration: 5 || Loss: 6.452789920326948
Iteration: 6 || Loss: 6.181052234316391
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.800285
Epoch 44 loss:6.181052234316391
waveform batch: 2/3
Using ADAM optimizer
Sum of params:80.800285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.091326165575257
Iteration: 2 || Loss: 17.09122360859242
Iteration: 3 || Loss: 17.091120172879926
Iteration: 4 || Loss: 17.091016563525255
Iteration: 5 || Loss: 17.090913879952883
Iteration: 6 || Loss: 17.090913879952883
saving ADAM checkpoint...
Sum of params:80.80033
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.090913879952883
Iteration: 2 || Loss: 17.08747448423349
Iteration: 3 || Loss: 16.90785539543583
Iteration: 4 || Loss: 16.468777436760313
Iteration: 5 || Loss: 16.361356665311465
Iteration: 6 || Loss: 16.05580937674732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.05039
Epoch 44 loss:16.05580937674732
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.05039
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 57.09367723778214
Iteration: 2 || Loss: 57.09341397874139
Iteration: 3 || Loss: 57.09315223069261
Iteration: 4 || Loss: 57.092882238852596
Iteration: 5 || Loss: 57.0926239186727
Iteration: 6 || Loss: 57.0926239186727
saving ADAM checkpoint...
Sum of params:81.050385
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 57.0926239186727
Iteration: 2 || Loss: 56.97545165020427
Iteration: 3 || Loss: 56.37922331570985
Iteration: 4 || Loss: 56.026521308285176
Iteration: 5 || Loss: 55.72918796300844
Iteration: 6 || Loss: 55.625597614205
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.034874
Epoch 44 loss:55.625597614205
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:16.13473679993684
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:89.13154931680879
waveform batch: 2/2
Test loss - extrapolation:51.97609296396627
Epoch 44 mean train loss:2.6849123870782314
Epoch 44 mean test loss - interpolation:2.689122799989473
Epoch 44 mean test loss - extrapolation:11.758970190064588
Start training epoch 45
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.034874
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.32925965503704
Iteration: 2 || Loss: 6.328714677585116
Iteration: 3 || Loss: 6.32816989973382
Iteration: 4 || Loss: 6.3276262530303615
Iteration: 5 || Loss: 6.327082873333165
Iteration: 6 || Loss: 6.327082873333165
saving ADAM checkpoint...
Sum of params:81.034874
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.327082873333165
Iteration: 2 || Loss: 6.139989632134943
Iteration: 3 || Loss: 5.82791245825628
Iteration: 4 || Loss: 5.803725365657449
Iteration: 5 || Loss: 5.7177145346906935
Iteration: 6 || Loss: 5.59574124967437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.22069
Epoch 45 loss:5.59574124967437
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.22069
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 16.090037930519344
Iteration: 2 || Loss: 16.089840934684922
Iteration: 3 || Loss: 16.089644253264595
Iteration: 4 || Loss: 16.089447879434154
Iteration: 5 || Loss: 16.08925250395055
Iteration: 6 || Loss: 16.08925250395055
saving ADAM checkpoint...
Sum of params:81.22071
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 16.08925250395055
Iteration: 2 || Loss: 16.070397525538933
Iteration: 3 || Loss: 16.00032138017117
Iteration: 4 || Loss: 15.923855563536426
Iteration: 5 || Loss: 15.855381279427522
Iteration: 6 || Loss: 15.775687646538188
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.03677
Epoch 45 loss:15.775687646538188
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.03677
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 56.29871764717857
Iteration: 2 || Loss: 56.29853231400976
Iteration: 3 || Loss: 56.29834932301525
Iteration: 4 || Loss: 56.2981652952428
Iteration: 5 || Loss: 56.297981359106515
Iteration: 6 || Loss: 56.297981359106515
saving ADAM checkpoint...
Sum of params:81.03673
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.297981359106515
Iteration: 2 || Loss: 56.26709297435534
Iteration: 3 || Loss: 56.197612492872715
Iteration: 4 || Loss: 55.824655089702645
Iteration: 5 || Loss: 55.58238166708902
Iteration: 6 || Loss: 55.43902947026027
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.08627
Epoch 45 loss:55.43902947026027
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:16.076312469215452
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:88.51084887804666
waveform batch: 2/2
Test loss - extrapolation:51.347274643930405
Epoch 45 mean train loss:2.648636495395615
Epoch 45 mean test loss - interpolation:2.679385411535909
Epoch 45 mean test loss - extrapolation:11.654843626831422
Start training epoch 46
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.08627
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.939121734941612
Iteration: 2 || Loss: 5.938632187731173
Iteration: 3 || Loss: 5.938143333185653
Iteration: 4 || Loss: 5.9376549838274135
Iteration: 5 || Loss: 5.937166211381608
Iteration: 6 || Loss: 5.937166211381608
saving ADAM checkpoint...
Sum of params:81.08629
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.937166211381608
Iteration: 2 || Loss: 5.788419118850071
Iteration: 3 || Loss: 5.6539139550926185
Iteration: 4 || Loss: 5.625614768518368
Iteration: 5 || Loss: 5.475313709240572
Iteration: 6 || Loss: 5.449481140228364
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.2491
Epoch 46 loss:5.449481140228364
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.2491
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.86685110343832
Iteration: 2 || Loss: 15.866739672033088
Iteration: 3 || Loss: 15.866628712329664
Iteration: 4 || Loss: 15.866518973776987
Iteration: 5 || Loss: 15.866409929609404
Iteration: 6 || Loss: 15.866409929609404
saving ADAM checkpoint...
Sum of params:81.24917
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.866409929609404
Iteration: 2 || Loss: 15.861410591345747
Iteration: 3 || Loss: 15.845577537103331
Iteration: 4 || Loss: 15.731598603108072
Iteration: 5 || Loss: 15.689589018609059
Iteration: 6 || Loss: 15.618667855421855
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.154396
Epoch 46 loss:15.618667855421855
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.154396
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 56.5121406852139
Iteration: 2 || Loss: 56.51193328748242
Iteration: 3 || Loss: 56.51172609551135
Iteration: 4 || Loss: 56.51151721160519
Iteration: 5 || Loss: 56.511310102469665
Iteration: 6 || Loss: 56.511310102469665
saving ADAM checkpoint...
Sum of params:81.1543
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.511310102469665
Iteration: 2 || Loss: 56.38593862136894
Iteration: 3 || Loss: 56.18725499705439
Iteration: 4 || Loss: 55.73172152097582
Iteration: 5 || Loss: 55.50138158069799
Iteration: 6 || Loss: 55.29991868922875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.14369
Epoch 46 loss:55.29991868922875
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.944835109955653
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:88.53164399765029
waveform batch: 2/2
Test loss - extrapolation:51.541723396468576
Epoch 46 mean train loss:2.633381644306171
Epoch 46 mean test loss - interpolation:2.657472518325942
Epoch 46 mean test loss - extrapolation:11.672780616176572
Start training epoch 47
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.14369
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.80505511552181
Iteration: 2 || Loss: 5.804566032667486
Iteration: 3 || Loss: 5.804079593067662
Iteration: 4 || Loss: 5.803593954048917
Iteration: 5 || Loss: 5.803108311080291
Iteration: 6 || Loss: 5.803108311080291
saving ADAM checkpoint...
Sum of params:81.14369
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.803108311080291
Iteration: 2 || Loss: 5.655899996468444
Iteration: 3 || Loss: 5.497450627132861
Iteration: 4 || Loss: 5.477232195160878
Iteration: 5 || Loss: 5.335497875718796
Iteration: 6 || Loss: 5.322508668472052
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.2965
Epoch 47 loss:5.322508668472052
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.2965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.688600249639514
Iteration: 2 || Loss: 15.68855863814821
Iteration: 3 || Loss: 15.688518549701085
Iteration: 4 || Loss: 15.688476992212404
Iteration: 5 || Loss: 15.688435525092961
Iteration: 6 || Loss: 15.688435525092961
saving ADAM checkpoint...
Sum of params:81.29664
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.688435525092961
Iteration: 2 || Loss: 15.6878936706026
Iteration: 3 || Loss: 15.67799976618482
Iteration: 4 || Loss: 15.58100537572375
Iteration: 5 || Loss: 15.497185608509435
Iteration: 6 || Loss: 15.28632323720511
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.12735
Epoch 47 loss:15.28632323720511
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.12735
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 56.750257027095124
Iteration: 2 || Loss: 56.74998206139317
Iteration: 3 || Loss: 56.74971067162732
Iteration: 4 || Loss: 56.74943786105517
Iteration: 5 || Loss: 56.74916794722491
Iteration: 6 || Loss: 56.74916794722491
saving ADAM checkpoint...
Sum of params:81.12733
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.74916794722491
Iteration: 2 || Loss: 56.700833452811025
Iteration: 3 || Loss: 56.340996275164365
Iteration: 4 || Loss: 55.90343548387376
Iteration: 5 || Loss: 55.54779877057243
Iteration: 6 || Loss: 55.35984777357366
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.23098
Epoch 47 loss:55.35984777357366
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.686782701483086
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:88.7488983102127
waveform batch: 2/2
Test loss - extrapolation:52.295155752518916
Epoch 47 mean train loss:2.6196096441120975
Epoch 47 mean test loss - interpolation:2.6144637835805145
Epoch 47 mean test loss - extrapolation:11.753671171894302
Start training epoch 48
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.23098
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.446126245669537
Iteration: 2 || Loss: 5.445855206178652
Iteration: 3 || Loss: 5.445584470224386
Iteration: 4 || Loss: 5.445315562344293
Iteration: 5 || Loss: 5.4450460874708195
Iteration: 6 || Loss: 5.4450460874708195
saving ADAM checkpoint...
Sum of params:81.23097
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.4450460874708195
Iteration: 2 || Loss: 5.403360676044271
Iteration: 3 || Loss: 5.223745799552233
Iteration: 4 || Loss: 5.19545747973164
Iteration: 5 || Loss: 5.093208833518483
Iteration: 6 || Loss: 5.0538846443911565
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.39798
Epoch 48 loss:5.0538846443911565
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.39798
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.377783287260769
Iteration: 2 || Loss: 15.377739871510423
Iteration: 3 || Loss: 15.377696663804835
Iteration: 4 || Loss: 15.377655176530025
Iteration: 5 || Loss: 15.377614889283771
Iteration: 6 || Loss: 15.377614889283771
saving ADAM checkpoint...
Sum of params:81.39801
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.377614889283771
Iteration: 2 || Loss: 15.375775976475262
Iteration: 3 || Loss: 15.369681542657542
Iteration: 4 || Loss: 15.230305850936128
Iteration: 5 || Loss: 15.167087311013022
Iteration: 6 || Loss: 15.054010270711965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.200005
Epoch 48 loss:15.054010270711965
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.200005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 56.44344580469349
Iteration: 2 || Loss: 56.44323383367282
Iteration: 3 || Loss: 56.443020348223946
Iteration: 4 || Loss: 56.44281302758756
Iteration: 5 || Loss: 56.44260052213998
Iteration: 6 || Loss: 56.44260052213998
saving ADAM checkpoint...
Sum of params:81.19998
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.44260052213998
Iteration: 2 || Loss: 56.399083022976924
Iteration: 3 || Loss: 56.08367836158991
Iteration: 4 || Loss: 55.74118925413143
Iteration: 5 || Loss: 55.5080596741811
Iteration: 6 || Loss: 55.19404388131441
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.31767
Epoch 48 loss:55.19404388131441
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.517651233501583
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:87.925444889627
waveform batch: 2/2
Test loss - extrapolation:51.90454784423504
Epoch 48 mean train loss:2.5966185791868113
Epoch 48 mean test loss - interpolation:2.5862752055835974
Epoch 48 mean test loss - extrapolation:11.652499394488503
Start training epoch 49
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.31767
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.166430691376103
Iteration: 2 || Loss: 5.166303713345737
Iteration: 3 || Loss: 5.166176858374439
Iteration: 4 || Loss: 5.166051331716901
Iteration: 5 || Loss: 5.16592661954244
Iteration: 6 || Loss: 5.16592661954244
saving ADAM checkpoint...
Sum of params:81.31766
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.16592661954244
Iteration: 2 || Loss: 5.158369793334956
Iteration: 3 || Loss: 5.093122913932706
Iteration: 4 || Loss: 5.014912334886422
Iteration: 5 || Loss: 4.91415631939659
Iteration: 6 || Loss: 4.854218653723789
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.4767
Epoch 49 loss:4.854218653723789
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.4767
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.255446525143384
Iteration: 2 || Loss: 15.255269040249786
Iteration: 3 || Loss: 15.255090140914751
Iteration: 4 || Loss: 15.254911899447146
Iteration: 5 || Loss: 15.254735357632565
Iteration: 6 || Loss: 15.254735357632565
saving ADAM checkpoint...
Sum of params:81.47676
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.254735357632565
Iteration: 2 || Loss: 15.240450556958839
Iteration: 3 || Loss: 15.200487747543649
Iteration: 4 || Loss: 15.057566309997734
Iteration: 5 || Loss: 15.033843636596695
Iteration: 6 || Loss: 14.908577362598624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.30486
Epoch 49 loss:14.908577362598624
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.30486
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 56.12998816698969
Iteration: 2 || Loss: 56.129819555168496
Iteration: 3 || Loss: 56.12965358000426
Iteration: 4 || Loss: 56.12948072453149
Iteration: 5 || Loss: 56.129311945866384
Iteration: 6 || Loss: 56.129311945866384
saving ADAM checkpoint...
Sum of params:81.30483
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 56.129311945866384
Iteration: 2 || Loss: 56.07768060077746
Iteration: 3 || Loss: 55.825094515651244
Iteration: 4 || Loss: 55.51671584563511
Iteration: 5 || Loss: 55.3641915550713
Iteration: 6 || Loss: 54.858039813897484
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.42342
Epoch 49 loss:54.858039813897484
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.556483917134853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:86.8035381780409
waveform batch: 2/2
Test loss - extrapolation:50.797709455642014
Epoch 49 mean train loss:2.5731322700075827
Epoch 49 mean test loss - interpolation:2.5927473195224757
Epoch 49 mean test loss - extrapolation:11.466770636140241
Start training epoch 50
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.42342
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.24229104273504
Iteration: 2 || Loss: 5.241816580811762
Iteration: 3 || Loss: 5.241343636470696
Iteration: 4 || Loss: 5.240870396772519
Iteration: 5 || Loss: 5.240398348537572
Iteration: 6 || Loss: 5.240398348537572
saving ADAM checkpoint...
Sum of params:81.42344
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.240398348537572
Iteration: 2 || Loss: 5.0991769310297155
Iteration: 3 || Loss: 5.041846763086518
Iteration: 4 || Loss: 5.007878781116265
Iteration: 5 || Loss: 4.828519184563227
Iteration: 6 || Loss: 4.806846929426435
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.577255
Epoch 50 loss:4.806846929426435
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.577255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.219974836249623
Iteration: 2 || Loss: 15.219872904373029
Iteration: 3 || Loss: 15.219771672451081
Iteration: 4 || Loss: 15.219669802916712
Iteration: 5 || Loss: 15.219569558870786
Iteration: 6 || Loss: 15.219569558870786
saving ADAM checkpoint...
Sum of params:81.57723
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.219569558870786
Iteration: 2 || Loss: 15.214023194389362
Iteration: 3 || Loss: 15.178602618891496
Iteration: 4 || Loss: 14.987549503806276
Iteration: 5 || Loss: 14.947020499082713
Iteration: 6 || Loss: 14.823129812711086
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.41722
Epoch 50 loss:14.823129812711086
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.41722
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 55.67947427643667
Iteration: 2 || Loss: 55.679305458708875
Iteration: 3 || Loss: 55.679138761850666
Iteration: 4 || Loss: 55.678968975938446
Iteration: 5 || Loss: 55.678800746025054
Iteration: 6 || Loss: 55.678800746025054
saving ADAM checkpoint...
Sum of params:81.41716
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 55.678800746025054
Iteration: 2 || Loss: 55.525833637568816
Iteration: 3 || Loss: 55.29227166682327
Iteration: 4 || Loss: 54.97748157578463
Iteration: 5 || Loss: 54.60341062912551
Iteration: 6 || Loss: 54.188154258823076
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.52148
Epoch 50 loss:54.188154258823076
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.59174824808055
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:85.76946644051446
waveform batch: 2/2
Test loss - extrapolation:49.6662236616572
Epoch 50 mean train loss:2.545452793136572
Epoch 50 mean test loss - interpolation:2.5986247080134253
Epoch 50 mean test loss - extrapolation:11.286307508514305
Start training epoch 51
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.52148
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.676954966601707
Iteration: 2 || Loss: 5.676219534549923
Iteration: 3 || Loss: 5.675483815040323
Iteration: 4 || Loss: 5.6747488845001985
Iteration: 5 || Loss: 5.6740159201380695
Iteration: 6 || Loss: 5.6740159201380695
saving ADAM checkpoint...
Sum of params:81.52149
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.6740159201380695
Iteration: 2 || Loss: 5.328268463812525
Iteration: 3 || Loss: 5.26137027641187
Iteration: 4 || Loss: 5.241899581498454
Iteration: 5 || Loss: 5.074958104394903
Iteration: 6 || Loss: 4.96760034149992
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.617645
Epoch 51 loss:4.96760034149992
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.617645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.05350186073816
Iteration: 2 || Loss: 15.05328496570667
Iteration: 3 || Loss: 15.05306826388701
Iteration: 4 || Loss: 15.052852983797438
Iteration: 5 || Loss: 15.052638918723176
Iteration: 6 || Loss: 15.052638918723176
saving ADAM checkpoint...
Sum of params:81.61776
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.052638918723176
Iteration: 2 || Loss: 15.02083985653334
Iteration: 3 || Loss: 15.015336581205611
Iteration: 4 || Loss: 14.982857315795448
Iteration: 5 || Loss: 14.930551586224782
Iteration: 6 || Loss: 14.890777459485498
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.603195
Epoch 51 loss:14.890777459485498
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.603195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 54.639796029035935
Iteration: 2 || Loss: 54.63956122349119
Iteration: 3 || Loss: 54.63932768033602
Iteration: 4 || Loss: 54.63909265707662
Iteration: 5 || Loss: 54.63885499144091
Iteration: 6 || Loss: 54.63885499144091
saving ADAM checkpoint...
Sum of params:81.60304
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 54.63885499144091
Iteration: 2 || Loss: 54.58723629165757
Iteration: 3 || Loss: 54.29298132640762
Iteration: 4 || Loss: 54.02796645522674
Iteration: 5 || Loss: 53.90771881882003
Iteration: 6 || Loss: 53.458740981645185
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.41068
Epoch 51 loss:53.458740981645185
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.203226577365982
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:86.95060188053975
waveform batch: 2/2
Test loss - extrapolation:50.52937659602367
Epoch 51 mean train loss:2.5281765097458826
Epoch 51 mean test loss - interpolation:2.5338710962276636
Epoch 51 mean test loss - extrapolation:11.456664873046952
Start training epoch 52
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.41068
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.070342665485743
Iteration: 2 || Loss: 6.06930705426661
Iteration: 3 || Loss: 6.068274447230899
Iteration: 4 || Loss: 6.067245286204218
Iteration: 5 || Loss: 6.066217677054343
Iteration: 6 || Loss: 6.066217677054343
saving ADAM checkpoint...
Sum of params:81.41068
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.066217677054343
Iteration: 2 || Loss: 5.360788837649607
Iteration: 3 || Loss: 5.298967448893864
Iteration: 4 || Loss: 5.256346632269912
Iteration: 5 || Loss: 5.1973616015851585
Iteration: 6 || Loss: 5.053055920340383
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.51804
Epoch 52 loss:5.053055920340383
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.51804
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.154964554270677
Iteration: 2 || Loss: 15.154809872576541
Iteration: 3 || Loss: 15.154655434245642
Iteration: 4 || Loss: 15.154502498448405
Iteration: 5 || Loss: 15.154348559269199
Iteration: 6 || Loss: 15.154348559269199
saving ADAM checkpoint...
Sum of params:81.5181
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.154348559269199
Iteration: 2 || Loss: 15.138559570104833
Iteration: 3 || Loss: 15.109567125979618
Iteration: 4 || Loss: 14.926993537006807
Iteration: 5 || Loss: 14.890281694022224
Iteration: 6 || Loss: 14.831016694641775
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.60962
Epoch 52 loss:14.831016694641775
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.60962
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 54.039497983913456
Iteration: 2 || Loss: 54.039302740411436
Iteration: 3 || Loss: 54.039111127292635
Iteration: 4 || Loss: 54.03891710831982
Iteration: 5 || Loss: 54.03872241201759
Iteration: 6 || Loss: 54.03872241201759
saving ADAM checkpoint...
Sum of params:81.60954
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 54.03872241201759
Iteration: 2 || Loss: 53.80475455628204
Iteration: 3 || Loss: 53.59926069845752
Iteration: 4 || Loss: 53.33616012281265
Iteration: 5 || Loss: 53.193532412247215
Iteration: 6 || Loss: 52.78803101749272
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.695305
Epoch 52 loss:52.78803101749272
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.233159883545472
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:84.37875269552178
waveform batch: 2/2
Test loss - extrapolation:48.5349990708163
Epoch 52 mean train loss:2.5059346080163754
Epoch 52 mean test loss - interpolation:2.538859980590912
Epoch 52 mean test loss - extrapolation:11.076145980528173
Start training epoch 53
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.695305
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.768519173551701
Iteration: 2 || Loss: 5.767745078184241
Iteration: 3 || Loss: 5.766972589032911
Iteration: 4 || Loss: 5.7661998792436355
Iteration: 5 || Loss: 5.765429943896955
Iteration: 6 || Loss: 5.765429943896955
saving ADAM checkpoint...
Sum of params:81.695305
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.765429943896955
Iteration: 2 || Loss: 5.3831022714759555
Iteration: 3 || Loss: 5.331433166008559
Iteration: 4 || Loss: 5.315392803135507
Iteration: 5 || Loss: 5.191402139330767
Iteration: 6 || Loss: 4.9272448228738455
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.90216
Epoch 53 loss:4.9272448228738455
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.90216
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.22575691568017
Iteration: 2 || Loss: 15.225642305607645
Iteration: 3 || Loss: 15.225526955695939
Iteration: 4 || Loss: 15.225410756458475
Iteration: 5 || Loss: 15.225296252424942
Iteration: 6 || Loss: 15.225296252424942
saving ADAM checkpoint...
Sum of params:81.90224
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.225296252424942
Iteration: 2 || Loss: 15.215036126761749
Iteration: 3 || Loss: 15.18710970753291
Iteration: 4 || Loss: 14.918857919143694
Iteration: 5 || Loss: 14.891147081008658
Iteration: 6 || Loss: 14.790826567246826
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.901794
Epoch 53 loss:14.790826567246826
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.901794
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 53.32032715164091
Iteration: 2 || Loss: 53.320141980117086
Iteration: 3 || Loss: 53.31995720403913
Iteration: 4 || Loss: 53.31976875477652
Iteration: 5 || Loss: 53.31958409581362
Iteration: 6 || Loss: 53.31958409581362
saving ADAM checkpoint...
Sum of params:81.90172
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 53.31958409581362
Iteration: 2 || Loss: 53.28199164450467
Iteration: 3 || Loss: 53.078683764653924
Iteration: 4 || Loss: 52.927427194725375
Iteration: 5 || Loss: 52.768556780290474
Iteration: 6 || Loss: 52.04124345683853
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.7294
Epoch 53 loss:52.04124345683853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.55773359121891
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:85.0595090468354
waveform batch: 2/2
Test loss - extrapolation:47.4477992761149
Epoch 53 mean train loss:2.4744591326537657
Epoch 53 mean test loss - interpolation:2.5929555985364847
Epoch 53 mean test loss - extrapolation:11.042275693579192
Start training epoch 54
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.7294
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.498014833968002
Iteration: 2 || Loss: 8.496001938118273
Iteration: 3 || Loss: 8.493980975898006
Iteration: 4 || Loss: 8.491969488129307
Iteration: 5 || Loss: 8.489960868754585
Iteration: 6 || Loss: 8.489960868754585
saving ADAM checkpoint...
Sum of params:81.72939
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.489960868754585
Iteration: 2 || Loss: 5.845024829227144
Iteration: 3 || Loss: 5.667955686482749
Iteration: 4 || Loss: 5.597803993052055
Iteration: 5 || Loss: 5.569619802110391
Iteration: 6 || Loss: 5.319170298652549
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.78575
Epoch 54 loss:5.319170298652549
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.78575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.24963866973798
Iteration: 2 || Loss: 15.24938344850594
Iteration: 3 || Loss: 15.249130039711517
Iteration: 4 || Loss: 15.248877890196942
Iteration: 5 || Loss: 15.248625672303254
Iteration: 6 || Loss: 15.248625672303254
saving ADAM checkpoint...
Sum of params:81.78578
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.248625672303254
Iteration: 2 || Loss: 15.20127501439911
Iteration: 3 || Loss: 15.159184157060178
Iteration: 4 || Loss: 15.063187660964282
Iteration: 5 || Loss: 14.939742174154137
Iteration: 6 || Loss: 14.891159068321763
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.90329
Epoch 54 loss:14.891159068321763
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.90329
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 52.402683909279396
Iteration: 2 || Loss: 52.40249282934809
Iteration: 3 || Loss: 52.402307012623
Iteration: 4 || Loss: 52.4021181782621
Iteration: 5 || Loss: 52.40192749575349
Iteration: 6 || Loss: 52.40192749575349
saving ADAM checkpoint...
Sum of params:81.90331
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 52.40192749575349
Iteration: 2 || Loss: 52.146522099540874
Iteration: 3 || Loss: 52.14042751305149
Iteration: 4 || Loss: 51.78380207741117
Iteration: 5 || Loss: 51.59370112574681
Iteration: 6 || Loss: 51.45948310283607
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.90071
Epoch 54 loss:51.45948310283607
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.068199905009678
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:82.41642415430721
waveform batch: 2/2
Test loss - extrapolation:46.71844169418193
Epoch 54 mean train loss:2.4713728437865647
Epoch 54 mean test loss - interpolation:2.511366650834946
Epoch 54 mean test loss - extrapolation:10.761238820707428
Start training epoch 55
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.90071
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.693458602880016
Iteration: 2 || Loss: 5.692862822008776
Iteration: 3 || Loss: 5.692266611278713
Iteration: 4 || Loss: 5.6916743609001115
Iteration: 5 || Loss: 5.691081784518785
Iteration: 6 || Loss: 5.691081784518785
saving ADAM checkpoint...
Sum of params:81.90071
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.691081784518785
Iteration: 2 || Loss: 5.472447102265659
Iteration: 3 || Loss: 5.351238549713779
Iteration: 4 || Loss: 5.336298268064903
Iteration: 5 || Loss: 5.245200633914868
Iteration: 6 || Loss: 5.140259855823651
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.096054
Epoch 55 loss:5.140259855823651
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.096054
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.09018162358721
Iteration: 2 || Loss: 15.089996329061284
Iteration: 3 || Loss: 15.089812771115133
Iteration: 4 || Loss: 15.089629865840365
Iteration: 5 || Loss: 15.08944782468943
Iteration: 6 || Loss: 15.08944782468943
saving ADAM checkpoint...
Sum of params:82.09611
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.08944782468943
Iteration: 2 || Loss: 15.074441244710856
Iteration: 3 || Loss: 15.050027507580756
Iteration: 4 || Loss: 14.87093509536674
Iteration: 5 || Loss: 14.851245511763812
Iteration: 6 || Loss: 14.774511624823065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.11546
Epoch 55 loss:14.774511624823065
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.11546
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 52.40816785377618
Iteration: 2 || Loss: 52.4079625415211
Iteration: 3 || Loss: 52.407760602999986
Iteration: 4 || Loss: 52.40755581612842
Iteration: 5 || Loss: 52.40735068768216
Iteration: 6 || Loss: 52.40735068768216
saving ADAM checkpoint...
Sum of params:82.11547
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 52.40735068768216
Iteration: 2 || Loss: 51.99488965185188
Iteration: 3 || Loss: 51.89579982738206
Iteration: 4 || Loss: 51.606484317889766
Iteration: 5 || Loss: 51.43200981634624
Iteration: 6 || Loss: 51.227595695057815
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.12948
Epoch 55 loss:51.227595695057815
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.967421765797718
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:81.76968291258981
waveform batch: 2/2
Test loss - extrapolation:46.42723274324442
Epoch 55 mean train loss:2.4531850750242943
Epoch 55 mean test loss - interpolation:2.4945702942996197
Epoch 55 mean test loss - extrapolation:10.683076304652852
Start training epoch 56
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.12948
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.70662342772763
Iteration: 2 || Loss: 5.7058844892020355
Iteration: 3 || Loss: 5.705147103133023
Iteration: 4 || Loss: 5.70441031088931
Iteration: 5 || Loss: 5.703672237689905
Iteration: 6 || Loss: 5.703672237689905
saving ADAM checkpoint...
Sum of params:82.12948
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.703672237689905
Iteration: 2 || Loss: 5.353939945391598
Iteration: 3 || Loss: 5.297161728652285
Iteration: 4 || Loss: 5.279806960936375
Iteration: 5 || Loss: 5.2354445272468055
Iteration: 6 || Loss: 5.005753535312063
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.35193
Epoch 56 loss:5.005753535312063
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.35193
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.081918871194295
Iteration: 2 || Loss: 15.081763830634904
Iteration: 3 || Loss: 15.081607532368183
Iteration: 4 || Loss: 15.08145320488471
Iteration: 5 || Loss: 15.081297893083216
Iteration: 6 || Loss: 15.081297893083216
saving ADAM checkpoint...
Sum of params:82.35202
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.081297893083216
Iteration: 2 || Loss: 15.061006748780905
Iteration: 3 || Loss: 15.02042458860541
Iteration: 4 || Loss: 14.816550654161654
Iteration: 5 || Loss: 14.761992106128746
Iteration: 6 || Loss: 14.679035376659181
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.34887
Epoch 56 loss:14.679035376659181
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.34887
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 51.91668447862309
Iteration: 2 || Loss: 51.91643942525332
Iteration: 3 || Loss: 51.91619500077947
Iteration: 4 || Loss: 51.91595263312023
Iteration: 5 || Loss: 51.91571056676296
Iteration: 6 || Loss: 51.91571056676296
saving ADAM checkpoint...
Sum of params:82.34893
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 51.91571056676296
Iteration: 2 || Loss: 51.85939407317134
Iteration: 3 || Loss: 51.55459375529761
Iteration: 4 || Loss: 51.43458842584736
Iteration: 5 || Loss: 51.300797197122236
Iteration: 6 || Loss: 50.80350427383006
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.263374
Epoch 56 loss:50.80350427383006
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.0447497288776
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:81.6575029093519
waveform batch: 2/2
Test loss - extrapolation:45.87584475756623
Epoch 56 mean train loss:2.4306307995103897
Epoch 56 mean test loss - interpolation:2.5074582881462666
Epoch 56 mean test loss - extrapolation:10.627778972243178
Start training epoch 57
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.263374
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.63495836775289
Iteration: 2 || Loss: 6.633622371720834
Iteration: 3 || Loss: 6.6322871637581375
Iteration: 4 || Loss: 6.630951537832959
Iteration: 5 || Loss: 6.629617604273138
Iteration: 6 || Loss: 6.629617604273138
saving ADAM checkpoint...
Sum of params:82.26338
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.629617604273138
Iteration: 2 || Loss: 5.4769507892899965
Iteration: 3 || Loss: 5.373050781625499
Iteration: 4 || Loss: 5.34026198377907
Iteration: 5 || Loss: 5.32957449815766
Iteration: 6 || Loss: 5.009049886687872
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.53419
Epoch 57 loss:5.009049886687872
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.53419
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.385449950211488
Iteration: 2 || Loss: 15.38509645523015
Iteration: 3 || Loss: 15.38474560688283
Iteration: 4 || Loss: 15.384396139547729
Iteration: 5 || Loss: 15.384047333015262
Iteration: 6 || Loss: 15.384047333015262
saving ADAM checkpoint...
Sum of params:82.53424
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.384047333015262
Iteration: 2 || Loss: 15.325861008732723
Iteration: 3 || Loss: 15.112597863982579
Iteration: 4 || Loss: 14.875534836313125
Iteration: 5 || Loss: 14.726278998267157
Iteration: 6 || Loss: 14.704776869280147
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.47486
Epoch 57 loss:14.704776869280147
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.47486
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 51.13657472579427
Iteration: 2 || Loss: 51.13633898177812
Iteration: 3 || Loss: 51.1361036472946
Iteration: 4 || Loss: 51.13586938317026
Iteration: 5 || Loss: 51.13563620832383
Iteration: 6 || Loss: 51.13563620832383
saving ADAM checkpoint...
Sum of params:82.47493
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 51.13563620832383
Iteration: 2 || Loss: 51.09472151323637
Iteration: 3 || Loss: 50.91355152834792
Iteration: 4 || Loss: 50.80383710366436
Iteration: 5 || Loss: 50.715042196751035
Iteration: 6 || Loss: 50.21846176392654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.35193
Epoch 57 loss:50.21846176392654
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:15.168008659063698
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:80.8704831122113
waveform batch: 2/2
Test loss - extrapolation:44.680141688399765
Epoch 57 mean train loss:2.4114582248239502
Epoch 57 mean test loss - interpolation:2.528001443177283
Epoch 57 mean test loss - extrapolation:10.46255206671759
Start training epoch 58
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.35193
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.006966741285899
Iteration: 2 || Loss: 7.005546604511344
Iteration: 3 || Loss: 7.0041268968072545
Iteration: 4 || Loss: 7.002707165038714
Iteration: 5 || Loss: 7.001282655749913
Iteration: 6 || Loss: 7.001282655749913
saving ADAM checkpoint...
Sum of params:82.35193
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.001282655749913
Iteration: 2 || Loss: 5.628831466337047
Iteration: 3 || Loss: 5.517171425533958
Iteration: 4 || Loss: 5.455451005743487
Iteration: 5 || Loss: 5.447580513175847
Iteration: 6 || Loss: 5.392023051143057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.440094
Epoch 58 loss:5.392023051143057
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.440094
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.135203173767847
Iteration: 2 || Loss: 15.134965484936101
Iteration: 3 || Loss: 15.13472944730811
Iteration: 4 || Loss: 15.134492656492956
Iteration: 5 || Loss: 15.134257046731513
Iteration: 6 || Loss: 15.134257046731513
saving ADAM checkpoint...
Sum of params:82.44021
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.134257046731513
Iteration: 2 || Loss: 15.088256074886676
Iteration: 3 || Loss: 15.046373311961295
Iteration: 4 || Loss: 14.948728891951024
Iteration: 5 || Loss: 14.80214654466391
Iteration: 6 || Loss: 14.776896100136137
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.55875
Epoch 58 loss:14.776896100136137
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.55875
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.5342290655653
Iteration: 2 || Loss: 50.53402622676806
Iteration: 3 || Loss: 50.533830271390066
Iteration: 4 || Loss: 50.53363264445216
Iteration: 5 || Loss: 50.53343451805232
Iteration: 6 || Loss: 50.53343451805232
saving ADAM checkpoint...
Sum of params:82.558716
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.53343451805232
Iteration: 2 || Loss: 50.44720043548799
Iteration: 3 || Loss: 50.227443649579094
Iteration: 4 || Loss: 49.95329840051434
Iteration: 5 || Loss: 49.84512334121531
Iteration: 6 || Loss: 49.66427800418752
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.61277
Epoch 58 loss:49.66427800418752
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.792521826806889
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:78.98078554020182
waveform batch: 2/2
Test loss - extrapolation:43.91240518102765
Epoch 58 mean train loss:2.4080412812229905
Epoch 58 mean test loss - interpolation:2.465420304467815
Epoch 58 mean test loss - extrapolation:10.241099226769123
Start training epoch 59
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.61277
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.997312370935481
Iteration: 2 || Loss: 5.9964235667039905
Iteration: 3 || Loss: 5.995535422118875
Iteration: 4 || Loss: 5.99464908184837
Iteration: 5 || Loss: 5.993763468289925
Iteration: 6 || Loss: 5.993763468289925
saving ADAM checkpoint...
Sum of params:82.61277
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.993763468289925
Iteration: 2 || Loss: 5.500674423126769
Iteration: 3 || Loss: 5.423218846095027
Iteration: 4 || Loss: 5.396376567351286
Iteration: 5 || Loss: 5.373745188771016
Iteration: 6 || Loss: 5.223838117849799
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.75572
Epoch 59 loss:5.223838117849799
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.75572
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.955002352414658
Iteration: 2 || Loss: 14.954947668300221
Iteration: 3 || Loss: 14.954892712515091
Iteration: 4 || Loss: 14.954837845437392
Iteration: 5 || Loss: 14.95478375736032
Iteration: 6 || Loss: 14.95478375736032
saving ADAM checkpoint...
Sum of params:82.75577
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.95478375736032
Iteration: 2 || Loss: 14.953781881587922
Iteration: 3 || Loss: 14.937919203864514
Iteration: 4 || Loss: 14.774083688201477
Iteration: 5 || Loss: 14.693703601439447
Iteration: 6 || Loss: 14.510324779678166
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.804855
Epoch 59 loss:14.510324779678166
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.804855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.74930610507664
Iteration: 2 || Loss: 50.7490407742966
Iteration: 3 || Loss: 50.74877754262332
Iteration: 4 || Loss: 50.74851322975752
Iteration: 5 || Loss: 50.74824834913843
Iteration: 6 || Loss: 50.74824834913843
saving ADAM checkpoint...
Sum of params:82.80485
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.74824834913843
Iteration: 2 || Loss: 50.69640209142403
Iteration: 3 || Loss: 50.36382930484834
Iteration: 4 || Loss: 50.148616494694465
Iteration: 5 || Loss: 49.981373844644665
Iteration: 6 || Loss: 49.801111592006706
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.78088
Epoch 59 loss:49.801111592006706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.631432291623303
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:79.04403447788744
waveform batch: 2/2
Test loss - extrapolation:44.464225927449206
Epoch 59 mean train loss:2.3977680858460233
Epoch 59 mean test loss - interpolation:2.4385720486038838
Epoch 59 mean test loss - extrapolation:10.292355033778053
Start training epoch 60
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.78088
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.373538006047018
Iteration: 2 || Loss: 5.373008074393046
Iteration: 3 || Loss: 5.372477270066616
Iteration: 4 || Loss: 5.3719479958800935
Iteration: 5 || Loss: 5.37142058428965
Iteration: 6 || Loss: 5.37142058428965
saving ADAM checkpoint...
Sum of params:82.7809
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.37142058428965
Iteration: 2 || Loss: 5.199671663560331
Iteration: 3 || Loss: 5.0334979172457235
Iteration: 4 || Loss: 5.022841486209594
Iteration: 5 || Loss: 4.88744730728017
Iteration: 6 || Loss: 4.848289423822258
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.97361
Epoch 60 loss:4.848289423822258
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.97361
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.685277516229892
Iteration: 2 || Loss: 14.685156032473666
Iteration: 3 || Loss: 14.685034425601708
Iteration: 4 || Loss: 14.68491352275497
Iteration: 5 || Loss: 14.684794924031642
Iteration: 6 || Loss: 14.684794924031642
saving ADAM checkpoint...
Sum of params:82.973656
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.684794924031642
Iteration: 2 || Loss: 14.679865624744059
Iteration: 3 || Loss: 14.647243741014085
Iteration: 4 || Loss: 14.442033122595832
Iteration: 5 || Loss: 14.42384747288329
Iteration: 6 || Loss: 14.358993513736738
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.953995
Epoch 60 loss:14.358993513736738
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.953995
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.78949530875772
Iteration: 2 || Loss: 50.789283313518546
Iteration: 3 || Loss: 50.789071266636874
Iteration: 4 || Loss: 50.788863829807276
Iteration: 5 || Loss: 50.78865663658548
Iteration: 6 || Loss: 50.78865663658548
saving ADAM checkpoint...
Sum of params:82.95397
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.78865663658548
Iteration: 2 || Loss: 50.47124568372101
Iteration: 3 || Loss: 50.34495911502073
Iteration: 4 || Loss: 50.095628532061994
Iteration: 5 || Loss: 49.95022436329274
Iteration: 6 || Loss: 49.61636412052758
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.87817
Epoch 60 loss:49.61636412052758
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.621392388930914
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:78.72504901180854
waveform batch: 2/2
Test loss - extrapolation:44.1206069894425
Epoch 60 mean train loss:2.373229208899537
Epoch 60 mean test loss - interpolation:2.4368987314884856
Epoch 60 mean test loss - extrapolation:10.237138000104252
Start training epoch 61
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.87817
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.637354626717543
Iteration: 2 || Loss: 5.63645024918008
Iteration: 3 || Loss: 5.6355466999066
Iteration: 4 || Loss: 5.634642132640255
Iteration: 5 || Loss: 5.63374029284079
Iteration: 6 || Loss: 5.63374029284079
saving ADAM checkpoint...
Sum of params:82.878174
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.63374029284079
Iteration: 2 || Loss: 5.117050017691251
Iteration: 3 || Loss: 5.050100226764707
Iteration: 4 || Loss: 5.026188897355412
Iteration: 5 || Loss: 5.0109827557573725
Iteration: 6 || Loss: 4.828020289890042
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.024704
Epoch 61 loss:4.828020289890042
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.024704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.505225317158645
Iteration: 2 || Loss: 14.50512617062292
Iteration: 3 || Loss: 14.50502680349798
Iteration: 4 || Loss: 14.504929243353834
Iteration: 5 || Loss: 14.50483177382758
Iteration: 6 || Loss: 14.50483177382758
saving ADAM checkpoint...
Sum of params:83.024796
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.50483177382758
Iteration: 2 || Loss: 14.499481118232584
Iteration: 3 || Loss: 14.48832062784731
Iteration: 4 || Loss: 14.382213219078215
Iteration: 5 || Loss: 14.357404750261555
Iteration: 6 || Loss: 14.295174588470436
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.08003
Epoch 61 loss:14.295174588470436
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.08003
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.56898542128172
Iteration: 2 || Loss: 50.56872865562157
Iteration: 3 || Loss: 50.568473781795994
Iteration: 4 || Loss: 50.56821790339416
Iteration: 5 || Loss: 50.567958542090544
Iteration: 6 || Loss: 50.567958542090544
saving ADAM checkpoint...
Sum of params:83.080055
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.567958542090544
Iteration: 2 || Loss: 50.47021663222821
Iteration: 3 || Loss: 50.05002518901215
Iteration: 4 || Loss: 49.846426996345734
Iteration: 5 || Loss: 49.70054752120913
Iteration: 6 || Loss: 49.51105762004084
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.948
Epoch 61 loss:49.51105762004084
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.546252313010456
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:78.57909511227294
waveform batch: 2/2
Test loss - extrapolation:44.06663733771515
Epoch 61 mean train loss:2.3666983620138384
Epoch 61 mean test loss - interpolation:2.4243753855017425
Epoch 61 mean test loss - extrapolation:10.220477704165674
Start training epoch 62
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.948
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.669840911764639
Iteration: 2 || Loss: 5.668829548665961
Iteration: 3 || Loss: 5.667816672500441
Iteration: 4 || Loss: 5.666808098474617
Iteration: 5 || Loss: 5.665799604791763
Iteration: 6 || Loss: 5.665799604791763
saving ADAM checkpoint...
Sum of params:82.94801
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.665799604791763
Iteration: 2 || Loss: 5.021264209783171
Iteration: 3 || Loss: 4.9372014242283
Iteration: 4 || Loss: 4.900922615472405
Iteration: 5 || Loss: 4.89154431801494
Iteration: 6 || Loss: 4.807134721562228
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.0504
Epoch 62 loss:4.807134721562228
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.0504
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.379976662611199
Iteration: 2 || Loss: 14.37991606491926
Iteration: 3 || Loss: 14.379856748341892
Iteration: 4 || Loss: 14.379799626774775
Iteration: 5 || Loss: 14.379742117494683
Iteration: 6 || Loss: 14.379742117494683
saving ADAM checkpoint...
Sum of params:83.050446
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.379742117494683
Iteration: 2 || Loss: 14.378210592104473
Iteration: 3 || Loss: 14.349438447816661
Iteration: 4 || Loss: 14.286118896353933
Iteration: 5 || Loss: 14.248063337002375
Iteration: 6 || Loss: 14.1350287226912
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.08623
Epoch 62 loss:14.1350287226912
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.08623
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.94151289538694
Iteration: 2 || Loss: 50.94115439594814
Iteration: 3 || Loss: 50.94079481892674
Iteration: 4 || Loss: 50.940438332436855
Iteration: 5 || Loss: 50.940083921119786
Iteration: 6 || Loss: 50.940083921119786
saving ADAM checkpoint...
Sum of params:83.08629
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.940083921119786
Iteration: 2 || Loss: 50.833171731464724
Iteration: 3 || Loss: 50.405450758146
Iteration: 4 || Loss: 50.26197268737692
Iteration: 5 || Loss: 49.8970443277002
Iteration: 6 || Loss: 49.85140161417915
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.02142
Epoch 62 loss:49.85140161417915
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.364127403233706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:78.70635254167591
waveform batch: 2/2
Test loss - extrapolation:44.73838315731179
Epoch 62 mean train loss:2.3721918985666406
Epoch 62 mean test loss - interpolation:2.394021233872284
Epoch 62 mean test loss - extrapolation:10.287061308248974
Start training epoch 63
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.02142
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.0902226945422955
Iteration: 2 || Loss: 5.089664846232246
Iteration: 3 || Loss: 5.0891094156504515
Iteration: 4 || Loss: 5.088552558684663
Iteration: 5 || Loss: 5.087996598521809
Iteration: 6 || Loss: 5.087996598521809
saving ADAM checkpoint...
Sum of params:83.02144
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.087996598521809
Iteration: 2 || Loss: 4.904011971993571
Iteration: 3 || Loss: 4.709035252157488
Iteration: 4 || Loss: 4.670167218127221
Iteration: 5 || Loss: 4.5496816638445425
Iteration: 6 || Loss: 4.525317919022892
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.16974
Epoch 63 loss:4.525317919022892
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.16974
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.128742531330388
Iteration: 2 || Loss: 14.128557738500684
Iteration: 3 || Loss: 14.128374479751823
Iteration: 4 || Loss: 14.128190964542203
Iteration: 5 || Loss: 14.128008441572856
Iteration: 6 || Loss: 14.128008441572856
saving ADAM checkpoint...
Sum of params:83.16977
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.128008441572856
Iteration: 2 || Loss: 14.111883451563449
Iteration: 3 || Loss: 14.066971778477305
Iteration: 4 || Loss: 14.027314330147412
Iteration: 5 || Loss: 14.00200179287066
Iteration: 6 || Loss: 13.966441635537045
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.0881
Epoch 63 loss:13.966441635537045
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.0881
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.50546425263268
Iteration: 2 || Loss: 50.505287750396015
Iteration: 3 || Loss: 50.505113752059444
Iteration: 4 || Loss: 50.504939825388384
Iteration: 5 || Loss: 50.50476553850641
Iteration: 6 || Loss: 50.50476553850641
saving ADAM checkpoint...
Sum of params:83.08803
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.50476553850641
Iteration: 2 || Loss: 50.24751494076414
Iteration: 3 || Loss: 50.138283663203985
Iteration: 4 || Loss: 49.92372584224951
Iteration: 5 || Loss: 49.75109957668793
Iteration: 6 || Loss: 49.2721271853951
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.07402
Epoch 63 loss:49.2721271853951
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.419580252701657
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:77.41072060593288
waveform batch: 2/2
Test loss - extrapolation:43.28496158555516
Epoch 63 mean train loss:2.336685749653622
Epoch 63 mean test loss - interpolation:2.403263375450276
Epoch 63 mean test loss - extrapolation:10.057973515957336
Start training epoch 64
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.07402
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.357274125803023
Iteration: 2 || Loss: 5.356510258941122
Iteration: 3 || Loss: 5.355746581820327
Iteration: 4 || Loss: 5.354984848284052
Iteration: 5 || Loss: 5.354223574431144
Iteration: 6 || Loss: 5.354223574431144
saving ADAM checkpoint...
Sum of params:83.074036
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.354223574431144
Iteration: 2 || Loss: 4.992774782025081
Iteration: 3 || Loss: 4.94547966482876
Iteration: 4 || Loss: 4.926871028690115
Iteration: 5 || Loss: 4.766837324118727
Iteration: 6 || Loss: 4.605733320219911
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.19824
Epoch 64 loss:4.605733320219911
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.19824
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.160308413638985
Iteration: 2 || Loss: 14.160166635849363
Iteration: 3 || Loss: 14.160024427428604
Iteration: 4 || Loss: 14.159883689542752
Iteration: 5 || Loss: 14.159743755543037
Iteration: 6 || Loss: 14.159743755543037
saving ADAM checkpoint...
Sum of params:83.1983
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.159743755543037
Iteration: 2 || Loss: 14.148677927363856
Iteration: 3 || Loss: 14.129582213998308
Iteration: 4 || Loss: 14.07116276233702
Iteration: 5 || Loss: 14.049916761150609
Iteration: 6 || Loss: 13.995178732131528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.188705
Epoch 64 loss:13.995178732131528
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.188705
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.80605479238999
Iteration: 2 || Loss: 49.805893030528004
Iteration: 3 || Loss: 49.80573554139134
Iteration: 4 || Loss: 49.8055803592795
Iteration: 5 || Loss: 49.80542198370086
Iteration: 6 || Loss: 49.80542198370086
saving ADAM checkpoint...
Sum of params:83.188705
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.80542198370086
Iteration: 2 || Loss: 49.57502674823858
Iteration: 3 || Loss: 49.57098348037649
Iteration: 4 || Loss: 49.34239085506378
Iteration: 5 || Loss: 49.221303943196794
Iteration: 6 || Loss: 48.70864539147101
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.8576
Epoch 64 loss:48.70864539147101
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.216677487391223
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:78.62601994090328
waveform batch: 2/2
Test loss - extrapolation:43.81594071496396
Epoch 64 mean train loss:2.3210192222007744
Epoch 64 mean test loss - interpolation:2.369446247898537
Epoch 64 mean test loss - extrapolation:10.203496721322269
Start training epoch 65
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.8576
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.230727496384692
Iteration: 2 || Loss: 6.229443909991398
Iteration: 3 || Loss: 6.22815885983355
Iteration: 4 || Loss: 6.226876905272939
Iteration: 5 || Loss: 6.225596375358855
Iteration: 6 || Loss: 6.225596375358855
saving ADAM checkpoint...
Sum of params:82.85761
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.225596375358855
Iteration: 2 || Loss: 5.16875412837218
Iteration: 3 || Loss: 5.096560126769965
Iteration: 4 || Loss: 5.051970656515873
Iteration: 5 || Loss: 5.021174085044811
Iteration: 6 || Loss: 4.858419950455701
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.93233
Epoch 65 loss:4.858419950455701
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.93233
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.366911277353937
Iteration: 2 || Loss: 14.366739080676997
Iteration: 3 || Loss: 14.36656786261547
Iteration: 4 || Loss: 14.366395410514386
Iteration: 5 || Loss: 14.366225622496456
Iteration: 6 || Loss: 14.366225622496456
saving ADAM checkpoint...
Sum of params:82.932365
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.366225622496456
Iteration: 2 || Loss: 14.34786866055557
Iteration: 3 || Loss: 14.323639763700584
Iteration: 4 || Loss: 14.156898864246987
Iteration: 5 || Loss: 14.120567772143298
Iteration: 6 || Loss: 14.050660645411714
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.077324
Epoch 65 loss:14.050660645411714
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.077324
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.396417367009036
Iteration: 2 || Loss: 49.39620599516082
Iteration: 3 || Loss: 49.39599755478413
Iteration: 4 || Loss: 49.395788040069355
Iteration: 5 || Loss: 49.39557657097291
Iteration: 6 || Loss: 49.39557657097291
saving ADAM checkpoint...
Sum of params:83.077354
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.39557657097291
Iteration: 2 || Loss: 49.15005937029442
Iteration: 3 || Loss: 48.81155533227366
Iteration: 4 || Loss: 48.57360972663859
Iteration: 5 || Loss: 48.4619037510932
Iteration: 6 || Loss: 48.31719110523443
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.095955
Epoch 65 loss:48.31719110523443
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.243619155961627
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:76.33844267401548
waveform batch: 2/2
Test loss - extrapolation:42.32797057334697
Epoch 65 mean train loss:2.318147300037994
Epoch 65 mean test loss - interpolation:2.3739365259936043
Epoch 65 mean test loss - extrapolation:9.888867770613537
Start training epoch 66
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.095955
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.4846668128662674
Iteration: 2 || Loss: 5.483838615033316
Iteration: 3 || Loss: 5.483013852908489
Iteration: 4 || Loss: 5.482185045397494
Iteration: 5 || Loss: 5.481358666806782
Iteration: 6 || Loss: 5.481358666806782
saving ADAM checkpoint...
Sum of params:83.095955
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.481358666806782
Iteration: 2 || Loss: 5.06002131039335
Iteration: 3 || Loss: 4.9661102368920425
Iteration: 4 || Loss: 4.9436857114593
Iteration: 5 || Loss: 4.905221196105698
Iteration: 6 || Loss: 4.729636853659622
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.284195
Epoch 66 loss:4.729636853659622
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.284195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.350928633190245
Iteration: 2 || Loss: 14.350841420722366
Iteration: 3 || Loss: 14.350754372451034
Iteration: 4 || Loss: 14.350667957669668
Iteration: 5 || Loss: 14.350581069609781
Iteration: 6 || Loss: 14.350581069609781
saving ADAM checkpoint...
Sum of params:83.28415
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.350581069609781
Iteration: 2 || Loss: 14.330425490531804
Iteration: 3 || Loss: 14.284020722849332
Iteration: 4 || Loss: 14.067526183203643
Iteration: 5 || Loss: 14.022719037496104
Iteration: 6 || Loss: 13.968293204102032
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.28171
Epoch 66 loss:13.968293204102032
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.28171
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.27251163519553
Iteration: 2 || Loss: 49.272319104220394
Iteration: 3 || Loss: 49.27212689658403
Iteration: 4 || Loss: 49.27193708394534
Iteration: 5 || Loss: 49.27174486934989
Iteration: 6 || Loss: 49.27174486934989
saving ADAM checkpoint...
Sum of params:83.28174
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.27174486934989
Iteration: 2 || Loss: 49.10263454683306
Iteration: 3 || Loss: 49.09164682308301
Iteration: 4 || Loss: 48.6552079966527
Iteration: 5 || Loss: 48.491299146730114
Iteration: 6 || Loss: 48.29601596582845
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.176796
Epoch 66 loss:48.29601596582845
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.135290164724706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:76.20976641612481
waveform batch: 2/2
Test loss - extrapolation:42.44672667461692
Epoch 66 mean train loss:2.310136069778969
Epoch 66 mean test loss - interpolation:2.3558816941207845
Epoch 66 mean test loss - extrapolation:9.888041090895143
Start training epoch 67
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.176796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.09016430798743
Iteration: 2 || Loss: 5.0895857698619125
Iteration: 3 || Loss: 5.08900740774637
Iteration: 4 || Loss: 5.0884308726717595
Iteration: 5 || Loss: 5.0878538692021955
Iteration: 6 || Loss: 5.0878538692021955
saving ADAM checkpoint...
Sum of params:83.1768
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.0878538692021955
Iteration: 2 || Loss: 4.886524296429142
Iteration: 3 || Loss: 4.800429223677157
Iteration: 4 || Loss: 4.788640344756495
Iteration: 5 || Loss: 4.687490503106235
Iteration: 6 || Loss: 4.617538113744999
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.35485
Epoch 67 loss:4.617538113744999
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.35485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.19372738371876
Iteration: 2 || Loss: 14.193520496089292
Iteration: 3 || Loss: 14.193315270264776
Iteration: 4 || Loss: 14.19311041539157
Iteration: 5 || Loss: 14.192906466904697
Iteration: 6 || Loss: 14.192906466904697
saving ADAM checkpoint...
Sum of params:83.35488
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.192906466904697
Iteration: 2 || Loss: 14.174720218340493
Iteration: 3 || Loss: 14.132504781487128
Iteration: 4 || Loss: 13.9797085060672
Iteration: 5 || Loss: 13.95934132799707
Iteration: 6 || Loss: 13.907299898221028
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.35522
Epoch 67 loss:13.907299898221028
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.35522
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.06173500686978
Iteration: 2 || Loss: 49.06155352866494
Iteration: 3 || Loss: 49.061371903871496
Iteration: 4 || Loss: 49.061191239691276
Iteration: 5 || Loss: 49.061008064596955
Iteration: 6 || Loss: 49.061008064596955
saving ADAM checkpoint...
Sum of params:83.35528
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.061008064596955
Iteration: 2 || Loss: 48.86051069732431
Iteration: 3 || Loss: 48.63574737421437
Iteration: 4 || Loss: 48.425317656349144
Iteration: 5 || Loss: 48.29325924232801
Iteration: 6 || Loss: 47.954644383113525
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.22388
Epoch 67 loss:47.954644383113525
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.213200785480968
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:76.02676057604485
waveform batch: 2/2
Test loss - extrapolation:41.93593406342097
Epoch 67 mean train loss:2.2923959446579154
Epoch 67 mean test loss - interpolation:2.368866797580161
Epoch 67 mean test loss - extrapolation:9.830224553288819
Start training epoch 68
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.22388
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.796763595619895
Iteration: 2 || Loss: 5.795686573187143
Iteration: 3 || Loss: 5.79461168843221
Iteration: 4 || Loss: 5.793535016751125
Iteration: 5 || Loss: 5.792459176467238
Iteration: 6 || Loss: 5.792459176467238
saving ADAM checkpoint...
Sum of params:83.22388
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.792459176467238
Iteration: 2 || Loss: 5.0603836159252085
Iteration: 3 || Loss: 4.997048557508808
Iteration: 4 || Loss: 4.97160061476333
Iteration: 5 || Loss: 4.936013392238022
Iteration: 6 || Loss: 4.697793763059287
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.436195
Epoch 68 loss:4.697793763059287
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.436195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.422531638875837
Iteration: 2 || Loss: 14.42200141781556
Iteration: 3 || Loss: 14.421468430718567
Iteration: 4 || Loss: 14.42093902243786
Iteration: 5 || Loss: 14.420411115531044
Iteration: 6 || Loss: 14.420411115531044
saving ADAM checkpoint...
Sum of params:83.436226
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.420411115531044
Iteration: 2 || Loss: 14.268654718412076
Iteration: 3 || Loss: 14.088121356497338
Iteration: 4 || Loss: 14.068401607691793
Iteration: 5 || Loss: 13.984156114538372
Iteration: 6 || Loss: 13.978026281078929
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.39463
Epoch 68 loss:13.978026281078929
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.39463
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 48.3833349013342
Iteration: 2 || Loss: 48.38302438331134
Iteration: 3 || Loss: 48.38271829502928
Iteration: 4 || Loss: 48.3824130082763
Iteration: 5 || Loss: 48.382105221038415
Iteration: 6 || Loss: 48.382105221038415
saving ADAM checkpoint...
Sum of params:83.394714
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 48.382105221038415
Iteration: 2 || Loss: 48.31394389655405
Iteration: 3 || Loss: 48.01421226556739
Iteration: 4 || Loss: 47.93401096670643
Iteration: 5 || Loss: 47.865051151096615
Iteration: 6 || Loss: 47.48051376179871
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.24551
Epoch 68 loss:47.48051376179871
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.526959827871003
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:75.09370131254174
waveform batch: 2/2
Test loss - extrapolation:40.29193955348348
Epoch 68 mean train loss:2.281252889859894
Epoch 68 mean test loss - interpolation:2.421159971311834
Epoch 68 mean test loss - extrapolation:9.615470072168769
Start training epoch 69
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.24551
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.739675329454139
Iteration: 2 || Loss: 6.738230319600347
Iteration: 3 || Loss: 6.73678778245325
Iteration: 4 || Loss: 6.735344178119211
Iteration: 5 || Loss: 6.733899603571539
Iteration: 6 || Loss: 6.733899603571539
saving ADAM checkpoint...
Sum of params:83.24552
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.733899603571539
Iteration: 2 || Loss: 5.3803164980102745
Iteration: 3 || Loss: 5.24952594797549
Iteration: 4 || Loss: 5.155797741702235
Iteration: 5 || Loss: 5.1471197273281915
Iteration: 6 || Loss: 5.114805126816024
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.34421
Epoch 69 loss:5.114805126816024
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.34421
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.406425730637674
Iteration: 2 || Loss: 14.406131714466923
Iteration: 3 || Loss: 14.40583785156933
Iteration: 4 || Loss: 14.405546308413806
Iteration: 5 || Loss: 14.40525503350277
Iteration: 6 || Loss: 14.40525503350277
saving ADAM checkpoint...
Sum of params:83.3443
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.40525503350277
Iteration: 2 || Loss: 14.342064879672582
Iteration: 3 || Loss: 14.322934901794273
Iteration: 4 || Loss: 14.241488738101072
Iteration: 5 || Loss: 14.139338112487655
Iteration: 6 || Loss: 14.089465110305504
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.43853
Epoch 69 loss:14.089465110305504
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.43853
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 48.02816902327061
Iteration: 2 || Loss: 48.02792694251748
Iteration: 3 || Loss: 48.02768267075561
Iteration: 4 || Loss: 48.027442813225136
Iteration: 5 || Loss: 48.02720103496084
Iteration: 6 || Loss: 48.02720103496084
saving ADAM checkpoint...
Sum of params:83.43853
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 48.02720103496084
Iteration: 2 || Loss: 47.941318320412485
Iteration: 3 || Loss: 47.58188044396135
Iteration: 4 || Loss: 47.319915927654826
Iteration: 5 || Loss: 47.22195525356218
Iteration: 6 || Loss: 47.107055348339934
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.36755
Epoch 69 loss:47.107055348339934
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.127448243537927
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:74.47016237928398
waveform batch: 2/2
Test loss - extrapolation:40.470754925610144
Epoch 69 mean train loss:2.28659743398143
Epoch 69 mean test loss - interpolation:2.354574707256321
Epoch 69 mean test loss - extrapolation:9.578409775407843
Start training epoch 70
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.36755
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.945401148145795
Iteration: 2 || Loss: 5.944323667929153
Iteration: 3 || Loss: 5.943245230695297
Iteration: 4 || Loss: 5.942168796572452
Iteration: 5 || Loss: 5.9410909210562695
Iteration: 6 || Loss: 5.9410909210562695
saving ADAM checkpoint...
Sum of params:83.367546
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.9410909210562695
Iteration: 2 || Loss: 5.225613850754173
Iteration: 3 || Loss: 5.12946356304873
Iteration: 4 || Loss: 5.083336191178584
Iteration: 5 || Loss: 5.071496185383067
Iteration: 6 || Loss: 4.983353619316749
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.48515
Epoch 70 loss:4.983353619316749
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.48515
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.182048834239522
Iteration: 2 || Loss: 14.181931445495943
Iteration: 3 || Loss: 14.181816823381068
Iteration: 4 || Loss: 14.181699864943809
Iteration: 5 || Loss: 14.181584427195409
Iteration: 6 || Loss: 14.181584427195409
saving ADAM checkpoint...
Sum of params:83.48525
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.181584427195409
Iteration: 2 || Loss: 14.175187221612562
Iteration: 3 || Loss: 14.162195389603982
Iteration: 4 || Loss: 14.058115830721745
Iteration: 5 || Loss: 14.035960193288117
Iteration: 6 || Loss: 13.917764544480352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.606415
Epoch 70 loss:13.917764544480352
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.606415
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 48.57144447086534
Iteration: 2 || Loss: 48.57113623002393
Iteration: 3 || Loss: 48.570825851972984
Iteration: 4 || Loss: 48.57051873538952
Iteration: 5 || Loss: 48.570209989992804
Iteration: 6 || Loss: 48.570209989992804
saving ADAM checkpoint...
Sum of params:83.606445
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 48.570209989992804
Iteration: 2 || Loss: 48.49259365052613
Iteration: 3 || Loss: 48.21415934984058
Iteration: 4 || Loss: 47.76776047158408
Iteration: 5 || Loss: 47.46397481866637
Iteration: 6 || Loss: 47.42075548996295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.485
Epoch 70 loss:47.42075548996295
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.990276534621792
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:74.11715088206523
waveform batch: 2/2
Test loss - extrapolation:40.7095948098541
Epoch 70 mean train loss:2.2869611604744846
Epoch 70 mean test loss - interpolation:2.331712755770299
Epoch 70 mean test loss - extrapolation:9.568895474326611
Start training epoch 71
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.096841670613866
Iteration: 2 || Loss: 5.096276999769705
Iteration: 3 || Loss: 5.09571244875116
Iteration: 4 || Loss: 5.095150163387283
Iteration: 5 || Loss: 5.09458769681873
Iteration: 6 || Loss: 5.09458769681873
saving ADAM checkpoint...
Sum of params:83.48501
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.09458769681873
Iteration: 2 || Loss: 4.907532060258594
Iteration: 3 || Loss: 4.736573781075888
Iteration: 4 || Loss: 4.719096015243478
Iteration: 5 || Loss: 4.597153147368558
Iteration: 6 || Loss: 4.588816921576103
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.63701
Epoch 71 loss:4.588816921576103
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.63701
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.91193695280646
Iteration: 2 || Loss: 13.911762335664525
Iteration: 3 || Loss: 13.911590315868802
Iteration: 4 || Loss: 13.911419693760443
Iteration: 5 || Loss: 13.911250400392126
Iteration: 6 || Loss: 13.911250400392126
saving ADAM checkpoint...
Sum of params:83.63706
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.911250400392126
Iteration: 2 || Loss: 13.897348955914694
Iteration: 3 || Loss: 13.88013793500299
Iteration: 4 || Loss: 13.820385374640846
Iteration: 5 || Loss: 13.803075199792241
Iteration: 6 || Loss: 13.761919033653657
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.62902
Epoch 71 loss:13.761919033653657
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.62902
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 48.288305512232434
Iteration: 2 || Loss: 48.28811203575016
Iteration: 3 || Loss: 48.28791753152309
Iteration: 4 || Loss: 48.287726104992466
Iteration: 5 || Loss: 48.28753541603796
Iteration: 6 || Loss: 48.28753541603796
saving ADAM checkpoint...
Sum of params:83.62906
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 48.28753541603796
Iteration: 2 || Loss: 48.03036396069543
Iteration: 3 || Loss: 47.79024233534471
Iteration: 4 || Loss: 47.57478094121094
Iteration: 5 || Loss: 47.48102756724192
Iteration: 6 || Loss: 47.17181124815875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.53844
Epoch 71 loss:47.17181124815875
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.994713845371445
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:74.27512098033269
waveform batch: 2/2
Test loss - extrapolation:40.6709175962019
Epoch 71 mean train loss:2.25939817942719
Epoch 71 mean test loss - interpolation:2.3324523075619075
Epoch 71 mean test loss - extrapolation:9.57883654804455
Start training epoch 72
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.53844
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.549064308294032
Iteration: 2 || Loss: 5.54807624459979
Iteration: 3 || Loss: 5.5470875784970435
Iteration: 4 || Loss: 5.5460998621225315
Iteration: 5 || Loss: 5.545112829849199
Iteration: 6 || Loss: 5.545112829849199
saving ADAM checkpoint...
Sum of params:83.538445
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.545112829849199
Iteration: 2 || Loss: 4.945747688810141
Iteration: 3 || Loss: 4.883462730984477
Iteration: 4 || Loss: 4.86046211456819
Iteration: 5 || Loss: 4.840718047445417
Iteration: 6 || Loss: 4.592787332715649
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.671455
Epoch 72 loss:4.592787332715649
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.671455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.9924866733262
Iteration: 2 || Loss: 13.992392953483174
Iteration: 3 || Loss: 13.99229941394033
Iteration: 4 || Loss: 13.992206580511018
Iteration: 5 || Loss: 13.992113768226826
Iteration: 6 || Loss: 13.992113768226826
saving ADAM checkpoint...
Sum of params:83.6716
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.992113768226826
Iteration: 2 || Loss: 13.987396330590688
Iteration: 3 || Loss: 13.960548800929605
Iteration: 4 || Loss: 13.815766608775856
Iteration: 5 || Loss: 13.801102462117765
Iteration: 6 || Loss: 13.734125303676743
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.753784
Epoch 72 loss:13.734125303676743
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.753784
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 48.00350609282998
Iteration: 2 || Loss: 48.00327793393338
Iteration: 3 || Loss: 48.003047782502
Iteration: 4 || Loss: 48.00282288870325
Iteration: 5 || Loss: 48.002599146710786
Iteration: 6 || Loss: 48.002599146710786
saving ADAM checkpoint...
Sum of params:83.75382
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 48.002599146710786
Iteration: 2 || Loss: 47.907385838211596
Iteration: 3 || Loss: 47.621269961032105
Iteration: 4 || Loss: 47.41606010228841
Iteration: 5 || Loss: 47.27580588167264
Iteration: 6 || Loss: 47.06478365261972
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.573616
Epoch 72 loss:47.06478365261972
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.898515645041913
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:74.28701764158482
waveform batch: 2/2
Test loss - extrapolation:40.763245008484034
Epoch 72 mean train loss:2.2548860789314524
Epoch 72 mean test loss - interpolation:2.316419274173652
Epoch 72 mean test loss - extrapolation:9.587521887505739
Start training epoch 73
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.573616
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.574115327218851
Iteration: 2 || Loss: 5.5730213121358165
Iteration: 3 || Loss: 5.5719236463096955
Iteration: 4 || Loss: 5.570828302246492
Iteration: 5 || Loss: 5.569733047414518
Iteration: 6 || Loss: 5.569733047414518
saving ADAM checkpoint...
Sum of params:83.57362
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.569733047414518
Iteration: 2 || Loss: 4.828354137166527
Iteration: 3 || Loss: 4.75290420687113
Iteration: 4 || Loss: 4.7192377066724
Iteration: 5 || Loss: 4.7112801414484355
Iteration: 6 || Loss: 4.603626136478684
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.66335
Epoch 73 loss:4.603626136478684
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.66335
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.87211092664591
Iteration: 2 || Loss: 13.872027024258948
Iteration: 3 || Loss: 13.87194378744191
Iteration: 4 || Loss: 13.871860403774392
Iteration: 5 || Loss: 13.871778968327595
Iteration: 6 || Loss: 13.871778968327595
saving ADAM checkpoint...
Sum of params:83.66339
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.871778968327595
Iteration: 2 || Loss: 13.867900464453784
Iteration: 3 || Loss: 13.857031987700601
Iteration: 4 || Loss: 13.759310602284572
Iteration: 5 || Loss: 13.724962950966454
Iteration: 6 || Loss: 13.697499003088305
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.74659
Epoch 73 loss:13.697499003088305
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.74659
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 47.81624331270133
Iteration: 2 || Loss: 47.816047486803605
Iteration: 3 || Loss: 47.81585160988029
Iteration: 4 || Loss: 47.81565562322715
Iteration: 5 || Loss: 47.815460193129645
Iteration: 6 || Loss: 47.815460193129645
saving ADAM checkpoint...
Sum of params:83.746635
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 47.815460193129645
Iteration: 2 || Loss: 47.535773623922566
Iteration: 3 || Loss: 47.53044078847708
Iteration: 4 || Loss: 47.21771391842308
Iteration: 5 || Loss: 47.091892476945404
Iteration: 6 || Loss: 46.868220640390525
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.61634
Epoch 73 loss:46.868220640390525
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.832245166185231
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:73.5328267011129
waveform batch: 2/2
Test loss - extrapolation:40.27464823193624
Epoch 73 mean train loss:2.247218819998535
Epoch 73 mean test loss - interpolation:2.305374194364205
Epoch 73 mean test loss - extrapolation:9.483956244420762
Start training epoch 74
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.61634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.133986958354702
Iteration: 2 || Loss: 5.133292211651285
Iteration: 3 || Loss: 5.132599259745503
Iteration: 4 || Loss: 5.131907404413163
Iteration: 5 || Loss: 5.131216798436437
Iteration: 6 || Loss: 5.131216798436437
saving ADAM checkpoint...
Sum of params:83.61633
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.131216798436437
Iteration: 2 || Loss: 4.843073595806493
Iteration: 3 || Loss: 4.778728434819565
Iteration: 4 || Loss: 4.761780488289815
Iteration: 5 || Loss: 4.66303153058955
Iteration: 6 || Loss: 4.572148542706277
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.78445
Epoch 74 loss:4.572148542706277
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.78445
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.96469740925658
Iteration: 2 || Loss: 13.964496970327323
Iteration: 3 || Loss: 13.964296629206832
Iteration: 4 || Loss: 13.964097431223623
Iteration: 5 || Loss: 13.963898564752965
Iteration: 6 || Loss: 13.963898564752965
saving ADAM checkpoint...
Sum of params:83.7845
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.963898564752965
Iteration: 2 || Loss: 13.947136846119879
Iteration: 3 || Loss: 13.90689292141298
Iteration: 4 || Loss: 13.744932710097157
Iteration: 5 || Loss: 13.727127793409466
Iteration: 6 || Loss: 13.676750290313143
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.79289
Epoch 74 loss:13.676750290313143
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.79289
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 47.61710322630339
Iteration: 2 || Loss: 47.61689717745401
Iteration: 3 || Loss: 47.61669260525783
Iteration: 4 || Loss: 47.61648644950032
Iteration: 5 || Loss: 47.616285887435524
Iteration: 6 || Loss: 47.616285887435524
saving ADAM checkpoint...
Sum of params:83.79298
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 47.616285887435524
Iteration: 2 || Loss: 47.4935039159016
Iteration: 3 || Loss: 47.358183839184484
Iteration: 4 || Loss: 47.06139678236278
Iteration: 5 || Loss: 46.93299518958636
Iteration: 6 || Loss: 46.66882326413697
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.64345
Epoch 74 loss:46.66882326413697
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.000466164732488
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:73.53757666421629
waveform batch: 2/2
Test loss - extrapolation:39.78752921799217
Epoch 74 mean train loss:2.2385421412812545
Epoch 74 mean test loss - interpolation:2.3334110274554147
Epoch 74 mean test loss - extrapolation:9.443758823517372
Start training epoch 75
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.64345
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.083065703724799
Iteration: 2 || Loss: 6.081700977199551
Iteration: 3 || Loss: 6.08033262749555
Iteration: 4 || Loss: 6.078969837875406
Iteration: 5 || Loss: 6.077607038332766
Iteration: 6 || Loss: 6.077607038332766
saving ADAM checkpoint...
Sum of params:83.64346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.077607038332766
Iteration: 2 || Loss: 4.9191088807793095
Iteration: 3 || Loss: 4.830254488816626
Iteration: 4 || Loss: 4.7938078607311825
Iteration: 5 || Loss: 4.784376107537067
Iteration: 6 || Loss: 4.617871299274777
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85364
Epoch 75 loss:4.617871299274777
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.85364
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.176176516276701
Iteration: 2 || Loss: 14.175691068204532
Iteration: 3 || Loss: 14.175206848599416
Iteration: 4 || Loss: 14.174722544039941
Iteration: 5 || Loss: 14.17424047947647
Iteration: 6 || Loss: 14.17424047947647
saving ADAM checkpoint...
Sum of params:83.85368
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.17424047947647
Iteration: 2 || Loss: 14.056431520872177
Iteration: 3 || Loss: 13.851245579482338
Iteration: 4 || Loss: 13.820883338761924
Iteration: 5 || Loss: 13.726307399530933
Iteration: 6 || Loss: 13.71581304923164
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.802376
Epoch 75 loss:13.71581304923164
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.802376
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 47.24378297472652
Iteration: 2 || Loss: 47.24340883563416
Iteration: 3 || Loss: 47.243040263013775
Iteration: 4 || Loss: 47.242670779885174
Iteration: 5 || Loss: 47.24230185624701
Iteration: 6 || Loss: 47.24230185624701
saving ADAM checkpoint...
Sum of params:83.80246
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 47.24230185624701
Iteration: 2 || Loss: 47.14816183237008
Iteration: 3 || Loss: 46.74983328075181
Iteration: 4 || Loss: 46.68839721290027
Iteration: 5 || Loss: 46.61774278451225
Iteration: 6 || Loss: 46.32621424833431
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.64051
Epoch 75 loss:46.32621424833431
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.174719780703175
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:72.82762683075951
waveform batch: 2/2
Test loss - extrapolation:38.68366872979991
Epoch 75 mean train loss:2.229651675753128
Epoch 75 mean test loss - interpolation:2.3624532967838623
Epoch 75 mean test loss - extrapolation:9.292607963379952
Start training epoch 76
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.64051
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.495392764318319
Iteration: 2 || Loss: 6.49395949663939
Iteration: 3 || Loss: 6.492522951949961
Iteration: 4 || Loss: 6.491086935537518
Iteration: 5 || Loss: 6.489653790304427
Iteration: 6 || Loss: 6.489653790304427
saving ADAM checkpoint...
Sum of params:83.64052
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.489653790304427
Iteration: 2 || Loss: 5.192613511980573
Iteration: 3 || Loss: 5.070176948209849
Iteration: 4 || Loss: 4.98861889356968
Iteration: 5 || Loss: 4.979170176719706
Iteration: 6 || Loss: 4.920444810137015
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.72919
Epoch 76 loss:4.920444810137015
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.72919
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.01785032200704
Iteration: 2 || Loss: 14.017646581047844
Iteration: 3 || Loss: 14.017445312822552
Iteration: 4 || Loss: 14.017243589403575
Iteration: 5 || Loss: 14.01704209097979
Iteration: 6 || Loss: 14.01704209097979
saving ADAM checkpoint...
Sum of params:83.72922
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.01704209097979
Iteration: 2 || Loss: 13.98504475285182
Iteration: 3 || Loss: 13.964491991022433
Iteration: 4 || Loss: 13.882955313963672
Iteration: 5 || Loss: 13.831144139803541
Iteration: 6 || Loss: 13.78340405828114
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.81177
Epoch 76 loss:13.78340405828114
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.81177
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 46.74908653137403
Iteration: 2 || Loss: 46.74890403352343
Iteration: 3 || Loss: 46.74872414668819
Iteration: 4 || Loss: 46.748543759908266
Iteration: 5 || Loss: 46.748364998283506
Iteration: 6 || Loss: 46.748364998283506
saving ADAM checkpoint...
Sum of params:83.81178
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 46.748364998283506
Iteration: 2 || Loss: 46.49887484841576
Iteration: 3 || Loss: 46.40951766629463
Iteration: 4 || Loss: 46.177938030844665
Iteration: 5 || Loss: 46.08566105334006
Iteration: 6 || Loss: 45.9518488708939
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.714035
Epoch 76 loss:45.9518488708939
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.758005296208294
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:72.30196787869248
waveform batch: 2/2
Test loss - extrapolation:39.00786555751084
Epoch 76 mean train loss:2.2295068185969673
Epoch 76 mean test loss - interpolation:2.2930008827013824
Epoch 76 mean test loss - extrapolation:9.275819453016943
Start training epoch 77
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.714035
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.569670617345973
Iteration: 2 || Loss: 5.568758336773938
Iteration: 3 || Loss: 5.56784593240693
Iteration: 4 || Loss: 5.5669352079019285
Iteration: 5 || Loss: 5.566023672472145
Iteration: 6 || Loss: 5.566023672472145
saving ADAM checkpoint...
Sum of params:83.71405
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.566023672472145
Iteration: 2 || Loss: 5.061438360279307
Iteration: 3 || Loss: 4.988103200800031
Iteration: 4 || Loss: 4.960701348865392
Iteration: 5 || Loss: 4.946110239575615
Iteration: 6 || Loss: 4.8128476225818115
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.860405
Epoch 77 loss:4.8128476225818115
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.860405
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.954930945440026
Iteration: 2 || Loss: 13.954880641822378
Iteration: 3 || Loss: 13.95483122150986
Iteration: 4 || Loss: 13.954781768079428
Iteration: 5 || Loss: 13.954732211259099
Iteration: 6 || Loss: 13.954732211259099
saving ADAM checkpoint...
Sum of params:83.860344
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.954732211259099
Iteration: 2 || Loss: 13.934834000625536
Iteration: 3 || Loss: 13.925626909874113
Iteration: 4 || Loss: 13.80474463891528
Iteration: 5 || Loss: 13.741217227001362
Iteration: 6 || Loss: 13.56646134780786
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.997314
Epoch 77 loss:13.56646134780786
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.997314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 47.31878134020211
Iteration: 2 || Loss: 47.31856261142474
Iteration: 3 || Loss: 47.31834392924865
Iteration: 4 || Loss: 47.31812517583827
Iteration: 5 || Loss: 47.317905475681485
Iteration: 6 || Loss: 47.317905475681485
saving ADAM checkpoint...
Sum of params:83.997345
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 47.317905475681485
Iteration: 2 || Loss: 47.221502034209685
Iteration: 3 || Loss: 46.96538154761599
Iteration: 4 || Loss: 46.640320762612525
Iteration: 5 || Loss: 46.432550635548175
Iteration: 6 || Loss: 46.37576031670833
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85122
Epoch 77 loss:46.37576031670833
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.673264691525953
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:72.33403341777485
waveform batch: 2/2
Test loss - extrapolation:39.49470927443517
Epoch 77 mean train loss:2.2329334236930345
Epoch 77 mean test loss - interpolation:2.2788774485876586
Epoch 77 mean test loss - extrapolation:9.319061891017503
Start training epoch 78
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.85122
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.041852296325326
Iteration: 2 || Loss: 5.041106162607335
Iteration: 3 || Loss: 5.040361598865886
Iteration: 4 || Loss: 5.039615220394874
Iteration: 5 || Loss: 5.038870504693871
Iteration: 6 || Loss: 5.038870504693871
saving ADAM checkpoint...
Sum of params:83.85122
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.038870504693871
Iteration: 2 || Loss: 4.707044454217607
Iteration: 3 || Loss: 4.544187560155583
Iteration: 4 || Loss: 4.526081327925718
Iteration: 5 || Loss: 4.505968435733115
Iteration: 6 || Loss: 4.393874731325168
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.01282
Epoch 78 loss:4.393874731325168
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.01282
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.624568360350498
Iteration: 2 || Loss: 13.624362924111514
Iteration: 3 || Loss: 13.624158199535307
Iteration: 4 || Loss: 13.62395465828585
Iteration: 5 || Loss: 13.623751340663311
Iteration: 6 || Loss: 13.623751340663311
saving ADAM checkpoint...
Sum of params:84.01284
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.623751340663311
Iteration: 2 || Loss: 13.605330102942572
Iteration: 3 || Loss: 13.573442698981674
Iteration: 4 || Loss: 13.480663830651954
Iteration: 5 || Loss: 13.461982421067164
Iteration: 6 || Loss: 13.42733693569506
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.9924
Epoch 78 loss:13.42733693569506
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.9924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 47.168994052101695
Iteration: 2 || Loss: 47.168793302832505
Iteration: 3 || Loss: 47.16859285475919
Iteration: 4 || Loss: 47.168392805934204
Iteration: 5 || Loss: 47.168192496022016
Iteration: 6 || Loss: 47.168192496022016
saving ADAM checkpoint...
Sum of params:83.99248
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 47.168192496022016
Iteration: 2 || Loss: 47.025316663191575
Iteration: 3 || Loss: 46.69604634136603
Iteration: 4 || Loss: 46.51211185642169
Iteration: 5 || Loss: 46.264133417002796
Iteration: 6 || Loss: 45.99774536624538
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.780396
Epoch 78 loss:45.99774536624538
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.75160423912996
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:72.56486000057114
waveform batch: 2/2
Test loss - extrapolation:39.15899408816203
Epoch 78 mean train loss:2.2006536908022625
Epoch 78 mean test loss - interpolation:2.2919340398549934
Epoch 78 mean test loss - extrapolation:9.310321174061098
Start training epoch 79
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.780396
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.894685830423134
Iteration: 2 || Loss: 5.893426682952698
Iteration: 3 || Loss: 5.8921687035242725
Iteration: 4 || Loss: 5.890913247250522
Iteration: 5 || Loss: 5.8896575806696125
Iteration: 6 || Loss: 5.8896575806696125
saving ADAM checkpoint...
Sum of params:83.780396
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.8896575806696125
Iteration: 2 || Loss: 4.920629867372416
Iteration: 3 || Loss: 4.8493328959459925
Iteration: 4 || Loss: 4.821562396684931
Iteration: 5 || Loss: 4.796147591388837
Iteration: 6 || Loss: 4.519746214184153
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.928825
Epoch 79 loss:4.519746214184153
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.928825
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.806354981914648
Iteration: 2 || Loss: 13.80585237019503
Iteration: 3 || Loss: 13.805347426889792
Iteration: 4 || Loss: 13.804846303785423
Iteration: 5 || Loss: 13.804344494742805
Iteration: 6 || Loss: 13.804344494742805
saving ADAM checkpoint...
Sum of params:83.92886
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.804344494742805
Iteration: 2 || Loss: 13.652298909360356
Iteration: 3 || Loss: 13.632448548316313
Iteration: 4 || Loss: 13.616593195959963
Iteration: 5 || Loss: 13.532423728296909
Iteration: 6 || Loss: 13.52619599790187
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.94032
Epoch 79 loss:13.52619599790187
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.94032
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 46.332368877626735
Iteration: 2 || Loss: 46.33210995684377
Iteration: 3 || Loss: 46.331849514999284
Iteration: 4 || Loss: 46.33159196843921
Iteration: 5 || Loss: 46.3313334178925
Iteration: 6 || Loss: 46.3313334178925
saving ADAM checkpoint...
Sum of params:83.94042
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 46.3313334178925
Iteration: 2 || Loss: 46.282650131075016
Iteration: 3 || Loss: 46.057532012670684
Iteration: 4 || Loss: 45.963883412763586
Iteration: 5 || Loss: 45.9040640549181
Iteration: 6 || Loss: 45.370289724149764
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.717285
Epoch 79 loss:45.370289724149764
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:14.117425895421457
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:71.5024567635029
waveform batch: 2/2
Test loss - extrapolation:37.42760458382583
Epoch 79 mean train loss:2.186766618490889
Epoch 79 mean test loss - interpolation:2.3529043159035763
Epoch 79 mean test loss - extrapolation:9.077505112277395
Start training epoch 80
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.717285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.319612996101501
Iteration: 2 || Loss: 7.317936714249877
Iteration: 3 || Loss: 7.316262661206892
Iteration: 4 || Loss: 7.314588543804618
Iteration: 5 || Loss: 7.312911356777306
Iteration: 6 || Loss: 7.312911356777306
saving ADAM checkpoint...
Sum of params:83.7173
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.312911356777306
Iteration: 2 || Loss: 5.516522476850608
Iteration: 3 || Loss: 5.357086007159378
Iteration: 4 || Loss: 5.211772444712757
Iteration: 5 || Loss: 5.1996611243181805
Iteration: 6 || Loss: 5.192894115408336
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85794
Epoch 80 loss:5.192894115408336
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.85794
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.293694365512184
Iteration: 2 || Loss: 14.293151182853666
Iteration: 3 || Loss: 14.292608820464011
Iteration: 4 || Loss: 14.292068020603999
Iteration: 5 || Loss: 14.291526929615236
Iteration: 6 || Loss: 14.291526929615236
saving ADAM checkpoint...
Sum of params:83.85806
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.291526929615236
Iteration: 2 || Loss: 14.118219765649828
Iteration: 3 || Loss: 14.073157051123525
Iteration: 4 || Loss: 14.030954869883429
Iteration: 5 || Loss: 13.795579040520266
Iteration: 6 || Loss: 13.776784021716974
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.91569
Epoch 80 loss:13.776784021716974
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.91569
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 46.20086718356508
Iteration: 2 || Loss: 46.200587792944596
Iteration: 3 || Loss: 46.200305552592134
Iteration: 4 || Loss: 46.200025630153384
Iteration: 5 || Loss: 46.19974161287489
Iteration: 6 || Loss: 46.19974161287489
saving ADAM checkpoint...
Sum of params:83.91569
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 46.19974161287489
Iteration: 2 || Loss: 45.97152680287707
Iteration: 3 || Loss: 45.490573375156345
Iteration: 4 || Loss: 45.11426284916623
Iteration: 5 || Loss: 45.04948570443207
Iteration: 6 || Loss: 44.969145788001995
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.82562
Epoch 80 loss:44.969145788001995
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.60458908899965
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:70.54144404835316
waveform batch: 2/2
Test loss - extrapolation:37.42863608369575
Epoch 80 mean train loss:2.2047870319009415
Epoch 80 mean test loss - interpolation:2.267431514833275
Epoch 80 mean test loss - extrapolation:8.997506677670742
Start training epoch 81
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.82562
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.835963324212816
Iteration: 2 || Loss: 5.834962181891423
Iteration: 3 || Loss: 5.83395767942838
Iteration: 4 || Loss: 5.832957854291139
Iteration: 5 || Loss: 5.831958022513846
Iteration: 6 || Loss: 5.831958022513846
saving ADAM checkpoint...
Sum of params:83.82562
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.831958022513846
Iteration: 2 || Loss: 5.236981491319583
Iteration: 3 || Loss: 5.127483503012542
Iteration: 4 || Loss: 5.084782311955068
Iteration: 5 || Loss: 5.064441362311424
Iteration: 6 || Loss: 4.916652086932867
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.98494
Epoch 81 loss:4.916652086932867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.98494
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.915985497202044
Iteration: 2 || Loss: 13.915870168997929
Iteration: 3 || Loss: 13.915754796597332
Iteration: 4 || Loss: 13.915639947836027
Iteration: 5 || Loss: 13.915527537689465
Iteration: 6 || Loss: 13.915527537689465
saving ADAM checkpoint...
Sum of params:83.98497
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.915527537689465
Iteration: 2 || Loss: 13.907942894185153
Iteration: 3 || Loss: 13.819429236374557
Iteration: 4 || Loss: 13.714480477365143
Iteration: 5 || Loss: 13.668508192063497
Iteration: 6 || Loss: 13.619955296390803
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.01076
Epoch 81 loss:13.619955296390803
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.01076
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.95752587686462
Iteration: 2 || Loss: 45.95731611737556
Iteration: 3 || Loss: 45.9571057115711
Iteration: 4 || Loss: 45.95689520105734
Iteration: 5 || Loss: 45.956684188945644
Iteration: 6 || Loss: 45.956684188945644
saving ADAM checkpoint...
Sum of params:84.01082
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.956684188945644
Iteration: 2 || Loss: 45.73540834635925
Iteration: 3 || Loss: 45.435407472049356
Iteration: 4 || Loss: 45.167257405409735
Iteration: 5 || Loss: 45.083300256792455
Iteration: 6 || Loss: 44.98177019765791
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.87445
Epoch 81 loss:44.98177019765791
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.541721319084044
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:70.46962832882897
waveform batch: 2/2
Test loss - extrapolation:37.59239386914991
Epoch 81 mean train loss:2.1902888821028133
Epoch 81 mean test loss - interpolation:2.256953553180674
Epoch 81 mean test loss - extrapolation:9.00516851649824
Start training epoch 82
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.87445
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.5589161162403355
Iteration: 2 || Loss: 5.5579583456885295
Iteration: 3 || Loss: 5.5569988211403665
Iteration: 4 || Loss: 5.556044912089425
Iteration: 5 || Loss: 5.5550884282776245
Iteration: 6 || Loss: 5.5550884282776245
saving ADAM checkpoint...
Sum of params:83.87446
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.5550884282776245
Iteration: 2 || Loss: 5.013760621847161
Iteration: 3 || Loss: 4.928945219780778
Iteration: 4 || Loss: 4.895670982658775
Iteration: 5 || Loss: 4.8830010376190875
Iteration: 6 || Loss: 4.782914110652058
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.01785
Epoch 82 loss:4.782914110652058
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.01785
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.76736539383417
Iteration: 2 || Loss: 13.767244752980787
Iteration: 3 || Loss: 13.767125341368931
Iteration: 4 || Loss: 13.76700641923817
Iteration: 5 || Loss: 13.766889099925194
Iteration: 6 || Loss: 13.766889099925194
saving ADAM checkpoint...
Sum of params:84.01795
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.766889099925194
Iteration: 2 || Loss: 13.761006451381526
Iteration: 3 || Loss: 13.718864096577722
Iteration: 4 || Loss: 13.61636375055532
Iteration: 5 || Loss: 13.573633301200674
Iteration: 6 || Loss: 13.54480246833394
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.08702
Epoch 82 loss:13.54480246833394
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.08702
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.97314377789253
Iteration: 2 || Loss: 45.9729064905979
Iteration: 3 || Loss: 45.972664768857925
Iteration: 4 || Loss: 45.972427862751424
Iteration: 5 || Loss: 45.97219065436915
Iteration: 6 || Loss: 45.97219065436915
saving ADAM checkpoint...
Sum of params:84.08703
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.97219065436915
Iteration: 2 || Loss: 45.642147262130756
Iteration: 3 || Loss: 45.40583815700694
Iteration: 4 || Loss: 45.16222803553076
Iteration: 5 || Loss: 45.09981042008131
Iteration: 6 || Loss: 44.92822213916554
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.918884
Epoch 82 loss:44.92822213916554
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.507607791677687
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:70.37856764015783
waveform batch: 2/2
Test loss - extrapolation:37.58629358698479
Epoch 82 mean train loss:2.1812392661431566
Epoch 82 mean test loss - interpolation:2.2512679652796144
Epoch 82 mean test loss - extrapolation:8.997071768928551
Start training epoch 83
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.918884
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.56903085003303
Iteration: 2 || Loss: 5.568011095215175
Iteration: 3 || Loss: 5.566991345908448
Iteration: 4 || Loss: 5.5659739333725975
Iteration: 5 || Loss: 5.564957297894213
Iteration: 6 || Loss: 5.564957297894213
saving ADAM checkpoint...
Sum of params:83.918884
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.564957297894213
Iteration: 2 || Loss: 4.948202521592288
Iteration: 3 || Loss: 4.869305553480952
Iteration: 4 || Loss: 4.834634477875559
Iteration: 5 || Loss: 4.82513252814352
Iteration: 6 || Loss: 4.745734136907384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.03804
Epoch 83 loss:4.745734136907384
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.03804
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.684308578151798
Iteration: 2 || Loss: 13.684177521060711
Iteration: 3 || Loss: 13.684047479149223
Iteration: 4 || Loss: 13.683918516650078
Iteration: 5 || Loss: 13.683789099982945
Iteration: 6 || Loss: 13.683789099982945
saving ADAM checkpoint...
Sum of params:84.03816
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.683789099982945
Iteration: 2 || Loss: 13.676371016264453
Iteration: 3 || Loss: 13.657753594418343
Iteration: 4 || Loss: 13.554795997053336
Iteration: 5 || Loss: 13.53146447826841
Iteration: 6 || Loss: 13.488381949519438
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.11885
Epoch 83 loss:13.488381949519438
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.11885
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.867933684520544
Iteration: 2 || Loss: 45.86771386089149
Iteration: 3 || Loss: 45.86749319089371
Iteration: 4 || Loss: 45.86726948202426
Iteration: 5 || Loss: 45.86704650461176
Iteration: 6 || Loss: 45.86704650461176
saving ADAM checkpoint...
Sum of params:84.11888
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.86704650461176
Iteration: 2 || Loss: 45.52566513753581
Iteration: 3 || Loss: 45.38545587799775
Iteration: 4 || Loss: 45.10215875187342
Iteration: 5 || Loss: 45.029583031161714
Iteration: 6 || Loss: 44.85100249824156
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.94768
Epoch 83 loss:44.85100249824156
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.459914379398555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:70.22503031401943
waveform batch: 2/2
Test loss - extrapolation:37.528360908294225
Epoch 83 mean train loss:2.175348916712703
Epoch 83 mean test loss - interpolation:2.2433190632330926
Epoch 83 mean test loss - extrapolation:8.979449268526139
Start training epoch 84
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.94768
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.526242807606585
Iteration: 2 || Loss: 5.52522558049397
Iteration: 3 || Loss: 5.524207595157512
Iteration: 4 || Loss: 5.523193842815724
Iteration: 5 || Loss: 5.522180436345596
Iteration: 6 || Loss: 5.522180436345596
saving ADAM checkpoint...
Sum of params:83.94768
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.522180436345596
Iteration: 2 || Loss: 4.910653898970318
Iteration: 3 || Loss: 4.829366268457223
Iteration: 4 || Loss: 4.794030940349453
Iteration: 5 || Loss: 4.784336795099877
Iteration: 6 || Loss: 4.705068544615873
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.06675
Epoch 84 loss:4.705068544615873
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.06675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.625927789068621
Iteration: 2 || Loss: 13.625800063000339
Iteration: 3 || Loss: 13.625671205697879
Iteration: 4 || Loss: 13.625544957883784
Iteration: 5 || Loss: 13.625419557154187
Iteration: 6 || Loss: 13.625419557154187
saving ADAM checkpoint...
Sum of params:84.06684
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.625419557154187
Iteration: 2 || Loss: 13.61841768435435
Iteration: 3 || Loss: 13.59506172588181
Iteration: 4 || Loss: 13.49587881439596
Iteration: 5 || Loss: 13.473092007505919
Iteration: 6 || Loss: 13.43229949873836
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.13526
Epoch 84 loss:13.43229949873836
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.13526
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.75595472347041
Iteration: 2 || Loss: 45.755736412879486
Iteration: 3 || Loss: 45.75551819157661
Iteration: 4 || Loss: 45.75530082663168
Iteration: 5 || Loss: 45.75508044799233
Iteration: 6 || Loss: 45.75508044799233
saving ADAM checkpoint...
Sum of params:84.13529
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.75508044799233
Iteration: 2 || Loss: 45.44561018976597
Iteration: 3 || Loss: 45.31359270951898
Iteration: 4 || Loss: 45.040126013937595
Iteration: 5 || Loss: 44.94767063666968
Iteration: 6 || Loss: 44.752544552079506
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.95417
Epoch 84 loss:44.752544552079506
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.422969822573874
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:70.08074353575773
waveform batch: 2/2
Test loss - extrapolation:37.4421900759878
Epoch 84 mean train loss:2.1686176757046116
Epoch 84 mean test loss - interpolation:2.2371616370956455
Epoch 84 mean test loss - extrapolation:8.960244467645461
Start training epoch 85
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.95417
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.524469565997015
Iteration: 2 || Loss: 5.523435301954087
Iteration: 3 || Loss: 5.522400716803472
Iteration: 4 || Loss: 5.521364488511233
Iteration: 5 || Loss: 5.520332753384855
Iteration: 6 || Loss: 5.520332753384855
saving ADAM checkpoint...
Sum of params:83.95418
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.520332753384855
Iteration: 2 || Loss: 4.88586967802814
Iteration: 3 || Loss: 4.80254111963219
Iteration: 4 || Loss: 4.765570181964175
Iteration: 5 || Loss: 4.757028789115903
Iteration: 6 || Loss: 4.6864432396354605
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.06998
Epoch 85 loss:4.6864432396354605
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.06998
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.576826134741388
Iteration: 2 || Loss: 13.576687471416845
Iteration: 3 || Loss: 13.576549785774997
Iteration: 4 || Loss: 13.576413269025377
Iteration: 5 || Loss: 13.576277544664332
Iteration: 6 || Loss: 13.576277544664332
saving ADAM checkpoint...
Sum of params:84.07008
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.576277544664332
Iteration: 2 || Loss: 13.567773839815134
Iteration: 3 || Loss: 13.545341895968981
Iteration: 4 || Loss: 13.448387770016769
Iteration: 5 || Loss: 13.424465310171938
Iteration: 6 || Loss: 13.36438606444062
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.14791
Epoch 85 loss:13.36438606444062
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.14791
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.75846661044448
Iteration: 2 || Loss: 45.758231133048746
Iteration: 3 || Loss: 45.7579925674443
Iteration: 4 || Loss: 45.75775402505222
Iteration: 5 || Loss: 45.75751884906393
Iteration: 6 || Loss: 45.75751884906393
saving ADAM checkpoint...
Sum of params:84.147934
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.75751884906393
Iteration: 2 || Loss: 45.619740030302175
Iteration: 3 || Loss: 45.46195386636516
Iteration: 4 || Loss: 45.043534422799276
Iteration: 5 || Loss: 44.898150218381865
Iteration: 6 || Loss: 44.744685762472116
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.966736
Epoch 85 loss:44.744685762472116
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.376202431274512
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:69.91889450355059
waveform batch: 2/2
Test loss - extrapolation:37.39603525261435
Epoch 85 mean train loss:2.165362588501662
Epoch 85 mean test loss - interpolation:2.2293670718790852
Epoch 85 mean test loss - extrapolation:8.942910813013745
Start training epoch 86
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.966736
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.42617760689721
Iteration: 2 || Loss: 5.425149168771086
Iteration: 3 || Loss: 5.424125136394448
Iteration: 4 || Loss: 5.423098478751951
Iteration: 5 || Loss: 5.422075203987989
Iteration: 6 || Loss: 5.422075203987989
saving ADAM checkpoint...
Sum of params:83.96675
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.422075203987989
Iteration: 2 || Loss: 4.802477762248378
Iteration: 3 || Loss: 4.703786965665552
Iteration: 4 || Loss: 4.661222116388551
Iteration: 5 || Loss: 4.652953714465399
Iteration: 6 || Loss: 4.590616737968464
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.097435
Epoch 86 loss:4.590616737968464
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.097435
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.482335930557234
Iteration: 2 || Loss: 13.482191946852087
Iteration: 3 || Loss: 13.482049915338864
Iteration: 4 || Loss: 13.481908865461008
Iteration: 5 || Loss: 13.481766329346195
Iteration: 6 || Loss: 13.481766329346195
saving ADAM checkpoint...
Sum of params:84.09755
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.481766329346195
Iteration: 2 || Loss: 13.472473453483818
Iteration: 3 || Loss: 13.42019064537076
Iteration: 4 || Loss: 13.349057654090352
Iteration: 5 || Loss: 13.326876969650861
Iteration: 6 || Loss: 13.219751590439113
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.14795
Epoch 86 loss:13.219751590439113
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.14795
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 46.002662056395
Iteration: 2 || Loss: 46.00237527190236
Iteration: 3 || Loss: 46.0020915228045
Iteration: 4 || Loss: 46.001806853917735
Iteration: 5 || Loss: 46.00152305392644
Iteration: 6 || Loss: 46.00152305392644
saving ADAM checkpoint...
Sum of params:84.147995
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 46.00152305392644
Iteration: 2 || Loss: 45.938575967317966
Iteration: 3 || Loss: 45.73732491594083
Iteration: 4 || Loss: 45.374952940311154
Iteration: 5 || Loss: 45.093991762638275
Iteration: 6 || Loss: 45.04562086293439
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.99182
Epoch 86 loss:45.04562086293439
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.311939002778391
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:69.57013660855365
waveform batch: 2/2
Test loss - extrapolation:37.369853116462906
Epoch 86 mean train loss:2.167447903149723
Epoch 86 mean test loss - interpolation:2.2186565004630654
Epoch 86 mean test loss - extrapolation:8.911665810418047
Start training epoch 87
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.99182
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.849106616701775
Iteration: 2 || Loss: 4.848398369249035
Iteration: 3 || Loss: 4.8476924948835896
Iteration: 4 || Loss: 4.846986334768434
Iteration: 5 || Loss: 4.846280272852296
Iteration: 6 || Loss: 4.846280272852296
saving ADAM checkpoint...
Sum of params:83.99182
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.846280272852296
Iteration: 2 || Loss: 4.561782588466495
Iteration: 3 || Loss: 4.420693436561167
Iteration: 4 || Loss: 4.4063758760053435
Iteration: 5 || Loss: 4.37234742565092
Iteration: 6 || Loss: 4.282341262000826
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.14145
Epoch 87 loss:4.282341262000826
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.14145
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.26281179449051
Iteration: 2 || Loss: 13.262634075053535
Iteration: 3 || Loss: 13.262455926803277
Iteration: 4 || Loss: 13.262279726855752
Iteration: 5 || Loss: 13.262103669329687
Iteration: 6 || Loss: 13.262103669329687
saving ADAM checkpoint...
Sum of params:84.14151
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.262103669329687
Iteration: 2 || Loss: 13.248730547661701
Iteration: 3 || Loss: 13.218811254534291
Iteration: 4 || Loss: 13.14963147573582
Iteration: 5 || Loss: 13.133806125146318
Iteration: 6 || Loss: 13.106287675274029
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.120186
Epoch 87 loss:13.106287675274029
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.120186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.79414789624942
Iteration: 2 || Loss: 45.793938084619974
Iteration: 3 || Loss: 45.79373092132013
Iteration: 4 || Loss: 45.7935229111236
Iteration: 5 || Loss: 45.79331578777146
Iteration: 6 || Loss: 45.79331578777146
saving ADAM checkpoint...
Sum of params:84.1203
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.79331578777146
Iteration: 2 || Loss: 45.68223919052597
Iteration: 3 || Loss: 45.495309316527795
Iteration: 4 || Loss: 45.24495551011238
Iteration: 5 || Loss: 45.1165442520746
Iteration: 6 || Loss: 44.747595910704796
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.866356
Epoch 87 loss:44.747595910704796
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.512762540169918
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:70.41080790650193
waveform batch: 2/2
Test loss - extrapolation:37.27677254319359
Epoch 87 mean train loss:2.142628443033781
Epoch 87 mean test loss - interpolation:2.2521270900283197
Epoch 87 mean test loss - extrapolation:8.973965037474626
Start training epoch 88
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.866356
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4162372509983445
Iteration: 2 || Loss: 6.414599989327868
Iteration: 3 || Loss: 6.412968544126869
Iteration: 4 || Loss: 6.411336571450907
Iteration: 5 || Loss: 6.409699352596768
Iteration: 6 || Loss: 6.409699352596768
saving ADAM checkpoint...
Sum of params:83.86637
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.409699352596768
Iteration: 2 || Loss: 4.791402491655709
Iteration: 3 || Loss: 4.696441893240381
Iteration: 4 || Loss: 4.659122176309179
Iteration: 5 || Loss: 4.6507512984871155
Iteration: 6 || Loss: 4.458967488212654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.96528
Epoch 88 loss:4.458967488212654
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.96528
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.35994465787819
Iteration: 2 || Loss: 13.35980612001358
Iteration: 3 || Loss: 13.35966831668678
Iteration: 4 || Loss: 13.359533302070467
Iteration: 5 || Loss: 13.35939513685662
Iteration: 6 || Loss: 13.35939513685662
saving ADAM checkpoint...
Sum of params:83.9654
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.35939513685662
Iteration: 2 || Loss: 13.346122912751552
Iteration: 3 || Loss: 13.30313023324496
Iteration: 4 || Loss: 13.221616366176619
Iteration: 5 || Loss: 13.183557075579284
Iteration: 6 || Loss: 13.13914398911659
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.0619
Epoch 88 loss:13.13914398911659
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.0619
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.327205162018004
Iteration: 2 || Loss: 45.32700482076665
Iteration: 3 || Loss: 45.32680608488458
Iteration: 4 || Loss: 45.32660718008987
Iteration: 5 || Loss: 45.326406882361304
Iteration: 6 || Loss: 45.326406882361304
saving ADAM checkpoint...
Sum of params:84.06201
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.326406882361304
Iteration: 2 || Loss: 45.21715391575136
Iteration: 3 || Loss: 45.10139844480906
Iteration: 4 || Loss: 44.821860698725885
Iteration: 5 || Loss: 44.69323521967345
Iteration: 6 || Loss: 44.43082054348534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.869995
Epoch 88 loss:44.43082054348534
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.374031331693319
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:69.67319585676871
waveform batch: 2/2
Test loss - extrapolation:36.93988730177634
Epoch 88 mean train loss:2.1389286903729166
Epoch 88 mean test loss - interpolation:2.2290052219488863
Epoch 88 mean test loss - extrapolation:8.884423596545421
Start training epoch 89
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.869995
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.072005903045985
Iteration: 2 || Loss: 6.070548466091221
Iteration: 3 || Loss: 6.069088403311508
Iteration: 4 || Loss: 6.067628817616063
Iteration: 5 || Loss: 6.066171689358984
Iteration: 6 || Loss: 6.066171689358984
saving ADAM checkpoint...
Sum of params:83.87
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.066171689358984
Iteration: 2 || Loss: 4.785816729842655
Iteration: 3 || Loss: 4.701509036496826
Iteration: 4 || Loss: 4.659004167170187
Iteration: 5 || Loss: 4.652420150841672
Iteration: 6 || Loss: 4.42775922752023
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.06443
Epoch 89 loss:4.42775922752023
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.06443
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.639333536700754
Iteration: 2 || Loss: 13.638817053581507
Iteration: 3 || Loss: 13.63829981681354
Iteration: 4 || Loss: 13.63778476044329
Iteration: 5 || Loss: 13.637269458573984
Iteration: 6 || Loss: 13.637269458573984
saving ADAM checkpoint...
Sum of params:84.06445
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.637269458573984
Iteration: 2 || Loss: 13.50702116048016
Iteration: 3 || Loss: 13.354299929890043
Iteration: 4 || Loss: 13.305735573289889
Iteration: 5 || Loss: 13.193659771156998
Iteration: 6 || Loss: 13.185498588146451
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.01616
Epoch 89 loss:13.185498588146451
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.01616
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 45.061234310840966
Iteration: 2 || Loss: 45.06079880967572
Iteration: 3 || Loss: 45.06036060001029
Iteration: 4 || Loss: 45.05992596242995
Iteration: 5 || Loss: 45.05949087091789
Iteration: 6 || Loss: 45.05949087091789
saving ADAM checkpoint...
Sum of params:84.01626
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 45.05949087091789
Iteration: 2 || Loss: 44.945981457628264
Iteration: 3 || Loss: 44.60156000124258
Iteration: 4 || Loss: 44.531630415772895
Iteration: 5 || Loss: 44.45915487417433
Iteration: 6 || Loss: 44.11858371895469
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.8286
Epoch 89 loss:44.11858371895469
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.603177772938961
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:68.53403742899859
waveform batch: 2/2
Test loss - extrapolation:35.53019750681259
Epoch 89 mean train loss:2.128684190849013
Epoch 89 mean test loss - interpolation:2.267196295489827
Epoch 89 mean test loss - extrapolation:8.672019577984265
Start training epoch 90
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.8286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.196928040336121
Iteration: 2 || Loss: 6.1956378042191895
Iteration: 3 || Loss: 6.194354541274854
Iteration: 4 || Loss: 6.193068542286173
Iteration: 5 || Loss: 6.191783453173504
Iteration: 6 || Loss: 6.191783453173504
saving ADAM checkpoint...
Sum of params:83.828606
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.191783453173504
Iteration: 2 || Loss: 5.179390692103195
Iteration: 3 || Loss: 4.981895607305064
Iteration: 4 || Loss: 4.875189709393467
Iteration: 5 || Loss: 4.8665976617505065
Iteration: 6 || Loss: 4.807366919679146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.04307
Epoch 90 loss:4.807366919679146
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.04307
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.709811711791742
Iteration: 2 || Loss: 13.709544233917537
Iteration: 3 || Loss: 13.709279237470648
Iteration: 4 || Loss: 13.70901528695482
Iteration: 5 || Loss: 13.7087518320517
Iteration: 6 || Loss: 13.7087518320517
saving ADAM checkpoint...
Sum of params:84.0432
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.7087518320517
Iteration: 2 || Loss: 13.67122723838932
Iteration: 3 || Loss: 13.434833048620778
Iteration: 4 || Loss: 13.352115921875251
Iteration: 5 || Loss: 13.314208513387538
Iteration: 6 || Loss: 13.255417972360762
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.013214
Epoch 90 loss:13.255417972360762
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.013214
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.964721991260696
Iteration: 2 || Loss: 44.96447330512052
Iteration: 3 || Loss: 44.964220675739455
Iteration: 4 || Loss: 44.96397392200543
Iteration: 5 || Loss: 44.96372395786615
Iteration: 6 || Loss: 44.96372395786615
saving ADAM checkpoint...
Sum of params:84.01329
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.96372395786615
Iteration: 2 || Loss: 44.5912120808089
Iteration: 3 || Loss: 44.23942602350291
Iteration: 4 || Loss: 43.98804378789272
Iteration: 5 || Loss: 43.930406724711816
Iteration: 6 || Loss: 43.81307164414732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.857315
Epoch 90 loss:43.81307164414732
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.17295268072655
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:68.4643487614083
waveform batch: 2/2
Test loss - extrapolation:36.09744682327744
Epoch 90 mean train loss:2.1336502253857663
Epoch 90 mean test loss - interpolation:2.1954921134544247
Epoch 90 mean test loss - extrapolation:8.713482965390478
Start training epoch 91
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.857315
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.5167401450284075
Iteration: 2 || Loss: 5.515692275105452
Iteration: 3 || Loss: 5.514650239905136
Iteration: 4 || Loss: 5.513606795034382
Iteration: 5 || Loss: 5.512562235704576
Iteration: 6 || Loss: 5.512562235704576
saving ADAM checkpoint...
Sum of params:83.857315
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.512562235704576
Iteration: 2 || Loss: 4.871214883627038
Iteration: 3 || Loss: 4.7871465027920275
Iteration: 4 || Loss: 4.750188480896897
Iteration: 5 || Loss: 4.7389241758274885
Iteration: 6 || Loss: 4.644288595913283
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.97277
Epoch 91 loss:4.644288595913283
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.97277
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.371112275902503
Iteration: 2 || Loss: 13.370999968196434
Iteration: 3 || Loss: 13.370890567751866
Iteration: 4 || Loss: 13.370780893398031
Iteration: 5 || Loss: 13.370672492413103
Iteration: 6 || Loss: 13.370672492413103
saving ADAM checkpoint...
Sum of params:83.97287
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.370672492413103
Iteration: 2 || Loss: 13.36579403466383
Iteration: 3 || Loss: 13.346471724312615
Iteration: 4 || Loss: 13.237300173106615
Iteration: 5 || Loss: 13.209800626608455
Iteration: 6 || Loss: 13.173528929517918
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.05678
Epoch 91 loss:13.173528929517918
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.05678
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.838800562779134
Iteration: 2 || Loss: 44.83856449688049
Iteration: 3 || Loss: 44.838325695708846
Iteration: 4 || Loss: 44.8380926200024
Iteration: 5 || Loss: 44.83785798014118
Iteration: 6 || Loss: 44.83785798014118
saving ADAM checkpoint...
Sum of params:84.05681
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.83785798014118
Iteration: 2 || Loss: 44.51527986025287
Iteration: 3 || Loss: 44.47318204586245
Iteration: 4 || Loss: 44.1106630415102
Iteration: 5 || Loss: 43.96229011349263
Iteration: 6 || Loss: 43.83639974658099
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.89039
Epoch 91 loss:43.83639974658099
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.193958351371684
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:68.27087389210706
waveform batch: 2/2
Test loss - extrapolation:35.92411404008181
Epoch 91 mean train loss:2.1260074921383514
Epoch 91 mean test loss - interpolation:2.1989930585619475
Epoch 91 mean test loss - extrapolation:8.68291566101574
Start training epoch 92
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.89039
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.705244188571532
Iteration: 2 || Loss: 5.7039814209615365
Iteration: 3 || Loss: 5.702717048753172
Iteration: 4 || Loss: 5.701457861888906
Iteration: 5 || Loss: 5.700195561737166
Iteration: 6 || Loss: 5.700195561737166
saving ADAM checkpoint...
Sum of params:83.89043
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.700195561737166
Iteration: 2 || Loss: 4.758963580611701
Iteration: 3 || Loss: 4.686863233960548
Iteration: 4 || Loss: 4.6464432364918125
Iteration: 5 || Loss: 4.635224897581953
Iteration: 6 || Loss: 4.481022119693046
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.9792
Epoch 92 loss:4.481022119693046
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.9792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.322455573715978
Iteration: 2 || Loss: 13.322196106842568
Iteration: 3 || Loss: 13.321937659911804
Iteration: 4 || Loss: 13.32168058564588
Iteration: 5 || Loss: 13.321424297107177
Iteration: 6 || Loss: 13.321424297107177
saving ADAM checkpoint...
Sum of params:83.97925
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.321424297107177
Iteration: 2 || Loss: 13.284984707648805
Iteration: 3 || Loss: 13.269183230658815
Iteration: 4 || Loss: 13.213277699732258
Iteration: 5 || Loss: 13.159855766050432
Iteration: 6 || Loss: 13.119016899142204
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.0524
Epoch 92 loss:13.119016899142204
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.0524
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.61583149462933
Iteration: 2 || Loss: 44.615567223295855
Iteration: 3 || Loss: 44.61530352701806
Iteration: 4 || Loss: 44.615038723852265
Iteration: 5 || Loss: 44.614775319326135
Iteration: 6 || Loss: 44.614775319326135
saving ADAM checkpoint...
Sum of params:84.052505
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.614775319326135
Iteration: 2 || Loss: 44.54761271474554
Iteration: 3 || Loss: 44.172940662319995
Iteration: 4 || Loss: 44.01071888955528
Iteration: 5 || Loss: 43.928634587969036
Iteration: 6 || Loss: 43.720166533488765
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.87549
Epoch 92 loss:43.720166533488765
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.22968866307539
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:68.28905862614631
waveform batch: 2/2
Test loss - extrapolation:35.85324247946551
Epoch 92 mean train loss:2.1144898466318627
Epoch 92 mean test loss - interpolation:2.204948110512565
Epoch 92 mean test loss - extrapolation:8.678525092134318
Start training epoch 93
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.87549
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.009151257802552
Iteration: 2 || Loss: 6.0077131121560345
Iteration: 3 || Loss: 6.00627107694078
Iteration: 4 || Loss: 6.004835780688286
Iteration: 5 || Loss: 6.0033989927684965
Iteration: 6 || Loss: 6.0033989927684965
saving ADAM checkpoint...
Sum of params:83.87548
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.0033989927684965
Iteration: 2 || Loss: 4.768807256642667
Iteration: 3 || Loss: 4.6888455977592285
Iteration: 4 || Loss: 4.6467754146957025
Iteration: 5 || Loss: 4.640614166783259
Iteration: 6 || Loss: 4.447172687332451
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.01878
Epoch 93 loss:4.447172687332451
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.01878
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.455288436297428
Iteration: 2 || Loss: 13.45483943270117
Iteration: 3 || Loss: 13.454391681993663
Iteration: 4 || Loss: 13.453945210336022
Iteration: 5 || Loss: 13.453498164513135
Iteration: 6 || Loss: 13.453498164513135
saving ADAM checkpoint...
Sum of params:84.018814
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.453498164513135
Iteration: 2 || Loss: 13.353053433819076
Iteration: 3 || Loss: 13.30213893302172
Iteration: 4 || Loss: 13.246187103147307
Iteration: 5 || Loss: 13.145050002192907
Iteration: 6 || Loss: 13.102188410216883
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.05934
Epoch 93 loss:13.102188410216883
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.05934
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.583867514649796
Iteration: 2 || Loss: 44.58348898963226
Iteration: 3 || Loss: 44.583106866875056
Iteration: 4 || Loss: 44.58272579699534
Iteration: 5 || Loss: 44.582344710976095
Iteration: 6 || Loss: 44.582344710976095
saving ADAM checkpoint...
Sum of params:84.05946
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.582344710976095
Iteration: 2 || Loss: 44.482393806144444
Iteration: 3 || Loss: 44.00921129359556
Iteration: 4 || Loss: 43.87706673897072
Iteration: 5 || Loss: 43.81021425268183
Iteration: 6 || Loss: 43.63411018979137
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.86014
Epoch 93 loss:43.63411018979137
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.296223939911256
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:68.05574299835685
waveform batch: 2/2
Test loss - extrapolation:35.56910345077503
Epoch 93 mean train loss:2.1097748719772658
Epoch 93 mean test loss - interpolation:2.2160373233185426
Epoch 93 mean test loss - extrapolation:8.63540387076099
Start training epoch 94
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.86014
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.953665365801021
Iteration: 2 || Loss: 5.952271007335167
Iteration: 3 || Loss: 5.950878697263832
Iteration: 4 || Loss: 5.949486534393636
Iteration: 5 || Loss: 5.948094302863707
Iteration: 6 || Loss: 5.948094302863707
saving ADAM checkpoint...
Sum of params:83.86014
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.948094302863707
Iteration: 2 || Loss: 4.786892912262998
Iteration: 3 || Loss: 4.681861879008177
Iteration: 4 || Loss: 4.612603008524452
Iteration: 5 || Loss: 4.605470465110257
Iteration: 6 || Loss: 4.58759851129579
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.961395
Epoch 94 loss:4.58759851129579
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.961395
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.330301300644239
Iteration: 2 || Loss: 13.33000675032346
Iteration: 3 || Loss: 13.329712330189025
Iteration: 4 || Loss: 13.329419236362549
Iteration: 5 || Loss: 13.329124325400903
Iteration: 6 || Loss: 13.329124325400903
saving ADAM checkpoint...
Sum of params:83.9615
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.329124325400903
Iteration: 2 || Loss: 13.276728961573896
Iteration: 3 || Loss: 13.263595130953192
Iteration: 4 || Loss: 13.207156981978338
Iteration: 5 || Loss: 13.159417415453158
Iteration: 6 || Loss: 13.1117228463471
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.028465
Epoch 94 loss:13.1117228463471
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.028465
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.280833354149514
Iteration: 2 || Loss: 44.28064731579887
Iteration: 3 || Loss: 44.28046166578179
Iteration: 4 || Loss: 44.28027615213009
Iteration: 5 || Loss: 44.28008805162323
Iteration: 6 || Loss: 44.28008805162323
saving ADAM checkpoint...
Sum of params:84.0285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.28008805162323
Iteration: 2 || Loss: 43.962959914897006
Iteration: 3 || Loss: 43.88655023326245
Iteration: 4 || Loss: 43.62280574622888
Iteration: 5 || Loss: 43.55035620322407
Iteration: 6 || Loss: 43.37087614458061
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.849686
Epoch 94 loss:43.37087614458061
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.068683938326005
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:67.72431566856925
waveform batch: 2/2
Test loss - extrapolation:35.610646772835835
Epoch 94 mean train loss:2.1058688793870175
Epoch 94 mean test loss - interpolation:2.178113989721001
Epoch 94 mean test loss - extrapolation:8.61124687011709
Start training epoch 95
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.849686
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.609700352653527
Iteration: 2 || Loss: 5.608540852033169
Iteration: 3 || Loss: 5.607378937577369
Iteration: 4 || Loss: 5.60622055190813
Iteration: 5 || Loss: 5.605060247918679
Iteration: 6 || Loss: 5.605060247918679
saving ADAM checkpoint...
Sum of params:83.84972
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.605060247918679
Iteration: 2 || Loss: 4.8122264393960705
Iteration: 3 || Loss: 4.744563142904283
Iteration: 4 || Loss: 4.709928411636386
Iteration: 5 || Loss: 4.700194477330938
Iteration: 6 || Loss: 4.548102609526098
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.9048
Epoch 95 loss:4.548102609526098
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.9048
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.30598565753473
Iteration: 2 || Loss: 13.30588671152779
Iteration: 3 || Loss: 13.30578616099625
Iteration: 4 || Loss: 13.305687403036256
Iteration: 5 || Loss: 13.305589572525578
Iteration: 6 || Loss: 13.305589572525578
saving ADAM checkpoint...
Sum of params:83.905
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.305589572525578
Iteration: 2 || Loss: 13.300264691992519
Iteration: 3 || Loss: 13.265988649798555
Iteration: 4 || Loss: 13.149646095159644
Iteration: 5 || Loss: 13.109611242329052
Iteration: 6 || Loss: 13.0716328360592
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.03066
Epoch 95 loss:13.0716328360592
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.03066
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.18175228624036
Iteration: 2 || Loss: 44.18141887748076
Iteration: 3 || Loss: 44.181084750132364
Iteration: 4 || Loss: 44.180751918712424
Iteration: 5 || Loss: 44.18041716780974
Iteration: 6 || Loss: 44.18041716780974
saving ADAM checkpoint...
Sum of params:84.03077
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.18041716780974
Iteration: 2 || Loss: 44.102512550615344
Iteration: 3 || Loss: 43.71606974388616
Iteration: 4 || Loss: 43.54140988539591
Iteration: 5 || Loss: 43.47839733818815
Iteration: 6 || Loss: 43.29297028671296
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.83656
Epoch 95 loss:43.29297028671296
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.170337535809317
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:67.59240545100667
waveform batch: 2/2
Test loss - extrapolation:35.316820704979754
Epoch 95 mean train loss:2.10043812869994
Epoch 95 mean test loss - interpolation:2.1950562559682196
Epoch 95 mean test loss - extrapolation:8.575768846332203
Start training epoch 96
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.83656
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.936523670425238
Iteration: 2 || Loss: 5.9351339043758315
Iteration: 3 || Loss: 5.933743499772801
Iteration: 4 || Loss: 5.932350237609881
Iteration: 5 || Loss: 5.930959409224311
Iteration: 6 || Loss: 5.930959409224311
saving ADAM checkpoint...
Sum of params:83.83656
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.930959409224311
Iteration: 2 || Loss: 4.77975860546494
Iteration: 3 || Loss: 4.689513402689019
Iteration: 4 || Loss: 4.633845564783956
Iteration: 5 || Loss: 4.6273276642566765
Iteration: 6 || Loss: 4.573861796826781
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.8981
Epoch 96 loss:4.573861796826781
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.8981
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.23610483476475
Iteration: 2 || Loss: 13.235903967350904
Iteration: 3 || Loss: 13.235704318101588
Iteration: 4 || Loss: 13.235505516528448
Iteration: 5 || Loss: 13.235307073178857
Iteration: 6 || Loss: 13.235307073178857
saving ADAM checkpoint...
Sum of params:83.89821
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.235307073178857
Iteration: 2 || Loss: 13.207658697511572
Iteration: 3 || Loss: 13.191324252943907
Iteration: 4 || Loss: 13.148793932962448
Iteration: 5 || Loss: 13.095154367472079
Iteration: 6 || Loss: 13.061871789548698
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.984375
Epoch 96 loss:13.061871789548698
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.984375
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.81144967473972
Iteration: 2 || Loss: 43.8112551045607
Iteration: 3 || Loss: 43.811061718901385
Iteration: 4 || Loss: 43.81086954384001
Iteration: 5 || Loss: 43.810678287311895
Iteration: 6 || Loss: 43.810678287311895
saving ADAM checkpoint...
Sum of params:83.984474
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.810678287311895
Iteration: 2 || Loss: 43.769460048910915
Iteration: 3 || Loss: 43.54982394254578
Iteration: 4 || Loss: 43.37808033219894
Iteration: 5 || Loss: 43.2925181004958
Iteration: 6 || Loss: 43.08410444666352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.81245
Epoch 96 loss:43.08410444666352
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.085334393404615
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:67.32152122523505
waveform batch: 2/2
Test loss - extrapolation:35.164100884030155
Epoch 96 mean train loss:2.093787518380655
Epoch 96 mean test loss - interpolation:2.180889065567436
Epoch 96 mean test loss - extrapolation:8.540468509105432
Start training epoch 97
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.81245
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.986144640721424
Iteration: 2 || Loss: 5.984744136315341
Iteration: 3 || Loss: 5.98334150796158
Iteration: 4 || Loss: 5.981942100763149
Iteration: 5 || Loss: 5.980543426354757
Iteration: 6 || Loss: 5.980543426354757
saving ADAM checkpoint...
Sum of params:83.812416
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.980543426354757
Iteration: 2 || Loss: 4.818365003012497
Iteration: 3 || Loss: 4.746722505401067
Iteration: 4 || Loss: 4.707720716535104
Iteration: 5 || Loss: 4.69998676715299
Iteration: 6 || Loss: 4.455317625094366
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.90125
Epoch 97 loss:4.455317625094366
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.90125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.363181002784595
Iteration: 2 || Loss: 13.362725105557955
Iteration: 3 || Loss: 13.362270029040724
Iteration: 4 || Loss: 13.36181626838173
Iteration: 5 || Loss: 13.361363192553615
Iteration: 6 || Loss: 13.361363192553615
saving ADAM checkpoint...
Sum of params:83.901276
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.361363192553615
Iteration: 2 || Loss: 13.247979264504663
Iteration: 3 || Loss: 13.230677258083228
Iteration: 4 || Loss: 13.190428988118715
Iteration: 5 || Loss: 13.072112588357955
Iteration: 6 || Loss: 13.060350694325635
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.93732
Epoch 97 loss:13.060350694325635
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.93732
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.57760800840209
Iteration: 2 || Loss: 43.57721589341697
Iteration: 3 || Loss: 43.57682505638743
Iteration: 4 || Loss: 43.576434126879064
Iteration: 5 || Loss: 43.57604270527223
Iteration: 6 || Loss: 43.57604270527223
saving ADAM checkpoint...
Sum of params:83.937416
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.57604270527223
Iteration: 2 || Loss: 43.487176357187806
Iteration: 3 || Loss: 43.28529934954817
Iteration: 4 || Loss: 43.20995648368844
Iteration: 5 || Loss: 43.15110419658897
Iteration: 6 || Loss: 42.73200716086009
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.685905
Epoch 97 loss:42.73200716086009
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.310341815229648
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:66.53503601929268
waveform batch: 2/2
Test loss - extrapolation:34.08749452243649
Epoch 97 mean train loss:2.077506051044141
Epoch 97 mean test loss - interpolation:2.2183903025382747
Epoch 97 mean test loss - extrapolation:8.38521087847743
Start training epoch 98
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.685905
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.050964645246558
Iteration: 2 || Loss: 6.049819366093495
Iteration: 3 || Loss: 6.048675963814191
Iteration: 4 || Loss: 6.047533217850406
Iteration: 5 || Loss: 6.046391705826266
Iteration: 6 || Loss: 6.046391705826266
saving ADAM checkpoint...
Sum of params:83.685905
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.046391705826266
Iteration: 2 || Loss: 5.244463527758928
Iteration: 3 || Loss: 5.0719046334957785
Iteration: 4 || Loss: 5.002866547261906
Iteration: 5 || Loss: 4.987155334333298
Iteration: 6 || Loss: 4.530854130353631
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.95302
Epoch 98 loss:4.530854130353631
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.95302
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.640092687901065
Iteration: 2 || Loss: 13.639639310364227
Iteration: 3 || Loss: 13.639187953267982
Iteration: 4 || Loss: 13.638736594553638
Iteration: 5 || Loss: 13.63828619928792
Iteration: 6 || Loss: 13.63828619928792
saving ADAM checkpoint...
Sum of params:83.95307
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.63828619928792
Iteration: 2 || Loss: 13.543591542544949
Iteration: 3 || Loss: 13.341770773570122
Iteration: 4 || Loss: 13.259407743802877
Iteration: 5 || Loss: 13.104297649371633
Iteration: 6 || Loss: 13.069311855377624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.957054
Epoch 98 loss:13.069311855377624
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.957054
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.58606743436322
Iteration: 2 || Loss: 43.585704896897326
Iteration: 3 || Loss: 43.58534105208996
Iteration: 4 || Loss: 43.58497855148014
Iteration: 5 || Loss: 43.58461591648692
Iteration: 6 || Loss: 43.58461591648692
saving ADAM checkpoint...
Sum of params:83.95718
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.58461591648692
Iteration: 2 || Loss: 43.486671029537355
Iteration: 3 || Loss: 43.01379667290434
Iteration: 4 || Loss: 42.90058478712892
Iteration: 5 || Loss: 42.811395401557164
Iteration: 6 || Loss: 42.664378401292765
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.77614
Epoch 98 loss:42.664378401292765
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.117378494633089
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:66.60563131255192
waveform batch: 2/2
Test loss - extrapolation:34.331123108240185
Epoch 98 mean train loss:2.0780877374835867
Epoch 98 mean test loss - interpolation:2.186229749105515
Epoch 98 mean test loss - extrapolation:8.411396201732677
Start training epoch 99
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.77614
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2297351898495705
Iteration: 2 || Loss: 6.228178125190735
Iteration: 3 || Loss: 6.2266218655048355
Iteration: 4 || Loss: 6.225064313439347
Iteration: 5 || Loss: 6.223508743165852
Iteration: 6 || Loss: 6.223508743165852
saving ADAM checkpoint...
Sum of params:83.776146
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.223508743165852
Iteration: 2 || Loss: 4.772139893830956
Iteration: 3 || Loss: 4.678554007735767
Iteration: 4 || Loss: 4.617899163249157
Iteration: 5 || Loss: 4.611584476704477
Iteration: 6 || Loss: 4.510499844710906
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.836716
Epoch 99 loss:4.510499844710906
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.836716
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.326616041831373
Iteration: 2 || Loss: 13.326247174468369
Iteration: 3 || Loss: 13.325878249109895
Iteration: 4 || Loss: 13.325511547285604
Iteration: 5 || Loss: 13.32514356172747
Iteration: 6 || Loss: 13.32514356172747
saving ADAM checkpoint...
Sum of params:83.836845
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.32514356172747
Iteration: 2 || Loss: 13.228867680737238
Iteration: 3 || Loss: 13.17078515619794
Iteration: 4 || Loss: 13.153068783355877
Iteration: 5 || Loss: 13.047238016772129
Iteration: 6 || Loss: 13.041003180487795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.960106
Epoch 99 loss:13.041003180487795
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.960106
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.33889875787712
Iteration: 2 || Loss: 43.33851643610029
Iteration: 3 || Loss: 43.3381361498217
Iteration: 4 || Loss: 43.337756413291224
Iteration: 5 || Loss: 43.33738040847193
Iteration: 6 || Loss: 43.33738040847193
saving ADAM checkpoint...
Sum of params:83.9602
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.33738040847193
Iteration: 2 || Loss: 43.24727207279358
Iteration: 3 || Loss: 42.873746290965855
Iteration: 4 || Loss: 42.7323423957688
Iteration: 5 || Loss: 42.671164944915525
Iteration: 6 || Loss: 42.51006196255909
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.77676
Epoch 99 loss:42.51006196255909
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.101667138168969
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:66.36526255045248
waveform batch: 2/2
Test loss - extrapolation:34.20368498664613
Epoch 99 mean train loss:2.071088447853717
Epoch 99 mean test loss - interpolation:2.1836111896948283
Epoch 99 mean test loss - extrapolation:8.380745628091551
Start training epoch 100
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.77676
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.099036828782035
Iteration: 2 || Loss: 6.097580969706148
Iteration: 3 || Loss: 6.096122132034584
Iteration: 4 || Loss: 6.094666202354651
Iteration: 5 || Loss: 6.093210468673732
Iteration: 6 || Loss: 6.093210468673732
saving ADAM checkpoint...
Sum of params:83.77676
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.093210468673732
Iteration: 2 || Loss: 4.830563925928593
Iteration: 3 || Loss: 4.733345606521584
Iteration: 4 || Loss: 4.6694967328204635
Iteration: 5 || Loss: 4.66229943953571
Iteration: 6 || Loss: 4.635521313610402
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85921
Epoch 100 loss:4.635521313610402
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.85921
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.260188240683089
Iteration: 2 || Loss: 13.259886932352687
Iteration: 3 || Loss: 13.259585489666346
Iteration: 4 || Loss: 13.259286636352943
Iteration: 5 || Loss: 13.258987787669748
Iteration: 6 || Loss: 13.258987787669748
saving ADAM checkpoint...
Sum of params:83.85931
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.258987787669748
Iteration: 2 || Loss: 13.201356685885685
Iteration: 3 || Loss: 13.1875021177689
Iteration: 4 || Loss: 13.145999057285852
Iteration: 5 || Loss: 13.087167376066049
Iteration: 6 || Loss: 13.030422437590705
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.96167
Epoch 100 loss:13.030422437590705
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.96167
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.293560443871
Iteration: 2 || Loss: 43.293353594092345
Iteration: 3 || Loss: 43.293148590948846
Iteration: 4 || Loss: 43.29294300240311
Iteration: 5 || Loss: 43.29273714785526
Iteration: 6 || Loss: 43.29273714785526
saving ADAM checkpoint...
Sum of params:83.96175
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.29273714785526
Iteration: 2 || Loss: 43.089763502612655
Iteration: 3 || Loss: 42.82632970688585
Iteration: 4 || Loss: 42.53696549556856
Iteration: 5 || Loss: 42.44678976541546
Iteration: 6 || Loss: 42.33781558545256
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.77287
Epoch 100 loss:42.33781558545256
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.883091405886194
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:65.94565553730301
waveform batch: 2/2
Test loss - extrapolation:34.20986053079975
Epoch 100 mean train loss:2.0690951495397814
Epoch 100 mean test loss - interpolation:2.1471819009810322
Epoch 100 mean test loss - extrapolation:8.34629300567523
Start training epoch 101
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.77287
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.572973746909472
Iteration: 2 || Loss: 5.571821548735714
Iteration: 3 || Loss: 5.570671934271368
Iteration: 4 || Loss: 5.569521770643129
Iteration: 5 || Loss: 5.568372798381157
Iteration: 6 || Loss: 5.568372798381157
saving ADAM checkpoint...
Sum of params:83.77287
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.568372798381157
Iteration: 2 || Loss: 4.799073154821507
Iteration: 3 || Loss: 4.7336756703316185
Iteration: 4 || Loss: 4.699583440844894
Iteration: 5 || Loss: 4.686884278788328
Iteration: 6 || Loss: 4.528332119282241
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.81123
Epoch 101 loss:4.528332119282241
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.81123
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.263773026544417
Iteration: 2 || Loss: 13.263617077744389
Iteration: 3 || Loss: 13.263460307717923
Iteration: 4 || Loss: 13.26330505226926
Iteration: 5 || Loss: 13.263149919419899
Iteration: 6 || Loss: 13.263149919419899
saving ADAM checkpoint...
Sum of params:83.81145
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.263149919419899
Iteration: 2 || Loss: 13.246966527591137
Iteration: 3 || Loss: 13.192331766410309
Iteration: 4 || Loss: 13.070750046363248
Iteration: 5 || Loss: 12.999918829870387
Iteration: 6 || Loss: 12.98326164893563
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.93537
Epoch 101 loss:12.98326164893563
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.93537
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.1026867045275
Iteration: 2 || Loss: 43.10214871437402
Iteration: 3 || Loss: 43.10161253045928
Iteration: 4 || Loss: 43.1010778996766
Iteration: 5 || Loss: 43.100543941265684
Iteration: 6 || Loss: 43.100543941265684
saving ADAM checkpoint...
Sum of params:83.935486
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.100543941265684
Iteration: 2 || Loss: 42.93527835804186
Iteration: 3 || Loss: 42.58854853861557
Iteration: 4 || Loss: 42.51019130611056
Iteration: 5 || Loss: 42.42788296942695
Iteration: 6 || Loss: 42.25053431103847
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.69855
Epoch 101 loss:42.25053431103847
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:13.047911929297648
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:65.69782701319694
waveform batch: 2/2
Test loss - extrapolation:33.69151384044765
Epoch 101 mean train loss:2.060763037215736
Epoch 101 mean test loss - interpolation:2.1746519882162745
Epoch 101 mean test loss - extrapolation:8.28244507113705
Start training epoch 102
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.69855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.831435518452897
Iteration: 2 || Loss: 5.830165706557561
Iteration: 3 || Loss: 5.82890056831564
Iteration: 4 || Loss: 5.827635095046551
Iteration: 5 || Loss: 5.8263681999310375
Iteration: 6 || Loss: 5.8263681999310375
saving ADAM checkpoint...
Sum of params:83.69854
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.8263681999310375
Iteration: 2 || Loss: 4.871608637388698
Iteration: 3 || Loss: 4.785836608898458
Iteration: 4 || Loss: 4.73025567787727
Iteration: 5 || Loss: 4.7206014779157135
Iteration: 6 || Loss: 4.686049320421234
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.81162
Epoch 102 loss:4.686049320421234
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.81162
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.266688446530456
Iteration: 2 || Loss: 13.266410824071702
Iteration: 3 || Loss: 13.266134931218957
Iteration: 4 || Loss: 13.265858349439682
Iteration: 5 || Loss: 13.265584060818643
Iteration: 6 || Loss: 13.265584060818643
saving ADAM checkpoint...
Sum of params:83.81173
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.265584060818643
Iteration: 2 || Loss: 13.219551060699322
Iteration: 3 || Loss: 13.200127018805938
Iteration: 4 || Loss: 13.086369268665832
Iteration: 5 || Loss: 13.053174042629218
Iteration: 6 || Loss: 12.995818113320514
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85321
Epoch 102 loss:12.995818113320514
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.85321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 42.6280412757137
Iteration: 2 || Loss: 42.62773778982467
Iteration: 3 || Loss: 42.62743466576148
Iteration: 4 || Loss: 42.62713038332867
Iteration: 5 || Loss: 42.626831597593814
Iteration: 6 || Loss: 42.626831597593814
saving ADAM checkpoint...
Sum of params:83.85331
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 42.626831597593814
Iteration: 2 || Loss: 42.576017810910066
Iteration: 3 || Loss: 42.393345168076024
Iteration: 4 || Loss: 42.21568158730105
Iteration: 5 || Loss: 42.1685944936586
Iteration: 6 || Loss: 42.017771761884894
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.70424
Epoch 102 loss:42.017771761884894
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.87527663353876
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:65.58125293662086
waveform batch: 2/2
Test loss - extrapolation:33.80820642889567
Epoch 102 mean train loss:2.058608248125057
Epoch 102 mean test loss - interpolation:2.1458794389231266
Epoch 102 mean test loss - extrapolation:8.282454947126377
Start training epoch 103
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.70424
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.870852966211645
Iteration: 2 || Loss: 5.869524119008199
Iteration: 3 || Loss: 5.8681982105324
Iteration: 4 || Loss: 5.866870618907752
Iteration: 5 || Loss: 5.865541805028449
Iteration: 6 || Loss: 5.865541805028449
saving ADAM checkpoint...
Sum of params:83.70423
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.865541805028449
Iteration: 2 || Loss: 4.836250052053641
Iteration: 3 || Loss: 4.76834025774819
Iteration: 4 || Loss: 4.728749835645268
Iteration: 5 || Loss: 4.717198900680869
Iteration: 6 || Loss: 4.5094884824196
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.73423
Epoch 103 loss:4.5094884824196
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.73423
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.29675271558717
Iteration: 2 || Loss: 13.296345775403195
Iteration: 3 || Loss: 13.295941141863944
Iteration: 4 || Loss: 13.295537237430914
Iteration: 5 || Loss: 13.295133859407398
Iteration: 6 || Loss: 13.295133859407398
saving ADAM checkpoint...
Sum of params:83.734344
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.295133859407398
Iteration: 2 || Loss: 13.187345296834197
Iteration: 3 || Loss: 13.138117779331845
Iteration: 4 || Loss: 13.101795756348498
Iteration: 5 || Loss: 12.967155201775377
Iteration: 6 || Loss: 12.960280199873663
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85257
Epoch 103 loss:12.960280199873663
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.85257
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 42.64697922925384
Iteration: 2 || Loss: 42.646524165491606
Iteration: 3 || Loss: 42.646068706365064
Iteration: 4 || Loss: 42.64561654310856
Iteration: 5 || Loss: 42.64516323560843
Iteration: 6 || Loss: 42.64516323560843
saving ADAM checkpoint...
Sum of params:83.852684
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 42.64516323560843
Iteration: 2 || Loss: 42.526703225250266
Iteration: 3 || Loss: 42.2502183311736
Iteration: 4 || Loss: 42.13009030800382
Iteration: 5 || Loss: 42.07131306880311
Iteration: 6 || Loss: 41.903658360378266
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.61471
Epoch 103 loss:41.903658360378266
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.953235052561867
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:65.26876630534598
waveform batch: 2/2
Test loss - extrapolation:33.404093354001986
Epoch 103 mean train loss:2.04735955319557
Epoch 103 mean test loss - interpolation:2.1588725087603113
Epoch 103 mean test loss - extrapolation:8.222738304945663
Start training epoch 104
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.61471
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.647557628735559
Iteration: 2 || Loss: 5.6464265062026575
Iteration: 3 || Loss: 5.645293908888223
Iteration: 4 || Loss: 5.644163984464581
Iteration: 5 || Loss: 5.643037038176239
Iteration: 6 || Loss: 5.643037038176239
saving ADAM checkpoint...
Sum of params:83.61472
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.643037038176239
Iteration: 2 || Loss: 4.885205166291907
Iteration: 3 || Loss: 4.778040081963052
Iteration: 4 || Loss: 4.724432346451926
Iteration: 5 || Loss: 4.716505933860044
Iteration: 6 || Loss: 4.665545811730467
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.769135
Epoch 104 loss:4.665545811730467
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.769135
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.181424261555337
Iteration: 2 || Loss: 13.181181316923897
Iteration: 3 || Loss: 13.18093914676242
Iteration: 4 || Loss: 13.180698872644259
Iteration: 5 || Loss: 13.180458493977103
Iteration: 6 || Loss: 13.180458493977103
saving ADAM checkpoint...
Sum of params:83.76925
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.180458493977103
Iteration: 2 || Loss: 13.150681968005369
Iteration: 3 || Loss: 13.105442071910964
Iteration: 4 || Loss: 13.028264081594944
Iteration: 5 || Loss: 12.999829399268156
Iteration: 6 || Loss: 12.923954179478013
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.80321
Epoch 104 loss:12.923954179478013
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.80321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 42.40190361222375
Iteration: 2 || Loss: 42.40163284527517
Iteration: 3 || Loss: 42.40136224966722
Iteration: 4 || Loss: 42.40109260355992
Iteration: 5 || Loss: 42.40082642850668
Iteration: 6 || Loss: 42.40082642850668
saving ADAM checkpoint...
Sum of params:83.803314
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 42.40082642850668
Iteration: 2 || Loss: 42.35272975281426
Iteration: 3 || Loss: 42.12606349590729
Iteration: 4 || Loss: 41.95555140760415
Iteration: 5 || Loss: 41.87610478004007
Iteration: 6 || Loss: 41.722681105591874
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.62422
Epoch 104 loss:41.722681105591874
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.783282301167715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:65.17720702624845
waveform batch: 2/2
Test loss - extrapolation:33.52246024170904
Epoch 104 mean train loss:2.0452476240275987
Epoch 104 mean test loss - interpolation:2.130547050194619
Epoch 104 mean test loss - extrapolation:8.224972272329792
Start training epoch 105
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.62422
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.886331686898218
Iteration: 2 || Loss: 5.884954582840138
Iteration: 3 || Loss: 5.883578732648406
Iteration: 4 || Loss: 5.882203645590407
Iteration: 5 || Loss: 5.880826465233692
Iteration: 6 || Loss: 5.880826465233692
saving ADAM checkpoint...
Sum of params:83.62422
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.880826465233692
Iteration: 2 || Loss: 4.774021566435054
Iteration: 3 || Loss: 4.713298536391185
Iteration: 4 || Loss: 4.679341518136567
Iteration: 5 || Loss: 4.668121419530335
Iteration: 6 || Loss: 4.469100374008769
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.693886
Epoch 105 loss:4.469100374008769
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.693886
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.32829140986854
Iteration: 2 || Loss: 13.327691004826086
Iteration: 3 || Loss: 13.327091007741547
Iteration: 4 || Loss: 13.326489860932812
Iteration: 5 || Loss: 13.32589217516448
Iteration: 6 || Loss: 13.32589217516448
saving ADAM checkpoint...
Sum of params:83.69399
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.32589217516448
Iteration: 2 || Loss: 13.121347235961505
Iteration: 3 || Loss: 13.098393317558932
Iteration: 4 || Loss: 13.060034945769104
Iteration: 5 || Loss: 12.902765649438187
Iteration: 6 || Loss: 12.896885354431962
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.77765
Epoch 105 loss:12.896885354431962
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.77765
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 42.405969206844404
Iteration: 2 || Loss: 42.40548558443251
Iteration: 3 || Loss: 42.40500072105051
Iteration: 4 || Loss: 42.40451528987638
Iteration: 5 || Loss: 42.404032700058686
Iteration: 6 || Loss: 42.404032700058686
saving ADAM checkpoint...
Sum of params:83.777756
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 42.404032700058686
Iteration: 2 || Loss: 42.27129522725277
Iteration: 3 || Loss: 42.0038127506609
Iteration: 4 || Loss: 41.86147051680166
Iteration: 5 || Loss: 41.79849801978306
Iteration: 6 || Loss: 41.60851665684592
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.54858
Epoch 105 loss:41.60851665684592
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.844298298328344
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:64.67576472955838
waveform batch: 2/2
Test loss - extrapolation:33.03758600435546
Epoch 105 mean train loss:2.033603530527126
Epoch 105 mean test loss - interpolation:2.140716383054724
Epoch 105 mean test loss - extrapolation:8.142779227826153
Start training epoch 106
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.54858
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.453599116746594
Iteration: 2 || Loss: 5.452593749330465
Iteration: 3 || Loss: 5.451589485233829
Iteration: 4 || Loss: 5.4505860412580915
Iteration: 5 || Loss: 5.449585491961385
Iteration: 6 || Loss: 5.449585491961385
saving ADAM checkpoint...
Sum of params:83.548584
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.449585491961385
Iteration: 2 || Loss: 4.856196520050144
Iteration: 3 || Loss: 4.7348782116467385
Iteration: 4 || Loss: 4.6949502663437395
Iteration: 5 || Loss: 4.654580013520815
Iteration: 6 || Loss: 4.465171314498691
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.706726
Epoch 106 loss:4.465171314498691
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.706726
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.263163171823326
Iteration: 2 || Loss: 13.262668517197897
Iteration: 3 || Loss: 13.262173715738527
Iteration: 4 || Loss: 13.2616790658676
Iteration: 5 || Loss: 13.261185686500832
Iteration: 6 || Loss: 13.261185686500832
saving ADAM checkpoint...
Sum of params:83.706764
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.261185686500832
Iteration: 2 || Loss: 13.128551162055059
Iteration: 3 || Loss: 13.103578256481402
Iteration: 4 || Loss: 13.025369535847986
Iteration: 5 || Loss: 12.867681224795744
Iteration: 6 || Loss: 12.86149369548527
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.76243
Epoch 106 loss:12.86149369548527
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.76243
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 42.286583221924026
Iteration: 2 || Loss: 42.286099046925784
Iteration: 3 || Loss: 42.28561654173666
Iteration: 4 || Loss: 42.28513782727452
Iteration: 5 || Loss: 42.284657723757455
Iteration: 6 || Loss: 42.284657723757455
saving ADAM checkpoint...
Sum of params:83.76253
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 42.284657723757455
Iteration: 2 || Loss: 42.151189405924285
Iteration: 3 || Loss: 41.80347693725568
Iteration: 4 || Loss: 41.7114400924301
Iteration: 5 || Loss: 41.638541464255404
Iteration: 6 || Loss: 41.44750807912581
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.53073
Epoch 106 loss:41.44750807912581
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.927010640497224
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:64.87468314413859
waveform batch: 2/2
Test loss - extrapolation:32.93381064369001
Epoch 106 mean train loss:2.026695623762406
Epoch 106 mean test loss - interpolation:2.154501773416204
Epoch 106 mean test loss - extrapolation:8.150707815652384
Start training epoch 107
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.53073
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.330202048867825
Iteration: 2 || Loss: 6.328630256222132
Iteration: 3 || Loss: 6.327057356512007
Iteration: 4 || Loss: 6.3254927709822395
Iteration: 5 || Loss: 6.323923023087937
Iteration: 6 || Loss: 6.323923023087937
saving ADAM checkpoint...
Sum of params:83.53073
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.323923023087937
Iteration: 2 || Loss: 4.861074867445153
Iteration: 3 || Loss: 4.774428751630418
Iteration: 4 || Loss: 4.714577664608953
Iteration: 5 || Loss: 4.705767092790271
Iteration: 6 || Loss: 4.61012519954474
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.56902
Epoch 107 loss:4.61012519954474
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.56902
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.275698929354245
Iteration: 2 || Loss: 13.275362475947789
Iteration: 3 || Loss: 13.275024866840617
Iteration: 4 || Loss: 13.274689199554475
Iteration: 5 || Loss: 13.274356984528897
Iteration: 6 || Loss: 13.274356984528897
saving ADAM checkpoint...
Sum of params:83.56914
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.274356984528897
Iteration: 2 || Loss: 13.180038411764544
Iteration: 3 || Loss: 13.04108069870582
Iteration: 4 || Loss: 13.008852377449617
Iteration: 5 || Loss: 12.916870179278584
Iteration: 6 || Loss: 12.903786073442125
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.70827
Epoch 107 loss:12.903786073442125
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.70827
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.950539107081354
Iteration: 2 || Loss: 41.950136682953186
Iteration: 3 || Loss: 41.949739427956956
Iteration: 4 || Loss: 41.94933874060957
Iteration: 5 || Loss: 41.948940362279835
Iteration: 6 || Loss: 41.948940362279835
saving ADAM checkpoint...
Sum of params:83.70838
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.948940362279835
Iteration: 2 || Loss: 41.85923654365254
Iteration: 3 || Loss: 41.568624208102634
Iteration: 4 || Loss: 41.39510028134998
Iteration: 5 || Loss: 41.35432917408988
Iteration: 6 || Loss: 41.19877302599396
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.51485
Epoch 107 loss:41.19877302599396
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.833763780845596
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:64.49610808737371
waveform batch: 2/2
Test loss - extrapolation:32.72026844746519
Epoch 107 mean train loss:2.0245753206545114
Epoch 107 mean test loss - interpolation:2.1389606301409327
Epoch 107 mean test loss - extrapolation:8.101364711236576
Start training epoch 108
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.51485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.245267249861429
Iteration: 2 || Loss: 6.243751520855535
Iteration: 3 || Loss: 6.242234476785453
Iteration: 4 || Loss: 6.240716804709714
Iteration: 5 || Loss: 6.239202196416903
Iteration: 6 || Loss: 6.239202196416903
saving ADAM checkpoint...
Sum of params:83.51485
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.239202196416903
Iteration: 2 || Loss: 4.883781103395909
Iteration: 3 || Loss: 4.8024837500778
Iteration: 4 || Loss: 4.750825596200293
Iteration: 5 || Loss: 4.740772213249171
Iteration: 6 || Loss: 4.570122061329726
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.53532
Epoch 108 loss:4.570122061329726
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.53532
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.261594013563577
Iteration: 2 || Loss: 13.261191388291852
Iteration: 3 || Loss: 13.260790348275123
Iteration: 4 || Loss: 13.260388710004973
Iteration: 5 || Loss: 13.25998774665492
Iteration: 6 || Loss: 13.25998774665492
saving ADAM checkpoint...
Sum of params:83.53543
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.25998774665492
Iteration: 2 || Loss: 13.137894124397162
Iteration: 3 || Loss: 13.026599273749694
Iteration: 4 || Loss: 12.999016280892262
Iteration: 5 || Loss: 12.898930226464637
Iteration: 6 || Loss: 12.887482622487997
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.672775
Epoch 108 loss:12.887482622487997
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.672775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.764706306747435
Iteration: 2 || Loss: 41.764269596411246
Iteration: 3 || Loss: 41.76383439465282
Iteration: 4 || Loss: 41.763398343634556
Iteration: 5 || Loss: 41.762965850395695
Iteration: 6 || Loss: 41.762965850395695
saving ADAM checkpoint...
Sum of params:83.67289
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.762965850395695
Iteration: 2 || Loss: 41.657492475903474
Iteration: 3 || Loss: 41.39708840311013
Iteration: 4 || Loss: 41.21056556269009
Iteration: 5 || Loss: 41.16707561313621
Iteration: 6 || Loss: 41.01546632614504
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.45256
Epoch 108 loss:41.01546632614504
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.77650716399162
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:64.2453888573117
waveform batch: 2/2
Test loss - extrapolation:32.571752397288535
Epoch 108 mean train loss:2.0163127934469918
Epoch 108 mean test loss - interpolation:2.1294178606652703
Epoch 108 mean test loss - extrapolation:8.06809510455002
Start training epoch 109
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.45256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.051821129590274
Iteration: 2 || Loss: 6.050416281834991
Iteration: 3 || Loss: 6.049010356465513
Iteration: 4 || Loss: 6.047606040472942
Iteration: 5 || Loss: 6.046203268031536
Iteration: 6 || Loss: 6.046203268031536
saving ADAM checkpoint...
Sum of params:83.452576
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.046203268031536
Iteration: 2 || Loss: 4.881542339004688
Iteration: 3 || Loss: 4.79731486796318
Iteration: 4 || Loss: 4.745623769666421
Iteration: 5 || Loss: 4.736598516673302
Iteration: 6 || Loss: 4.674745352872476
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.50216
Epoch 109 loss:4.674745352872476
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.50216
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.105915540860344
Iteration: 2 || Loss: 13.105707689538209
Iteration: 3 || Loss: 13.105500620310902
Iteration: 4 || Loss: 13.10529466796198
Iteration: 5 || Loss: 13.105088854935005
Iteration: 6 || Loss: 13.105088854935005
saving ADAM checkpoint...
Sum of params:83.50227
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.105088854935005
Iteration: 2 || Loss: 13.069448459488989
Iteration: 3 || Loss: 13.008630328955713
Iteration: 4 || Loss: 12.95362361883809
Iteration: 5 || Loss: 12.894483916334384
Iteration: 6 || Loss: 12.825364987347045
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.65578
Epoch 109 loss:12.825364987347045
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.65578
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.62671391136246
Iteration: 2 || Loss: 41.626531674265884
Iteration: 3 || Loss: 41.62634811717893
Iteration: 4 || Loss: 41.626170650921935
Iteration: 5 || Loss: 41.625990672353154
Iteration: 6 || Loss: 41.625990672353154
saving ADAM checkpoint...
Sum of params:83.65588
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.625990672353154
Iteration: 2 || Loss: 41.52589715494486
Iteration: 3 || Loss: 41.44377248499245
Iteration: 4 || Loss: 41.149486226128126
Iteration: 5 || Loss: 41.0138492954303
Iteration: 6 || Loss: 40.86871846760355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.44661
Epoch 109 loss:40.86871846760355
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.661029228643464
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:64.00987729476043
waveform batch: 2/2
Test loss - extrapolation:32.47298963406077
Epoch 109 mean train loss:2.0127182347525197
Epoch 109 mean test loss - interpolation:2.110171538107244
Epoch 109 mean test loss - extrapolation:8.0402389107351
Start training epoch 110
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.44661
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.219266713487859
Iteration: 2 || Loss: 6.217687181191728
Iteration: 3 || Loss: 6.216107121933745
Iteration: 4 || Loss: 6.214527235197587
Iteration: 5 || Loss: 6.212952108348305
Iteration: 6 || Loss: 6.212952108348305
saving ADAM checkpoint...
Sum of params:83.4466
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.212952108348305
Iteration: 2 || Loss: 4.760283644853113
Iteration: 3 || Loss: 4.702629800861387
Iteration: 4 || Loss: 4.675598466773946
Iteration: 5 || Loss: 4.656025216387974
Iteration: 6 || Loss: 4.480447372527167
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.50659
Epoch 110 loss:4.480447372527167
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.50659
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.200265991227711
Iteration: 2 || Loss: 13.199664244843486
Iteration: 3 || Loss: 13.199058142744093
Iteration: 4 || Loss: 13.198456318983613
Iteration: 5 || Loss: 13.197855526269954
Iteration: 6 || Loss: 13.197855526269954
saving ADAM checkpoint...
Sum of params:83.50666
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.197855526269954
Iteration: 2 || Loss: 12.984223771733024
Iteration: 3 || Loss: 12.958091455059499
Iteration: 4 || Loss: 12.873155277619679
Iteration: 5 || Loss: 12.806790118993458
Iteration: 6 || Loss: 12.796784076144318
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.609634
Epoch 110 loss:12.796784076144318
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.609634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.552739989671004
Iteration: 2 || Loss: 41.55227042031584
Iteration: 3 || Loss: 41.551801651178714
Iteration: 4 || Loss: 41.551332741478845
Iteration: 5 || Loss: 41.550868596993936
Iteration: 6 || Loss: 41.550868596993936
saving ADAM checkpoint...
Sum of params:83.60974
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.550868596993936
Iteration: 2 || Loss: 41.42806760001466
Iteration: 3 || Loss: 41.13258931491417
Iteration: 4 || Loss: 40.95114392145421
Iteration: 5 || Loss: 40.899123659678594
Iteration: 6 || Loss: 40.74661811633944
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.36865
Epoch 110 loss:40.74661811633944
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.67401519883491
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:63.652725781863566
waveform batch: 2/2
Test loss - extrapolation:32.22405893464545
Epoch 110 mean train loss:2.0008223987934803
Epoch 110 mean test loss - interpolation:2.112335866472485
Epoch 110 mean test loss - extrapolation:7.989732059709084
Start training epoch 111
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.36865
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.742352792628836
Iteration: 2 || Loss: 5.741105009659704
Iteration: 3 || Loss: 5.739858206634844
Iteration: 4 || Loss: 5.738612858674951
Iteration: 5 || Loss: 5.737367086648258
Iteration: 6 || Loss: 5.737367086648258
saving ADAM checkpoint...
Sum of params:83.36867
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.737367086648258
Iteration: 2 || Loss: 4.817060686220037
Iteration: 3 || Loss: 4.7224254148528795
Iteration: 4 || Loss: 4.67212256834763
Iteration: 5 || Loss: 4.664117708647716
Iteration: 6 || Loss: 4.624259226816842
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.47819
Epoch 111 loss:4.624259226816842
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.47819
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.955556332630918
Iteration: 2 || Loss: 12.955285074532203
Iteration: 3 || Loss: 12.955013697460602
Iteration: 4 || Loss: 12.95474395797016
Iteration: 5 || Loss: 12.954476756673891
Iteration: 6 || Loss: 12.954476756673891
saving ADAM checkpoint...
Sum of params:83.4783
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.954476756673891
Iteration: 2 || Loss: 12.91150231404834
Iteration: 3 || Loss: 12.900794541367555
Iteration: 4 || Loss: 12.845107421051603
Iteration: 5 || Loss: 12.812497848033479
Iteration: 6 || Loss: 12.739946856207359
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.56013
Epoch 111 loss:12.739946856207359
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.56013
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.29725203605754
Iteration: 2 || Loss: 41.29701423097197
Iteration: 3 || Loss: 41.296781447254105
Iteration: 4 || Loss: 41.296543509309124
Iteration: 5 || Loss: 41.296308519054264
Iteration: 6 || Loss: 41.296308519054264
saving ADAM checkpoint...
Sum of params:83.56023
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.296308519054264
Iteration: 2 || Loss: 41.257601060225774
Iteration: 3 || Loss: 41.06249391098268
Iteration: 4 || Loss: 40.797748405464965
Iteration: 5 || Loss: 40.73204095010404
Iteration: 6 || Loss: 40.581959779862665
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.36914
Epoch 111 loss:40.581959779862665
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.504121135299894
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:63.50067871757716
waveform batch: 2/2
Test loss - extrapolation:32.25226222539599
Epoch 111 mean train loss:1.9981436504443746
Epoch 111 mean test loss - interpolation:2.0840201892166488
Epoch 111 mean test loss - extrapolation:7.979411745247762
Start training epoch 112
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.36914
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.960865266588235
Iteration: 2 || Loss: 5.959372914817968
Iteration: 3 || Loss: 5.957882024734924
Iteration: 4 || Loss: 5.956390279521452
Iteration: 5 || Loss: 5.954900990322255
Iteration: 6 || Loss: 5.954900990322255
saving ADAM checkpoint...
Sum of params:83.36913
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.954900990322255
Iteration: 2 || Loss: 4.6798766752713945
Iteration: 3 || Loss: 4.628357266807488
Iteration: 4 || Loss: 4.602675826165491
Iteration: 5 || Loss: 4.579168764477206
Iteration: 6 || Loss: 4.433609695714984
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.41549
Epoch 112 loss:4.433609695714984
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.41549
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.097211132136097
Iteration: 2 || Loss: 13.096615645512475
Iteration: 3 || Loss: 13.096021253382874
Iteration: 4 || Loss: 13.095427347589398
Iteration: 5 || Loss: 13.094833752739607
Iteration: 6 || Loss: 13.094833752739607
saving ADAM checkpoint...
Sum of params:83.4156
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.094833752739607
Iteration: 2 || Loss: 12.89079587748623
Iteration: 3 || Loss: 12.860520356017226
Iteration: 4 || Loss: 12.79016312124596
Iteration: 5 || Loss: 12.719730491820842
Iteration: 6 || Loss: 12.696319182310054
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.51649
Epoch 112 loss:12.696319182310054
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.51649
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 41.16318067501059
Iteration: 2 || Loss: 41.16280271903377
Iteration: 3 || Loss: 41.162426509843534
Iteration: 4 || Loss: 41.16205122297319
Iteration: 5 || Loss: 41.16167417303467
Iteration: 6 || Loss: 41.16167417303467
saving ADAM checkpoint...
Sum of params:83.5166
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 41.16167417303467
Iteration: 2 || Loss: 41.08224988221564
Iteration: 3 || Loss: 40.834335325313425
Iteration: 4 || Loss: 40.65809056016233
Iteration: 5 || Loss: 40.61986033871671
Iteration: 6 || Loss: 40.41534275579986
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.25766
Epoch 112 loss:40.41534275579986
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.634031887431853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:63.478604478503264
waveform batch: 2/2
Test loss - extrapolation:31.987382122131866
Epoch 112 mean train loss:1.9843197115112032
Epoch 112 mean test loss - interpolation:2.105671981238642
Epoch 112 mean test loss - extrapolation:7.955498883386261
Start training epoch 113
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.25766
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.265538653185308
Iteration: 2 || Loss: 6.263963607028072
Iteration: 3 || Loss: 6.262386786691203
Iteration: 4 || Loss: 6.260812234995557
Iteration: 5 || Loss: 6.259237379930953
Iteration: 6 || Loss: 6.259237379930953
saving ADAM checkpoint...
Sum of params:83.257675
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.259237379930953
Iteration: 2 || Loss: 4.8020804948609435
Iteration: 3 || Loss: 4.724420382399298
Iteration: 4 || Loss: 4.674649487832794
Iteration: 5 || Loss: 4.666237847580574
Iteration: 6 || Loss: 4.510521370415172
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.27831
Epoch 113 loss:4.510521370415172
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.27831
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.158833931070951
Iteration: 2 || Loss: 13.158317026019052
Iteration: 3 || Loss: 13.157802423847782
Iteration: 4 || Loss: 13.157288032563557
Iteration: 5 || Loss: 13.156775201848397
Iteration: 6 || Loss: 13.156775201848397
saving ADAM checkpoint...
Sum of params:83.27841
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.156775201848397
Iteration: 2 || Loss: 12.967997260045461
Iteration: 3 || Loss: 12.859483176519479
Iteration: 4 || Loss: 12.823455101393868
Iteration: 5 || Loss: 12.7296448506712
Iteration: 6 || Loss: 12.720454385828745
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.40682
Epoch 113 loss:12.720454385828745
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.40682
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.90344913489492
Iteration: 2 || Loss: 40.90294207548928
Iteration: 3 || Loss: 40.90243508385643
Iteration: 4 || Loss: 40.90192948782565
Iteration: 5 || Loss: 40.901423404620246
Iteration: 6 || Loss: 40.901423404620246
saving ADAM checkpoint...
Sum of params:83.40695
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.901423404620246
Iteration: 2 || Loss: 40.76147464541873
Iteration: 3 || Loss: 40.53199961138721
Iteration: 4 || Loss: 40.383496286818854
Iteration: 5 || Loss: 40.331139652795386
Iteration: 6 || Loss: 40.12447545230102
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.219284
Epoch 113 loss:40.12447545230102
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.488589114317966
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:62.686005189174814
waveform batch: 2/2
Test loss - extrapolation:31.490872050089507
Epoch 113 mean train loss:1.9777741796049977
Epoch 113 mean test loss - interpolation:2.081431519052994
Epoch 113 mean test loss - extrapolation:7.848073103272026
Start training epoch 114
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.219284
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.878274658293095
Iteration: 2 || Loss: 5.876973895793605
Iteration: 3 || Loss: 5.875678486543434
Iteration: 4 || Loss: 5.874379122822905
Iteration: 5 || Loss: 5.873081787799039
Iteration: 6 || Loss: 5.873081787799039
saving ADAM checkpoint...
Sum of params:83.219284
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.873081787799039
Iteration: 2 || Loss: 4.913476459427537
Iteration: 3 || Loss: 4.864590312439835
Iteration: 4 || Loss: 4.8286787319124596
Iteration: 5 || Loss: 4.766054540788368
Iteration: 6 || Loss: 4.571581517155367
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.26033
Epoch 114 loss:4.571581517155367
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.26033
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.264543520906011
Iteration: 2 || Loss: 13.263824991760401
Iteration: 3 || Loss: 13.263109259690529
Iteration: 4 || Loss: 13.262393756111381
Iteration: 5 || Loss: 13.261678805311972
Iteration: 6 || Loss: 13.261678805311972
saving ADAM checkpoint...
Sum of params:83.26046
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.261678805311972
Iteration: 2 || Loss: 12.951505213964017
Iteration: 3 || Loss: 12.91944004324706
Iteration: 4 || Loss: 12.866728647638942
Iteration: 5 || Loss: 12.745205328086806
Iteration: 6 || Loss: 12.733534533324082
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.365524
Epoch 114 loss:12.733534533324082
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.365524
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.69593878958157
Iteration: 2 || Loss: 40.69544569780696
Iteration: 3 || Loss: 40.694952567845945
Iteration: 4 || Loss: 40.694460796774024
Iteration: 5 || Loss: 40.69397304467797
Iteration: 6 || Loss: 40.69397304467797
saving ADAM checkpoint...
Sum of params:83.36567
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.69397304467797
Iteration: 2 || Loss: 40.56094907905014
Iteration: 3 || Loss: 40.28113816781675
Iteration: 4 || Loss: 40.08964473960621
Iteration: 5 || Loss: 40.043447375775585
Iteration: 6 || Loss: 39.93525729624697
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.144875
Epoch 114 loss:39.93525729624697
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.472234307376151
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:62.73516941563466
waveform batch: 2/2
Test loss - extrapolation:31.460375078734987
Epoch 114 mean train loss:1.9738059774733245
Epoch 114 mean test loss - interpolation:2.078705717896025
Epoch 114 mean test loss - extrapolation:7.849628707864137
Start training epoch 115
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.144875
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.9929702236781575
Iteration: 2 || Loss: 5.991536280392004
Iteration: 3 || Loss: 5.99009950639819
Iteration: 4 || Loss: 5.988664012749786
Iteration: 5 || Loss: 5.987226787017771
Iteration: 6 || Loss: 5.987226787017771
saving ADAM checkpoint...
Sum of params:83.14488
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.987226787017771
Iteration: 2 || Loss: 4.792486162264478
Iteration: 3 || Loss: 4.72215602030767
Iteration: 4 || Loss: 4.681952674368684
Iteration: 5 || Loss: 4.672002139758534
Iteration: 6 || Loss: 4.52276451062079
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.16431
Epoch 115 loss:4.52276451062079
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.16431
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.152721759112298
Iteration: 2 || Loss: 13.152300128656439
Iteration: 3 || Loss: 13.151878223323674
Iteration: 4 || Loss: 13.151458430917742
Iteration: 5 || Loss: 13.15103954585565
Iteration: 6 || Loss: 13.15103954585565
saving ADAM checkpoint...
Sum of params:83.16442
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.15103954585565
Iteration: 2 || Loss: 13.014090088451017
Iteration: 3 || Loss: 12.850948857737357
Iteration: 4 || Loss: 12.80124685677021
Iteration: 5 || Loss: 12.693098574948998
Iteration: 6 || Loss: 12.68220794525262
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.311584
Epoch 115 loss:12.68220794525262
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.311584
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.49046581705551
Iteration: 2 || Loss: 40.48997467664572
Iteration: 3 || Loss: 40.48948530220572
Iteration: 4 || Loss: 40.48899541968852
Iteration: 5 || Loss: 40.488507107196035
Iteration: 6 || Loss: 40.488507107196035
saving ADAM checkpoint...
Sum of params:83.31171
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.488507107196035
Iteration: 2 || Loss: 40.357772878434
Iteration: 3 || Loss: 40.135654133259905
Iteration: 4 || Loss: 39.95750708070812
Iteration: 5 || Loss: 39.91007436180214
Iteration: 6 || Loss: 39.73484501212237
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.12408
Epoch 115 loss:39.73484501212237
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.404736356043522
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:62.357799658627094
waveform batch: 2/2
Test loss - extrapolation:31.248694399813793
Epoch 115 mean train loss:1.9634419816550268
Epoch 115 mean test loss - interpolation:2.067456059340587
Epoch 115 mean test loss - extrapolation:7.80054117153674
Start training epoch 116
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.12408
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.016502940073855
Iteration: 2 || Loss: 6.0150745089669995
Iteration: 3 || Loss: 6.013646397355773
Iteration: 4 || Loss: 6.012213881020283
Iteration: 5 || Loss: 6.010784631497545
Iteration: 6 || Loss: 6.010784631497545
saving ADAM checkpoint...
Sum of params:83.12408
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.010784631497545
Iteration: 2 || Loss: 4.838699313134714
Iteration: 3 || Loss: 4.786050192066981
Iteration: 4 || Loss: 4.752887502543743
Iteration: 5 || Loss: 4.722396482232543
Iteration: 6 || Loss: 4.553454420060427
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.17383
Epoch 116 loss:4.553454420060427
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.17383
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.194765511854477
Iteration: 2 || Loss: 13.194045565634218
Iteration: 3 || Loss: 13.193325480312277
Iteration: 4 || Loss: 13.192609210317729
Iteration: 5 || Loss: 13.191892242902826
Iteration: 6 || Loss: 13.191892242902826
saving ADAM checkpoint...
Sum of params:83.17394
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.191892242902826
Iteration: 2 || Loss: 12.879335140498428
Iteration: 3 || Loss: 12.851008399661197
Iteration: 4 || Loss: 12.807351108938887
Iteration: 5 || Loss: 12.699767836096362
Iteration: 6 || Loss: 12.687058237530078
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.28096
Epoch 116 loss:12.687058237530078
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.28096
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.32831049026015
Iteration: 2 || Loss: 40.3278082073632
Iteration: 3 || Loss: 40.327306661913234
Iteration: 4 || Loss: 40.32680481880155
Iteration: 5 || Loss: 40.32630223957798
Iteration: 6 || Loss: 40.32630223957798
saving ADAM checkpoint...
Sum of params:83.281075
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.32630223957798
Iteration: 2 || Loss: 40.18886754160043
Iteration: 3 || Loss: 39.91344971885208
Iteration: 4 || Loss: 39.7245842128669
Iteration: 5 || Loss: 39.67916579369895
Iteration: 6 || Loss: 39.54927930753146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.05656
Epoch 116 loss:39.54927930753146
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.402094700889998
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:62.2241114520957
waveform batch: 2/2
Test loss - extrapolation:31.069501965461928
Epoch 116 mean train loss:1.9582686884524816
Epoch 116 mean test loss - interpolation:2.0670157834816663
Epoch 116 mean test loss - extrapolation:7.774467784796468
Start training epoch 117
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.05656
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.0615340916720255
Iteration: 2 || Loss: 6.060065879046346
Iteration: 3 || Loss: 6.058592722559501
Iteration: 4 || Loss: 6.057119336576096
Iteration: 5 || Loss: 6.05564833947299
Iteration: 6 || Loss: 6.05564833947299
saving ADAM checkpoint...
Sum of params:83.05656
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.05564833947299
Iteration: 2 || Loss: 4.803751761214313
Iteration: 3 || Loss: 4.736990996575193
Iteration: 4 || Loss: 4.699823419487485
Iteration: 5 || Loss: 4.688605134367729
Iteration: 6 || Loss: 4.5189762554951285
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.078766
Epoch 117 loss:4.5189762554951285
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.078766
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.225195160229564
Iteration: 2 || Loss: 13.22462546068674
Iteration: 3 || Loss: 13.224056571816973
Iteration: 4 || Loss: 13.22348957741498
Iteration: 5 || Loss: 13.222922208948683
Iteration: 6 || Loss: 13.222922208948683
saving ADAM checkpoint...
Sum of params:83.078865
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.222922208948683
Iteration: 2 || Loss: 12.992783423035252
Iteration: 3 || Loss: 12.847305795808433
Iteration: 4 || Loss: 12.790102920717
Iteration: 5 || Loss: 12.660713556058923
Iteration: 6 || Loss: 12.647665086900197
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.22318
Epoch 117 loss:12.647665086900197
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.22318
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.12258349668864
Iteration: 2 || Loss: 40.12204072481809
Iteration: 3 || Loss: 40.12149587453275
Iteration: 4 || Loss: 40.12095320914339
Iteration: 5 || Loss: 40.12041058529249
Iteration: 6 || Loss: 40.12041058529249
saving ADAM checkpoint...
Sum of params:83.223305
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 40.12041058529249
Iteration: 2 || Loss: 39.95819055393087
Iteration: 3 || Loss: 39.74567024854649
Iteration: 4 || Loss: 39.562938777231764
Iteration: 5 || Loss: 39.507247937619276
Iteration: 6 || Loss: 39.36143703754459
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.0719
Epoch 117 loss:39.36143703754459
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.237343993651644
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:61.5339388843613
waveform batch: 2/2
Test loss - extrapolation:30.750370528757426
Epoch 117 mean train loss:1.9492440820668935
Epoch 117 mean test loss - interpolation:2.039557332275274
Epoch 117 mean test loss - extrapolation:7.690359117759893
Start training epoch 118
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.0719
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.692868097517673
Iteration: 2 || Loss: 5.691605541191964
Iteration: 3 || Loss: 5.690343198767555
Iteration: 4 || Loss: 5.689078514420797
Iteration: 5 || Loss: 5.687819130855724
Iteration: 6 || Loss: 5.687819130855724
saving ADAM checkpoint...
Sum of params:83.0719
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.687819130855724
Iteration: 2 || Loss: 4.781132372475682
Iteration: 3 || Loss: 4.74303650304777
Iteration: 4 || Loss: 4.714603717936021
Iteration: 5 || Loss: 4.63754632673032
Iteration: 6 || Loss: 4.524215952596193
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.10739
Epoch 118 loss:4.524215952596193
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.10739
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.087695357545671
Iteration: 2 || Loss: 13.087050545478943
Iteration: 3 || Loss: 13.086407611971614
Iteration: 4 || Loss: 13.085767107487008
Iteration: 5 || Loss: 13.085124449098414
Iteration: 6 || Loss: 13.085124449098414
saving ADAM checkpoint...
Sum of params:83.10751
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.085124449098414
Iteration: 2 || Loss: 12.83217980086182
Iteration: 3 || Loss: 12.801559158255424
Iteration: 4 || Loss: 12.7412912687865
Iteration: 5 || Loss: 12.630691351983286
Iteration: 6 || Loss: 12.614580766338726
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.214584
Epoch 118 loss:12.614580766338726
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.214584
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.924086438760874
Iteration: 2 || Loss: 39.9235967669749
Iteration: 3 || Loss: 39.923103285705885
Iteration: 4 || Loss: 39.92261385946143
Iteration: 5 || Loss: 39.92212236179929
Iteration: 6 || Loss: 39.92212236179929
saving ADAM checkpoint...
Sum of params:83.214714
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.92212236179929
Iteration: 2 || Loss: 39.79051683768636
Iteration: 3 || Loss: 39.57178235433688
Iteration: 4 || Loss: 39.367867870256056
Iteration: 5 || Loss: 39.32636765480871
Iteration: 6 || Loss: 39.199130824080775
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.99831
Epoch 118 loss:39.199130824080775
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.26482128951648
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:61.76170969056139
waveform batch: 2/2
Test loss - extrapolation:30.8225590471325
Epoch 118 mean train loss:1.9426871566557136
Epoch 118 mean test loss - interpolation:2.04413688158608
Epoch 118 mean test loss - extrapolation:7.715355728141158
Start training epoch 119
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.99831
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.00448867389794
Iteration: 2 || Loss: 6.002993677304639
Iteration: 3 || Loss: 6.001501546218805
Iteration: 4 || Loss: 6.000011734083164
Iteration: 5 || Loss: 5.9985239340526375
Iteration: 6 || Loss: 5.9985239340526375
saving ADAM checkpoint...
Sum of params:82.9983
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.9985239340526375
Iteration: 2 || Loss: 4.718987237378569
Iteration: 3 || Loss: 4.661350290688716
Iteration: 4 || Loss: 4.63350829508547
Iteration: 5 || Loss: 4.622132099652766
Iteration: 6 || Loss: 4.474133899500101
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.061615
Epoch 119 loss:4.474133899500101
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.061615
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.124500774236575
Iteration: 2 || Loss: 13.123784474522093
Iteration: 3 || Loss: 13.123069583935846
Iteration: 4 || Loss: 13.12235522956649
Iteration: 5 || Loss: 13.121641506161396
Iteration: 6 || Loss: 13.121641506161396
saving ADAM checkpoint...
Sum of params:83.06165
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.121641506161396
Iteration: 2 || Loss: 12.808504108156852
Iteration: 3 || Loss: 12.773191594216641
Iteration: 4 || Loss: 12.714457920783804
Iteration: 5 || Loss: 12.588111724748275
Iteration: 6 || Loss: 12.576142187835927
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.17467
Epoch 119 loss:12.576142187835927
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.17467
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.82007390604174
Iteration: 2 || Loss: 39.81953635077458
Iteration: 3 || Loss: 39.81899990164756
Iteration: 4 || Loss: 39.818466685367376
Iteration: 5 || Loss: 39.81792997933249
Iteration: 6 || Loss: 39.81792997933249
saving ADAM checkpoint...
Sum of params:83.1748
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.81792997933249
Iteration: 2 || Loss: 39.66132110999534
Iteration: 3 || Loss: 39.45766299626103
Iteration: 4 || Loss: 39.22932684535048
Iteration: 5 || Loss: 39.180396863985344
Iteration: 6 || Loss: 39.0254165533901
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.99465
Epoch 119 loss:39.0254165533901
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.12140559179341
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:61.11345018716085
waveform batch: 2/2
Test loss - extrapolation:30.561125103889893
Epoch 119 mean train loss:1.9336445738181425
Epoch 119 mean test loss - interpolation:2.020234265298902
Epoch 119 mean test loss - extrapolation:7.639547940920895
Start training epoch 120
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.99465
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.529573745352193
Iteration: 2 || Loss: 5.528367155858675
Iteration: 3 || Loss: 5.527163987647932
Iteration: 4 || Loss: 5.525957420081945
Iteration: 5 || Loss: 5.5247519253826765
Iteration: 6 || Loss: 5.5247519253826765
saving ADAM checkpoint...
Sum of params:82.99467
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.5247519253826765
Iteration: 2 || Loss: 4.698166010902714
Iteration: 3 || Loss: 4.662240466808832
Iteration: 4 || Loss: 4.639956427078493
Iteration: 5 || Loss: 4.579189166826116
Iteration: 6 || Loss: 4.486986227262643
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.03105
Epoch 120 loss:4.486986227262643
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.03105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.037651853762547
Iteration: 2 || Loss: 13.03698586855172
Iteration: 3 || Loss: 13.036320529103767
Iteration: 4 || Loss: 13.03565402343974
Iteration: 5 || Loss: 13.034989676851414
Iteration: 6 || Loss: 13.034989676851414
saving ADAM checkpoint...
Sum of params:83.03108
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.034989676851414
Iteration: 2 || Loss: 12.762849161944926
Iteration: 3 || Loss: 12.729951348459437
Iteration: 4 || Loss: 12.666066513616839
Iteration: 5 || Loss: 12.558302197189573
Iteration: 6 || Loss: 12.544556859312024
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.13605
Epoch 120 loss:12.544556859312024
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.13605
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.58197622215158
Iteration: 2 || Loss: 39.58143721149717
Iteration: 3 || Loss: 39.58089900656883
Iteration: 4 || Loss: 39.5803623333193
Iteration: 5 || Loss: 39.57982580007058
Iteration: 6 || Loss: 39.57982580007058
saving ADAM checkpoint...
Sum of params:83.13619
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.57982580007058
Iteration: 2 || Loss: 39.42184029922626
Iteration: 3 || Loss: 39.22439700078218
Iteration: 4 || Loss: 39.025214406300506
Iteration: 5 || Loss: 38.97780889085001
Iteration: 6 || Loss: 38.83383772493806
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.940506
Epoch 120 loss:38.83383772493806
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:12.089636727459196
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:60.99626227676248
waveform batch: 2/2
Test loss - extrapolation:30.424731758979323
Epoch 120 mean train loss:1.9263924417763008
Epoch 120 mean test loss - interpolation:2.0149394545765325
Epoch 120 mean test loss - extrapolation:7.61841616964515
Start training epoch 121
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.940506
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.652909543896003
Iteration: 2 || Loss: 5.651603420754752
Iteration: 3 || Loss: 5.650296471830542
Iteration: 4 || Loss: 5.648992285762982
Iteration: 5 || Loss: 5.64768890898214
Iteration: 6 || Loss: 5.64768890898214
saving ADAM checkpoint...
Sum of params:82.940506
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.64768890898214
Iteration: 2 || Loss: 4.678705698946523
Iteration: 3 || Loss: 4.641467848295894
Iteration: 4 || Loss: 4.621785923446528
Iteration: 5 || Loss: 4.5714070154452315
Iteration: 6 || Loss: 4.46496828368329
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.98602
Epoch 121 loss:4.46496828368329
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.98602
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.030523008566368
Iteration: 2 || Loss: 13.02983916670037
Iteration: 3 || Loss: 13.029158868402634
Iteration: 4 || Loss: 13.028477155140129
Iteration: 5 || Loss: 13.02779602787309
Iteration: 6 || Loss: 13.02779602787309
saving ADAM checkpoint...
Sum of params:82.98606
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.02779602787309
Iteration: 2 || Loss: 12.744050416207205
Iteration: 3 || Loss: 12.708334567068663
Iteration: 4 || Loss: 12.641709053702963
Iteration: 5 || Loss: 12.52349554799338
Iteration: 6 || Loss: 12.510652752852256
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.092514
Epoch 121 loss:12.510652752852256
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.092514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.401350603758196
Iteration: 2 || Loss: 39.400796561973124
Iteration: 3 || Loss: 39.400240819914316
Iteration: 4 || Loss: 39.39968521068116
Iteration: 5 || Loss: 39.399131006430466
Iteration: 6 || Loss: 39.399131006430466
saving ADAM checkpoint...
Sum of params:83.09266
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.399131006430466
Iteration: 2 || Loss: 39.230209196933366
Iteration: 3 || Loss: 39.05426766068223
Iteration: 4 || Loss: 38.84030330781343
Iteration: 5 || Loss: 38.79112639912666
Iteration: 6 || Loss: 38.675166173978404
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.94275
Epoch 121 loss:38.675166173978404
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.993535253687762
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:60.52207643655194
waveform batch: 2/2
Test loss - extrapolation:30.165011776919684
Epoch 121 mean train loss:1.9189926624315157
Epoch 121 mean test loss - interpolation:1.9989225422812937
Epoch 121 mean test loss - extrapolation:7.557257351122636
Start training epoch 122
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.94275
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.455699145265322
Iteration: 2 || Loss: 5.454489445011355
Iteration: 3 || Loss: 5.453279798071231
Iteration: 4 || Loss: 5.452071568811115
Iteration: 5 || Loss: 5.450865738635391
Iteration: 6 || Loss: 5.450865738635391
saving ADAM checkpoint...
Sum of params:82.94273
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.450865738635391
Iteration: 2 || Loss: 4.623961452345096
Iteration: 3 || Loss: 4.5946193560181054
Iteration: 4 || Loss: 4.572627040834593
Iteration: 5 || Loss: 4.48469225181103
Iteration: 6 || Loss: 4.435339406773023
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.98084
Epoch 122 loss:4.435339406773023
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.98084
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.971312393460167
Iteration: 2 || Loss: 12.970644784281511
Iteration: 3 || Loss: 12.969977303803844
Iteration: 4 || Loss: 12.969314003500063
Iteration: 5 || Loss: 12.968649911092744
Iteration: 6 || Loss: 12.968649911092744
saving ADAM checkpoint...
Sum of params:82.98087
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.968649911092744
Iteration: 2 || Loss: 12.70334726849176
Iteration: 3 || Loss: 12.665799454285462
Iteration: 4 || Loss: 12.584686958036912
Iteration: 5 || Loss: 12.466619111889118
Iteration: 6 || Loss: 12.455215853552202
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.08243
Epoch 122 loss:12.455215853552202
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.08243
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.31225957574842
Iteration: 2 || Loss: 39.31168698294344
Iteration: 3 || Loss: 39.3111136379754
Iteration: 4 || Loss: 39.310539409092264
Iteration: 5 || Loss: 39.309970954013515
Iteration: 6 || Loss: 39.309970954013515
saving ADAM checkpoint...
Sum of params:83.082535
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.309970954013515
Iteration: 2 || Loss: 39.13096115810279
Iteration: 3 || Loss: 38.91566504906042
Iteration: 4 || Loss: 38.70678396840969
Iteration: 5 || Loss: 38.64997839167314
Iteration: 6 || Loss: 38.53184371576139
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.908226
Epoch 122 loss:38.53184371576139
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.984495328857639
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:60.45528950373093
waveform batch: 2/2
Test loss - extrapolation:30.080155700838198
Epoch 122 mean train loss:1.9111172060719521
Epoch 122 mean test loss - interpolation:1.9974158881429398
Epoch 122 mean test loss - extrapolation:7.544620433714094
Start training epoch 123
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.908226
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.551100379893025
Iteration: 2 || Loss: 5.549804202082714
Iteration: 3 || Loss: 5.548504435213599
Iteration: 4 || Loss: 5.547207123624492
Iteration: 5 || Loss: 5.545913461760933
Iteration: 6 || Loss: 5.545913461760933
saving ADAM checkpoint...
Sum of params:82.90822
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.545913461760933
Iteration: 2 || Loss: 4.596834319917892
Iteration: 3 || Loss: 4.56064864173056
Iteration: 4 || Loss: 4.543896131596378
Iteration: 5 || Loss: 4.501570545410949
Iteration: 6 || Loss: 4.400161113481096
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.96602
Epoch 123 loss:4.400161113481096
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.96602
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.909490676074896
Iteration: 2 || Loss: 12.908803915541153
Iteration: 3 || Loss: 12.908119285213413
Iteration: 4 || Loss: 12.907434884937533
Iteration: 5 || Loss: 12.90675046096515
Iteration: 6 || Loss: 12.90675046096515
saving ADAM checkpoint...
Sum of params:82.96606
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.90675046096515
Iteration: 2 || Loss: 12.627117321252294
Iteration: 3 || Loss: 12.594968709274706
Iteration: 4 || Loss: 12.532764294199199
Iteration: 5 || Loss: 12.41866556083597
Iteration: 6 || Loss: 12.405630709061644
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.066635
Epoch 123 loss:12.405630709061644
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.066635
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.14095268169212
Iteration: 2 || Loss: 39.14039973190415
Iteration: 3 || Loss: 39.139846078566706
Iteration: 4 || Loss: 39.139296292709645
Iteration: 5 || Loss: 39.13874449995748
Iteration: 6 || Loss: 39.13874449995748
saving ADAM checkpoint...
Sum of params:83.06678
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.13874449995748
Iteration: 2 || Loss: 38.97265713643436
Iteration: 3 || Loss: 38.766948484270785
Iteration: 4 || Loss: 38.55938929859849
Iteration: 5 || Loss: 38.50941993493924
Iteration: 6 || Loss: 38.37760497698844
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.869965
Epoch 123 loss:38.37760497698844
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.949670528132769
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:60.35756370357022
waveform batch: 2/2
Test loss - extrapolation:30.02699346394022
Epoch 123 mean train loss:1.9028757517079717
Epoch 123 mean test loss - interpolation:1.9916117546887948
Epoch 123 mean test loss - extrapolation:7.532046430625869
Start training epoch 124
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.869965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.577461453886467
Iteration: 2 || Loss: 5.5761237969777815
Iteration: 3 || Loss: 5.574786188885029
Iteration: 4 || Loss: 5.573450411630131
Iteration: 5 || Loss: 5.572116718265057
Iteration: 6 || Loss: 5.572116718265057
saving ADAM checkpoint...
Sum of params:82.869965
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.572116718265057
Iteration: 2 || Loss: 4.562044341420695
Iteration: 3 || Loss: 4.522428573544587
Iteration: 4 || Loss: 4.506872047157749
Iteration: 5 || Loss: 4.482394314216015
Iteration: 6 || Loss: 4.369498954819314
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.93906
Epoch 124 loss:4.369498954819314
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.93906
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.888179139570429
Iteration: 2 || Loss: 12.887461679395027
Iteration: 3 || Loss: 12.886744816134845
Iteration: 4 || Loss: 12.886031498573896
Iteration: 5 || Loss: 12.885317168592422
Iteration: 6 || Loss: 12.885317168592422
saving ADAM checkpoint...
Sum of params:82.93907
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.885317168592422
Iteration: 2 || Loss: 12.583537756223556
Iteration: 3 || Loss: 12.5493893338112
Iteration: 4 || Loss: 12.487222256777775
Iteration: 5 || Loss: 12.374595494610613
Iteration: 6 || Loss: 12.363531028829737
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.03795
Epoch 124 loss:12.363531028829737
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.03795
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.026757197806894
Iteration: 2 || Loss: 39.026177650432125
Iteration: 3 || Loss: 39.02560214636515
Iteration: 4 || Loss: 39.025025158147976
Iteration: 5 || Loss: 39.02444844316725
Iteration: 6 || Loss: 39.02444844316725
saving ADAM checkpoint...
Sum of params:83.03806
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.02444844316725
Iteration: 2 || Loss: 38.84326356418902
Iteration: 3 || Loss: 38.618642780881245
Iteration: 4 || Loss: 38.407100019057
Iteration: 5 || Loss: 38.35167464096061
Iteration: 6 || Loss: 38.22359701614047
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.85829
Epoch 124 loss:38.22359701614047
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.88666734626551
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:59.99445926441663
waveform batch: 2/2
Test loss - extrapolation:29.807458366544395
Epoch 124 mean train loss:1.895056103441018
Epoch 124 mean test loss - interpolation:1.981111224377585
Epoch 124 mean test loss - extrapolation:7.483493135913419
Start training epoch 125
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.85829
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.4457613241255665
Iteration: 2 || Loss: 5.444492857176949
Iteration: 3 || Loss: 5.443220584881576
Iteration: 4 || Loss: 5.441952242014647
Iteration: 5 || Loss: 5.440682916556796
Iteration: 6 || Loss: 5.440682916556796
saving ADAM checkpoint...
Sum of params:82.85828
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.440682916556796
Iteration: 2 || Loss: 4.533628170600007
Iteration: 3 || Loss: 4.499845623512415
Iteration: 4 || Loss: 4.485333785519561
Iteration: 5 || Loss: 4.441350085229483
Iteration: 6 || Loss: 4.349493823579457
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.917366
Epoch 125 loss:4.349493823579457
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.917366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.81421338587729
Iteration: 2 || Loss: 12.81352935178611
Iteration: 3 || Loss: 12.81284353068523
Iteration: 4 || Loss: 12.81215991562328
Iteration: 5 || Loss: 12.811478125897006
Iteration: 6 || Loss: 12.811478125897006
saving ADAM checkpoint...
Sum of params:82.91741
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.811478125897006
Iteration: 2 || Loss: 12.534533554691643
Iteration: 3 || Loss: 12.501383793668278
Iteration: 4 || Loss: 12.435220773023266
Iteration: 5 || Loss: 12.325035344229116
Iteration: 6 || Loss: 12.312282369858009
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.01262
Epoch 125 loss:12.312282369858009
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.01262
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.8080933398711
Iteration: 2 || Loss: 38.807529952330036
Iteration: 3 || Loss: 38.80696668017929
Iteration: 4 || Loss: 38.80640384526664
Iteration: 5 || Loss: 38.80584372127577
Iteration: 6 || Loss: 38.80584372127577
saving ADAM checkpoint...
Sum of params:83.01275
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.80584372127577
Iteration: 2 || Loss: 38.63256766253526
Iteration: 3 || Loss: 38.4435410935328
Iteration: 4 || Loss: 38.23883252541816
Iteration: 5 || Loss: 38.189010403827844
Iteration: 6 || Loss: 38.06747687488776
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.8322
Epoch 125 loss:38.06747687488776
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.82914974379112
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:59.84689987780708
waveform batch: 2/2
Test loss - extrapolation:29.72595504888524
Epoch 125 mean train loss:1.8872156230456973
Epoch 125 mean test loss - interpolation:1.97152495729852
Epoch 125 mean test loss - extrapolation:7.46440457722436
Start training epoch 126
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.8322
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.459397640425475
Iteration: 2 || Loss: 5.4580857469355735
Iteration: 3 || Loss: 5.456770564096333
Iteration: 4 || Loss: 5.455457438696616
Iteration: 5 || Loss: 5.454146478567118
Iteration: 6 || Loss: 5.454146478567118
saving ADAM checkpoint...
Sum of params:82.83221
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.454146478567118
Iteration: 2 || Loss: 4.48474189508911
Iteration: 3 || Loss: 4.450767168933012
Iteration: 4 || Loss: 4.438412125210105
Iteration: 5 || Loss: 4.396049217680808
Iteration: 6 || Loss: 4.300509682068162
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.90086
Epoch 126 loss:4.300509682068162
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.90086
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.751977312087464
Iteration: 2 || Loss: 12.751309155787418
Iteration: 3 || Loss: 12.750642682649023
Iteration: 4 || Loss: 12.749979998783708
Iteration: 5 || Loss: 12.749315365286899
Iteration: 6 || Loss: 12.749315365286899
saving ADAM checkpoint...
Sum of params:82.9009
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.749315365286899
Iteration: 2 || Loss: 12.490938396670401
Iteration: 3 || Loss: 12.458186118810131
Iteration: 4 || Loss: 12.388915907225597
Iteration: 5 || Loss: 12.268891848016851
Iteration: 6 || Loss: 12.255677097122858
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.99651
Epoch 126 loss:12.255677097122858
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.99651
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.69320710702407
Iteration: 2 || Loss: 38.69263569685355
Iteration: 3 || Loss: 38.692062789051896
Iteration: 4 || Loss: 38.69148913544987
Iteration: 5 || Loss: 38.690919282156294
Iteration: 6 || Loss: 38.690919282156294
saving ADAM checkpoint...
Sum of params:82.996635
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.690919282156294
Iteration: 2 || Loss: 38.51161062254896
Iteration: 3 || Loss: 38.3287802007907
Iteration: 4 || Loss: 38.10547574605482
Iteration: 5 || Loss: 38.054041058063845
Iteration: 6 || Loss: 37.94302142040375
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.83401
Epoch 126 loss:37.94302142040375
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.761642666524533
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:59.55003202057605
waveform batch: 2/2
Test loss - extrapolation:29.5887197453299
Epoch 126 mean train loss:1.8792830413653372
Epoch 126 mean test loss - interpolation:1.9602737777540888
Epoch 126 mean test loss - extrapolation:7.428229313825496
Start training epoch 127
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.83401
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.317022588138292
Iteration: 2 || Loss: 5.315760844023247
Iteration: 3 || Loss: 5.3145009391626195
Iteration: 4 || Loss: 5.313239129137234
Iteration: 5 || Loss: 5.311980186106978
Iteration: 6 || Loss: 5.311980186106978
saving ADAM checkpoint...
Sum of params:82.834015
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.311980186106978
Iteration: 2 || Loss: 4.4207823853469685
Iteration: 3 || Loss: 4.3894062485912
Iteration: 4 || Loss: 4.37850502696232
Iteration: 5 || Loss: 4.336124398247675
Iteration: 6 || Loss: 4.252989242444325
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.90099
Epoch 127 loss:4.252989242444325
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.90099
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.669186348759565
Iteration: 2 || Loss: 12.668533947016309
Iteration: 3 || Loss: 12.667881921972432
Iteration: 4 || Loss: 12.667230680587817
Iteration: 5 || Loss: 12.666578961709137
Iteration: 6 || Loss: 12.666578961709137
saving ADAM checkpoint...
Sum of params:82.901024
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.666578961709137
Iteration: 2 || Loss: 12.421057087767405
Iteration: 3 || Loss: 12.386489421215547
Iteration: 4 || Loss: 12.317592658952181
Iteration: 5 || Loss: 12.204889248277274
Iteration: 6 || Loss: 12.192523075463608
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.99209
Epoch 127 loss:12.192523075463608
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.99209
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.587681510254356
Iteration: 2 || Loss: 38.5871097820047
Iteration: 3 || Loss: 38.58653735617956
Iteration: 4 || Loss: 38.58596467553317
Iteration: 5 || Loss: 38.585393409638066
Iteration: 6 || Loss: 38.585393409638066
saving ADAM checkpoint...
Sum of params:82.992195
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.585393409638066
Iteration: 2 || Loss: 38.406912530208686
Iteration: 3 || Loss: 38.2056111574381
Iteration: 4 || Loss: 37.98799640675524
Iteration: 5 || Loss: 37.93521957787835
Iteration: 6 || Loss: 37.8089385731778
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.80827
Epoch 127 loss:37.8089385731778
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.734820626683911
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:59.46382430436839
waveform batch: 2/2
Test loss - extrapolation:29.52884674708363
Epoch 127 mean train loss:1.87084313417537
Epoch 127 mean test loss - interpolation:1.955803437780652
Epoch 127 mean test loss - extrapolation:7.416055920954335
Start training epoch 128
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.80827
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.34758525243114
Iteration: 2 || Loss: 5.346277492123126
Iteration: 3 || Loss: 5.344969932315718
Iteration: 4 || Loss: 5.343665035050709
Iteration: 5 || Loss: 5.342359660901945
Iteration: 6 || Loss: 5.342359660901945
saving ADAM checkpoint...
Sum of params:82.808266
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.342359660901945
Iteration: 2 || Loss: 4.386662799331554
Iteration: 3 || Loss: 4.351461074628334
Iteration: 4 || Loss: 4.340595500369883
Iteration: 5 || Loss: 4.317933362084288
Iteration: 6 || Loss: 4.208566655462055
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.89369
Epoch 128 loss:4.208566655462055
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.89369
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.6314671774548
Iteration: 2 || Loss: 12.630774735521582
Iteration: 3 || Loss: 12.630084726624561
Iteration: 4 || Loss: 12.629396116335036
Iteration: 5 || Loss: 12.628706019854798
Iteration: 6 || Loss: 12.628706019854798
saving ADAM checkpoint...
Sum of params:82.89373
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.628706019854798
Iteration: 2 || Loss: 12.358461493223594
Iteration: 3 || Loss: 12.317926829400344
Iteration: 4 || Loss: 12.250881284265137
Iteration: 5 || Loss: 12.147803649342134
Iteration: 6 || Loss: 12.136791643288456
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.97781
Epoch 128 loss:12.136791643288456
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.97781
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.46355585985482
Iteration: 2 || Loss: 38.462977567530345
Iteration: 3 || Loss: 38.462403250478985
Iteration: 4 || Loss: 38.46182754021802
Iteration: 5 || Loss: 38.461252665645105
Iteration: 6 || Loss: 38.461252665645105
saving ADAM checkpoint...
Sum of params:82.977936
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.461252665645105
Iteration: 2 || Loss: 38.28175090207556
Iteration: 3 || Loss: 38.07337071165673
Iteration: 4 || Loss: 37.85331793736655
Iteration: 5 || Loss: 37.80096432489705
Iteration: 6 || Loss: 37.662366420087146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.79059
Epoch 128 loss:37.662366420087146
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.681646712354626
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:59.231351078195594
waveform batch: 2/2
Test loss - extrapolation:29.403087661520647
Epoch 128 mean train loss:1.862335335132333
Epoch 128 mean test loss - interpolation:1.946941118725771
Epoch 128 mean test loss - extrapolation:7.386203228309687
Start training epoch 129
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.79059
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.268859774034363
Iteration: 2 || Loss: 5.267578504521176
Iteration: 3 || Loss: 5.266296216434444
Iteration: 4 || Loss: 5.2650182386354825
Iteration: 5 || Loss: 5.263738325873751
Iteration: 6 || Loss: 5.263738325873751
saving ADAM checkpoint...
Sum of params:82.79058
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.263738325873751
Iteration: 2 || Loss: 4.348134774585719
Iteration: 3 || Loss: 4.315294389611397
Iteration: 4 || Loss: 4.305283286459752
Iteration: 5 || Loss: 4.277702333879652
Iteration: 6 || Loss: 4.174191971311425
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.87438
Epoch 129 loss:4.174191971311425
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.87438
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.605151418850157
Iteration: 2 || Loss: 12.604449284953125
Iteration: 3 || Loss: 12.60374727525127
Iteration: 4 || Loss: 12.603047131040555
Iteration: 5 || Loss: 12.602346745756005
Iteration: 6 || Loss: 12.602346745756005
saving ADAM checkpoint...
Sum of params:82.87442
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.602346745756005
Iteration: 2 || Loss: 12.324065522581087
Iteration: 3 || Loss: 12.278197732908904
Iteration: 4 || Loss: 12.199805183060253
Iteration: 5 || Loss: 12.093933263559618
Iteration: 6 || Loss: 12.082626532443633
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.95862
Epoch 129 loss:12.082626532443633
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.95862
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.31994018855223
Iteration: 2 || Loss: 38.31936128631504
Iteration: 3 || Loss: 38.31878478441099
Iteration: 4 || Loss: 38.31820598363198
Iteration: 5 || Loss: 38.31763070647598
Iteration: 6 || Loss: 38.31763070647598
saving ADAM checkpoint...
Sum of params:82.958755
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.31763070647598
Iteration: 2 || Loss: 38.13787989576107
Iteration: 3 || Loss: 37.936746206091556
Iteration: 4 || Loss: 37.70100271302749
Iteration: 5 || Loss: 37.65007189414874
Iteration: 6 || Loss: 37.51433358496616
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.78325
Epoch 129 loss:37.51433358496616
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.614009690931548
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:58.97903887117073
waveform batch: 2/2
Test loss - extrapolation:29.272446029654198
Epoch 129 mean train loss:1.8541776582317662
Epoch 129 mean test loss - interpolation:1.9356682818219246
Epoch 129 mean test loss - extrapolation:7.354290408402076
Start training epoch 130
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.78325
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.1899038720819535
Iteration: 2 || Loss: 5.188636704945007
Iteration: 3 || Loss: 5.18737117830056
Iteration: 4 || Loss: 5.186108213973235
Iteration: 5 || Loss: 5.184843299167315
Iteration: 6 || Loss: 5.184843299167315
saving ADAM checkpoint...
Sum of params:82.783264
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.184843299167315
Iteration: 2 || Loss: 4.295132099565449
Iteration: 3 || Loss: 4.266097305149305
Iteration: 4 || Loss: 4.2571236263097285
Iteration: 5 || Loss: 4.209974325774539
Iteration: 6 || Loss: 4.135014308301135
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.85193
Epoch 130 loss:4.135014308301135
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.85193
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.502228263352572
Iteration: 2 || Loss: 12.501571972532386
Iteration: 3 || Loss: 12.500918947511483
Iteration: 4 || Loss: 12.500266752925926
Iteration: 5 || Loss: 12.499615563310066
Iteration: 6 || Loss: 12.499615563310066
saving ADAM checkpoint...
Sum of params:82.85197
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.499615563310066
Iteration: 2 || Loss: 12.256549997697828
Iteration: 3 || Loss: 12.22037829325542
Iteration: 4 || Loss: 12.149067365886019
Iteration: 5 || Loss: 12.038231547570941
Iteration: 6 || Loss: 12.024951043222163
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.93836
Epoch 130 loss:12.024951043222163
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.93836
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.133585245046326
Iteration: 2 || Loss: 38.13300091903417
Iteration: 3 || Loss: 38.13241835140845
Iteration: 4 || Loss: 38.13183703547939
Iteration: 5 || Loss: 38.13125649491316
Iteration: 6 || Loss: 38.13125649491316
saving ADAM checkpoint...
Sum of params:82.93849
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.13125649491316
Iteration: 2 || Loss: 37.94646642958261
Iteration: 3 || Loss: 37.77378853852523
Iteration: 4 || Loss: 37.5465858832458
Iteration: 5 || Loss: 37.49650309608396
Iteration: 6 || Loss: 37.387674365755444
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.781334
Epoch 130 loss:37.387674365755444
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.55758111635478
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:58.75584682645337
waveform batch: 2/2
Test loss - extrapolation:29.147788821297077
Epoch 130 mean train loss:1.8464703350785774
Epoch 130 mean test loss - interpolation:1.9262635193924635
Epoch 130 mean test loss - extrapolation:7.325302970645871
Start training epoch 131
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.781334
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.128503074488065
Iteration: 2 || Loss: 5.127234833726853
Iteration: 3 || Loss: 5.1259653469461
Iteration: 4 || Loss: 5.124694676793024
Iteration: 5 || Loss: 5.123427200473769
Iteration: 6 || Loss: 5.123427200473769
saving ADAM checkpoint...
Sum of params:82.781334
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.123427200473769
Iteration: 2 || Loss: 4.231790358404342
Iteration: 3 || Loss: 4.202753586983831
Iteration: 4 || Loss: 4.1952376757895955
Iteration: 5 || Loss: 4.152946503264483
Iteration: 6 || Loss: 4.058943761830263
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.88223
Epoch 131 loss:4.058943761830263
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.88223
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.404622640836994
Iteration: 2 || Loss: 12.404001221464492
Iteration: 3 || Loss: 12.40337906800457
Iteration: 4 || Loss: 12.402760249948654
Iteration: 5 || Loss: 12.402141522096963
Iteration: 6 || Loss: 12.402141522096963
saving ADAM checkpoint...
Sum of params:82.88227
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.402141522096963
Iteration: 2 || Loss: 12.1943583509812
Iteration: 3 || Loss: 12.134977607598756
Iteration: 4 || Loss: 11.944991968061409
Iteration: 5 || Loss: 11.930209232736154
Iteration: 6 || Loss: 11.90293307889061
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.9494
Epoch 131 loss:11.90293307889061
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.9494
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.799943077415755
Iteration: 2 || Loss: 37.79948098566511
Iteration: 3 || Loss: 37.7990188538251
Iteration: 4 || Loss: 37.798559445709564
Iteration: 5 || Loss: 37.798100368180606
Iteration: 6 || Loss: 37.798100368180606
saving ADAM checkpoint...
Sum of params:82.94943
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.798100368180606
Iteration: 2 || Loss: 37.67837794298022
Iteration: 3 || Loss: 37.57144392382889
Iteration: 4 || Loss: 37.457874643447006
Iteration: 5 || Loss: 37.4149200418575
Iteration: 6 || Loss: 37.259481102436204
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.735085
Epoch 131 loss:37.259481102436204
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.577860218676932
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:58.910206278235705
waveform batch: 2/2
Test loss - extrapolation:29.194765424880373
Epoch 131 mean train loss:1.8352192394192095
Epoch 131 mean test loss - interpolation:1.9296433697794886
Epoch 131 mean test loss - extrapolation:7.342080975259673
Start training epoch 132
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.735085
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.352363808070742
Iteration: 2 || Loss: 5.350926156976802
Iteration: 3 || Loss: 5.349484969484578
Iteration: 4 || Loss: 5.348041693950308
Iteration: 5 || Loss: 5.346603915438471
Iteration: 6 || Loss: 5.346603915438471
saving ADAM checkpoint...
Sum of params:82.73509
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.346603915438471
Iteration: 2 || Loss: 4.19553472368612
Iteration: 3 || Loss: 4.147586456766999
Iteration: 4 || Loss: 4.133181064365963
Iteration: 5 || Loss: 4.128930981468138
Iteration: 6 || Loss: 4.050733492344538
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.81412
Epoch 132 loss:4.050733492344538
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.81412
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.18344793222715
Iteration: 2 || Loss: 12.182995914974736
Iteration: 3 || Loss: 12.18254472185118
Iteration: 4 || Loss: 12.182094999223649
Iteration: 5 || Loss: 12.181645707903943
Iteration: 6 || Loss: 12.181645707903943
saving ADAM checkpoint...
Sum of params:82.81416
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.181645707903943
Iteration: 2 || Loss: 12.050385360599469
Iteration: 3 || Loss: 11.977879585386564
Iteration: 4 || Loss: 11.956545644266013
Iteration: 5 || Loss: 11.860876032704832
Iteration: 6 || Loss: 11.852890050647506
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.91042
Epoch 132 loss:11.852890050647506
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.91042
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.67106340930075
Iteration: 2 || Loss: 37.670487447481605
Iteration: 3 || Loss: 37.66991296092047
Iteration: 4 || Loss: 37.66933921287322
Iteration: 5 || Loss: 37.66876404447604
Iteration: 6 || Loss: 37.66876404447604
saving ADAM checkpoint...
Sum of params:82.910446
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.66876404447604
Iteration: 2 || Loss: 37.47701611314944
Iteration: 3 || Loss: 37.32932278854218
Iteration: 4 || Loss: 37.29156517401822
Iteration: 5 || Loss: 37.227351543448286
Iteration: 6 || Loss: 37.08640623292808
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.68934
Epoch 132 loss:37.08640623292808
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.61332666833362
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:58.84120661896905
waveform batch: 2/2
Test loss - extrapolation:28.90875812828985
Epoch 132 mean train loss:1.8272424060662111
Epoch 132 mean test loss - interpolation:1.9355544447222701
Epoch 132 mean test loss - extrapolation:7.312497062271575
Start training epoch 133
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.68934
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.7975971175460765
Iteration: 2 || Loss: 5.795920250808704
Iteration: 3 || Loss: 5.794243918564789
Iteration: 4 || Loss: 5.79257094882668
Iteration: 5 || Loss: 5.790896633581489
Iteration: 6 || Loss: 5.790896633581489
saving ADAM checkpoint...
Sum of params:82.68936
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.790896633581489
Iteration: 2 || Loss: 4.233818198813768
Iteration: 3 || Loss: 4.176286870811851
Iteration: 4 || Loss: 4.15678400951662
Iteration: 5 || Loss: 4.14210870243166
Iteration: 6 || Loss: 4.082034849093651
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.73494
Epoch 133 loss:4.082034849093651
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.73494
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.22395298160077
Iteration: 2 || Loss: 12.223356302869199
Iteration: 3 || Loss: 12.222763338766475
Iteration: 4 || Loss: 12.222169044500754
Iteration: 5 || Loss: 12.22157693308758
Iteration: 6 || Loss: 12.22157693308758
saving ADAM checkpoint...
Sum of params:82.73497
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.22157693308758
Iteration: 2 || Loss: 11.997040638657024
Iteration: 3 || Loss: 11.937151201056496
Iteration: 4 || Loss: 11.925492131215336
Iteration: 5 || Loss: 11.86786175526981
Iteration: 6 || Loss: 11.855803021692662
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.82901
Epoch 133 loss:11.855803021692662
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.82901
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.402140827736815
Iteration: 2 || Loss: 37.401647154664396
Iteration: 3 || Loss: 37.40115912316696
Iteration: 4 || Loss: 37.40066981022147
Iteration: 5 || Loss: 37.40018329126869
Iteration: 6 || Loss: 37.40018329126869
saving ADAM checkpoint...
Sum of params:82.829056
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.40018329126869
Iteration: 2 || Loss: 37.267093322038164
Iteration: 3 || Loss: 37.162721430502764
Iteration: 4 || Loss: 37.01813341002657
Iteration: 5 || Loss: 36.98795149513978
Iteration: 6 || Loss: 36.827986263548695
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.62914
Epoch 133 loss:36.827986263548695
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.435251376994422
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:58.4698187673267
waveform batch: 2/2
Test loss - extrapolation:28.884247048793352
Epoch 133 mean train loss:1.8195111770460346
Epoch 133 mean test loss - interpolation:1.9058752294990704
Epoch 133 mean test loss - extrapolation:7.279505484676672
Start training epoch 134
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.62914
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.402896255995327
Iteration: 2 || Loss: 5.401399878733718
Iteration: 3 || Loss: 5.399907893689771
Iteration: 4 || Loss: 5.398417702460562
Iteration: 5 || Loss: 5.396926187550061
Iteration: 6 || Loss: 5.396926187550061
saving ADAM checkpoint...
Sum of params:82.629135
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.396926187550061
Iteration: 2 || Loss: 4.172982756003337
Iteration: 3 || Loss: 4.134803028041922
Iteration: 4 || Loss: 4.11884965361681
Iteration: 5 || Loss: 4.072767318625511
Iteration: 6 || Loss: 4.043137565923393
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.69422
Epoch 134 loss:4.043137565923393
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.69422
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.229727352648323
Iteration: 2 || Loss: 12.229039178183669
Iteration: 3 || Loss: 12.228353970645248
Iteration: 4 || Loss: 12.227668453922947
Iteration: 5 || Loss: 12.226982770080356
Iteration: 6 || Loss: 12.226982770080356
saving ADAM checkpoint...
Sum of params:82.69427
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.226982770080356
Iteration: 2 || Loss: 11.948911246094063
Iteration: 3 || Loss: 11.910034486463744
Iteration: 4 || Loss: 11.891516373086152
Iteration: 5 || Loss: 11.817841790385868
Iteration: 6 || Loss: 11.789720510896435
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.75714
Epoch 134 loss:11.789720510896435
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.75714
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 37.11732592996747
Iteration: 2 || Loss: 37.1167537562721
Iteration: 3 || Loss: 37.116182560292295
Iteration: 4 || Loss: 37.115613516466674
Iteration: 5 || Loss: 37.11504494999074
Iteration: 6 || Loss: 37.11504494999074
saving ADAM checkpoint...
Sum of params:82.75717
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 37.11504494999074
Iteration: 2 || Loss: 36.917132239421214
Iteration: 3 || Loss: 36.86468905601796
Iteration: 4 || Loss: 36.82011375985983
Iteration: 5 || Loss: 36.769993336876176
Iteration: 6 || Loss: 36.66659548117669
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.62885
Epoch 134 loss:36.66659548117669
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.415891375523163
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:57.98180849256183
waveform batch: 2/2
Test loss - extrapolation:28.414323975680983
Epoch 134 mean train loss:1.8103259847585007
Epoch 134 mean test loss - interpolation:1.9026485625871938
Epoch 134 mean test loss - extrapolation:7.199677705686901
Start training epoch 135
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.62885
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.479427528807964
Iteration: 2 || Loss: 5.4778862739345415
Iteration: 3 || Loss: 5.476340952648556
Iteration: 4 || Loss: 5.474797922853603
Iteration: 5 || Loss: 5.473258692782263
Iteration: 6 || Loss: 5.473258692782263
saving ADAM checkpoint...
Sum of params:82.62886
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.473258692782263
Iteration: 2 || Loss: 4.172610370661653
Iteration: 3 || Loss: 4.134485285119182
Iteration: 4 || Loss: 4.104847452851104
Iteration: 5 || Loss: 4.0585887430524386
Iteration: 6 || Loss: 4.036669598341875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.682816
Epoch 135 loss:4.036669598341875
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.682816
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.09781817816198
Iteration: 2 || Loss: 12.097152354527873
Iteration: 3 || Loss: 12.09648637842787
Iteration: 4 || Loss: 12.095820887899873
Iteration: 5 || Loss: 12.095156985433833
Iteration: 6 || Loss: 12.095156985433833
saving ADAM checkpoint...
Sum of params:82.68285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.095156985433833
Iteration: 2 || Loss: 11.835855628521447
Iteration: 3 || Loss: 11.810130288390114
Iteration: 4 || Loss: 11.801537969066286
Iteration: 5 || Loss: 11.727754439596971
Iteration: 6 || Loss: 11.716940495074887
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.74489
Epoch 135 loss:11.716940495074887
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.74489
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.97973349583223
Iteration: 2 || Loss: 36.97918723081211
Iteration: 3 || Loss: 36.9786445424203
Iteration: 4 || Loss: 36.97809691084027
Iteration: 5 || Loss: 36.97755479637024
Iteration: 6 || Loss: 36.97755479637024
saving ADAM checkpoint...
Sum of params:82.744835
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.97755479637024
Iteration: 2 || Loss: 36.802379380259275
Iteration: 3 || Loss: 36.69263738587252
Iteration: 4 || Loss: 36.661704429424745
Iteration: 5 || Loss: 36.577684077029446
Iteration: 6 || Loss: 36.433041028342544
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.51776
Epoch 135 loss:36.433041028342544
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.432328813536353
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:58.544136922620154
waveform batch: 2/2
Test loss - extrapolation:28.637226908417844
Epoch 135 mean train loss:1.799539693853769
Epoch 135 mean test loss - interpolation:1.9053881355893922
Epoch 135 mean test loss - extrapolation:7.2651136525865
Start training epoch 136
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.51776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.277250431321132
Iteration: 2 || Loss: 6.2752876956770836
Iteration: 3 || Loss: 6.273328855862282
Iteration: 4 || Loss: 6.2713699190058705
Iteration: 5 || Loss: 6.269408117880931
Iteration: 6 || Loss: 6.269408117880931
saving ADAM checkpoint...
Sum of params:82.51776
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.269408117880931
Iteration: 2 || Loss: 4.1692351488002455
Iteration: 3 || Loss: 4.107300029934691
Iteration: 4 || Loss: 4.083706496649596
Iteration: 5 || Loss: 4.0528538563363306
Iteration: 6 || Loss: 4.025936435976282
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.5596
Epoch 136 loss:4.025936435976282
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.5596
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.191087971599373
Iteration: 2 || Loss: 12.190323363657814
Iteration: 3 || Loss: 12.189560608499352
Iteration: 4 || Loss: 12.188797887553623
Iteration: 5 || Loss: 12.18803613927682
Iteration: 6 || Loss: 12.18803613927682
saving ADAM checkpoint...
Sum of params:82.55965
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.18803613927682
Iteration: 2 || Loss: 11.84561652342684
Iteration: 3 || Loss: 11.809579801309694
Iteration: 4 || Loss: 11.79974208587793
Iteration: 5 || Loss: 11.714291498024371
Iteration: 6 || Loss: 11.698365990350037
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.64541
Epoch 136 loss:11.698365990350037
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.64541
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.728643819366475
Iteration: 2 || Loss: 36.728017090718716
Iteration: 3 || Loss: 36.72738965622818
Iteration: 4 || Loss: 36.72676310187931
Iteration: 5 || Loss: 36.72613440623309
Iteration: 6 || Loss: 36.72613440623309
saving ADAM checkpoint...
Sum of params:82.64543
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.72613440623309
Iteration: 2 || Loss: 36.48570002102747
Iteration: 3 || Loss: 36.414688567175105
Iteration: 4 || Loss: 36.39752543806722
Iteration: 5 || Loss: 36.34110644762744
Iteration: 6 || Loss: 36.24574862611685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.49053
Epoch 136 loss:36.24574862611685
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.402132679969208
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:57.703465555673716
waveform batch: 2/2
Test loss - extrapolation:28.03666747947835
Epoch 136 mean train loss:1.7920707259463162
Epoch 136 mean test loss - interpolation:1.9003554466615347
Epoch 136 mean test loss - extrapolation:7.145011086262673
Start training epoch 137
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.49053
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.860374602683131
Iteration: 2 || Loss: 5.858629143701166
Iteration: 3 || Loss: 5.856888286813655
Iteration: 4 || Loss: 5.855144627722789
Iteration: 5 || Loss: 5.85340365245837
Iteration: 6 || Loss: 5.85340365245837
saving ADAM checkpoint...
Sum of params:82.49055
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.85340365245837
Iteration: 2 || Loss: 4.196347004135243
Iteration: 3 || Loss: 4.14182565641101
Iteration: 4 || Loss: 4.116843018092665
Iteration: 5 || Loss: 4.09351852372334
Iteration: 6 || Loss: 4.044835190285796
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.53151
Epoch 137 loss:4.044835190285796
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.53151
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.962400401867523
Iteration: 2 || Loss: 11.961826565090242
Iteration: 3 || Loss: 11.961254255994247
Iteration: 4 || Loss: 11.96068035199413
Iteration: 5 || Loss: 11.960107435010253
Iteration: 6 || Loss: 11.960107435010253
saving ADAM checkpoint...
Sum of params:82.531525
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.960107435010253
Iteration: 2 || Loss: 11.76156537059249
Iteration: 3 || Loss: 11.731966888551938
Iteration: 4 || Loss: 11.719591179071648
Iteration: 5 || Loss: 11.673329126542795
Iteration: 6 || Loss: 11.650504878206668
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.599915
Epoch 137 loss:11.650504878206668
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.599915
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.48092833216905
Iteration: 2 || Loss: 36.480504786563245
Iteration: 3 || Loss: 36.48008193347853
Iteration: 4 || Loss: 36.47965890425299
Iteration: 5 || Loss: 36.4792383253601
Iteration: 6 || Loss: 36.4792383253601
saving ADAM checkpoint...
Sum of params:82.59996
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.4792383253601
Iteration: 2 || Loss: 36.37949065642508
Iteration: 3 || Loss: 36.3153507322019
Iteration: 4 || Loss: 36.16214992344607
Iteration: 5 || Loss: 36.13782045984573
Iteration: 6 || Loss: 36.01487140435036
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.43653
Epoch 137 loss:36.01487140435036
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.220382877943512
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:57.64766199732552
waveform batch: 2/2
Test loss - extrapolation:28.265975365166877
Epoch 137 mean train loss:1.783110740442856
Epoch 137 mean test loss - interpolation:1.8700638129905853
Epoch 137 mean test loss - extrapolation:7.1594697802077
Start training epoch 138
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.43653
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.60769388310066
Iteration: 2 || Loss: 5.606018460966421
Iteration: 3 || Loss: 5.604343046031261
Iteration: 4 || Loss: 5.602676124477269
Iteration: 5 || Loss: 5.6010028675225545
Iteration: 6 || Loss: 5.6010028675225545
saving ADAM checkpoint...
Sum of params:82.43654
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.6010028675225545
Iteration: 2 || Loss: 4.090707587129744
Iteration: 3 || Loss: 4.048628981026452
Iteration: 4 || Loss: 4.037768716286271
Iteration: 5 || Loss: 4.004793739110969
Iteration: 6 || Loss: 3.9695691063633043
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.48075
Epoch 138 loss:3.9695691063633043
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.48075
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.011273460652776
Iteration: 2 || Loss: 12.010599975837854
Iteration: 3 || Loss: 12.009926097889682
Iteration: 4 || Loss: 12.009254224002348
Iteration: 5 || Loss: 12.008583575011915
Iteration: 6 || Loss: 12.008583575011915
saving ADAM checkpoint...
Sum of params:82.4808
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.008583575011915
Iteration: 2 || Loss: 11.73993636278663
Iteration: 3 || Loss: 11.689298288285121
Iteration: 4 || Loss: 11.67374756688377
Iteration: 5 || Loss: 11.599900871630267
Iteration: 6 || Loss: 11.586488301482007
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.54917
Epoch 138 loss:11.586488301482007
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.54917
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.372199266942054
Iteration: 2 || Loss: 36.37157109765281
Iteration: 3 || Loss: 36.37094213309606
Iteration: 4 || Loss: 36.370314122018044
Iteration: 5 || Loss: 36.36968920664967
Iteration: 6 || Loss: 36.36968920664967
saving ADAM checkpoint...
Sum of params:82.549194
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.36968920664967
Iteration: 2 || Loss: 36.134454448863174
Iteration: 3 || Loss: 36.061629035305046
Iteration: 4 || Loss: 36.028462021721076
Iteration: 5 || Loss: 35.98030774029096
Iteration: 6 || Loss: 35.85915657566393
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.42192
Epoch 138 loss:35.85915657566393
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.203194550425605
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:57.06265605370593
waveform batch: 2/2
Test loss - extrapolation:27.73703910279238
Epoch 138 mean train loss:1.7729384132244566
Epoch 138 mean test loss - interpolation:1.867199091737601
Epoch 138 mean test loss - extrapolation:7.066641263041525
Start training epoch 139
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.42192
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.552511935263946
Iteration: 2 || Loss: 5.550886449848727
Iteration: 3 || Loss: 5.549263311857268
Iteration: 4 || Loss: 5.54764146709147
Iteration: 5 || Loss: 5.546018333944531
Iteration: 6 || Loss: 5.546018333944531
saving ADAM checkpoint...
Sum of params:82.421936
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.546018333944531
Iteration: 2 || Loss: 4.1203447116764345
Iteration: 3 || Loss: 4.082091964960914
Iteration: 4 || Loss: 4.000016957945221
Iteration: 5 || Loss: 3.990154132945929
Iteration: 6 || Loss: 3.9770049232004783
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.46337
Epoch 139 loss:3.9770049232004783
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.46337
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.855962951675956
Iteration: 2 || Loss: 11.855324734221938
Iteration: 3 || Loss: 11.854689951159619
Iteration: 4 || Loss: 11.854055196494818
Iteration: 5 || Loss: 11.853420308966973
Iteration: 6 || Loss: 11.853420308966973
saving ADAM checkpoint...
Sum of params:82.46341
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.853420308966973
Iteration: 2 || Loss: 11.625054052423662
Iteration: 3 || Loss: 11.614312447976346
Iteration: 4 || Loss: 11.604723657414128
Iteration: 5 || Loss: 11.551473123099493
Iteration: 6 || Loss: 11.503428325346043
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.56285
Epoch 139 loss:11.503428325346043
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.56285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.1768114186138
Iteration: 2 || Loss: 36.176513971189664
Iteration: 3 || Loss: 36.17621805306274
Iteration: 4 || Loss: 36.17592064543366
Iteration: 5 || Loss: 36.175626295702386
Iteration: 6 || Loss: 36.175626295702386
saving ADAM checkpoint...
Sum of params:82.56279
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.175626295702386
Iteration: 2 || Loss: 36.12665140075263
Iteration: 3 || Loss: 36.02214616474628
Iteration: 4 || Loss: 35.823205264792676
Iteration: 5 || Loss: 35.76426797961873
Iteration: 6 || Loss: 35.686095607596165
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.374695
Epoch 139 loss:35.686095607596165
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.168137934820628
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:57.33703384764913
waveform batch: 2/2
Test loss - extrapolation:27.964458250085332
Epoch 139 mean train loss:1.7643630640049204
Epoch 139 mean test loss - interpolation:1.8613563224701046
Epoch 139 mean test loss - extrapolation:7.108457674811206
Start training epoch 140
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.374695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.808089313085903
Iteration: 2 || Loss: 5.8062622501548855
Iteration: 3 || Loss: 5.804436557046697
Iteration: 4 || Loss: 5.802617823055361
Iteration: 5 || Loss: 5.800795020868947
Iteration: 6 || Loss: 5.800795020868947
saving ADAM checkpoint...
Sum of params:82.3747
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.800795020868947
Iteration: 2 || Loss: 4.014521674070499
Iteration: 3 || Loss: 3.958252775239872
Iteration: 4 || Loss: 3.9449665169305206
Iteration: 5 || Loss: 3.940933729307212
Iteration: 6 || Loss: 3.8742683359813634
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.441895
Epoch 140 loss:3.8742683359813634
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.441895
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.83133311977868
Iteration: 2 || Loss: 11.830667109314488
Iteration: 3 || Loss: 11.829999127226484
Iteration: 4 || Loss: 11.829335379355951
Iteration: 5 || Loss: 11.828670482288134
Iteration: 6 || Loss: 11.828670482288134
saving ADAM checkpoint...
Sum of params:82.441956
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.828670482288134
Iteration: 2 || Loss: 11.570647300918528
Iteration: 3 || Loss: 11.526000819672886
Iteration: 4 || Loss: 11.508462442031776
Iteration: 5 || Loss: 11.428205113049572
Iteration: 6 || Loss: 11.419023845819307
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.50355
Epoch 140 loss:11.419023845819307
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.50355
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.071551643758504
Iteration: 2 || Loss: 36.070964771038774
Iteration: 3 || Loss: 36.07037886286693
Iteration: 4 || Loss: 36.06979471278991
Iteration: 5 || Loss: 36.069209292831395
Iteration: 6 || Loss: 36.069209292831395
saving ADAM checkpoint...
Sum of params:82.503586
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.069209292831395
Iteration: 2 || Loss: 35.87077178551868
Iteration: 3 || Loss: 35.774629670544044
Iteration: 4 || Loss: 35.73152015938883
Iteration: 5 || Loss: 35.680059629068154
Iteration: 6 || Loss: 35.5626669906393
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.33928
Epoch 140 loss:35.5626669906393
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:11.095527360712422
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:56.94591768538285
waveform batch: 2/2
Test loss - extrapolation:27.738428208062672
Epoch 140 mean train loss:1.7536537645668955
Epoch 140 mean test loss - interpolation:1.8492545601187371
Epoch 140 mean test loss - extrapolation:7.057028824453794
Start training epoch 141
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.33928
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.452684351985079
Iteration: 2 || Loss: 5.451056854337669
Iteration: 3 || Loss: 5.449435673380065
Iteration: 4 || Loss: 5.447814258524282
Iteration: 5 || Loss: 5.446191891121251
Iteration: 6 || Loss: 5.446191891121251
saving ADAM checkpoint...
Sum of params:82.3393
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.446191891121251
Iteration: 2 || Loss: 4.0359549091320455
Iteration: 3 || Loss: 3.993075296329662
Iteration: 4 || Loss: 3.975228556818596
Iteration: 5 || Loss: 3.950996472056534
Iteration: 6 || Loss: 3.9063252271026796
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.37671
Epoch 141 loss:3.9063252271026796
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.37671
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.747795759926406
Iteration: 2 || Loss: 11.747161910562363
Iteration: 3 || Loss: 11.746529075821721
Iteration: 4 || Loss: 11.74589750073147
Iteration: 5 || Loss: 11.745266076419734
Iteration: 6 || Loss: 11.745266076419734
saving ADAM checkpoint...
Sum of params:82.376755
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.745266076419734
Iteration: 2 || Loss: 11.50689149101488
Iteration: 3 || Loss: 11.470966010677342
Iteration: 4 || Loss: 11.463414744197758
Iteration: 5 || Loss: 11.407557063035147
Iteration: 6 || Loss: 11.391830244950041
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.46085
Epoch 141 loss:11.391830244950041
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.46085
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.8351051938549
Iteration: 2 || Loss: 35.83467980957767
Iteration: 3 || Loss: 35.83425617160533
Iteration: 4 || Loss: 35.833833238584496
Iteration: 5 || Loss: 35.833409911055355
Iteration: 6 || Loss: 35.833409911055355
saving ADAM checkpoint...
Sum of params:82.46088
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.833409911055355
Iteration: 2 || Loss: 35.73336679615262
Iteration: 3 || Loss: 35.66488902070946
Iteration: 4 || Loss: 35.50450748242641
Iteration: 5 || Loss: 35.47867834614016
Iteration: 6 || Loss: 35.362526615090005
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.29441
Epoch 141 loss:35.362526615090005
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.988807302276754
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:56.80388905023687
waveform batch: 2/2
Test loss - extrapolation:27.82175327597662
Epoch 141 mean train loss:1.7469200719704387
Epoch 141 mean test loss - interpolation:1.8314678837127925
Epoch 141 mean test loss - extrapolation:7.052136860517791
Start training epoch 142
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.29441
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.32142929883999
Iteration: 2 || Loss: 5.319826576691746
Iteration: 3 || Loss: 5.3182251928598525
Iteration: 4 || Loss: 5.316626631258884
Iteration: 5 || Loss: 5.31502625729316
Iteration: 6 || Loss: 5.31502625729316
saving ADAM checkpoint...
Sum of params:82.2944
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.31502625729316
Iteration: 2 || Loss: 3.9478492495054884
Iteration: 3 || Loss: 3.9075668335207343
Iteration: 4 || Loss: 3.899084712096726
Iteration: 5 || Loss: 3.8847676377673714
Iteration: 6 || Loss: 3.8419006716463517
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.3472
Epoch 142 loss:3.8419006716463517
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.3472
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.759241267113142
Iteration: 2 || Loss: 11.758507707951829
Iteration: 3 || Loss: 11.757773570129196
Iteration: 4 || Loss: 11.757042458092313
Iteration: 5 || Loss: 11.756311298788637
Iteration: 6 || Loss: 11.756311298788637
saving ADAM checkpoint...
Sum of params:82.34726
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.756311298788637
Iteration: 2 || Loss: 11.450187243540736
Iteration: 3 || Loss: 11.421654238047516
Iteration: 4 || Loss: 11.410502488446214
Iteration: 5 || Loss: 11.342933592585698
Iteration: 6 || Loss: 11.327229519881984
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.41146
Epoch 142 loss:11.327229519881984
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.41146
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.68326155631264
Iteration: 2 || Loss: 35.682675569795656
Iteration: 3 || Loss: 35.682090158464206
Iteration: 4 || Loss: 35.68150858681433
Iteration: 5 || Loss: 35.68092394679158
Iteration: 6 || Loss: 35.68092394679158
saving ADAM checkpoint...
Sum of params:82.411476
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.68092394679158
Iteration: 2 || Loss: 35.478377511985904
Iteration: 3 || Loss: 35.4099866960531
Iteration: 4 || Loss: 35.37634567901482
Iteration: 5 || Loss: 35.32291004714297
Iteration: 6 || Loss: 35.21407020616898
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.25685
Epoch 142 loss:35.21407020616898
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.9966080995827
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:56.5604145270222
waveform batch: 2/2
Test loss - extrapolation:27.49134532840811
Epoch 142 mean train loss:1.7373517378516312
Epoch 142 mean test loss - interpolation:1.8327680165971165
Epoch 142 mean test loss - extrapolation:7.004313321285859
Start training epoch 143
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.25685
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.423377475512548
Iteration: 2 || Loss: 5.421739735941187
Iteration: 3 || Loss: 5.420100562875792
Iteration: 4 || Loss: 5.418460465792745
Iteration: 5 || Loss: 5.416822457497707
Iteration: 6 || Loss: 5.416822457497707
saving ADAM checkpoint...
Sum of params:82.25686
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.416822457497707
Iteration: 2 || Loss: 3.988553098726037
Iteration: 3 || Loss: 3.9437734087660545
Iteration: 4 || Loss: 3.923599000955585
Iteration: 5 || Loss: 3.9009343596011536
Iteration: 6 || Loss: 3.858228330837946
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.292046
Epoch 143 loss:3.858228330837946
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.292046
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.631910601138056
Iteration: 2 || Loss: 11.631286565213435
Iteration: 3 || Loss: 11.630662675998991
Iteration: 4 || Loss: 11.630038719821183
Iteration: 5 || Loss: 11.629415333041347
Iteration: 6 || Loss: 11.629415333041347
saving ADAM checkpoint...
Sum of params:82.29208
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.629415333041347
Iteration: 2 || Loss: 11.401603182436052
Iteration: 3 || Loss: 11.37267866524252
Iteration: 4 || Loss: 11.363196423799197
Iteration: 5 || Loss: 11.315896846685593
Iteration: 6 || Loss: 11.296053402458577
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.36377
Epoch 143 loss:11.296053402458577
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.36377
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.461280204469176
Iteration: 2 || Loss: 35.46084319640781
Iteration: 3 || Loss: 35.4604085598309
Iteration: 4 || Loss: 35.45997135163332
Iteration: 5 || Loss: 35.459536881487466
Iteration: 6 || Loss: 35.459536881487466
saving ADAM checkpoint...
Sum of params:82.36382
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.459536881487466
Iteration: 2 || Loss: 35.35315457474692
Iteration: 3 || Loss: 35.29684832775201
Iteration: 4 || Loss: 35.14931374619547
Iteration: 5 || Loss: 35.12492141582543
Iteration: 6 || Loss: 35.013427782956406
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.21977
Epoch 143 loss:35.013427782956406
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.868817005009438
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:56.38127361390579
waveform batch: 2/2
Test loss - extrapolation:27.54137299072321
Epoch 143 mean train loss:1.7299210178018252
Epoch 143 mean test loss - interpolation:1.8114695008349064
Epoch 143 mean test loss - extrapolation:6.993553883719083
Start training epoch 144
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.21977
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.24467184025222
Iteration: 2 || Loss: 5.243080320766962
Iteration: 3 || Loss: 5.241492616262813
Iteration: 4 || Loss: 5.239902194441634
Iteration: 5 || Loss: 5.238312436784815
Iteration: 6 || Loss: 5.238312436784815
saving ADAM checkpoint...
Sum of params:82.2198
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.238312436784815
Iteration: 2 || Loss: 3.90159765013016
Iteration: 3 || Loss: 3.8659399994858545
Iteration: 4 || Loss: 3.858051132098733
Iteration: 5 || Loss: 3.832870494421103
Iteration: 6 || Loss: 3.8034911033194594
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.254
Epoch 144 loss:3.8034911033194594
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.254
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.653217692599988
Iteration: 2 || Loss: 11.65250437916936
Iteration: 3 || Loss: 11.651793119562354
Iteration: 4 || Loss: 11.651082105433641
Iteration: 5 || Loss: 11.650371483158574
Iteration: 6 || Loss: 11.650371483158574
saving ADAM checkpoint...
Sum of params:82.25403
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.650371483158574
Iteration: 2 || Loss: 11.358306551715696
Iteration: 3 || Loss: 11.31824738134495
Iteration: 4 || Loss: 11.307860948770381
Iteration: 5 || Loss: 11.240965879757438
Iteration: 6 || Loss: 11.230976048814394
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.323166
Epoch 144 loss:11.230976048814394
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.323166
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.33366374665196
Iteration: 2 || Loss: 35.33306980783506
Iteration: 3 || Loss: 35.332475363047536
Iteration: 4 || Loss: 35.33188353371567
Iteration: 5 || Loss: 35.33129289980795
Iteration: 6 || Loss: 35.33129289980795
saving ADAM checkpoint...
Sum of params:82.3232
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.33129289980795
Iteration: 2 || Loss: 35.12604833047393
Iteration: 3 || Loss: 35.04596344355946
Iteration: 4 || Loss: 35.01421495548456
Iteration: 5 || Loss: 34.96542973388781
Iteration: 6 || Loss: 34.84663357316103
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.165985
Epoch 144 loss:34.84663357316103
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.893761851580665
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:56.19843196673539
waveform batch: 2/2
Test loss - extrapolation:27.218959036984515
Epoch 144 mean train loss:1.7200379560446513
Epoch 144 mean test loss - interpolation:1.8156269752634442
Epoch 144 mean test loss - extrapolation:6.951449250309992
Start training epoch 145
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.165985
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.423287456944477
Iteration: 2 || Loss: 5.4216274837843965
Iteration: 3 || Loss: 5.419969726265128
Iteration: 4 || Loss: 5.4183106143005295
Iteration: 5 || Loss: 5.416655657653449
Iteration: 6 || Loss: 5.416655657653449
saving ADAM checkpoint...
Sum of params:82.16599
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.416655657653449
Iteration: 2 || Loss: 3.9593781680847187
Iteration: 3 || Loss: 3.914988111071823
Iteration: 4 || Loss: 3.8929875031531456
Iteration: 5 || Loss: 3.8693716117890777
Iteration: 6 || Loss: 3.8262939802309095
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.19807
Epoch 145 loss:3.8262939802309095
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.19807
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.54804982537072
Iteration: 2 || Loss: 11.547413266826133
Iteration: 3 || Loss: 11.54677796323557
Iteration: 4 || Loss: 11.546142237294802
Iteration: 5 || Loss: 11.545510948535606
Iteration: 6 || Loss: 11.545510948535606
saving ADAM checkpoint...
Sum of params:82.1981
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.545510948535606
Iteration: 2 || Loss: 11.311162670936227
Iteration: 3 || Loss: 11.282085531384693
Iteration: 4 || Loss: 11.272383122065884
Iteration: 5 || Loss: 11.224300196449839
Iteration: 6 || Loss: 11.202129409833036
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.269165
Epoch 145 loss:11.202129409833036
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.269165
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.07168384788761
Iteration: 2 || Loss: 35.07125001776521
Iteration: 3 || Loss: 35.070815968744064
Iteration: 4 || Loss: 35.07038296486951
Iteration: 5 || Loss: 35.06995261048477
Iteration: 6 || Loss: 35.06995261048477
saving ADAM checkpoint...
Sum of params:82.269196
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.06995261048477
Iteration: 2 || Loss: 34.964311664117425
Iteration: 3 || Loss: 34.9136465775032
Iteration: 4 || Loss: 34.76963160378399
Iteration: 5 || Loss: 34.74598719506196
Iteration: 6 || Loss: 34.64358742248903
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.13756
Epoch 145 loss:34.64358742248903
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.758957971386092
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:55.999585184727266
waveform batch: 2/2
Test loss - extrapolation:27.260206803374572
Epoch 145 mean train loss:1.7128279590535507
Epoch 145 mean test loss - interpolation:1.793159661897682
Epoch 145 mean test loss - extrapolation:6.938315999008487
Start training epoch 146
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.13756
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.2286899704259255
Iteration: 2 || Loss: 5.227080365510963
Iteration: 3 || Loss: 5.225474403925129
Iteration: 4 || Loss: 5.2238676325625955
Iteration: 5 || Loss: 5.222258884372602
Iteration: 6 || Loss: 5.222258884372602
saving ADAM checkpoint...
Sum of params:82.13756
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.222258884372602
Iteration: 2 || Loss: 3.860560055983631
Iteration: 3 || Loss: 3.8257423599861013
Iteration: 4 || Loss: 3.817973849513514
Iteration: 5 || Loss: 3.7919757901971867
Iteration: 6 || Loss: 3.7642778965118797
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.16962
Epoch 146 loss:3.7642778965118797
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.16962
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.555039261740196
Iteration: 2 || Loss: 11.554314682770878
Iteration: 3 || Loss: 11.553591998798861
Iteration: 4 || Loss: 11.552872869650495
Iteration: 5 || Loss: 11.552152347558154
Iteration: 6 || Loss: 11.552152347558154
saving ADAM checkpoint...
Sum of params:82.169655
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.552152347558154
Iteration: 2 || Loss: 11.254648576664147
Iteration: 3 || Loss: 11.21640788054755
Iteration: 4 || Loss: 11.205960340425897
Iteration: 5 || Loss: 11.139944542966152
Iteration: 6 || Loss: 11.1294036486654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.236595
Epoch 146 loss:11.1294036486654
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.236595
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.95937467888101
Iteration: 2 || Loss: 34.958786158964806
Iteration: 3 || Loss: 34.95820062842987
Iteration: 4 || Loss: 34.95761415027901
Iteration: 5 || Loss: 34.957027454816014
Iteration: 6 || Loss: 34.957027454816014
saving ADAM checkpoint...
Sum of params:82.236626
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.957027454816014
Iteration: 2 || Loss: 34.75627803234959
Iteration: 3 || Loss: 34.678717346345195
Iteration: 4 || Loss: 34.646242822652155
Iteration: 5 || Loss: 34.599011420556586
Iteration: 6 || Loss: 34.49050327179908
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.086075
Epoch 146 loss:34.49050327179908
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.779645771518243
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:55.86422302154703
waveform batch: 2/2
Test loss - extrapolation:26.995671133931914
Epoch 146 mean train loss:1.7029029247233227
Epoch 146 mean test loss - interpolation:1.796607628586374
Epoch 146 mean test loss - extrapolation:6.904991179623245
Start training epoch 147
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.086075
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.373904368006997
Iteration: 2 || Loss: 5.372242961405476
Iteration: 3 || Loss: 5.370579030513027
Iteration: 4 || Loss: 5.368916307529518
Iteration: 5 || Loss: 5.3672556937877856
Iteration: 6 || Loss: 5.3672556937877856
saving ADAM checkpoint...
Sum of params:82.086075
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.3672556937877856
Iteration: 2 || Loss: 3.90990468267941
Iteration: 3 || Loss: 3.865438253670247
Iteration: 4 || Loss: 3.845170122763038
Iteration: 5 || Loss: 3.818312492110014
Iteration: 6 || Loss: 3.7758573202802563
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.11388
Epoch 147 loss:3.7758573202802563
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.11388
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.472991878731763
Iteration: 2 || Loss: 11.472331606362959
Iteration: 3 || Loss: 11.471671744654762
Iteration: 4 || Loss: 11.471013981694206
Iteration: 5 || Loss: 11.470357197133389
Iteration: 6 || Loss: 11.470357197133389
saving ADAM checkpoint...
Sum of params:82.113914
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.470357197133389
Iteration: 2 || Loss: 11.215438264051626
Iteration: 3 || Loss: 11.174363811282937
Iteration: 4 || Loss: 11.165321352733061
Iteration: 5 || Loss: 11.11718507693317
Iteration: 6 || Loss: 11.102109098662435
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.18792
Epoch 147 loss:11.102109098662435
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.18792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.736967364368006
Iteration: 2 || Loss: 34.73650615479945
Iteration: 3 || Loss: 34.736045086084175
Iteration: 4 || Loss: 34.7355868981514
Iteration: 5 || Loss: 34.73512870432493
Iteration: 6 || Loss: 34.73512870432493
saving ADAM checkpoint...
Sum of params:82.18796
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.73512870432493
Iteration: 2 || Loss: 34.617572171146485
Iteration: 3 || Loss: 34.55640723196209
Iteration: 4 || Loss: 34.41784444663829
Iteration: 5 || Loss: 34.39471392410803
Iteration: 6 || Loss: 34.291178296669706
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.058945
Epoch 147 loss:34.291178296669706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.651412925452473
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:55.63815154965396
waveform batch: 2/2
Test loss - extrapolation:27.00938274669407
Epoch 147 mean train loss:1.6954877488142206
Epoch 147 mean test loss - interpolation:1.7752354875754122
Epoch 147 mean test loss - extrapolation:6.887294524695669
Start training epoch 148
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.058945
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.173272884965136
Iteration: 2 || Loss: 5.171667674958907
Iteration: 3 || Loss: 5.170065698437137
Iteration: 4 || Loss: 5.168464639246035
Iteration: 5 || Loss: 5.166863145494788
Iteration: 6 || Loss: 5.166863145494788
saving ADAM checkpoint...
Sum of params:82.05892
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.166863145494788
Iteration: 2 || Loss: 3.821231286487793
Iteration: 3 || Loss: 3.7874035258652223
Iteration: 4 || Loss: 3.7793887504076196
Iteration: 5 || Loss: 3.7521638350619044
Iteration: 6 || Loss: 3.7275756811123597
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.08946
Epoch 148 loss:3.7275756811123597
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.08946
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.466390180589693
Iteration: 2 || Loss: 11.465648346451738
Iteration: 3 || Loss: 11.464908053741576
Iteration: 4 || Loss: 11.464168478233974
Iteration: 5 || Loss: 11.46343094016249
Iteration: 6 || Loss: 11.46343094016249
saving ADAM checkpoint...
Sum of params:82.08951
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.46343094016249
Iteration: 2 || Loss: 11.153685527651827
Iteration: 3 || Loss: 11.119145658522662
Iteration: 4 || Loss: 11.109366397339448
Iteration: 5 || Loss: 11.042856989901914
Iteration: 6 || Loss: 11.03180490905777
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.157005
Epoch 148 loss:11.03180490905777
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.157005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.591266446019866
Iteration: 2 || Loss: 34.59067914723184
Iteration: 3 || Loss: 34.59009486984288
Iteration: 4 || Loss: 34.58951269497916
Iteration: 5 || Loss: 34.588926840842255
Iteration: 6 || Loss: 34.588926840842255
saving ADAM checkpoint...
Sum of params:82.157036
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.588926840842255
Iteration: 2 || Loss: 34.39258054757703
Iteration: 3 || Loss: 34.31760102240348
Iteration: 4 || Loss: 34.285436665887964
Iteration: 5 || Loss: 34.23784337905598
Iteration: 6 || Loss: 34.14121008662446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.010574
Epoch 148 loss:34.14121008662446
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.671441772225906
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:55.53375766849604
waveform batch: 2/2
Test loss - extrapolation:26.777828124524603
Epoch 148 mean train loss:1.6862272647170549
Epoch 148 mean test loss - interpolation:1.7785736287043177
Epoch 148 mean test loss - extrapolation:6.859298816085054
Start training epoch 149
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.010574
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.323211944844954
Iteration: 2 || Loss: 5.321542966795252
Iteration: 3 || Loss: 5.319877386430276
Iteration: 4 || Loss: 5.318209347021908
Iteration: 5 || Loss: 5.316545852008718
Iteration: 6 || Loss: 5.316545852008718
saving ADAM checkpoint...
Sum of params:82.01059
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.316545852008718
Iteration: 2 || Loss: 3.860885856085101
Iteration: 3 || Loss: 3.814893196207186
Iteration: 4 || Loss: 3.7952887364652907
Iteration: 5 || Loss: 3.769359462951616
Iteration: 6 || Loss: 3.7225827614726645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.05148
Epoch 149 loss:3.7225827614726645
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.05148
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.434776780130134
Iteration: 2 || Loss: 11.433990341007279
Iteration: 3 || Loss: 11.433204596080008
Iteration: 4 || Loss: 11.432422857402345
Iteration: 5 || Loss: 11.431640901361737
Iteration: 6 || Loss: 11.431640901361737
saving ADAM checkpoint...
Sum of params:82.05154
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.431640901361737
Iteration: 2 || Loss: 11.090368193225721
Iteration: 3 || Loss: 11.065100873235044
Iteration: 4 || Loss: 11.058561146073387
Iteration: 5 || Loss: 10.996612516747023
Iteration: 6 || Loss: 10.97971148730973
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.1286
Epoch 149 loss:10.97971148730973
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.1286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.373955109481486
Iteration: 2 || Loss: 34.37353305022596
Iteration: 3 || Loss: 34.37311069779735
Iteration: 4 || Loss: 34.37269056439193
Iteration: 5 || Loss: 34.372269010525095
Iteration: 6 || Loss: 34.372269010525095
saving ADAM checkpoint...
Sum of params:82.12863
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.372269010525095
Iteration: 2 || Loss: 34.27359211830342
Iteration: 3 || Loss: 34.216567645825464
Iteration: 4 || Loss: 34.08724338898018
Iteration: 5 || Loss: 34.06045852271662
Iteration: 6 || Loss: 33.969831565670574
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.98973
Epoch 149 loss:33.969831565670574
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.549561735188941
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:55.338118218986644
waveform batch: 2/2
Test loss - extrapolation:26.820891301400415
Epoch 149 mean train loss:1.6783491660156196
Epoch 149 mean test loss - interpolation:1.758260289198157
Epoch 149 mean test loss - extrapolation:6.846584126698922
Start training epoch 150
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.98973
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.115031006496787
Iteration: 2 || Loss: 5.113408716272536
Iteration: 3 || Loss: 5.111786218361051
Iteration: 4 || Loss: 5.1101687465924766
Iteration: 5 || Loss: 5.1085477613387225
Iteration: 6 || Loss: 5.1085477613387225
saving ADAM checkpoint...
Sum of params:81.98973
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.1085477613387225
Iteration: 2 || Loss: 3.762843263105506
Iteration: 3 || Loss: 3.7238039062521855
Iteration: 4 || Loss: 3.7138667643430865
Iteration: 5 || Loss: 3.7075041857829985
Iteration: 6 || Loss: 3.6664231688743323
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.024635
Epoch 150 loss:3.6664231688743323
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.024635
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.288137942627175
Iteration: 2 || Loss: 11.287474259568656
Iteration: 3 || Loss: 11.286810880117018
Iteration: 4 || Loss: 11.286151086514847
Iteration: 5 || Loss: 11.285490698010564
Iteration: 6 || Loss: 11.285490698010564
saving ADAM checkpoint...
Sum of params:82.0247
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.285490698010564
Iteration: 2 || Loss: 11.037921488883981
Iteration: 3 || Loss: 10.992100700382176
Iteration: 4 || Loss: 10.981795355900378
Iteration: 5 || Loss: 10.910809628039138
Iteration: 6 || Loss: 10.904378970618154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.103905
Epoch 150 loss:10.904378970618154
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.103905
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.294468082183386
Iteration: 2 || Loss: 34.29391506125978
Iteration: 3 || Loss: 34.29336478359512
Iteration: 4 || Loss: 34.29281318067986
Iteration: 5 || Loss: 34.29226331197833
Iteration: 6 || Loss: 34.29226331197833
saving ADAM checkpoint...
Sum of params:82.10395
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.29226331197833
Iteration: 2 || Loss: 34.12234507000325
Iteration: 3 || Loss: 34.02485693096737
Iteration: 4 || Loss: 33.97979070401171
Iteration: 5 || Loss: 33.932638048317024
Iteration: 6 || Loss: 33.853612292178006
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.95073
Epoch 150 loss:33.853612292178006
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.5637664941946
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:55.23777453577737
waveform batch: 2/2
Test loss - extrapolation:26.6499080240988
Epoch 150 mean train loss:1.6698073941955343
Epoch 150 mean test loss - interpolation:1.7606277490324331
Epoch 150 mean test loss - extrapolation:6.823973546656347
Start training epoch 151
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.95073
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.121461629322409
Iteration: 2 || Loss: 5.119852815498206
Iteration: 3 || Loss: 5.11824241940208
Iteration: 4 || Loss: 5.116635879979382
Iteration: 5 || Loss: 5.115030084689626
Iteration: 6 || Loss: 5.115030084689626
saving ADAM checkpoint...
Sum of params:81.95073
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.115030084689626
Iteration: 2 || Loss: 3.7939434756034904
Iteration: 3 || Loss: 3.7411135294033366
Iteration: 4 || Loss: 3.7185298278319654
Iteration: 5 || Loss: 3.7106508424762112
Iteration: 6 || Loss: 3.671980892788957
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.00203
Epoch 151 loss:3.671980892788957
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.00203
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.193566612151175
Iteration: 2 || Loss: 11.192945386185206
Iteration: 3 || Loss: 11.192323807574262
Iteration: 4 || Loss: 11.191703527811546
Iteration: 5 || Loss: 11.19108590596345
Iteration: 6 || Loss: 11.19108590596345
saving ADAM checkpoint...
Sum of params:82.0019
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.19108590596345
Iteration: 2 || Loss: 10.97736880863777
Iteration: 3 || Loss: 10.955743011418235
Iteration: 4 || Loss: 10.93826835640636
Iteration: 5 || Loss: 10.900816613720087
Iteration: 6 || Loss: 10.87057876028787
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.05989
Epoch 151 loss:10.87057876028787
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.05989
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.08876581503984
Iteration: 2 || Loss: 34.08831808391249
Iteration: 3 || Loss: 34.08787164531925
Iteration: 4 || Loss: 34.087426896194735
Iteration: 5 || Loss: 34.086980899535284
Iteration: 6 || Loss: 34.086980899535284
saving ADAM checkpoint...
Sum of params:82.05993
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.086980899535284
Iteration: 2 || Loss: 33.97710373734028
Iteration: 3 || Loss: 33.928830179058764
Iteration: 4 || Loss: 33.80977793727741
Iteration: 5 || Loss: 33.78622234116879
Iteration: 6 || Loss: 33.68154015093932
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.93256
Epoch 151 loss:33.68154015093932
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.43574204501227
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:55.07711541313503
waveform batch: 2/2
Test loss - extrapolation:26.631203922784064
Epoch 151 mean train loss:1.6628999932419362
Epoch 151 mean test loss - interpolation:1.7392903408353784
Epoch 151 mean test loss - extrapolation:6.809026611326591
Start training epoch 152
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.93256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.091598157004779
Iteration: 2 || Loss: 5.089955016155854
Iteration: 3 || Loss: 5.088313005697934
Iteration: 4 || Loss: 5.086674941690119
Iteration: 5 || Loss: 5.085037908541733
Iteration: 6 || Loss: 5.085037908541733
saving ADAM checkpoint...
Sum of params:81.93256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.085037908541733
Iteration: 2 || Loss: 3.7166324809362905
Iteration: 3 || Loss: 3.682128468924084
Iteration: 4 || Loss: 3.6735844745556445
Iteration: 5 || Loss: 3.655911316646224
Iteration: 6 || Loss: 3.624705657039727
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.962006
Epoch 152 loss:3.624705657039727
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.962006
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.239262003795929
Iteration: 2 || Loss: 11.238504162754715
Iteration: 3 || Loss: 11.237748838731306
Iteration: 4 || Loss: 11.236994173242179
Iteration: 5 || Loss: 11.23624147302011
Iteration: 6 || Loss: 11.23624147302011
saving ADAM checkpoint...
Sum of params:81.96207
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.23624147302011
Iteration: 2 || Loss: 10.923420501144285
Iteration: 3 || Loss: 10.890923891462108
Iteration: 4 || Loss: 10.882397406265696
Iteration: 5 || Loss: 10.814980085849873
Iteration: 6 || Loss: 10.80703849699905
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.035034
Epoch 152 loss:10.80703849699905
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.035034
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.97769836960712
Iteration: 2 || Loss: 33.977136941429826
Iteration: 3 || Loss: 33.976573706257895
Iteration: 4 || Loss: 33.976011480270216
Iteration: 5 || Loss: 33.97545118343363
Iteration: 6 || Loss: 33.97545118343363
saving ADAM checkpoint...
Sum of params:82.03509
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.97545118343363
Iteration: 2 || Loss: 33.7978240014943
Iteration: 3 || Loss: 33.70938533810205
Iteration: 4 || Loss: 33.67160323571264
Iteration: 5 || Loss: 33.6240360530922
Iteration: 6 || Loss: 33.54189253563835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.885124
Epoch 152 loss:33.54189253563835
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.460275309482437
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.953146994440864
waveform batch: 2/2
Test loss - extrapolation:26.43968001574704
Epoch 152 mean train loss:1.6542633341267974
Epoch 152 mean test loss - interpolation:1.7433792182470729
Epoch 152 mean test loss - extrapolation:6.782735584182325
Start training epoch 153
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.885124
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.108952415590484
Iteration: 2 || Loss: 5.107325118562265
Iteration: 3 || Loss: 5.105696873082486
Iteration: 4 || Loss: 5.10406871977178
Iteration: 5 || Loss: 5.102444658722628
Iteration: 6 || Loss: 5.102444658722628
saving ADAM checkpoint...
Sum of params:81.88513
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.102444658722628
Iteration: 2 || Loss: 3.7538465975420654
Iteration: 3 || Loss: 3.701279770342091
Iteration: 4 || Loss: 3.677952191421411
Iteration: 5 || Loss: 3.669399982597674
Iteration: 6 || Loss: 3.629630604817249
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.93463
Epoch 153 loss:3.629630604817249
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.93463
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.094547479190533
Iteration: 2 || Loss: 11.093922558616056
Iteration: 3 || Loss: 11.093297488647037
Iteration: 4 || Loss: 11.092673955609904
Iteration: 5 || Loss: 11.092052260597866
Iteration: 6 || Loss: 11.092052260597866
saving ADAM checkpoint...
Sum of params:81.9346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.092052260597866
Iteration: 2 || Loss: 10.877750948463035
Iteration: 3 || Loss: 10.857296172184022
Iteration: 4 || Loss: 10.83919802465336
Iteration: 5 || Loss: 10.800677270829523
Iteration: 6 || Loss: 10.766929502697662
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.9946
Epoch 153 loss:10.766929502697662
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.9946
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.762698043130236
Iteration: 2 || Loss: 33.762255843143606
Iteration: 3 || Loss: 33.76181522554344
Iteration: 4 || Loss: 33.76137570282744
Iteration: 5 || Loss: 33.76093451108162
Iteration: 6 || Loss: 33.76093451108162
saving ADAM checkpoint...
Sum of params:81.99465
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.76093451108162
Iteration: 2 || Loss: 33.654174396968884
Iteration: 3 || Loss: 33.605225551533614
Iteration: 4 || Loss: 33.49102713658116
Iteration: 5 || Loss: 33.46749212775095
Iteration: 6 || Loss: 33.36831685534398
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.86873
Epoch 153 loss:33.36831685534398
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.33366745604277
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.79577614588392
waveform batch: 2/2
Test loss - extrapolation:26.430695002729014
Epoch 153 mean train loss:1.6470647228572033
Epoch 153 mean test loss - interpolation:1.7222779093404617
Epoch 153 mean test loss - extrapolation:6.7688725957177445
Start training epoch 154
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.86873
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.047124327390608
Iteration: 2 || Loss: 5.045476973017632
Iteration: 3 || Loss: 5.043834350335536
Iteration: 4 || Loss: 5.042191438421829
Iteration: 5 || Loss: 5.040548230419461
Iteration: 6 || Loss: 5.040548230419461
saving ADAM checkpoint...
Sum of params:81.86874
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.040548230419461
Iteration: 2 || Loss: 3.6727644195566596
Iteration: 3 || Loss: 3.6363773746790633
Iteration: 4 || Loss: 3.62659893765254
Iteration: 5 || Loss: 3.61601080872753
Iteration: 6 || Loss: 3.5802500291871
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.895706
Epoch 154 loss:3.5802500291871
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.895706
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.103786824742476
Iteration: 2 || Loss: 11.103074926247329
Iteration: 3 || Loss: 11.102365838657986
Iteration: 4 || Loss: 11.1016577558142
Iteration: 5 || Loss: 11.10095009922228
Iteration: 6 || Loss: 11.10095009922228
saving ADAM checkpoint...
Sum of params:81.89577
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.10095009922228
Iteration: 2 || Loss: 10.822043155593702
Iteration: 3 || Loss: 10.782487701299088
Iteration: 4 || Loss: 10.773953098278328
Iteration: 5 || Loss: 10.704034852604224
Iteration: 6 || Loss: 10.698157053511643
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.97779
Epoch 154 loss:10.698157053511643
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.97779
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.668628939731605
Iteration: 2 || Loss: 33.66809682492739
Iteration: 3 || Loss: 33.66756304293325
Iteration: 4 || Loss: 33.667028914826425
Iteration: 5 || Loss: 33.66649782883873
Iteration: 6 || Loss: 33.66649782883873
saving ADAM checkpoint...
Sum of params:81.97784
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.66649782883873
Iteration: 2 || Loss: 33.50965184873069
Iteration: 3 || Loss: 33.41047554497065
Iteration: 4 || Loss: 33.36039884193866
Iteration: 5 || Loss: 33.31557817212062
Iteration: 6 || Loss: 33.24317753281201
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.826004
Epoch 154 loss:33.24317753281201
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.351362753788985
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.66327979988051
waveform batch: 2/2
Test loss - extrapolation:26.28181439682218
Epoch 154 mean train loss:1.6386753315693363
Epoch 154 mean test loss - interpolation:1.7252271256314975
Epoch 154 mean test loss - extrapolation:6.745424516391892
Start training epoch 155
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.826004
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.971100087196792
Iteration: 2 || Loss: 4.969517508565829
Iteration: 3 || Loss: 4.967936318306298
Iteration: 4 || Loss: 4.966358457128432
Iteration: 5 || Loss: 4.964782011768335
Iteration: 6 || Loss: 4.964782011768335
saving ADAM checkpoint...
Sum of params:81.82599
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.964782011768335
Iteration: 2 || Loss: 3.700629413432082
Iteration: 3 || Loss: 3.6420111077450192
Iteration: 4 || Loss: 3.617163045357855
Iteration: 5 || Loss: 3.6120654952346105
Iteration: 6 || Loss: 3.5767454023256993
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.892654
Epoch 155 loss:3.5767454023256993
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.892654
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.958397301161767
Iteration: 2 || Loss: 10.957770168772244
Iteration: 3 || Loss: 10.957143683414843
Iteration: 4 || Loss: 10.956518004312931
Iteration: 5 || Loss: 10.955893633448145
Iteration: 6 || Loss: 10.955893633448145
saving ADAM checkpoint...
Sum of params:81.89261
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.955893633448145
Iteration: 2 || Loss: 10.747348120342348
Iteration: 3 || Loss: 10.736248403543307
Iteration: 4 || Loss: 10.722242331765075
Iteration: 5 || Loss: 10.685373529638438
Iteration: 6 || Loss: 10.616019390893289
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.98753
Epoch 155 loss:10.616019390893289
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.98753
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.53547986336716
Iteration: 2 || Loss: 33.535148108443806
Iteration: 3 || Loss: 33.53481901889667
Iteration: 4 || Loss: 33.53449073948999
Iteration: 5 || Loss: 33.53416281360968
Iteration: 6 || Loss: 33.53416281360968
saving ADAM checkpoint...
Sum of params:81.98749
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.53416281360968
Iteration: 2 || Loss: 33.47711860969579
Iteration: 3 || Loss: 33.36926224273211
Iteration: 4 || Loss: 33.222820195278494
Iteration: 5 || Loss: 33.17952511974465
Iteration: 6 || Loss: 33.108860051497764
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.813
Epoch 155 loss:33.108860051497764
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.297739255427922
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.7550399344606
waveform batch: 2/2
Test loss - extrapolation:26.28457496727866
Epoch 155 mean train loss:1.6310905118867847
Epoch 155 mean test loss - interpolation:1.7162898759046537
Epoch 155 mean test loss - extrapolation:6.753301241811605
Start training epoch 156
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.813
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.288698219859929
Iteration: 2 || Loss: 5.286884456121265
Iteration: 3 || Loss: 5.285073480707021
Iteration: 4 || Loss: 5.283262387688259
Iteration: 5 || Loss: 5.281448926614609
Iteration: 6 || Loss: 5.281448926614609
saving ADAM checkpoint...
Sum of params:81.81302
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.281448926614609
Iteration: 2 || Loss: 3.6282492803641504
Iteration: 3 || Loss: 3.567215917458664
Iteration: 4 || Loss: 3.543502927398404
Iteration: 5 || Loss: 3.541629614767035
Iteration: 6 || Loss: 3.525834312888046
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.86493
Epoch 156 loss:3.525834312888046
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.86493
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.872994537797362
Iteration: 2 || Loss: 10.872357023072725
Iteration: 3 || Loss: 10.871720590771647
Iteration: 4 || Loss: 10.871082462150826
Iteration: 5 || Loss: 10.870448360935457
Iteration: 6 || Loss: 10.870448360935457
saving ADAM checkpoint...
Sum of params:81.864876
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.870448360935457
Iteration: 2 || Loss: 10.652459418052807
Iteration: 3 || Loss: 10.636859173322787
Iteration: 4 || Loss: 10.623809329228862
Iteration: 5 || Loss: 10.589501394849202
Iteration: 6 || Loss: 10.549102351187543
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.93439
Epoch 156 loss:10.549102351187543
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.93439
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.36155127721399
Iteration: 2 || Loss: 33.36117297690918
Iteration: 3 || Loss: 33.360793764974595
Iteration: 4 || Loss: 33.36041509301268
Iteration: 5 || Loss: 33.36003802138633
Iteration: 6 || Loss: 33.36003802138633
saving ADAM checkpoint...
Sum of params:81.9344
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.36003802138633
Iteration: 2 || Loss: 33.28315568325605
Iteration: 3 || Loss: 33.2304758747413
Iteration: 4 || Loss: 33.10165387698897
Iteration: 5 || Loss: 33.07374432872732
Iteration: 6 || Loss: 32.97449703173952
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.77809
Epoch 156 loss:32.97449703173952
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.185623187100093
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.49606754405261
waveform batch: 2/2
Test loss - extrapolation:26.256262893821145
Epoch 156 mean train loss:1.6223942653729346
Epoch 156 mean test loss - interpolation:1.6976038645166822
Epoch 156 mean test loss - extrapolation:6.729360869822813
Start training epoch 157
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.77809
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.97746674982723
Iteration: 2 || Loss: 4.975804900159468
Iteration: 3 || Loss: 4.974148069615833
Iteration: 4 || Loss: 4.972491067750697
Iteration: 5 || Loss: 4.970835560021624
Iteration: 6 || Loss: 4.970835560021624
saving ADAM checkpoint...
Sum of params:81.7781
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.970835560021624
Iteration: 2 || Loss: 3.5894595695539286
Iteration: 3 || Loss: 3.5397586553456204
Iteration: 4 || Loss: 3.522202691588716
Iteration: 5 || Loss: 3.519539187837557
Iteration: 6 || Loss: 3.4942882647805797
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.82664
Epoch 157 loss:3.4942882647805797
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.82664
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.815673119073674
Iteration: 2 || Loss: 10.815030721161046
Iteration: 3 || Loss: 10.814391515581208
Iteration: 4 || Loss: 10.81375055826465
Iteration: 5 || Loss: 10.813113030055062
Iteration: 6 || Loss: 10.813113030055062
saving ADAM checkpoint...
Sum of params:81.8266
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.813113030055062
Iteration: 2 || Loss: 10.594849877018454
Iteration: 3 || Loss: 10.58001697253801
Iteration: 4 || Loss: 10.569922998308643
Iteration: 5 || Loss: 10.518877143132709
Iteration: 6 || Loss: 10.499373598308924
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.90456
Epoch 157 loss:10.499373598308924
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.90456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.25941071856152
Iteration: 2 || Loss: 33.258983004368865
Iteration: 3 || Loss: 33.25855220545492
Iteration: 4 || Loss: 33.258125061520495
Iteration: 5 || Loss: 33.25769684701733
Iteration: 6 || Loss: 33.25769684701733
saving ADAM checkpoint...
Sum of params:81.9046
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.25769684701733
Iteration: 2 || Loss: 33.15889158300519
Iteration: 3 || Loss: 33.085083456857696
Iteration: 4 || Loss: 32.95821661905687
Iteration: 5 || Loss: 32.929557078696924
Iteration: 6 || Loss: 32.844658162997376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.75615
Epoch 157 loss:32.844658162997376
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.133348201417297
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.29951752311104
waveform batch: 2/2
Test loss - extrapolation:26.15689768732884
Epoch 157 mean train loss:1.6151144836581681
Epoch 157 mean test loss - interpolation:1.6888913669028829
Epoch 157 mean test loss - extrapolation:6.704701267536657
Start training epoch 158
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.75615
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.817295812329651
Iteration: 2 || Loss: 4.81571975123782
Iteration: 3 || Loss: 4.81414558539491
Iteration: 4 || Loss: 4.812570629417271
Iteration: 5 || Loss: 4.81099686797126
Iteration: 6 || Loss: 4.81099686797126
saving ADAM checkpoint...
Sum of params:81.75614
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.81099686797126
Iteration: 2 || Loss: 3.5662381961922502
Iteration: 3 || Loss: 3.510747687096654
Iteration: 4 || Loss: 3.4921386381781385
Iteration: 5 || Loss: 3.4899122567682372
Iteration: 6 || Loss: 3.46191686065639
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.820465
Epoch 158 loss:3.46191686065639
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.820465
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.720598893954982
Iteration: 2 || Loss: 10.719980735697511
Iteration: 3 || Loss: 10.71936511970593
Iteration: 4 || Loss: 10.718747346482182
Iteration: 5 || Loss: 10.718132376618277
Iteration: 6 || Loss: 10.718132376618277
saving ADAM checkpoint...
Sum of params:81.82042
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.718132376618277
Iteration: 2 || Loss: 10.520618092854507
Iteration: 3 || Loss: 10.512238492965226
Iteration: 4 || Loss: 10.503439345301508
Iteration: 5 || Loss: 10.447863825511208
Iteration: 6 || Loss: 10.432699175168718
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.89673
Epoch 158 loss:10.432699175168718
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.89673
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.16834535807006
Iteration: 2 || Loss: 33.167924393956596
Iteration: 3 || Loss: 33.16750251382236
Iteration: 4 || Loss: 33.16708076105529
Iteration: 5 || Loss: 33.166661479835504
Iteration: 6 || Loss: 33.166661479835504
saving ADAM checkpoint...
Sum of params:81.896706
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.166661479835504
Iteration: 2 || Loss: 33.07184263780256
Iteration: 3 || Loss: 32.96555620559284
Iteration: 4 || Loss: 32.84637435024603
Iteration: 5 || Loss: 32.805728859736135
Iteration: 6 || Loss: 32.73455179338053
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.727936
Epoch 158 loss:32.73455179338053
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.127295551031038
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.31640978252042
waveform batch: 2/2
Test loss - extrapolation:26.096175392900584
Epoch 158 mean train loss:1.6079023389381255
Epoch 158 mean test loss - interpolation:1.6878825918385063
Epoch 158 mean test loss - extrapolation:6.701048764618417
Start training epoch 159
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.727936
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.911148807037642
Iteration: 2 || Loss: 4.909507724305424
Iteration: 3 || Loss: 4.907870273961381
Iteration: 4 || Loss: 4.906229630526839
Iteration: 5 || Loss: 4.9045946368045765
Iteration: 6 || Loss: 4.9045946368045765
saving ADAM checkpoint...
Sum of params:81.72792
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.9045946368045765
Iteration: 2 || Loss: 3.5609526528688242
Iteration: 3 || Loss: 3.485244265847717
Iteration: 4 || Loss: 3.457142483857764
Iteration: 5 || Loss: 3.4552682071601897
Iteration: 6 || Loss: 3.426317713088376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.81456
Epoch 159 loss:3.426317713088376
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.81456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.649513483555696
Iteration: 2 || Loss: 10.648877021527959
Iteration: 3 || Loss: 10.648241145311117
Iteration: 4 || Loss: 10.647607004025991
Iteration: 5 || Loss: 10.646973592640457
Iteration: 6 || Loss: 10.646973592640457
saving ADAM checkpoint...
Sum of params:81.81452
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.646973592640457
Iteration: 2 || Loss: 10.443730360164151
Iteration: 3 || Loss: 10.434347356952577
Iteration: 4 || Loss: 10.427162435239909
Iteration: 5 || Loss: 10.379906177854858
Iteration: 6 || Loss: 10.359769314072578
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.85674
Epoch 159 loss:10.359769314072578
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.85674
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.99320160654737
Iteration: 2 || Loss: 32.99277416934519
Iteration: 3 || Loss: 32.99234727608853
Iteration: 4 || Loss: 32.991922782812736
Iteration: 5 || Loss: 32.99149961476303
Iteration: 6 || Loss: 32.99149961476303
saving ADAM checkpoint...
Sum of params:81.85678
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.99149961476303
Iteration: 2 || Loss: 32.89528090399102
Iteration: 3 || Loss: 32.82083047242711
Iteration: 4 || Loss: 32.729112992704664
Iteration: 5 || Loss: 32.69793187530077
Iteration: 6 || Loss: 32.58881420019912
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.68897
Epoch 159 loss:32.58881420019912
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.033932999185806
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.17992479267323
waveform batch: 2/2
Test loss - extrapolation:26.064895719174583
Epoch 159 mean train loss:1.5991345250813818
Epoch 159 mean test loss - interpolation:1.6723221665309678
Epoch 159 mean test loss - extrapolation:6.687068375987319
Start training epoch 160
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.68897
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.816446913454426
Iteration: 2 || Loss: 4.8148430271884335
Iteration: 3 || Loss: 4.813238573494711
Iteration: 4 || Loss: 4.811635426248765
Iteration: 5 || Loss: 4.8100296391263955
Iteration: 6 || Loss: 4.8100296391263955
saving ADAM checkpoint...
Sum of params:81.68898
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.8100296391263955
Iteration: 2 || Loss: 3.524632936575424
Iteration: 3 || Loss: 3.4639654527886936
Iteration: 4 || Loss: 3.4409463082265823
Iteration: 5 || Loss: 3.438221015951367
Iteration: 6 || Loss: 3.408905827946345
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.7584
Epoch 160 loss:3.408905827946345
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.7584
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.636813998490585
Iteration: 2 || Loss: 10.636144104615514
Iteration: 3 || Loss: 10.635476600974703
Iteration: 4 || Loss: 10.634809211635867
Iteration: 5 || Loss: 10.634142240632107
Iteration: 6 || Loss: 10.634142240632107
saving ADAM checkpoint...
Sum of params:81.75836
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.634142240632107
Iteration: 2 || Loss: 10.405955505551647
Iteration: 3 || Loss: 10.396646499498654
Iteration: 4 || Loss: 10.389730813837172
Iteration: 5 || Loss: 10.33560153752065
Iteration: 6 || Loss: 10.320975025277818
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.8394
Epoch 160 loss:10.320975025277818
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.8394
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.8858810461191
Iteration: 2 || Loss: 32.88548908048115
Iteration: 3 || Loss: 32.88510102786755
Iteration: 4 || Loss: 32.88471231736686
Iteration: 5 || Loss: 32.8843236744216
Iteration: 6 || Loss: 32.8843236744216
saving ADAM checkpoint...
Sum of params:81.83942
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.8843236744216
Iteration: 2 || Loss: 32.80620350674399
Iteration: 3 || Loss: 32.68334802999748
Iteration: 4 || Loss: 32.55960496535247
Iteration: 5 || Loss: 32.5260156892507
Iteration: 6 || Loss: 32.44949915919288
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.67229
Epoch 160 loss:32.44949915919288
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:10.031448995133161
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.12026165797821
waveform batch: 2/2
Test loss - extrapolation:25.921452699169826
Epoch 160 mean train loss:1.5923924142212773
Epoch 160 mean test loss - interpolation:1.6719081658555268
Epoch 160 mean test loss - extrapolation:6.67014286309567
Start training epoch 161
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.67229
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.969381206992767
Iteration: 2 || Loss: 4.967679650855786
Iteration: 3 || Loss: 4.965975710895545
Iteration: 4 || Loss: 4.964271972973476
Iteration: 5 || Loss: 4.96257097151675
Iteration: 6 || Loss: 4.96257097151675
saving ADAM checkpoint...
Sum of params:81.67231
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.96257097151675
Iteration: 2 || Loss: 3.5130703075912857
Iteration: 3 || Loss: 3.436551355271459
Iteration: 4 || Loss: 3.406142347248264
Iteration: 5 || Loss: 3.404349397401501
Iteration: 6 || Loss: 3.380081203061558
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.754166
Epoch 161 loss:3.380081203061558
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.754166
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.539846318672533
Iteration: 2 || Loss: 10.539213028113045
Iteration: 3 || Loss: 10.538580588747415
Iteration: 4 || Loss: 10.53795091614514
Iteration: 5 || Loss: 10.537319881114016
Iteration: 6 || Loss: 10.537319881114016
saving ADAM checkpoint...
Sum of params:81.75412
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.537319881114016
Iteration: 2 || Loss: 10.337195864345071
Iteration: 3 || Loss: 10.328070098403447
Iteration: 4 || Loss: 10.321201090643687
Iteration: 5 || Loss: 10.277880129335575
Iteration: 6 || Loss: 10.251819165107944
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.806366
Epoch 161 loss:10.251819165107944
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.806366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.69675681058894
Iteration: 2 || Loss: 32.696393835702914
Iteration: 3 || Loss: 32.69602981844463
Iteration: 4 || Loss: 32.69566821130192
Iteration: 5 || Loss: 32.69530624654648
Iteration: 6 || Loss: 32.69530624654648
saving ADAM checkpoint...
Sum of params:81.80641
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.69530624654648
Iteration: 2 || Loss: 32.62702611608696
Iteration: 3 || Loss: 32.54242972174507
Iteration: 4 || Loss: 32.43383361391875
Iteration: 5 || Loss: 32.402667610672715
Iteration: 6 || Loss: 32.30535013616555
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.63497
Epoch 161 loss:32.30535013616555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.962682337177151
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:54.040660720605395
waveform batch: 2/2
Test loss - extrapolation:25.88610400935699
Epoch 161 mean train loss:1.58404312083914
Epoch 161 mean test loss - interpolation:1.660447056196192
Epoch 161 mean test loss - extrapolation:6.660563727496865
Start training epoch 162
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.63497
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.959750638746905
Iteration: 2 || Loss: 4.958034242527921
Iteration: 3 || Loss: 4.956319234778861
Iteration: 4 || Loss: 4.95460254795838
Iteration: 5 || Loss: 4.952884924418575
Iteration: 6 || Loss: 4.952884924418575
saving ADAM checkpoint...
Sum of params:81.63499
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.952884924418575
Iteration: 2 || Loss: 3.48554366493626
Iteration: 3 || Loss: 3.414664642072695
Iteration: 4 || Loss: 3.3855274894747884
Iteration: 5 || Loss: 3.383522520278923
Iteration: 6 || Loss: 3.3603274725610195
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.710045
Epoch 162 loss:3.3603274725610195
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.710045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.510845492483329
Iteration: 2 || Loss: 10.510186422543578
Iteration: 3 || Loss: 10.509530336050931
Iteration: 4 || Loss: 10.508874792596707
Iteration: 5 || Loss: 10.508219845479942
Iteration: 6 || Loss: 10.508219845479942
saving ADAM checkpoint...
Sum of params:81.70999
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.508219845479942
Iteration: 2 || Loss: 10.29144598081349
Iteration: 3 || Loss: 10.280448239572682
Iteration: 4 || Loss: 10.26971130515967
Iteration: 5 || Loss: 10.22714230284341
Iteration: 6 || Loss: 10.195263831208223
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.76864
Epoch 162 loss:10.195263831208223
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.76864
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.54375929670844
Iteration: 2 || Loss: 32.54333901445121
Iteration: 3 || Loss: 32.54291936479791
Iteration: 4 || Loss: 32.5425013757201
Iteration: 5 || Loss: 32.542083338518886
Iteration: 6 || Loss: 32.542083338518886
saving ADAM checkpoint...
Sum of params:81.76865
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.542083338518886
Iteration: 2 || Loss: 32.44904980743833
Iteration: 3 || Loss: 32.365706766546836
Iteration: 4 || Loss: 32.28123850653229
Iteration: 5 || Loss: 32.24894820706396
Iteration: 6 || Loss: 32.151220324132055
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.600685
Epoch 162 loss:32.151220324132055
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.88940441666972
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:53.829479172444564
waveform batch: 2/2
Test loss - extrapolation:25.794460667653443
Epoch 162 mean train loss:1.5760969526862516
Epoch 162 mean test loss - interpolation:1.6482340694449533
Epoch 162 mean test loss - extrapolation:6.635328320008167
Start training epoch 163
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.600685
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.743132568181386
Iteration: 2 || Loss: 4.741533523148013
Iteration: 3 || Loss: 4.739935261936371
Iteration: 4 || Loss: 4.73833634549264
Iteration: 5 || Loss: 4.736742749419795
Iteration: 6 || Loss: 4.736742749419795
saving ADAM checkpoint...
Sum of params:81.60071
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.736742749419795
Iteration: 2 || Loss: 3.4702077427462674
Iteration: 3 || Loss: 3.3957672073239697
Iteration: 4 || Loss: 3.3686407943383743
Iteration: 5 || Loss: 3.3660332187296054
Iteration: 6 || Loss: 3.3278777865774742
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.68127
Epoch 163 loss:3.3278777865774742
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.68127
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.488142123542989
Iteration: 2 || Loss: 10.48742656366501
Iteration: 3 || Loss: 10.486712566864869
Iteration: 4 || Loss: 10.485997660767916
Iteration: 5 || Loss: 10.485284557136886
Iteration: 6 || Loss: 10.485284557136886
saving ADAM checkpoint...
Sum of params:81.68123
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.485284557136886
Iteration: 2 || Loss: 10.229101062064748
Iteration: 3 || Loss: 10.218129672974635
Iteration: 4 || Loss: 10.193026683264273
Iteration: 5 || Loss: 10.158379839100345
Iteration: 6 || Loss: 10.130104420267896
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.76237
Epoch 163 loss:10.130104420267896
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.76237
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.56715516551846
Iteration: 2 || Loss: 32.56665041146023
Iteration: 3 || Loss: 32.56614497665398
Iteration: 4 || Loss: 32.565640150308795
Iteration: 5 || Loss: 32.565134446396804
Iteration: 6 || Loss: 32.565134446396804
saving ADAM checkpoint...
Sum of params:81.7624
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.565134446396804
Iteration: 2 || Loss: 32.43569668290706
Iteration: 3 || Loss: 32.253999097832946
Iteration: 4 || Loss: 32.150778552534334
Iteration: 5 || Loss: 32.10802787983652
Iteration: 6 || Loss: 32.03276700024765
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.57796
Epoch 163 loss:32.03276700024765
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.884343919602658
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:53.75697286466988
waveform batch: 2/2
Test loss - extrapolation:25.672124794189035
Epoch 163 mean train loss:1.568646524382518
Epoch 163 mean test loss - interpolation:1.6473906532671097
Epoch 163 mean test loss - extrapolation:6.6190914715715765
Start training epoch 164
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.57796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.737305988453628
Iteration: 2 || Loss: 4.735716212912957
Iteration: 3 || Loss: 4.734121477974859
Iteration: 4 || Loss: 4.73253038533186
Iteration: 5 || Loss: 4.730942602909338
Iteration: 6 || Loss: 4.730942602909338
saving ADAM checkpoint...
Sum of params:81.57796
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.730942602909338
Iteration: 2 || Loss: 3.475979053874908
Iteration: 3 || Loss: 3.3714519491671657
Iteration: 4 || Loss: 3.334198801722582
Iteration: 5 || Loss: 3.3308323890826164
Iteration: 6 || Loss: 3.2949871936696975
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.64973
Epoch 164 loss:3.2949871936696975
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.64973
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.51643851207084
Iteration: 2 || Loss: 10.515628141831613
Iteration: 3 || Loss: 10.514814549419608
Iteration: 4 || Loss: 10.514005882298225
Iteration: 5 || Loss: 10.513194031536349
Iteration: 6 || Loss: 10.513194031536349
saving ADAM checkpoint...
Sum of params:81.64967
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.513194031536349
Iteration: 2 || Loss: 10.163824537359451
Iteration: 3 || Loss: 10.14082045852978
Iteration: 4 || Loss: 10.135085715909666
Iteration: 5 || Loss: 10.092997169756309
Iteration: 6 || Loss: 10.080523508341663
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.71491
Epoch 164 loss:10.080523508341663
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.71491
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.304766722849806
Iteration: 2 || Loss: 32.30435428617783
Iteration: 3 || Loss: 32.30394334651377
Iteration: 4 || Loss: 32.30353278030394
Iteration: 5 || Loss: 32.30312343057102
Iteration: 6 || Loss: 32.30312343057102
saving ADAM checkpoint...
Sum of params:81.714935
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.30312343057102
Iteration: 2 || Loss: 32.21654019147715
Iteration: 3 || Loss: 32.12557520749923
Iteration: 4 || Loss: 32.01078877431235
Iteration: 5 || Loss: 31.985143463718707
Iteration: 6 || Loss: 31.886146527457186
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.55331
Epoch 164 loss:31.886146527457186
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.79374954215903
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:53.617151984068265
waveform batch: 2/2
Test loss - extrapolation:25.61972490234476
Epoch 164 mean train loss:1.5607468010161567
Epoch 164 mean test loss - interpolation:1.6322915903598385
Epoch 164 mean test loss - extrapolation:6.603073073867752
Start training epoch 165
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.55331
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.763007978507194
Iteration: 2 || Loss: 4.761366544606183
Iteration: 3 || Loss: 4.759723010387693
Iteration: 4 || Loss: 4.758081325557634
Iteration: 5 || Loss: 4.756441022052288
Iteration: 6 || Loss: 4.756441022052288
saving ADAM checkpoint...
Sum of params:81.55332
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.756441022052288
Iteration: 2 || Loss: 3.4223474616790512
Iteration: 3 || Loss: 3.343747656217276
Iteration: 4 || Loss: 3.3140289587496246
Iteration: 5 || Loss: 3.311632759343597
Iteration: 6 || Loss: 3.27397909109655
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.634514
Epoch 165 loss:3.27397909109655
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.634514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.377618234326475
Iteration: 2 || Loss: 10.376892418259574
Iteration: 3 || Loss: 10.376168238298728
Iteration: 4 || Loss: 10.375445249878789
Iteration: 5 || Loss: 10.374722835235133
Iteration: 6 || Loss: 10.374722835235133
saving ADAM checkpoint...
Sum of params:81.63451
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.374722835235133
Iteration: 2 || Loss: 10.113760013421956
Iteration: 3 || Loss: 10.103420286959093
Iteration: 4 || Loss: 10.095749711149653
Iteration: 5 || Loss: 10.0445130858813
Iteration: 6 || Loss: 10.033759906518306
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.67247
Epoch 165 loss:10.033759906518306
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.67247
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.14648389505341
Iteration: 2 || Loss: 32.14602154753326
Iteration: 3 || Loss: 32.14556127830372
Iteration: 4 || Loss: 32.145100114948335
Iteration: 5 || Loss: 32.14463988539345
Iteration: 6 || Loss: 32.14463988539345
saving ADAM checkpoint...
Sum of params:81.67251
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.14463988539345
Iteration: 2 || Loss: 32.03504828388767
Iteration: 3 || Loss: 31.935841812279943
Iteration: 4 || Loss: 31.85740025373752
Iteration: 5 || Loss: 31.832973619361287
Iteration: 6 || Loss: 31.70486254841428
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.5058
Epoch 165 loss:31.70486254841428
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.745455006907417
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:53.4658668579264
waveform batch: 2/2
Test loss - extrapolation:25.485995502703872
Epoch 165 mean train loss:1.5521586740010045
Epoch 165 mean test loss - interpolation:1.624242501151236
Epoch 165 mean test loss - extrapolation:6.579321863385856
Start training epoch 166
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.5058
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.721558443482434
Iteration: 2 || Loss: 4.719941257690253
Iteration: 3 || Loss: 4.718332577000264
Iteration: 4 || Loss: 4.716724573641688
Iteration: 5 || Loss: 4.715114727951467
Iteration: 6 || Loss: 4.715114727951467
saving ADAM checkpoint...
Sum of params:81.50581
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.715114727951467
Iteration: 2 || Loss: 3.432075901616693
Iteration: 3 || Loss: 3.354883291263943
Iteration: 4 || Loss: 3.324026299739683
Iteration: 5 || Loss: 3.3202902946622337
Iteration: 6 || Loss: 3.2748413952719724
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.58505
Epoch 166 loss:3.2748413952719724
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.58505
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.377990127729854
Iteration: 2 || Loss: 10.377231793361698
Iteration: 3 || Loss: 10.376475808405187
Iteration: 4 || Loss: 10.375721152737338
Iteration: 5 || Loss: 10.374968982276092
Iteration: 6 || Loss: 10.374968982276092
saving ADAM checkpoint...
Sum of params:81.58501
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.374968982276092
Iteration: 2 || Loss: 10.090093574087659
Iteration: 3 || Loss: 10.078954591217842
Iteration: 4 || Loss: 10.067771151232273
Iteration: 5 || Loss: 10.011193225240481
Iteration: 6 || Loss: 10.002156662241639
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.62972
Epoch 166 loss:10.002156662241639
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.62972
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.944843777177468
Iteration: 2 || Loss: 31.944348607626647
Iteration: 3 || Loss: 31.943852885531054
Iteration: 4 || Loss: 31.943359071085794
Iteration: 5 || Loss: 31.942866298554737
Iteration: 6 || Loss: 31.942866298554737
saving ADAM checkpoint...
Sum of params:81.62979
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.942866298554737
Iteration: 2 || Loss: 31.814833823967458
Iteration: 3 || Loss: 31.713022606995825
Iteration: 4 || Loss: 31.648480056205322
Iteration: 5 || Loss: 31.62188307706632
Iteration: 6 || Loss: 31.51446257197239
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.48055
Epoch 166 loss:31.51446257197239
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.689600097048285
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:53.24888227062395
waveform batch: 2/2
Test loss - extrapolation:25.312872535646427
Epoch 166 mean train loss:1.5445331251546899
Epoch 166 mean test loss - interpolation:1.6149333495080473
Epoch 166 mean test loss - extrapolation:6.546812900522532
Start training epoch 167
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.48055
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.627316203656604
Iteration: 2 || Loss: 4.625758088252533
Iteration: 3 || Loss: 4.624198190483635
Iteration: 4 || Loss: 4.6226387855527
Iteration: 5 || Loss: 4.621082170798021
Iteration: 6 || Loss: 4.621082170798021
saving ADAM checkpoint...
Sum of params:81.48057
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.621082170798021
Iteration: 2 || Loss: 3.423178369333041
Iteration: 3 || Loss: 3.3446072112360583
Iteration: 4 || Loss: 3.3158547829024205
Iteration: 5 || Loss: 3.3106473419883193
Iteration: 6 || Loss: 3.2610484961676582
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.550026
Epoch 167 loss:3.2610484961676582
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.550026
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.344428989233432
Iteration: 2 || Loss: 10.343664620367141
Iteration: 3 || Loss: 10.342901552016217
Iteration: 4 || Loss: 10.34214050390615
Iteration: 5 || Loss: 10.341379131000458
Iteration: 6 || Loss: 10.341379131000458
saving ADAM checkpoint...
Sum of params:81.54999
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.341379131000458
Iteration: 2 || Loss: 10.044083058384414
Iteration: 3 || Loss: 10.03325677023949
Iteration: 4 || Loss: 10.026166760290106
Iteration: 5 || Loss: 9.965888568493531
Iteration: 6 || Loss: 9.956823821260091
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.62243
Epoch 167 loss:9.956823821260091
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.62243
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.7783942060033
Iteration: 2 || Loss: 31.77796586666176
Iteration: 3 || Loss: 31.777538010789186
Iteration: 4 || Loss: 31.777109558277555
Iteration: 5 || Loss: 31.776683796557908
Iteration: 6 || Loss: 31.776683796557908
saving ADAM checkpoint...
Sum of params:81.62247
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.776683796557908
Iteration: 2 || Loss: 31.681896668018084
Iteration: 3 || Loss: 31.548722425491786
Iteration: 4 || Loss: 31.473694602587624
Iteration: 5 || Loss: 31.435611062998287
Iteration: 6 || Loss: 31.36717134792782
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.45346
Epoch 167 loss:31.36717134792782
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.686091012947081
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:53.24349388667297
waveform batch: 2/2
Test loss - extrapolation:25.22694116091074
Epoch 167 mean train loss:1.5374152988053644
Epoch 167 mean test loss - interpolation:1.6143485021578468
Epoch 167 mean test loss - extrapolation:6.539202920631976
Start training epoch 168
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.45346
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.766322631331865
Iteration: 2 || Loss: 4.764668746898492
Iteration: 3 || Loss: 4.763018942075419
Iteration: 4 || Loss: 4.76136542831114
Iteration: 5 || Loss: 4.759715320008158
Iteration: 6 || Loss: 4.759715320008158
saving ADAM checkpoint...
Sum of params:81.45346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.759715320008158
Iteration: 2 || Loss: 3.415723932941964
Iteration: 3 || Loss: 3.3065207428064904
Iteration: 4 || Loss: 3.2702668083259194
Iteration: 5 || Loss: 3.2673699079109726
Iteration: 6 || Loss: 3.2338707135394653
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.51598
Epoch 168 loss:3.2338707135394653
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.51598
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.332578127347452
Iteration: 2 || Loss: 10.331786735771804
Iteration: 3 || Loss: 10.330996431014727
Iteration: 4 || Loss: 10.330206687783507
Iteration: 5 || Loss: 10.329418055539259
Iteration: 6 || Loss: 10.329418055539259
saving ADAM checkpoint...
Sum of params:81.51593
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.329418055539259
Iteration: 2 || Loss: 9.997198134686414
Iteration: 3 || Loss: 9.966431110004596
Iteration: 4 || Loss: 9.959248928946828
Iteration: 5 || Loss: 9.908038243159899
Iteration: 6 || Loss: 9.902075745184371
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.58628
Epoch 168 loss:9.902075745184371
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.58628
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.652541540994694
Iteration: 2 || Loss: 31.65210252887705
Iteration: 3 || Loss: 31.65166335518891
Iteration: 4 || Loss: 31.65122442954494
Iteration: 5 || Loss: 31.650787578658452
Iteration: 6 || Loss: 31.650787578658452
saving ADAM checkpoint...
Sum of params:81.58631
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.650787578658452
Iteration: 2 || Loss: 31.553335409214643
Iteration: 3 || Loss: 31.44102905244416
Iteration: 4 || Loss: 31.341281490187853
Iteration: 5 || Loss: 31.318471330533477
Iteration: 6 || Loss: 31.2295235651091
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.43237
Epoch 168 loss:31.2295235651091
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.607529835811677
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:53.03564581433522
waveform batch: 2/2
Test loss - extrapolation:25.127315543368567
Epoch 168 mean train loss:1.5298437939252738
Epoch 168 mean test loss - interpolation:1.6012549726352796
Epoch 168 mean test loss - extrapolation:6.513580113141981
Start training epoch 169
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.43237
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.675039175369844
Iteration: 2 || Loss: 4.673420730532207
Iteration: 3 || Loss: 4.671806457160445
Iteration: 4 || Loss: 4.670189344704756
Iteration: 5 || Loss: 4.668575141723946
Iteration: 6 || Loss: 4.668575141723946
saving ADAM checkpoint...
Sum of params:81.43237
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.668575141723946
Iteration: 2 || Loss: 3.38205175450526
Iteration: 3 || Loss: 3.2867550535974512
Iteration: 4 || Loss: 3.255634307649346
Iteration: 5 || Loss: 3.252198247227698
Iteration: 6 || Loss: 3.214007078828366
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.49423
Epoch 169 loss:3.214007078828366
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.49423
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.268854275126731
Iteration: 2 || Loss: 10.26807217401959
Iteration: 3 || Loss: 10.267293154058216
Iteration: 4 || Loss: 10.266513851326572
Iteration: 5 || Loss: 10.26573562806874
Iteration: 6 || Loss: 10.26573562806874
saving ADAM checkpoint...
Sum of params:81.494194
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.26573562806874
Iteration: 2 || Loss: 9.948163958769245
Iteration: 3 || Loss: 9.927698250848135
Iteration: 4 || Loss: 9.919856719611028
Iteration: 5 || Loss: 9.862727780803597
Iteration: 6 || Loss: 9.856889257472394
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.5635
Epoch 169 loss:9.856889257472394
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.5635
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.516670359686785
Iteration: 2 || Loss: 31.51620939827244
Iteration: 3 || Loss: 31.51575022212818
Iteration: 4 || Loss: 31.515292848017626
Iteration: 5 || Loss: 31.51483574331257
Iteration: 6 || Loss: 31.51483574331257
saving ADAM checkpoint...
Sum of params:81.56354
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.51483574331257
Iteration: 2 || Loss: 31.40723019402875
Iteration: 3 || Loss: 31.28099541109461
Iteration: 4 || Loss: 31.20747697621046
Iteration: 5 || Loss: 31.178107686406786
Iteration: 6 || Loss: 31.094242594376844
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.40161
Epoch 169 loss:31.094242594376844
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.57634832659507
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.937804273885774
waveform batch: 2/2
Test loss - extrapolation:25.04029181295528
Epoch 169 mean train loss:1.5229358251957796
Epoch 169 mean test loss - interpolation:1.5960580544325118
Epoch 169 mean test loss - extrapolation:6.498174673903421
Start training epoch 170
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.40161
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.636452306924881
Iteration: 2 || Loss: 4.634855047047396
Iteration: 3 || Loss: 4.633260217035597
Iteration: 4 || Loss: 4.631667584358053
Iteration: 5 || Loss: 4.630070871719045
Iteration: 6 || Loss: 4.630070871719045
saving ADAM checkpoint...
Sum of params:81.40162
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.630070871719045
Iteration: 2 || Loss: 3.3829142334595614
Iteration: 3 || Loss: 3.270597210024684
Iteration: 4 || Loss: 3.23767950881937
Iteration: 5 || Loss: 3.2334342467234314
Iteration: 6 || Loss: 3.1962819317757334
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.46366
Epoch 170 loss:3.1962819317757334
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.46366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.188752646525304
Iteration: 2 || Loss: 10.188029848227407
Iteration: 3 || Loss: 10.187310236896518
Iteration: 4 || Loss: 10.186590764178744
Iteration: 5 || Loss: 10.185872530889789
Iteration: 6 || Loss: 10.185872530889789
saving ADAM checkpoint...
Sum of params:81.46361
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.185872530889789
Iteration: 2 || Loss: 9.910126140943907
Iteration: 3 || Loss: 9.88041626497589
Iteration: 4 || Loss: 9.872034681267534
Iteration: 5 || Loss: 9.818490701551825
Iteration: 6 || Loss: 9.813333283898116
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.536354
Epoch 170 loss:9.813333283898116
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.536354
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.381888588837175
Iteration: 2 || Loss: 31.38143449349972
Iteration: 3 || Loss: 31.380986603668816
Iteration: 4 || Loss: 31.38053469079438
Iteration: 5 || Loss: 31.38008772872177
Iteration: 6 || Loss: 31.38008772872177
saving ADAM checkpoint...
Sum of params:81.5364
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.38008772872177
Iteration: 2 || Loss: 31.27819558291261
Iteration: 3 || Loss: 31.149168728146215
Iteration: 4 || Loss: 31.06460337542599
Iteration: 5 || Loss: 31.040442294262608
Iteration: 6 || Loss: 30.95051488502139
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.3776
Epoch 170 loss:30.95051488502139
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.533519768148674
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.82298160339949
waveform batch: 2/2
Test loss - extrapolation:24.93455327181174
Epoch 170 mean train loss:1.5158665551963875
Epoch 170 mean test loss - interpolation:1.5889199613581122
Epoch 170 mean test loss - extrapolation:6.47979457293427
Start training epoch 171
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.3776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.665833847153565
Iteration: 2 || Loss: 4.664210266189674
Iteration: 3 || Loss: 4.662587232105007
Iteration: 4 || Loss: 4.660968548961433
Iteration: 5 || Loss: 4.659346022420024
Iteration: 6 || Loss: 4.659346022420024
saving ADAM checkpoint...
Sum of params:81.3776
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.659346022420024
Iteration: 2 || Loss: 3.3688465014857276
Iteration: 3 || Loss: 3.2580497911878177
Iteration: 4 || Loss: 3.2237397303707027
Iteration: 5 || Loss: 3.2198391492607947
Iteration: 6 || Loss: 3.1848683431271705
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.43682
Epoch 171 loss:3.1848683431271705
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.43682
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.17303278070478
Iteration: 2 || Loss: 10.172282140400007
Iteration: 3 || Loss: 10.171535410865916
Iteration: 4 || Loss: 10.17078430922586
Iteration: 5 || Loss: 10.170037458860907
Iteration: 6 || Loss: 10.170037458860907
saving ADAM checkpoint...
Sum of params:81.436775
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.170037458860907
Iteration: 2 || Loss: 9.872517339296662
Iteration: 3 || Loss: 9.843208223626041
Iteration: 4 || Loss: 9.834490987322361
Iteration: 5 || Loss: 9.781585014454157
Iteration: 6 || Loss: 9.77489665579723
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.50987
Epoch 171 loss:9.77489665579723
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.50987
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.234944542855047
Iteration: 2 || Loss: 31.234499706453374
Iteration: 3 || Loss: 31.2340531010959
Iteration: 4 || Loss: 31.233607130935933
Iteration: 5 || Loss: 31.23316257010998
Iteration: 6 || Loss: 31.23316257010998
saving ADAM checkpoint...
Sum of params:81.50989
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.23316257010998
Iteration: 2 || Loss: 31.133821807522214
Iteration: 3 || Loss: 31.00978960670979
Iteration: 4 || Loss: 30.917504119624734
Iteration: 5 || Loss: 30.895512800775204
Iteration: 6 || Loss: 30.807307344748665
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.35708
Epoch 171 loss:30.807307344748665
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.484658368282487
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.68625221136706
waveform batch: 2/2
Test loss - extrapolation:24.8262671263608
Epoch 171 mean train loss:1.5092093911611402
Epoch 171 mean test loss - interpolation:1.5807763947137479
Epoch 171 mean test loss - extrapolation:6.459376611477322
Start training epoch 172
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.35708
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.645703259404887
Iteration: 2 || Loss: 4.644079667821326
Iteration: 3 || Loss: 4.642458418110897
Iteration: 4 || Loss: 4.640837492582947
Iteration: 5 || Loss: 4.639212957693134
Iteration: 6 || Loss: 4.639212957693134
saving ADAM checkpoint...
Sum of params:81.357086
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.639212957693134
Iteration: 2 || Loss: 3.3529331029969267
Iteration: 3 || Loss: 3.242259454551933
Iteration: 4 || Loss: 3.2092459989923277
Iteration: 5 || Loss: 3.205196989955546
Iteration: 6 || Loss: 3.1674985560647486
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.41587
Epoch 172 loss:3.1674985560647486
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.41587
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.105358581199658
Iteration: 2 || Loss: 10.104636382363925
Iteration: 3 || Loss: 10.103914374703281
Iteration: 4 || Loss: 10.103193554080336
Iteration: 5 || Loss: 10.102472755353164
Iteration: 6 || Loss: 10.102472755353164
saving ADAM checkpoint...
Sum of params:81.41583
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.102472755353164
Iteration: 2 || Loss: 9.827185341073859
Iteration: 3 || Loss: 9.799129933760677
Iteration: 4 || Loss: 9.7901921603741
Iteration: 5 || Loss: 9.736045346628318
Iteration: 6 || Loss: 9.730689477929772
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.488235
Epoch 172 loss:9.730689477929772
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.488235
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.09836566275598
Iteration: 2 || Loss: 31.09791541757848
Iteration: 3 || Loss: 31.097466096963856
Iteration: 4 || Loss: 31.0970180544498
Iteration: 5 || Loss: 31.096568571220242
Iteration: 6 || Loss: 31.096568571220242
saving ADAM checkpoint...
Sum of params:81.48826
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.096568571220242
Iteration: 2 || Loss: 30.995807669411352
Iteration: 3 || Loss: 30.86117295912535
Iteration: 4 || Loss: 30.77941193431431
Iteration: 5 || Loss: 30.755958278597937
Iteration: 6 || Loss: 30.668078825000986
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.33004
Epoch 172 loss:30.668078825000986
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.451792464296407
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.58553997444585
waveform batch: 2/2
Test loss - extrapolation:24.728624790358143
Epoch 172 mean train loss:1.5022850641032932
Epoch 172 mean test loss - interpolation:1.575298744049401
Epoch 172 mean test loss - extrapolation:6.442847063733666
Start training epoch 173
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.33004
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.646919383857056
Iteration: 2 || Loss: 4.645289951062877
Iteration: 3 || Loss: 4.643663595839493
Iteration: 4 || Loss: 4.642038332403894
Iteration: 5 || Loss: 4.640413523341643
Iteration: 6 || Loss: 4.640413523341643
saving ADAM checkpoint...
Sum of params:81.330055
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.640413523341643
Iteration: 2 || Loss: 3.350559655047181
Iteration: 3 || Loss: 3.2295850243531534
Iteration: 4 || Loss: 3.1948813203796926
Iteration: 5 || Loss: 3.1909459950191748
Iteration: 6 || Loss: 3.146817615298648
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.396935
Epoch 173 loss:3.146817615298648
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.396935
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.014987893750352
Iteration: 2 || Loss: 10.01431339203037
Iteration: 3 || Loss: 10.013640183731226
Iteration: 4 || Loss: 10.01296939732846
Iteration: 5 || Loss: 10.012299982065093
Iteration: 6 || Loss: 10.012299982065093
saving ADAM checkpoint...
Sum of params:81.39689
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.012299982065093
Iteration: 2 || Loss: 9.776599208839716
Iteration: 3 || Loss: 9.753981789704282
Iteration: 4 || Loss: 9.744085554919453
Iteration: 5 || Loss: 9.690140570851808
Iteration: 6 || Loss: 9.684614761963045
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.46508
Epoch 173 loss:9.684614761963045
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.46508
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.962950926838126
Iteration: 2 || Loss: 30.96248014397297
Iteration: 3 || Loss: 30.962010610216435
Iteration: 4 || Loss: 30.961541089223505
Iteration: 5 || Loss: 30.961073718905585
Iteration: 6 || Loss: 30.961073718905585
saving ADAM checkpoint...
Sum of params:81.465096
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.961073718905585
Iteration: 2 || Loss: 30.85082896275797
Iteration: 3 || Loss: 30.705339767950026
Iteration: 4 || Loss: 30.643365951491734
Iteration: 5 || Loss: 30.616060499838643
Iteration: 6 || Loss: 30.52637727781753
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.29866
Epoch 173 loss:30.52637727781753
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.421879833646626
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.48763973100692
waveform batch: 2/2
Test loss - extrapolation:24.62673484588693
Epoch 173 mean train loss:1.4950968846579042
Epoch 173 mean test loss - interpolation:1.570313305607771
Epoch 173 mean test loss - extrapolation:6.426197881407821
Start training epoch 174
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.29866
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.637761691153702
Iteration: 2 || Loss: 4.6361402758665955
Iteration: 3 || Loss: 4.634521272231205
Iteration: 4 || Loss: 4.632903955915373
Iteration: 5 || Loss: 4.631286440032193
Iteration: 6 || Loss: 4.631286440032193
saving ADAM checkpoint...
Sum of params:81.29866
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.631286440032193
Iteration: 2 || Loss: 3.3566265466736054
Iteration: 3 || Loss: 3.2209920350647696
Iteration: 4 || Loss: 3.1835419050984437
Iteration: 5 || Loss: 3.1794434009204178
Iteration: 6 || Loss: 3.1259597779985295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.3862
Epoch 174 loss:3.1259597779985295
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.3862
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.941627096077632
Iteration: 2 || Loss: 9.940959243667775
Iteration: 3 || Loss: 9.940292318573153
Iteration: 4 || Loss: 9.939628313909838
Iteration: 5 || Loss: 9.938963289516474
Iteration: 6 || Loss: 9.938963289516474
saving ADAM checkpoint...
Sum of params:81.38616
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.938963289516474
Iteration: 2 || Loss: 9.717107653725616
Iteration: 3 || Loss: 9.707264968923111
Iteration: 4 || Loss: 9.697422267192861
Iteration: 5 || Loss: 9.648029781077797
Iteration: 6 || Loss: 9.62821535160356
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.455475
Epoch 174 loss:9.62821535160356
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.455475
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.803928833678754
Iteration: 2 || Loss: 30.8035214209266
Iteration: 3 || Loss: 30.803112453022955
Iteration: 4 || Loss: 30.802707076318086
Iteration: 5 || Loss: 30.802302030791417
Iteration: 6 || Loss: 30.802302030791417
saving ADAM checkpoint...
Sum of params:81.45549
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.802302030791417
Iteration: 2 || Loss: 30.71971439344967
Iteration: 3 || Loss: 30.57899356678498
Iteration: 4 || Loss: 30.502377138248605
Iteration: 5 || Loss: 30.469074453025133
Iteration: 6 || Loss: 30.398422053849494
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.287
Epoch 174 loss:30.398422053849494
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.366807779911069
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.37532586817512
waveform batch: 2/2
Test loss - extrapolation:24.549052210735706
Epoch 174 mean train loss:1.4880205925328132
Epoch 174 mean test loss - interpolation:1.561134629985178
Epoch 174 mean test loss - extrapolation:6.410364839909235
Start training epoch 175
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.287
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.620424341704342
Iteration: 2 || Loss: 4.6187863748139275
Iteration: 3 || Loss: 4.6171477249823605
Iteration: 4 || Loss: 4.615506061491427
Iteration: 5 || Loss: 4.6138694049602496
Iteration: 6 || Loss: 4.6138694049602496
saving ADAM checkpoint...
Sum of params:81.28701
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.6138694049602496
Iteration: 2 || Loss: 3.3145051091013493
Iteration: 3 || Loss: 3.1664681440177893
Iteration: 4 || Loss: 3.133997822824228
Iteration: 5 || Loss: 3.131477020576333
Iteration: 6 || Loss: 3.0820260576475245
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.39119
Epoch 175 loss:3.0820260576475245
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.39119
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.916229355197773
Iteration: 2 || Loss: 9.9154825916133
Iteration: 3 || Loss: 9.914735709374431
Iteration: 4 || Loss: 9.913988674669515
Iteration: 5 || Loss: 9.913246377416373
Iteration: 6 || Loss: 9.913246377416373
saving ADAM checkpoint...
Sum of params:81.391136
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.913246377416373
Iteration: 2 || Loss: 9.64918285551973
Iteration: 3 || Loss: 9.63558613680711
Iteration: 4 || Loss: 9.624958645244135
Iteration: 5 || Loss: 9.575179907858502
Iteration: 6 || Loss: 9.56656864312013
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.41884
Epoch 175 loss:9.56656864312013
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.41884
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.698779371525262
Iteration: 2 || Loss: 30.6983188996767
Iteration: 3 || Loss: 30.697859847280103
Iteration: 4 || Loss: 30.697401755964705
Iteration: 5 || Loss: 30.69694429367161
Iteration: 6 || Loss: 30.69694429367161
saving ADAM checkpoint...
Sum of params:81.4188
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.69694429367161
Iteration: 2 || Loss: 30.593313281964015
Iteration: 3 || Loss: 30.455782482212154
Iteration: 4 || Loss: 30.386949019572974
Iteration: 5 || Loss: 30.362460694843275
Iteration: 6 || Loss: 30.258908770852297
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.24794
Epoch 175 loss:30.258908770852297
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.318093737234646
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.25220356818921
waveform batch: 2/2
Test loss - extrapolation:24.45196471663477
Epoch 175 mean train loss:1.4795690852282743
Epoch 175 mean test loss - interpolation:1.553015622872441
Epoch 175 mean test loss - extrapolation:6.392014023735332
Start training epoch 176
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.24794
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.600894665949737
Iteration: 2 || Loss: 4.599268657465162
Iteration: 3 || Loss: 4.5976434834269435
Iteration: 4 || Loss: 4.596016107201768
Iteration: 5 || Loss: 4.5943904427778826
Iteration: 6 || Loss: 4.5943904427778826
saving ADAM checkpoint...
Sum of params:81.24794
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.5943904427778826
Iteration: 2 || Loss: 3.316332014968133
Iteration: 3 || Loss: 3.1670530820580716
Iteration: 4 || Loss: 3.1304513643535783
Iteration: 5 || Loss: 3.1269536664639537
Iteration: 6 || Loss: 3.07657864652774
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.35243
Epoch 176 loss:3.07657864652774
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.35243
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.884528492835328
Iteration: 2 || Loss: 9.883776102722994
Iteration: 3 || Loss: 9.88302590312885
Iteration: 4 || Loss: 9.882274853567344
Iteration: 5 || Loss: 9.8815219397816
Iteration: 6 || Loss: 9.8815219397816
saving ADAM checkpoint...
Sum of params:81.352394
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.8815219397816
Iteration: 2 || Loss: 9.612284400471735
Iteration: 3 || Loss: 9.599458580478677
Iteration: 4 || Loss: 9.589850044724686
Iteration: 5 || Loss: 9.538156670541614
Iteration: 6 || Loss: 9.53065254577444
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.391
Epoch 176 loss:9.53065254577444
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.391
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.5418977854714
Iteration: 2 || Loss: 30.541423583716373
Iteration: 3 || Loss: 30.540946658350265
Iteration: 4 || Loss: 30.540471348105484
Iteration: 5 || Loss: 30.53999642064242
Iteration: 6 || Loss: 30.53999642064242
saving ADAM checkpoint...
Sum of params:81.390976
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.53999642064242
Iteration: 2 || Loss: 30.428125169738852
Iteration: 3 || Loss: 30.27628852762098
Iteration: 4 || Loss: 30.22289147445371
Iteration: 5 || Loss: 30.196274389154457
Iteration: 6 || Loss: 30.097444061724723
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.22176
Epoch 176 loss:30.097444061724723
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.2911597910778
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:52.1498304420052
waveform batch: 2/2
Test loss - extrapolation:24.32912111838638
Epoch 176 mean train loss:1.4725750087595484
Epoch 176 mean test loss - interpolation:1.5485266318462998
Epoch 176 mean test loss - extrapolation:6.373245963365965
Start training epoch 177
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.22176
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.609342116112972
Iteration: 2 || Loss: 4.607712574889948
Iteration: 3 || Loss: 4.606084134982524
Iteration: 4 || Loss: 4.604453531564407
Iteration: 5 || Loss: 4.602826563309998
Iteration: 6 || Loss: 4.602826563309998
saving ADAM checkpoint...
Sum of params:81.22176
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.602826563309998
Iteration: 2 || Loss: 3.3235396926072416
Iteration: 3 || Loss: 3.165031277579411
Iteration: 4 || Loss: 3.1249241393076796
Iteration: 5 || Loss: 3.120890102554839
Iteration: 6 || Loss: 3.0685385476121465
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.32976
Epoch 177 loss:3.0685385476121465
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.32976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.84634079826872
Iteration: 2 || Loss: 9.845584845511084
Iteration: 3 || Loss: 9.84482809352904
Iteration: 4 || Loss: 9.844073396822713
Iteration: 5 || Loss: 9.843318563171723
Iteration: 6 || Loss: 9.843318563171723
saving ADAM checkpoint...
Sum of params:81.32972
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.843318563171723
Iteration: 2 || Loss: 9.571419658467592
Iteration: 3 || Loss: 9.557534564927396
Iteration: 4 || Loss: 9.548534339486242
Iteration: 5 || Loss: 9.496525508315022
Iteration: 6 || Loss: 9.488937209066675
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.3717
Epoch 177 loss:9.488937209066675
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.3717
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.369112414952628
Iteration: 2 || Loss: 30.368654744450573
Iteration: 3 || Loss: 30.3681961940252
Iteration: 4 || Loss: 30.367739908365795
Iteration: 5 || Loss: 30.367283786253942
Iteration: 6 || Loss: 30.367283786253942
saving ADAM checkpoint...
Sum of params:81.37166
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.367283786253942
Iteration: 2 || Loss: 30.264208620021105
Iteration: 3 || Loss: 30.11703142578688
Iteration: 4 || Loss: 30.056514107114147
Iteration: 5 || Loss: 30.030272718373997
Iteration: 6 || Loss: 29.94663267916602
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.208
Epoch 177 loss:29.94663267916602
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.232659664282801
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.97035391554003
waveform batch: 2/2
Test loss - extrapolation:24.217649245349687
Epoch 177 mean train loss:1.4656589115808567
Epoch 177 mean test loss - interpolation:1.5387766107138001
Epoch 177 mean test loss - extrapolation:6.3490002634074765
Start training epoch 178
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.208
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.525559583925464
Iteration: 2 || Loss: 4.5239609697480345
Iteration: 3 || Loss: 4.522366161003315
Iteration: 4 || Loss: 4.520770056069185
Iteration: 5 || Loss: 4.519177995236742
Iteration: 6 || Loss: 4.519177995236742
saving ADAM checkpoint...
Sum of params:81.20799
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.519177995236742
Iteration: 2 || Loss: 3.297101510820842
Iteration: 3 || Loss: 3.1294107617461067
Iteration: 4 || Loss: 3.097756141109971
Iteration: 5 || Loss: 3.094559443450663
Iteration: 6 || Loss: 3.0481177286937906
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.31472
Epoch 178 loss:3.0481177286937906
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.31472
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.81299375289343
Iteration: 2 || Loss: 9.812213677501244
Iteration: 3 || Loss: 9.811435237611972
Iteration: 4 || Loss: 9.810657476028258
Iteration: 5 || Loss: 9.809882198727234
Iteration: 6 || Loss: 9.809882198727234
saving ADAM checkpoint...
Sum of params:81.314674
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.809882198727234
Iteration: 2 || Loss: 9.523387390146828
Iteration: 3 || Loss: 9.506993311373144
Iteration: 4 || Loss: 9.498118101131531
Iteration: 5 || Loss: 9.448426827570238
Iteration: 6 || Loss: 9.439546172710113
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.35398
Epoch 178 loss:9.439546172710113
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.35398
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.223719543226867
Iteration: 2 || Loss: 30.22329092809756
Iteration: 3 || Loss: 30.222864407982243
Iteration: 4 || Loss: 30.22243620911459
Iteration: 5 || Loss: 30.222011119960776
Iteration: 6 || Loss: 30.222011119960776
saving ADAM checkpoint...
Sum of params:81.35395
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.222011119960776
Iteration: 2 || Loss: 30.13356841372926
Iteration: 3 || Loss: 29.992035420241066
Iteration: 4 || Loss: 29.919361880273144
Iteration: 5 || Loss: 29.895006419984792
Iteration: 6 || Loss: 29.814841862049423
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.19225
Epoch 178 loss:29.814841862049423
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.180129743947322
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.845024879769994
waveform batch: 2/2
Test loss - extrapolation:24.126249447601285
Epoch 178 mean train loss:1.458707095291494
Epoch 178 mean test loss - interpolation:1.5300216239912203
Epoch 178 mean test loss - extrapolation:6.33093952728094
Start training epoch 179
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.19225
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.5079134327821375
Iteration: 2 || Loss: 4.506307575525505
Iteration: 3 || Loss: 4.504703308020812
Iteration: 4 || Loss: 4.503098670260552
Iteration: 5 || Loss: 4.50149595949315
Iteration: 6 || Loss: 4.50149595949315
saving ADAM checkpoint...
Sum of params:81.192245
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.50149595949315
Iteration: 2 || Loss: 3.270206816674596
Iteration: 3 || Loss: 3.1000932787925874
Iteration: 4 || Loss: 3.0710277850245826
Iteration: 5 || Loss: 3.0682186009536276
Iteration: 6 || Loss: 3.020131857109506
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.29818
Epoch 179 loss:3.020131857109506
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.29818
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.752678908451697
Iteration: 2 || Loss: 9.751910772878604
Iteration: 3 || Loss: 9.751143792543461
Iteration: 4 || Loss: 9.750376972951944
Iteration: 5 || Loss: 9.749609644772105
Iteration: 6 || Loss: 9.749609644772105
saving ADAM checkpoint...
Sum of params:81.29814
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.749609644772105
Iteration: 2 || Loss: 9.473753382569306
Iteration: 3 || Loss: 9.45557631462197
Iteration: 4 || Loss: 9.44581890960824
Iteration: 5 || Loss: 9.396645291295538
Iteration: 6 || Loss: 9.387818595885756
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.333
Epoch 179 loss:9.387818595885756
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.333
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.110301128936584
Iteration: 2 || Loss: 30.109857657182392
Iteration: 3 || Loss: 30.109415086874492
Iteration: 4 || Loss: 30.10897341806506
Iteration: 5 || Loss: 30.10853202260223
Iteration: 6 || Loss: 30.10853202260223
saving ADAM checkpoint...
Sum of params:81.33296
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.10853202260223
Iteration: 2 || Loss: 30.014204616061047
Iteration: 3 || Loss: 29.86712287291208
Iteration: 4 || Loss: 29.797454636149716
Iteration: 5 || Loss: 29.77317334951269
Iteration: 6 || Loss: 29.687549376968292
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.16752
Epoch 179 loss:29.687549376968292
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.140759578494572
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.74083414501292
waveform batch: 2/2
Test loss - extrapolation:24.037006227963506
Epoch 179 mean train loss:1.4515689596539159
Epoch 179 mean test loss - interpolation:1.5234599297490954
Epoch 179 mean test loss - extrapolation:6.314820031081369
Start training epoch 180
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.16752
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.489439873815243
Iteration: 2 || Loss: 4.487836964154452
Iteration: 3 || Loss: 4.486238419442897
Iteration: 4 || Loss: 4.4846388906596895
Iteration: 5 || Loss: 4.483042969594366
Iteration: 6 || Loss: 4.483042969594366
saving ADAM checkpoint...
Sum of params:81.16751
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.483042969594366
Iteration: 2 || Loss: 3.2641749519156082
Iteration: 3 || Loss: 3.081334548850494
Iteration: 4 || Loss: 3.0523327322482974
Iteration: 5 || Loss: 3.0493513177807223
Iteration: 6 || Loss: 3.002082323560836
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.27615
Epoch 180 loss:3.002082323560836
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.27615
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.71411728909463
Iteration: 2 || Loss: 9.713338650059717
Iteration: 3 || Loss: 9.712561791410005
Iteration: 4 || Loss: 9.7117850553998
Iteration: 5 || Loss: 9.711009195609892
Iteration: 6 || Loss: 9.711009195609892
saving ADAM checkpoint...
Sum of params:81.27611
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.711009195609892
Iteration: 2 || Loss: 9.428875140236215
Iteration: 3 || Loss: 9.40933167831538
Iteration: 4 || Loss: 9.399787905612753
Iteration: 5 || Loss: 9.351697646292982
Iteration: 6 || Loss: 9.341116419841748
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.31566
Epoch 180 loss:9.341116419841748
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.31566
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.974676883649092
Iteration: 2 || Loss: 29.974246572416316
Iteration: 3 || Loss: 29.97381789177311
Iteration: 4 || Loss: 29.97338826225983
Iteration: 5 || Loss: 29.97296000519791
Iteration: 6 || Loss: 29.97296000519791
saving ADAM checkpoint...
Sum of params:81.31564
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.97296000519791
Iteration: 2 || Loss: 29.88480830412679
Iteration: 3 || Loss: 29.73674398353499
Iteration: 4 || Loss: 29.665656755853398
Iteration: 5 || Loss: 29.64107988305237
Iteration: 6 || Loss: 29.559132001586
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.150635
Epoch 180 loss:29.559132001586
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.096512985399166
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.631851281577745
waveform batch: 2/2
Test loss - extrapolation:23.949713224872397
Epoch 180 mean train loss:1.4449079567237444
Epoch 180 mean test loss - interpolation:1.5160854975665277
Epoch 180 mean test loss - extrapolation:6.2984637088708455
Start training epoch 181
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.150635
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.469561977896989
Iteration: 2 || Loss: 4.4679633599200175
Iteration: 3 || Loss: 4.466363193018923
Iteration: 4 || Loss: 4.46476785879294
Iteration: 5 || Loss: 4.463169573596206
Iteration: 6 || Loss: 4.463169573596206
saving ADAM checkpoint...
Sum of params:81.15065
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.463169573596206
Iteration: 2 || Loss: 3.246740541006431
Iteration: 3 || Loss: 3.0562673383783268
Iteration: 4 || Loss: 3.0288466400481875
Iteration: 5 || Loss: 3.0259634341496957
Iteration: 6 || Loss: 2.9769000566920183
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.26085
Epoch 181 loss:2.9769000566920183
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.26085
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.654392025975124
Iteration: 2 || Loss: 9.653626028337086
Iteration: 3 || Loss: 9.652863318216733
Iteration: 4 || Loss: 9.6520988071065
Iteration: 5 || Loss: 9.651336434578255
Iteration: 6 || Loss: 9.651336434578255
saving ADAM checkpoint...
Sum of params:81.26081
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.651336434578255
Iteration: 2 || Loss: 9.380796264188577
Iteration: 3 || Loss: 9.359031490454862
Iteration: 4 || Loss: 9.348864742618781
Iteration: 5 || Loss: 9.301348790831778
Iteration: 6 || Loss: 9.289931063378196
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.29817
Epoch 181 loss:9.289931063378196
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.29817
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.859656109753214
Iteration: 2 || Loss: 29.859212365347112
Iteration: 3 || Loss: 29.858770221877755
Iteration: 4 || Loss: 29.858326571443328
Iteration: 5 || Loss: 29.857884328994324
Iteration: 6 || Loss: 29.857884328994324
saving ADAM checkpoint...
Sum of params:81.29813
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.857884328994324
Iteration: 2 || Loss: 29.76410728450607
Iteration: 3 || Loss: 29.61248929523621
Iteration: 4 || Loss: 29.543502670865447
Iteration: 5 || Loss: 29.51840355734585
Iteration: 6 || Loss: 29.43449200142562
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.13074
Epoch 181 loss:29.43449200142562
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.053416692961115
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.52150382410244
waveform batch: 2/2
Test loss - extrapolation:23.863966971072074
Epoch 181 mean train loss:1.4379766593619252
Epoch 181 mean test loss - interpolation:1.5089027821601857
Epoch 181 mean test loss - extrapolation:6.282122566264543
Start training epoch 182
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.13074
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.429113308898578
Iteration: 2 || Loss: 4.427530595369424
Iteration: 3 || Loss: 4.425950875690644
Iteration: 4 || Loss: 4.424370036440095
Iteration: 5 || Loss: 4.422788920403622
Iteration: 6 || Loss: 4.422788920403622
saving ADAM checkpoint...
Sum of params:81.130745
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.422788920403622
Iteration: 2 || Loss: 3.237030340661263
Iteration: 3 || Loss: 3.0330565430376364
Iteration: 4 || Loss: 3.007640582467939
Iteration: 5 || Loss: 3.0046067867418587
Iteration: 6 || Loss: 2.953246966935531
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.24422
Epoch 182 loss:2.953246966935531
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.24422
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.595392326058597
Iteration: 2 || Loss: 9.594641236478672
Iteration: 3 || Loss: 9.593889028871224
Iteration: 4 || Loss: 9.593136897708453
Iteration: 5 || Loss: 9.592388689593163
Iteration: 6 || Loss: 9.592388689593163
saving ADAM checkpoint...
Sum of params:81.24418
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.592388689593163
Iteration: 2 || Loss: 9.33284045109306
Iteration: 3 || Loss: 9.309242674463476
Iteration: 4 || Loss: 9.298695295139217
Iteration: 5 || Loss: 9.252019720182263
Iteration: 6 || Loss: 9.239359857946072
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.28183
Epoch 182 loss:9.239359857946072
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.28183
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.74603953908252
Iteration: 2 || Loss: 29.745582353544922
Iteration: 3 || Loss: 29.745127364238915
Iteration: 4 || Loss: 29.744671369411144
Iteration: 5 || Loss: 29.744215721907796
Iteration: 6 || Loss: 29.744215721907796
saving ADAM checkpoint...
Sum of params:81.28179
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.744215721907796
Iteration: 2 || Loss: 29.64502330263952
Iteration: 3 || Loss: 29.486808172648086
Iteration: 4 || Loss: 29.420686033526717
Iteration: 5 || Loss: 29.39486126456951
Iteration: 6 || Loss: 29.30944726701311
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.11208
Epoch 182 loss:29.30944726701311
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:9.011988224861783
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.42314302848118
waveform batch: 2/2
Test loss - extrapolation:23.780256016657525
Epoch 182 mean train loss:1.4311053135136107
Epoch 182 mean test loss - interpolation:1.501998037476964
Epoch 182 mean test loss - extrapolation:6.266949920428225
Start training epoch 183
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.11208
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.4019404269286895
Iteration: 2 || Loss: 4.400369043077607
Iteration: 3 || Loss: 4.3987938894318965
Iteration: 4 || Loss: 4.39722193134249
Iteration: 5 || Loss: 4.395649084802345
Iteration: 6 || Loss: 4.395649084802345
saving ADAM checkpoint...
Sum of params:81.112076
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.395649084802345
Iteration: 2 || Loss: 3.227715662273146
Iteration: 3 || Loss: 3.011256308944351
Iteration: 4 || Loss: 2.98692656909471
Iteration: 5 || Loss: 2.9837251368421196
Iteration: 6 || Loss: 2.930441619466799
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.2283
Epoch 183 loss:2.930441619466799
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.2283
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.537449704018945
Iteration: 2 || Loss: 9.536707649957394
Iteration: 3 || Loss: 9.535965652614909
Iteration: 4 || Loss: 9.535226231207501
Iteration: 5 || Loss: 9.534488773633056
Iteration: 6 || Loss: 9.534488773633056
saving ADAM checkpoint...
Sum of params:81.22827
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.534488773633056
Iteration: 2 || Loss: 9.284046874740277
Iteration: 3 || Loss: 9.258804753799655
Iteration: 4 || Loss: 9.248122799897972
Iteration: 5 || Loss: 9.202673616183969
Iteration: 6 || Loss: 9.18816469190864
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.26779
Epoch 183 loss:9.18816469190864
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.26779
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.628299491683556
Iteration: 2 || Loss: 29.62783633709658
Iteration: 3 || Loss: 29.62737326310432
Iteration: 4 || Loss: 29.626911214505917
Iteration: 5 || Loss: 29.626448122810718
Iteration: 6 || Loss: 29.626448122810718
saving ADAM checkpoint...
Sum of params:81.26776
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.626448122810718
Iteration: 2 || Loss: 29.524734924211305
Iteration: 3 || Loss: 29.36196734553042
Iteration: 4 || Loss: 29.297414237325093
Iteration: 5 || Loss: 29.270724379485557
Iteration: 6 || Loss: 29.185398363938848
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.0961
Epoch 183 loss:29.185398363938848
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.96883329120728
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.321512139906986
waveform batch: 2/2
Test loss - extrapolation:23.697953163857832
Epoch 183 mean train loss:1.4242760232866996
Epoch 183 mean test loss - interpolation:1.4948055485345468
Epoch 183 mean test loss - extrapolation:6.2516221086470685
Start training epoch 184
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.0961
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.369880423691471
Iteration: 2 || Loss: 4.368315817715438
Iteration: 3 || Loss: 4.366752655964333
Iteration: 4 || Loss: 4.365193977380019
Iteration: 5 || Loss: 4.363632543424517
Iteration: 6 || Loss: 4.363632543424517
saving ADAM checkpoint...
Sum of params:81.09609
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.363632543424517
Iteration: 2 || Loss: 3.215740853221924
Iteration: 3 || Loss: 2.9867813269348145
Iteration: 4 || Loss: 2.9641454680472785
Iteration: 5 || Loss: 2.9608520641326748
Iteration: 6 || Loss: 2.904849526266963
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.214645
Epoch 184 loss:2.904849526266963
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.214645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.472918013531688
Iteration: 2 || Loss: 9.472193884085401
Iteration: 3 || Loss: 9.471472317440035
Iteration: 4 || Loss: 9.470749450661124
Iteration: 5 || Loss: 9.470027704947821
Iteration: 6 || Loss: 9.470027704947821
saving ADAM checkpoint...
Sum of params:81.21459
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.470027704947821
Iteration: 2 || Loss: 9.2335574387666
Iteration: 3 || Loss: 9.206811720732151
Iteration: 4 || Loss: 9.195497039257125
Iteration: 5 || Loss: 9.150959964366523
Iteration: 6 || Loss: 9.13479701842023
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.252686
Epoch 184 loss:9.13479701842023
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.252686
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.521757640863484
Iteration: 2 || Loss: 29.52127501273382
Iteration: 3 || Loss: 29.52079569302217
Iteration: 4 || Loss: 29.520316252490048
Iteration: 5 || Loss: 29.519835422997595
Iteration: 6 || Loss: 29.519835422997595
saving ADAM checkpoint...
Sum of params:81.25266
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.519835422997595
Iteration: 2 || Loss: 29.41073485874745
Iteration: 3 || Loss: 29.241453297624798
Iteration: 4 || Loss: 29.176649380531828
Iteration: 5 || Loss: 29.150457477643002
Iteration: 6 || Loss: 29.06016058160885
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.080215
Epoch 184 loss:29.06016058160885
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.923872603975767
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.22587363477761
waveform batch: 2/2
Test loss - extrapolation:23.614129641589113
Epoch 184 mean train loss:1.417234728492967
Epoch 184 mean test loss - interpolation:1.4873121006626278
Epoch 184 mean test loss - extrapolation:6.236666939697226
Start training epoch 185
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.080215
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.346400843042005
Iteration: 2 || Loss: 4.344842668735158
Iteration: 3 || Loss: 4.343284373230539
Iteration: 4 || Loss: 4.341729856785908
Iteration: 5 || Loss: 4.34017654681542
Iteration: 6 || Loss: 4.34017654681542
saving ADAM checkpoint...
Sum of params:81.080215
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.34017654681542
Iteration: 2 || Loss: 3.2047339517300215
Iteration: 3 || Loss: 2.9662953181282528
Iteration: 4 || Loss: 2.944359691855499
Iteration: 5 || Loss: 2.940836622139743
Iteration: 6 || Loss: 2.8821817873314783
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.20024
Epoch 185 loss:2.8821817873314783
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.20024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.412904544262402
Iteration: 2 || Loss: 9.412192914500059
Iteration: 3 || Loss: 9.41148519663097
Iteration: 4 || Loss: 9.410775160822618
Iteration: 5 || Loss: 9.410066681772731
Iteration: 6 || Loss: 9.410066681772731
saving ADAM checkpoint...
Sum of params:81.20022
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.410066681772731
Iteration: 2 || Loss: 9.18297481106385
Iteration: 3 || Loss: 9.155069997448656
Iteration: 4 || Loss: 9.143645311325134
Iteration: 5 || Loss: 9.100353404349278
Iteration: 6 || Loss: 9.082485955312617
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.239784
Epoch 185 loss:9.082485955312617
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.239784
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.405747046303784
Iteration: 2 || Loss: 29.40525569018315
Iteration: 3 || Loss: 29.404766477468
Iteration: 4 || Loss: 29.404278697974075
Iteration: 5 || Loss: 29.403790876089275
Iteration: 6 || Loss: 29.403790876089275
saving ADAM checkpoint...
Sum of params:81.23973
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.403790876089275
Iteration: 2 || Loss: 29.29172750273932
Iteration: 3 || Loss: 29.116326944808637
Iteration: 4 || Loss: 29.051888531280206
Iteration: 5 || Loss: 29.025770525771044
Iteration: 6 || Loss: 28.933188230088533
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.06605
Epoch 185 loss:28.933188230088533
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.880364455150788
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.13384507915577
waveform batch: 2/2
Test loss - extrapolation:23.53141443754167
Epoch 185 mean train loss:1.41027089561147
Epoch 185 mean test loss - interpolation:1.4800607425251313
Epoch 185 mean test loss - extrapolation:6.222104959724786
Start training epoch 186
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.06605
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.330084838251853
Iteration: 2 || Loss: 4.328525371754432
Iteration: 3 || Loss: 4.326968000358685
Iteration: 4 || Loss: 4.325411334700461
Iteration: 5 || Loss: 4.3238579352629465
Iteration: 6 || Loss: 4.3238579352629465
saving ADAM checkpoint...
Sum of params:81.06604
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.3238579352629465
Iteration: 2 || Loss: 3.193411651923994
Iteration: 3 || Loss: 2.9450300268484106
Iteration: 4 || Loss: 2.923732402866593
Iteration: 5 || Loss: 2.920068350164541
Iteration: 6 || Loss: 2.858671908116628
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.18769
Epoch 186 loss:2.858671908116628
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.18769
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.351435901617377
Iteration: 2 || Loss: 9.350737670901662
Iteration: 3 || Loss: 9.350043320055702
Iteration: 4 || Loss: 9.349350045501248
Iteration: 5 || Loss: 9.348655758564316
Iteration: 6 || Loss: 9.348655758564316
saving ADAM checkpoint...
Sum of params:81.18765
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.348655758564316
Iteration: 2 || Loss: 9.13281271693176
Iteration: 3 || Loss: 9.103169870958562
Iteration: 4 || Loss: 9.091227368898181
Iteration: 5 || Loss: 9.048880670510917
Iteration: 6 || Loss: 9.028939108502952
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.22647
Epoch 186 loss:9.028939108502952
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.22647
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.29728041918443
Iteration: 2 || Loss: 29.296777209825468
Iteration: 3 || Loss: 29.296274666058324
Iteration: 4 || Loss: 29.295773007095953
Iteration: 5 || Loss: 29.29527167553211
Iteration: 6 || Loss: 29.29527167553211
saving ADAM checkpoint...
Sum of params:81.226456
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.29527167553211
Iteration: 2 || Loss: 29.178109451146163
Iteration: 3 || Loss: 28.994210562698935
Iteration: 4 || Loss: 28.926396962916577
Iteration: 5 || Loss: 28.901377775612342
Iteration: 6 || Loss: 28.802596736559767
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.05236
Epoch 186 loss:28.802596736559767
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.834838556779118
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:51.04692404125284
waveform batch: 2/2
Test loss - extrapolation:23.446462964319892
Epoch 186 mean train loss:1.4031106121785983
Epoch 186 mean test loss - interpolation:1.4724730927965197
Epoch 186 mean test loss - extrapolation:6.207782250464395
Start training epoch 187
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.05236
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.326250707614325
Iteration: 2 || Loss: 4.32468512078157
Iteration: 3 || Loss: 4.32312027208096
Iteration: 4 || Loss: 4.321556489293399
Iteration: 5 || Loss: 4.319989838222104
Iteration: 6 || Loss: 4.319989838222104
saving ADAM checkpoint...
Sum of params:81.05235
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.319989838222104
Iteration: 2 || Loss: 3.1822978809627576
Iteration: 3 || Loss: 2.927283850355292
Iteration: 4 || Loss: 2.9059165184373277
Iteration: 5 || Loss: 2.9020626225156976
Iteration: 6 || Loss: 2.8376077650086
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.17477
Epoch 187 loss:2.8376077650086
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.17477
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.296110897727871
Iteration: 2 || Loss: 9.29542057854207
Iteration: 3 || Loss: 9.294732649076126
Iteration: 4 || Loss: 9.294043703497014
Iteration: 5 || Loss: 9.293356619970185
Iteration: 6 || Loss: 9.293356619970185
saving ADAM checkpoint...
Sum of params:81.17473
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.293356619970185
Iteration: 2 || Loss: 9.083551951411442
Iteration: 3 || Loss: 9.051972506666823
Iteration: 4 || Loss: 9.039775948665767
Iteration: 5 || Loss: 8.998095136832287
Iteration: 6 || Loss: 8.976734111268074
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.21426
Epoch 187 loss:8.976734111268074
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.21426
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.17950811456084
Iteration: 2 || Loss: 29.17899716816713
Iteration: 3 || Loss: 29.178484781142103
Iteration: 4 || Loss: 29.177974696250615
Iteration: 5 || Loss: 29.17746508631015
Iteration: 6 || Loss: 29.17746508631015
saving ADAM checkpoint...
Sum of params:81.21422
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.17746508631015
Iteration: 2 || Loss: 29.05782023279624
Iteration: 3 || Loss: 28.864683511430542
Iteration: 4 || Loss: 28.79542858330249
Iteration: 5 || Loss: 28.771073772879355
Iteration: 6 || Loss: 28.668050042202253
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.03886
Epoch 187 loss:28.668050042202253
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.792444536361801
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.961417285465146
waveform batch: 2/2
Test loss - extrapolation:23.358179083044234
Epoch 187 mean train loss:1.3959445489130664
Epoch 187 mean test loss - interpolation:1.465407422726967
Epoch 187 mean test loss - extrapolation:6.193299697375782
Start training epoch 188
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.03886
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.330874047785999
Iteration: 2 || Loss: 4.329298276647297
Iteration: 3 || Loss: 4.327719279157625
Iteration: 4 || Loss: 4.3261422402912455
Iteration: 5 || Loss: 4.324566584252596
Iteration: 6 || Loss: 4.324566584252596
saving ADAM checkpoint...
Sum of params:81.03888
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.324566584252596
Iteration: 2 || Loss: 3.1741947902476544
Iteration: 3 || Loss: 2.9107019273982506
Iteration: 4 || Loss: 2.8890964688060716
Iteration: 5 || Loss: 2.8850907196681277
Iteration: 6 || Loss: 2.8173543991819443
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.16264
Epoch 188 loss:2.8173543991819443
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.16264
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.24207217006955
Iteration: 2 || Loss: 9.241389344626619
Iteration: 3 || Loss: 9.240708869414922
Iteration: 4 || Loss: 9.240029381241817
Iteration: 5 || Loss: 9.239348466444655
Iteration: 6 || Loss: 9.239348466444655
saving ADAM checkpoint...
Sum of params:81.16262
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.239348466444655
Iteration: 2 || Loss: 9.036024190658033
Iteration: 3 || Loss: 9.001838115456051
Iteration: 4 || Loss: 8.989152764610472
Iteration: 5 || Loss: 8.94783803438992
Iteration: 6 || Loss: 8.924455289572734
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.20186
Epoch 188 loss:8.924455289572734
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.20186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.06129894672467
Iteration: 2 || Loss: 29.060781902712325
Iteration: 3 || Loss: 29.06026088154326
Iteration: 4 || Loss: 29.059744490351996
Iteration: 5 || Loss: 29.05922823733412
Iteration: 6 || Loss: 29.05922823733412
saving ADAM checkpoint...
Sum of params:81.20184
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.05922823733412
Iteration: 2 || Loss: 28.937384468107513
Iteration: 3 || Loss: 28.733992480944057
Iteration: 4 || Loss: 28.660618894839374
Iteration: 5 || Loss: 28.637271795762313
Iteration: 6 || Loss: 28.52779652225324
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.02545
Epoch 188 loss:28.52779652225324
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.7492380721715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.88210582181646
waveform batch: 2/2
Test loss - extrapolation:23.267396368450548
Epoch 188 mean train loss:1.388607110724411
Epoch 188 mean test loss - interpolation:1.4582063453619167
Epoch 188 mean test loss - extrapolation:6.1791251825222515
Start training epoch 189
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.02545
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.352467932997781
Iteration: 2 || Loss: 4.350867669684263
Iteration: 3 || Loss: 4.349267981047745
Iteration: 4 || Loss: 4.347668366302389
Iteration: 5 || Loss: 4.346070289791731
Iteration: 6 || Loss: 4.346070289791731
saving ADAM checkpoint...
Sum of params:81.02545
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.346070289791731
Iteration: 2 || Loss: 3.167121186831321
Iteration: 3 || Loss: 2.8976617665251543
Iteration: 4 || Loss: 2.8750917823466793
Iteration: 5 || Loss: 2.870941175402873
Iteration: 6 || Loss: 2.799662917111542
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.15024
Epoch 189 loss:2.799662917111542
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.15024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.196311608295698
Iteration: 2 || Loss: 9.19562808340503
Iteration: 3 || Loss: 9.194945159202161
Iteration: 4 || Loss: 9.194264116217795
Iteration: 5 || Loss: 9.193582976441323
Iteration: 6 || Loss: 9.193582976441323
saving ADAM checkpoint...
Sum of params:81.15022
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.193582976441323
Iteration: 2 || Loss: 8.990674483451555
Iteration: 3 || Loss: 8.952886983497399
Iteration: 4 || Loss: 8.93991420278416
Iteration: 5 || Loss: 8.898407801402568
Iteration: 6 || Loss: 8.87381146192482
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.191
Epoch 189 loss:8.87381146192482
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.191
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.93224294238656
Iteration: 2 || Loss: 28.931718444254457
Iteration: 3 || Loss: 28.931195399060357
Iteration: 4 || Loss: 28.93067162131364
Iteration: 5 || Loss: 28.93015123752847
Iteration: 6 || Loss: 28.93015123752847
saving ADAM checkpoint...
Sum of params:81.19099
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.93015123752847
Iteration: 2 || Loss: 28.806754032288467
Iteration: 3 || Loss: 28.59166698534605
Iteration: 4 || Loss: 28.519308030288006
Iteration: 5 || Loss: 28.496098417332856
Iteration: 6 || Loss: 28.38397413556142
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.01334
Epoch 189 loss:28.38397413556142
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.70905011648748
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.789855411410606
waveform batch: 2/2
Test loss - extrapolation:23.167927278390753
Epoch 189 mean train loss:1.3812913280895787
Epoch 189 mean test loss - interpolation:1.4515083527479133
Epoch 189 mean test loss - extrapolation:6.163148557483447
Start training epoch 190
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.01334
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.363864578329983
Iteration: 2 || Loss: 4.362247717235658
Iteration: 3 || Loss: 4.36063121798424
Iteration: 4 || Loss: 4.359019791629885
Iteration: 5 || Loss: 4.357406439089791
Iteration: 6 || Loss: 4.357406439089791
saving ADAM checkpoint...
Sum of params:81.01335
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.357406439089791
Iteration: 2 || Loss: 3.163225640206392
Iteration: 3 || Loss: 2.883180445636628
Iteration: 4 || Loss: 2.860500639270689
Iteration: 5 || Loss: 2.856212388822058
Iteration: 6 || Loss: 2.7802320266350264
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.13991
Epoch 190 loss:2.7802320266350264
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.13991
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.14191988486691
Iteration: 2 || Loss: 9.141248839441902
Iteration: 3 || Loss: 9.140578014068268
Iteration: 4 || Loss: 9.139909921870139
Iteration: 5 || Loss: 9.139240215587686
Iteration: 6 || Loss: 9.139240215587686
saving ADAM checkpoint...
Sum of params:81.139885
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.139240215587686
Iteration: 2 || Loss: 8.945609029091683
Iteration: 3 || Loss: 8.904405970199688
Iteration: 4 || Loss: 8.890346111130937
Iteration: 5 || Loss: 8.848769441593403
Iteration: 6 || Loss: 8.822166639563171
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.175766
Epoch 190 loss:8.822166639563171
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.175766
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.814964211638134
Iteration: 2 || Loss: 28.814431436381636
Iteration: 3 || Loss: 28.813900099672104
Iteration: 4 || Loss: 28.813371103029482
Iteration: 5 || Loss: 28.81284144255415
Iteration: 6 || Loss: 28.81284144255415
saving ADAM checkpoint...
Sum of params:81.17574
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.81284144255415
Iteration: 2 || Loss: 28.68876163194386
Iteration: 3 || Loss: 28.458428640452645
Iteration: 4 || Loss: 28.373913046114556
Iteration: 5 || Loss: 28.352968543889812
Iteration: 6 || Loss: 28.21939344713022
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.997055
Epoch 190 loss:28.21939344713022
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.667005618827105
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.762527490544706
waveform batch: 2/2
Test loss - extrapolation:23.067868242078976
Epoch 190 mean train loss:1.3731652452871868
Epoch 190 mean test loss - interpolation:1.444500936471184
Epoch 190 mean test loss - extrapolation:6.15253297771864
Start training epoch 191
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.997055
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.496968438531484
Iteration: 2 || Loss: 4.495263233165915
Iteration: 3 || Loss: 4.493556246342586
Iteration: 4 || Loss: 4.4918522018291585
Iteration: 5 || Loss: 4.490151255294096
Iteration: 6 || Loss: 4.490151255294096
saving ADAM checkpoint...
Sum of params:80.99704
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.490151255294096
Iteration: 2 || Loss: 3.1635845136042646
Iteration: 3 || Loss: 2.893845864677661
Iteration: 4 || Loss: 2.8647341972975497
Iteration: 5 || Loss: 2.8602295339278045
Iteration: 6 || Loss: 2.7799165850196452
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.12315
Epoch 191 loss:2.7799165850196452
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.12315
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.147295045556284
Iteration: 2 || Loss: 9.146565021945925
Iteration: 3 || Loss: 9.145835346739728
Iteration: 4 || Loss: 9.145107946006227
Iteration: 5 || Loss: 9.144381012100672
Iteration: 6 || Loss: 9.144381012100672
saving ADAM checkpoint...
Sum of params:81.12313
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.144381012100672
Iteration: 2 || Loss: 8.915240485331369
Iteration: 3 || Loss: 8.864377636537595
Iteration: 4 || Loss: 8.851575006419834
Iteration: 5 || Loss: 8.807052297233467
Iteration: 6 || Loss: 8.782927523527805
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.17881
Epoch 191 loss:8.782927523527805
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.17881
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.619766826799314
Iteration: 2 || Loss: 28.61925802001035
Iteration: 3 || Loss: 28.618747507934852
Iteration: 4 || Loss: 28.618243445436395
Iteration: 5 || Loss: 28.617738431257553
Iteration: 6 || Loss: 28.617738431257553
saving ADAM checkpoint...
Sum of params:81.17879
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.617738431257553
Iteration: 2 || Loss: 28.50325931829574
Iteration: 3 || Loss: 28.253758363892683
Iteration: 4 || Loss: 28.204317565051614
Iteration: 5 || Loss: 28.177144976872746
Iteration: 6 || Loss: 28.089341240416623
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.998116
Epoch 191 loss:28.089341240416623
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.637083941284024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.50465427466642
waveform batch: 2/2
Test loss - extrapolation:22.929338249899573
Epoch 191 mean train loss:1.3673167361711749
Epoch 191 mean test loss - interpolation:1.439513990214004
Epoch 191 mean test loss - extrapolation:6.119499377047166
Start training epoch 192
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.998116
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.285027174993134
Iteration: 2 || Loss: 4.283453335827913
Iteration: 3 || Loss: 4.281880694408761
Iteration: 4 || Loss: 4.280308213797844
Iteration: 5 || Loss: 4.278738388749791
Iteration: 6 || Loss: 4.278738388749791
saving ADAM checkpoint...
Sum of params:80.99812
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.278738388749791
Iteration: 2 || Loss: 3.1548687523986927
Iteration: 3 || Loss: 2.8341919581352215
Iteration: 4 || Loss: 2.8165734945338796
Iteration: 5 || Loss: 2.8120061354182533
Iteration: 6 || Loss: 2.7246034463033535
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.12681
Epoch 192 loss:2.7246034463033535
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.12681
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.9777373102008
Iteration: 2 || Loss: 8.977197182099438
Iteration: 3 || Loss: 8.976658133487684
Iteration: 4 || Loss: 8.976119683397368
Iteration: 5 || Loss: 8.975582487848303
Iteration: 6 || Loss: 8.975582487848303
saving ADAM checkpoint...
Sum of params:81.126755
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.975582487848303
Iteration: 2 || Loss: 8.856206035719216
Iteration: 3 || Loss: 8.815131684012142
Iteration: 4 || Loss: 8.786078613642607
Iteration: 5 || Loss: 8.747791096734415
Iteration: 6 || Loss: 8.717510618772053
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.14446
Epoch 192 loss:8.717510618772053
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.14446
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.520033817309056
Iteration: 2 || Loss: 28.519577779939308
Iteration: 3 || Loss: 28.519120307760392
Iteration: 4 || Loss: 28.518660930392677
Iteration: 5 || Loss: 28.51820538987926
Iteration: 6 || Loss: 28.51820538987926
saving ADAM checkpoint...
Sum of params:81.14444
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.51820538987926
Iteration: 2 || Loss: 28.431702118642068
Iteration: 3 || Loss: 28.22394665634873
Iteration: 4 || Loss: 28.085667254002942
Iteration: 5 || Loss: 28.07047386945857
Iteration: 6 || Loss: 27.942673255091346
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.982864
Epoch 192 loss:27.942673255091346
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.56298116758145
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.49135205084986
waveform batch: 2/2
Test loss - extrapolation:22.877256333942373
Epoch 192 mean train loss:1.3580961144885086
Epoch 192 mean test loss - interpolation:1.4271635279302417
Epoch 192 mean test loss - extrapolation:6.114050698732687
Start training epoch 193
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.982864
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.429769319096696
Iteration: 2 || Loss: 4.428074790543068
Iteration: 3 || Loss: 4.426378296859928
Iteration: 4 || Loss: 4.424684893484104
Iteration: 5 || Loss: 4.422992944682027
Iteration: 6 || Loss: 4.422992944682027
saving ADAM checkpoint...
Sum of params:80.98287
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.422992944682027
Iteration: 2 || Loss: 3.1216067208863394
Iteration: 3 || Loss: 2.8364770769745857
Iteration: 4 || Loss: 2.8128101234387697
Iteration: 5 || Loss: 2.808716651807956
Iteration: 6 || Loss: 2.7185117977532722
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.11092
Epoch 193 loss:2.7185117977532722
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.11092
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.011278742076994
Iteration: 2 || Loss: 9.01061990465617
Iteration: 3 || Loss: 9.009963337993863
Iteration: 4 || Loss: 9.009306133919178
Iteration: 5 || Loss: 9.008650259329988
Iteration: 6 || Loss: 9.008650259329988
saving ADAM checkpoint...
Sum of params:81.11095
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.008650259329988
Iteration: 2 || Loss: 8.82920940285397
Iteration: 3 || Loss: 8.77119693253228
Iteration: 4 || Loss: 8.75029426078867
Iteration: 5 || Loss: 8.70376858151704
Iteration: 6 || Loss: 8.677673985329028
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.13695
Epoch 193 loss:8.677673985329028
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.13695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.39883138409737
Iteration: 2 || Loss: 28.39830164243123
Iteration: 3 || Loss: 28.397771929884026
Iteration: 4 || Loss: 28.39724533286664
Iteration: 5 || Loss: 28.396716455511356
Iteration: 6 || Loss: 28.396716455511356
saving ADAM checkpoint...
Sum of params:81.13693
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.396716455511356
Iteration: 2 || Loss: 28.278099686677805
Iteration: 3 || Loss: 28.02925785491867
Iteration: 4 || Loss: 27.928355905347946
Iteration: 5 || Loss: 27.910720155999922
Iteration: 6 || Loss: 27.735377085969862
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.95792
Epoch 193 loss:27.735377085969862
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.557312428392887
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.55377610194575
waveform batch: 2/2
Test loss - extrapolation:22.72384267960334
Epoch 193 mean train loss:1.3493642368638676
Epoch 193 mean test loss - interpolation:1.4262187380654812
Epoch 193 mean test loss - extrapolation:6.106468231795758
Start training epoch 194
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.95792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.785116341316343
Iteration: 2 || Loss: 4.783233731246242
Iteration: 3 || Loss: 4.781355831373523
Iteration: 4 || Loss: 4.779478883231863
Iteration: 5 || Loss: 4.777604501197569
Iteration: 6 || Loss: 4.777604501197569
saving ADAM checkpoint...
Sum of params:80.95791
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.777604501197569
Iteration: 2 || Loss: 3.181796382848506
Iteration: 3 || Loss: 2.910647542842888
Iteration: 4 || Loss: 2.8711627182635957
Iteration: 5 || Loss: 2.8660140128070952
Iteration: 6 || Loss: 2.7593118593534003
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.0894
Epoch 194 loss:2.7593118593534003
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.0894
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.10432104175073
Iteration: 2 || Loss: 9.103528588414438
Iteration: 3 || Loss: 9.102738616513957
Iteration: 4 || Loss: 9.101947850047443
Iteration: 5 || Loss: 9.101157431183124
Iteration: 6 || Loss: 9.101157431183124
saving ADAM checkpoint...
Sum of params:81.089455
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.101157431183124
Iteration: 2 || Loss: 8.837173637077806
Iteration: 3 || Loss: 8.756035305861467
Iteration: 4 || Loss: 8.7385208418452
Iteration: 5 || Loss: 8.682830923840577
Iteration: 6 || Loss: 8.657773742698534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.15224
Epoch 194 loss:8.657773742698534
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.15224
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.13689073517013
Iteration: 2 || Loss: 28.136412603387733
Iteration: 3 || Loss: 28.135938712608244
Iteration: 4 || Loss: 28.13546304306737
Iteration: 5 || Loss: 28.13498814699683
Iteration: 6 || Loss: 28.13498814699683
saving ADAM checkpoint...
Sum of params:81.15222
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.13498814699683
Iteration: 2 || Loss: 28.03830297291809
Iteration: 3 || Loss: 27.731724161215368
Iteration: 4 || Loss: 27.69409812007768
Iteration: 5 || Loss: 27.66939874825435
Iteration: 6 || Loss: 27.584517177122724
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.9734
Epoch 194 loss:27.584517177122724
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.520931873956728
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.12096765302914
waveform batch: 2/2
Test loss - extrapolation:22.539197512003103
Epoch 194 mean train loss:1.3448828544542986
Epoch 194 mean test loss - interpolation:1.4201553123261215
Epoch 194 mean test loss - extrapolation:6.055013763752687
Start training epoch 195
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.9734
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.3577425754775385
Iteration: 2 || Loss: 4.356104541758564
Iteration: 3 || Loss: 4.354469482802417
Iteration: 4 || Loss: 4.352837231599767
Iteration: 5 || Loss: 4.35120444365367
Iteration: 6 || Loss: 4.35120444365367
saving ADAM checkpoint...
Sum of params:80.9734
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.35120444365367
Iteration: 2 || Loss: 3.147879645527969
Iteration: 3 || Loss: 2.8088020425760662
Iteration: 4 || Loss: 2.7905386612296987
Iteration: 5 || Loss: 2.7858066087045916
Iteration: 6 || Loss: 2.6759710737054667
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.10328
Epoch 195 loss:2.6759710737054667
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.10328
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.866222304122632
Iteration: 2 || Loss: 8.865735088718619
Iteration: 3 || Loss: 8.865247101350494
Iteration: 4 || Loss: 8.864762331935262
Iteration: 5 || Loss: 8.864278472264656
Iteration: 6 || Loss: 8.864278472264656
saving ADAM checkpoint...
Sum of params:81.10339
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.864278472264656
Iteration: 2 || Loss: 8.772924800508566
Iteration: 3 || Loss: 8.71209800808904
Iteration: 4 || Loss: 8.658608868798023
Iteration: 5 || Loss: 8.6195709423058
Iteration: 6 || Loss: 8.568940507356274
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.09765
Epoch 195 loss:8.568940507356274
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.09765
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.034378143608237
Iteration: 2 || Loss: 28.033918410812124
Iteration: 3 || Loss: 28.033459900714192
Iteration: 4 || Loss: 28.03300231865092
Iteration: 5 || Loss: 28.032546112073717
Iteration: 6 || Loss: 28.032546112073717
saving ADAM checkpoint...
Sum of params:81.097626
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.032546112073717
Iteration: 2 || Loss: 27.94794961331263
Iteration: 3 || Loss: 27.752054856886435
Iteration: 4 || Loss: 27.59052225718835
Iteration: 5 || Loss: 27.578264685001752
Iteration: 6 || Loss: 27.36150780306978
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.01004
Epoch 195 loss:27.36150780306978
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.37422107271358
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.205451249389256
waveform batch: 2/2
Test loss - extrapolation:22.434712656148186
Epoch 195 mean train loss:1.3312558408321213
Epoch 195 mean test loss - interpolation:1.3957035121189298
Epoch 195 mean test loss - extrapolation:6.0533469921281196
Start training epoch 196
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.01004
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.824411391577429
Iteration: 2 || Loss: 4.82243525728435
Iteration: 3 || Loss: 4.820462937741651
Iteration: 4 || Loss: 4.818487640633846
Iteration: 5 || Loss: 4.816519277063999
Iteration: 6 || Loss: 4.816519277063999
saving ADAM checkpoint...
Sum of params:81.01004
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.816519277063999
Iteration: 2 || Loss: 3.0820822125759
Iteration: 3 || Loss: 2.884076519102811
Iteration: 4 || Loss: 2.8567195507460212
Iteration: 5 || Loss: 2.851607252899805
Iteration: 6 || Loss: 2.6886688951852604
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.10672
Epoch 196 loss:2.6886688951852604
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.10672
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.032587718649653
Iteration: 2 || Loss: 9.031925191973965
Iteration: 3 || Loss: 9.031261848103325
Iteration: 4 || Loss: 9.03060138232405
Iteration: 5 || Loss: 9.029941235543046
Iteration: 6 || Loss: 9.029941235543046
saving ADAM checkpoint...
Sum of params:81.10685
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.029941235543046
Iteration: 2 || Loss: 8.857149416378592
Iteration: 3 || Loss: 8.758701104208818
Iteration: 4 || Loss: 8.6747663230744
Iteration: 5 || Loss: 8.606849572711814
Iteration: 6 || Loss: 8.572966833272327
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.121475
Epoch 196 loss:8.572966833272327
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.121475
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.617677657824427
Iteration: 2 || Loss: 27.617203218283855
Iteration: 3 || Loss: 27.616730683937803
Iteration: 4 || Loss: 27.616260558562615
Iteration: 5 || Loss: 27.615792143926598
Iteration: 6 || Loss: 27.615792143926598
saving ADAM checkpoint...
Sum of params:81.12143
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.615792143926598
Iteration: 2 || Loss: 27.51672594223532
Iteration: 3 || Loss: 27.38465576831658
Iteration: 4 || Loss: 27.332135159494033
Iteration: 5 || Loss: 27.31270555988224
Iteration: 6 || Loss: 27.233272904126327
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.96766
Epoch 196 loss:27.233272904126327
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.316740466729168
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:49.540246514548876
waveform batch: 2/2
Test loss - extrapolation:22.27554190591251
Epoch 196 mean train loss:1.3274106425028935
Epoch 196 mean test loss - interpolation:1.3861234111215281
Epoch 196 mean test loss - extrapolation:5.984649035038449
Start training epoch 197
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.96766
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8760321333581165
Iteration: 2 || Loss: 3.874671319086677
Iteration: 3 || Loss: 3.873313463544636
Iteration: 4 || Loss: 3.8719565548051236
Iteration: 5 || Loss: 3.8705987040124885
Iteration: 6 || Loss: 3.8705987040124885
saving ADAM checkpoint...
Sum of params:80.96767
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8705987040124885
Iteration: 2 || Loss: 3.056484298185248
Iteration: 3 || Loss: 2.729891336620262
Iteration: 4 || Loss: 2.7227917694487034
Iteration: 5 || Loss: 2.6856583506285934
Iteration: 6 || Loss: 2.611180054286435
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.08102
Epoch 197 loss:2.611180054286435
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.08102
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.778038864636228
Iteration: 2 || Loss: 8.777672919483601
Iteration: 3 || Loss: 8.777306892069307
Iteration: 4 || Loss: 8.776943110447815
Iteration: 5 || Loss: 8.7765792929022
Iteration: 6 || Loss: 8.7765792929022
saving ADAM checkpoint...
Sum of params:81.08108
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.7765792929022
Iteration: 2 || Loss: 8.728988780890562
Iteration: 3 || Loss: 8.66803352516954
Iteration: 4 || Loss: 8.541546087000645
Iteration: 5 || Loss: 8.522259338578147
Iteration: 6 || Loss: 8.496649439604823
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.11086
Epoch 197 loss:8.496649439604823
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.11086
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.56974944650509
Iteration: 2 || Loss: 27.56938066962198
Iteration: 3 || Loss: 27.569012941379707
Iteration: 4 || Loss: 27.56864556895862
Iteration: 5 || Loss: 27.56827771291334
Iteration: 6 || Loss: 27.56827771291334
saving ADAM checkpoint...
Sum of params:81.11085
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.56827771291334
Iteration: 2 || Loss: 27.514323215350593
Iteration: 3 || Loss: 27.293574698391698
Iteration: 4 || Loss: 27.245229312393363
Iteration: 5 || Loss: 27.22141300508974
Iteration: 6 || Loss: 27.07797138272666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.9507
Epoch 197 loss:27.07797138272666
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.357219268776719
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:50.02117356704878
waveform batch: 2/2
Test loss - extrapolation:22.26276966893545
Epoch 197 mean train loss:1.3167517543661351
Epoch 197 mean test loss - interpolation:1.392869878129453
Epoch 197 mean test loss - extrapolation:6.023661936332019
Start training epoch 198
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.9507
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.812975171212653
Iteration: 2 || Loss: 4.811015495521256
Iteration: 3 || Loss: 4.809057823769091
Iteration: 4 || Loss: 4.807097712022386
Iteration: 5 || Loss: 4.80513824954284
Iteration: 6 || Loss: 4.80513824954284
saving ADAM checkpoint...
Sum of params:80.950714
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.80513824954284
Iteration: 2 || Loss: 3.10273388965476
Iteration: 3 || Loss: 2.837271950071034
Iteration: 4 || Loss: 2.806819935437219
Iteration: 5 || Loss: 2.80097042694911
Iteration: 6 || Loss: 2.651316594789946
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.066956
Epoch 198 loss:2.651316594789946
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.066956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.938223929365389
Iteration: 2 || Loss: 8.937561325964094
Iteration: 3 || Loss: 8.936900946041824
Iteration: 4 || Loss: 8.936238824582386
Iteration: 5 || Loss: 8.935580002574802
Iteration: 6 || Loss: 8.935580002574802
saving ADAM checkpoint...
Sum of params:81.0671
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.935580002574802
Iteration: 2 || Loss: 8.762616520492793
Iteration: 3 || Loss: 8.6611119557293
Iteration: 4 || Loss: 8.578175326324866
Iteration: 5 || Loss: 8.514953624846484
Iteration: 6 || Loss: 8.4911504612415
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.105736
Epoch 198 loss:8.4911504612415
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.105736
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.34892082926943
Iteration: 2 || Loss: 27.348459430678847
Iteration: 3 || Loss: 27.34799981707025
Iteration: 4 || Loss: 27.347541673219183
Iteration: 5 || Loss: 27.34708351803449
Iteration: 6 || Loss: 27.34708351803449
saving ADAM checkpoint...
Sum of params:81.1057
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.34708351803449
Iteration: 2 || Loss: 27.25635731355741
Iteration: 3 || Loss: 27.09713504436912
Iteration: 4 || Loss: 27.050073142761264
Iteration: 5 || Loss: 27.034387348822285
Iteration: 6 || Loss: 26.93863072966913
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.96394
Epoch 198 loss:26.93863072966913
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.285018297118961
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:49.33273931514676
waveform batch: 2/2
Test loss - extrapolation:22.03124050283132
Epoch 198 mean train loss:1.3131413029551924
Epoch 198 mean test loss - interpolation:1.3808363828531602
Epoch 198 mean test loss - extrapolation:5.94699831816484
Start training epoch 199
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.96394
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.020672617900547
Iteration: 2 || Loss: 4.019227115687729
Iteration: 3 || Loss: 4.017784313160566
Iteration: 4 || Loss: 4.016342133731847
Iteration: 5 || Loss: 4.014902840015956
Iteration: 6 || Loss: 4.014902840015956
saving ADAM checkpoint...
Sum of params:80.96395
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.014902840015956
Iteration: 2 || Loss: 3.0995057928663834
Iteration: 3 || Loss: 2.7357843493741205
Iteration: 4 || Loss: 2.7274896995608175
Iteration: 5 || Loss: 2.621440294409278
Iteration: 6 || Loss: 2.5980420606183037
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.08629
Epoch 199 loss:2.5980420606183037
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.08629
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.736368415752095
Iteration: 2 || Loss: 8.736018019590507
Iteration: 3 || Loss: 8.735667579977452
Iteration: 4 || Loss: 8.735320146008696
Iteration: 5 || Loss: 8.734973471633797
Iteration: 6 || Loss: 8.734973471633797
saving ADAM checkpoint...
Sum of params:81.08638
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.734973471633797
Iteration: 2 || Loss: 8.692901189159947
Iteration: 3 || Loss: 8.621519269676602
Iteration: 4 || Loss: 8.479243454476363
Iteration: 5 || Loss: 8.460640042131187
Iteration: 6 || Loss: 8.432010002906354
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.1232
Epoch 199 loss:8.432010002906354
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.1232
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.268463040044836
Iteration: 2 || Loss: 27.268091576679648
Iteration: 3 || Loss: 27.2677209414583
Iteration: 4 || Loss: 27.267353856069306
Iteration: 5 || Loss: 27.26698704736569
Iteration: 6 || Loss: 27.26698704736569
saving ADAM checkpoint...
Sum of params:81.12315
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.26698704736569
Iteration: 2 || Loss: 27.213300938601506
Iteration: 3 || Loss: 26.97589694934339
Iteration: 4 || Loss: 26.935031474716524
Iteration: 5 || Loss: 26.911405234153772
Iteration: 6 || Loss: 26.771007620195686
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:80.98213
Epoch 199 loss:26.771007620195686
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.284089758532225
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:49.7713569202733
waveform batch: 2/2
Test loss - extrapolation:22.033254738164814
Epoch 199 mean train loss:1.3034848166800117
Epoch 199 mean test loss - interpolation:1.3806816264220376
Epoch 199 mean test loss - extrapolation:5.983717638203177
Start training epoch 200
waveform batch: 1/3
Using ADAM optimizer
Sum of params:80.98213
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.821552082700897
Iteration: 2 || Loss: 4.81956372744964
Iteration: 3 || Loss: 4.817572897039896
Iteration: 4 || Loss: 4.815587897887256
Iteration: 5 || Loss: 4.813601587889463
Iteration: 6 || Loss: 4.813601587889463
saving ADAM checkpoint...
Sum of params:80.98212
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.813601587889463
Iteration: 2 || Loss: 3.0791732408513517
Iteration: 3 || Loss: 2.82271422723071
Iteration: 4 || Loss: 2.7928263467098935
Iteration: 5 || Loss: 2.7862665857859934
Iteration: 6 || Loss: 2.6287433610201463
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.08877
Epoch 200 loss:2.6287433610201463
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.08877
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.866923186994054
Iteration: 2 || Loss: 8.866297496875063
Iteration: 3 || Loss: 8.865672496464793
Iteration: 4 || Loss: 8.865045731553904
Iteration: 5 || Loss: 8.864422110259477
Iteration: 6 || Loss: 8.864422110259477
saving ADAM checkpoint...
Sum of params:81.08891
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.864422110259477
Iteration: 2 || Loss: 8.71159198898445
Iteration: 3 || Loss: 8.612481759313875
Iteration: 4 || Loss: 8.51056342691949
Iteration: 5 || Loss: 8.454340632896166
Iteration: 6 || Loss: 8.429553676950832
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.12948
Epoch 200 loss:8.429553676950832
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.12948
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.04129024016618
Iteration: 2 || Loss: 27.04080915112552
Iteration: 3 || Loss: 27.04032805077962
Iteration: 4 || Loss: 27.039848612782404
Iteration: 5 || Loss: 27.03936967084667
Iteration: 6 || Loss: 27.03936967084667
saving ADAM checkpoint...
Sum of params:81.129456
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.03936967084667
Iteration: 2 || Loss: 26.9402728134048
Iteration: 3 || Loss: 26.7903730934915
Iteration: 4 || Loss: 26.743473796371177
Iteration: 5 || Loss: 26.72870993119896
Iteration: 6 || Loss: 26.63692159448094
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.00375
Epoch 200 loss:26.63692159448094
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.190308052375673
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.9628191911687
waveform batch: 2/2
Test loss - extrapolation:21.769974393904054
Epoch 200 mean train loss:1.2998351252569624
Epoch 200 mean test loss - interpolation:1.3650513420626122
Epoch 200 mean test loss - extrapolation:5.89439946542273
Start training epoch 201
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.00375
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.890855676802577
Iteration: 2 || Loss: 3.8894905711314425
Iteration: 3 || Loss: 3.8881268524987007
Iteration: 4 || Loss: 3.8867628602622193
Iteration: 5 || Loss: 3.885400813692394
Iteration: 6 || Loss: 3.885400813692394
saving ADAM checkpoint...
Sum of params:81.00375
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.885400813692394
Iteration: 2 || Loss: 3.0768541137591634
Iteration: 3 || Loss: 2.713518280301064
Iteration: 4 || Loss: 2.70627185661257
Iteration: 5 || Loss: 2.6694511994118026
Iteration: 6 || Loss: 2.581327265246804
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.11679
Epoch 201 loss:2.581327265246804
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.11679
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.700782057338355
Iteration: 2 || Loss: 8.700433782748243
Iteration: 3 || Loss: 8.7000862954136
Iteration: 4 || Loss: 8.699740751119142
Iteration: 5 || Loss: 8.699396552273637
Iteration: 6 || Loss: 8.699396552273637
saving ADAM checkpoint...
Sum of params:81.11685
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.699396552273637
Iteration: 2 || Loss: 8.65807527824964
Iteration: 3 || Loss: 8.5782902513049
Iteration: 4 || Loss: 8.414209437499874
Iteration: 5 || Loss: 8.396986784929641
Iteration: 6 || Loss: 8.370401640914036
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.16114
Epoch 201 loss:8.370401640914036
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.16114
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.96278100313154
Iteration: 2 || Loss: 26.962424817085587
Iteration: 3 || Loss: 26.962066644702492
Iteration: 4 || Loss: 26.961710454754197
Iteration: 5 || Loss: 26.96135553601768
Iteration: 6 || Loss: 26.96135553601768
saving ADAM checkpoint...
Sum of params:81.161095
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.96135553601768
Iteration: 2 || Loss: 26.912044844022677
Iteration: 3 || Loss: 26.675755551761164
Iteration: 4 || Loss: 26.634214894716443
Iteration: 5 || Loss: 26.609866366498068
Iteration: 6 || Loss: 26.494811764619612
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.02221
Epoch 201 loss:26.494811764619612
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.18591793387343
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:49.418719056292346
waveform batch: 2/2
Test loss - extrapolation:21.835517680793167
Epoch 201 mean train loss:1.2912600231303604
Epoch 201 mean test loss - interpolation:1.3643196556455717
Epoch 201 mean test loss - extrapolation:5.937853061423792
Start training epoch 202
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.02221
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.573154091332383
Iteration: 2 || Loss: 4.571270015124323
Iteration: 3 || Loss: 4.569386713781352
Iteration: 4 || Loss: 4.567506867635216
Iteration: 5 || Loss: 4.565625766637932
Iteration: 6 || Loss: 4.565625766637932
saving ADAM checkpoint...
Sum of params:81.02222
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.565625766637932
Iteration: 2 || Loss: 3.027873371099287
Iteration: 3 || Loss: 2.764062309996074
Iteration: 4 || Loss: 2.7422614416432536
Iteration: 5 || Loss: 2.7349307644666356
Iteration: 6 || Loss: 2.5857351989746906
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.12203
Epoch 202 loss:2.5857351989746906
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.12203
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.73723848060558
Iteration: 2 || Loss: 8.736704503156155
Iteration: 3 || Loss: 8.736171661455822
Iteration: 4 || Loss: 8.735640370177522
Iteration: 5 || Loss: 8.735110655562728
Iteration: 6 || Loss: 8.735110655562728
saving ADAM checkpoint...
Sum of params:81.12216
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.735110655562728
Iteration: 2 || Loss: 8.628873616202638
Iteration: 3 || Loss: 8.54020836575342
Iteration: 4 || Loss: 8.421768665292026
Iteration: 5 || Loss: 8.38192900022435
Iteration: 6 || Loss: 8.356869594194347
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.167564
Epoch 202 loss:8.356869594194347
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.167564
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.777231063358187
Iteration: 2 || Loss: 26.776768369743497
Iteration: 3 || Loss: 26.77630519390924
Iteration: 4 || Loss: 26.77584120090205
Iteration: 5 || Loss: 26.775378604925432
Iteration: 6 || Loss: 26.775378604925432
saving ADAM checkpoint...
Sum of params:81.16753
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.775378604925432
Iteration: 2 || Loss: 26.68567491476094
Iteration: 3 || Loss: 26.52857267890266
Iteration: 4 || Loss: 26.48387372212896
Iteration: 5 || Loss: 26.469155498774157
Iteration: 6 || Loss: 26.36201256590493
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.04845
Epoch 202 loss:26.36201256590493
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.131081120182765
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.71596014742713
waveform batch: 2/2
Test loss - extrapolation:21.526622304259934
Epoch 202 mean train loss:1.2863661158301367
Epoch 202 mean test loss - interpolation:1.3551801866971276
Epoch 202 mean test loss - extrapolation:5.853548537640589
Start training epoch 203
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.04845
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.9940577338664567
Iteration: 2 || Loss: 3.9926186607190246
Iteration: 3 || Loss: 3.9911776549065934
Iteration: 4 || Loss: 3.9897364186109767
Iteration: 5 || Loss: 3.988300126162259
Iteration: 6 || Loss: 3.988300126162259
saving ADAM checkpoint...
Sum of params:81.04846
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.988300126162259
Iteration: 2 || Loss: 3.094342521578141
Iteration: 3 || Loss: 2.7190575675818622
Iteration: 4 || Loss: 2.7116039994954977
Iteration: 5 || Loss: 2.64583811077023
Iteration: 6 || Loss: 2.5767679740730163
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.1582
Epoch 203 loss:2.5767679740730163
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.1582
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.678610955199069
Iteration: 2 || Loss: 8.678228179542371
Iteration: 3 || Loss: 8.67784376646846
Iteration: 4 || Loss: 8.6774620482803
Iteration: 5 || Loss: 8.677081353047424
Iteration: 6 || Loss: 8.677081353047424
saving ADAM checkpoint...
Sum of params:81.15826
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.677081353047424
Iteration: 2 || Loss: 8.624910327218036
Iteration: 3 || Loss: 8.53830106668372
Iteration: 4 || Loss: 8.358751319121588
Iteration: 5 || Loss: 8.341187566161272
Iteration: 6 || Loss: 8.316070696381688
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.20714
Epoch 203 loss:8.316070696381688
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.20714
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.665592211445183
Iteration: 2 || Loss: 26.665220106096918
Iteration: 3 || Loss: 26.664851131266996
Iteration: 4 || Loss: 26.664481886014652
Iteration: 5 || Loss: 26.664113900483578
Iteration: 6 || Loss: 26.664113900483578
saving ADAM checkpoint...
Sum of params:81.20711
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.664113900483578
Iteration: 2 || Loss: 26.611693376072616
Iteration: 3 || Loss: 26.384663074456736
Iteration: 4 || Loss: 26.347146776694377
Iteration: 5 || Loss: 26.325920927118926
Iteration: 6 || Loss: 26.221362385678486
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.080986
Epoch 203 loss:26.221362385678486
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.099228592768444
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:49.17237933434029
waveform batch: 2/2
Test loss - extrapolation:21.656883615068118
Epoch 203 mean train loss:1.279800036418386
Epoch 203 mean test loss - interpolation:1.349871432128074
Epoch 203 mean test loss - extrapolation:5.902438579117368
Start training epoch 204
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.080986
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.443731146428097
Iteration: 2 || Loss: 4.441888773267069
Iteration: 3 || Loss: 4.440046183885602
Iteration: 4 || Loss: 4.438205788800172
Iteration: 5 || Loss: 4.4363673687474545
Iteration: 6 || Loss: 4.4363673687474545
saving ADAM checkpoint...
Sum of params:81.08099
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.4363673687474545
Iteration: 2 || Loss: 2.98529305411661
Iteration: 3 || Loss: 2.7394440197818724
Iteration: 4 || Loss: 2.72075742880694
Iteration: 5 || Loss: 2.7109479219017203
Iteration: 6 || Loss: 2.5618898689054697
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.16878
Epoch 204 loss:2.5618898689054697
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.16878
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.644330969665116
Iteration: 2 || Loss: 8.64384648334533
Iteration: 3 || Loss: 8.643365311386237
Iteration: 4 || Loss: 8.642885487629437
Iteration: 5 || Loss: 8.642409291724615
Iteration: 6 || Loss: 8.642409291724615
saving ADAM checkpoint...
Sum of params:81.16889
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.642409291724615
Iteration: 2 || Loss: 8.55843274807691
Iteration: 3 || Loss: 8.477167533714255
Iteration: 4 || Loss: 8.351950334336387
Iteration: 5 || Loss: 8.321497301639411
Iteration: 6 || Loss: 8.292951324556162
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.22307
Epoch 204 loss:8.292951324556162
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.22307
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.502834785734866
Iteration: 2 || Loss: 26.502378610272395
Iteration: 3 || Loss: 26.50192406209421
Iteration: 4 || Loss: 26.501469766541668
Iteration: 5 || Loss: 26.5010152961465
Iteration: 6 || Loss: 26.5010152961465
saving ADAM checkpoint...
Sum of params:81.22305
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.5010152961465
Iteration: 2 || Loss: 26.415506210516583
Iteration: 3 || Loss: 26.263125454708383
Iteration: 4 || Loss: 26.219628224249416
Iteration: 5 || Loss: 26.20414483119282
Iteration: 6 || Loss: 26.107414625898706
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.118164
Epoch 204 loss:26.107414625898706
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.044704525200231
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.39820582570762
waveform batch: 2/2
Test loss - extrapolation:21.316815382901222
Epoch 204 mean train loss:1.274560545495184
Epoch 204 mean test loss - interpolation:1.3407840875333719
Epoch 204 mean test loss - extrapolation:5.8095851007174035
Start training epoch 205
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.118164
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8866924506827583
Iteration: 2 || Loss: 3.885303392322825
Iteration: 3 || Loss: 3.883916500244175
Iteration: 4 || Loss: 3.882529604170629
Iteration: 5 || Loss: 3.881143244752866
Iteration: 6 || Loss: 3.881143244752866
saving ADAM checkpoint...
Sum of params:81.118164
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.881143244752866
Iteration: 2 || Loss: 3.0616516594452157
Iteration: 3 || Loss: 2.6848555585803933
Iteration: 4 || Loss: 2.677642978759949
Iteration: 5 || Loss: 2.635073965040444
Iteration: 6 || Loss: 2.553466280421866
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.21996
Epoch 205 loss:2.553466280421866
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.21996
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.625423037635109
Iteration: 2 || Loss: 8.625036325334039
Iteration: 3 || Loss: 8.624651119821728
Iteration: 4 || Loss: 8.624268492865122
Iteration: 5 || Loss: 8.623884621880592
Iteration: 6 || Loss: 8.623884621880592
saving ADAM checkpoint...
Sum of params:81.22
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.623884621880592
Iteration: 2 || Loss: 8.571250078495149
Iteration: 3 || Loss: 8.482256941418854
Iteration: 4 || Loss: 8.295747826321175
Iteration: 5 || Loss: 8.279059951385626
Iteration: 6 || Loss: 8.253843447824735
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.26706
Epoch 205 loss:8.253843447824735
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.26706
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.41315254714292
Iteration: 2 || Loss: 26.41277721504994
Iteration: 3 || Loss: 26.4124050487547
Iteration: 4 || Loss: 26.412032181427094
Iteration: 5 || Loss: 26.411661298089577
Iteration: 6 || Loss: 26.411661298089577
saving ADAM checkpoint...
Sum of params:81.26703
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.411661298089577
Iteration: 2 || Loss: 26.358818772558646
Iteration: 3 || Loss: 26.13725430063575
Iteration: 4 || Loss: 26.100969644152308
Iteration: 5 || Loss: 26.07996881941441
Iteration: 6 || Loss: 25.981523914604715
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.13945
Epoch 205 loss:25.981523914604715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:8.024156118732474
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.9023798927037
waveform batch: 2/2
Test loss - extrapolation:21.469224557562633
Epoch 205 mean train loss:1.268580470443149
Epoch 205 mean test loss - interpolation:1.337359353122079
Epoch 205 mean test loss - extrapolation:5.864300370855528
Start training epoch 206
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.13945
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.360405330868307
Iteration: 2 || Loss: 4.358592638744802
Iteration: 3 || Loss: 4.356775697709956
Iteration: 4 || Loss: 4.354963004655136
Iteration: 5 || Loss: 4.353149785919632
Iteration: 6 || Loss: 4.353149785919632
saving ADAM checkpoint...
Sum of params:81.139465
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.353149785919632
Iteration: 2 || Loss: 2.9620380635818764
Iteration: 3 || Loss: 2.7093926669304116
Iteration: 4 || Loss: 2.6935567141377166
Iteration: 5 || Loss: 2.681235224623235
Iteration: 6 || Loss: 2.5363732355585844
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.22238
Epoch 206 loss:2.5363732355585844
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.22238
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.576871275948664
Iteration: 2 || Loss: 8.576405732774866
Iteration: 3 || Loss: 8.575942715515778
Iteration: 4 || Loss: 8.575479725727565
Iteration: 5 || Loss: 8.575017489004752
Iteration: 6 || Loss: 8.575017489004752
saving ADAM checkpoint...
Sum of params:81.2225
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.575017489004752
Iteration: 2 || Loss: 8.49781914639238
Iteration: 3 || Loss: 8.418635532330244
Iteration: 4 || Loss: 8.285080307340642
Iteration: 5 || Loss: 8.25866022597927
Iteration: 6 || Loss: 8.229751459873102
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.28203
Epoch 206 loss:8.229751459873102
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.28203
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.26618594222452
Iteration: 2 || Loss: 26.26573276609769
Iteration: 3 || Loss: 26.265278832636348
Iteration: 4 || Loss: 26.26482854300781
Iteration: 5 || Loss: 26.26437850116318
Iteration: 6 || Loss: 26.26437850116318
saving ADAM checkpoint...
Sum of params:81.28199
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.26437850116318
Iteration: 2 || Loss: 26.181121724027292
Iteration: 3 || Loss: 26.027710538037663
Iteration: 4 || Loss: 25.98498628291956
Iteration: 5 || Loss: 25.969506831032
Iteration: 6 || Loss: 25.874463280074472
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.17824
Epoch 206 loss:25.874463280074472
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.978316577438138
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.184322492596216
waveform batch: 2/2
Test loss - extrapolation:21.146871589068652
Epoch 206 mean train loss:1.2634685508795227
Epoch 206 mean test loss - interpolation:1.329719429573023
Epoch 206 mean test loss - extrapolation:5.777599506805406
Start training epoch 207
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.17824
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8863147711679518
Iteration: 2 || Loss: 3.884904454774359
Iteration: 3 || Loss: 3.8834973518093183
Iteration: 4 || Loss: 3.8820913212264205
Iteration: 5 || Loss: 3.880684409686636
Iteration: 6 || Loss: 3.880684409686636
saving ADAM checkpoint...
Sum of params:81.17825
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.880684409686636
Iteration: 2 || Loss: 3.046732304343482
Iteration: 3 || Loss: 2.6643396375856168
Iteration: 4 || Loss: 2.657240433414339
Iteration: 5 || Loss: 2.6077852275468447
Iteration: 6 || Loss: 2.531108776477884
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.277756
Epoch 207 loss:2.531108776477884
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.277756
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.569728878700213
Iteration: 2 || Loss: 8.56934324048224
Iteration: 3 || Loss: 8.568957778816026
Iteration: 4 || Loss: 8.56857335242821
Iteration: 5 || Loss: 8.568188280104566
Iteration: 6 || Loss: 8.568188280104566
saving ADAM checkpoint...
Sum of params:81.2778
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.568188280104566
Iteration: 2 || Loss: 8.515987668991983
Iteration: 3 || Loss: 8.4250083352995
Iteration: 4 || Loss: 8.235450346272732
Iteration: 5 || Loss: 8.21915226500654
Iteration: 6 || Loss: 8.193861488815537
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.32761
Epoch 207 loss:8.193861488815537
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.32761
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.178772208250248
Iteration: 2 || Loss: 26.178393203656263
Iteration: 3 || Loss: 26.178013115717032
Iteration: 4 || Loss: 26.177635429883317
Iteration: 5 || Loss: 26.177259025862288
Iteration: 6 || Loss: 26.177259025862288
saving ADAM checkpoint...
Sum of params:81.32758
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.177259025862288
Iteration: 2 || Loss: 26.123300269061975
Iteration: 3 || Loss: 25.905052379052044
Iteration: 4 || Loss: 25.870312712225925
Iteration: 5 || Loss: 25.849968575421453
Iteration: 6 || Loss: 25.75239341501607
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.199554
Epoch 207 loss:25.75239341501607
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.961819527371114
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.66580953943511
waveform batch: 2/2
Test loss - extrapolation:21.28692571925314
Epoch 207 mean train loss:1.257840126907224
Epoch 207 mean test loss - interpolation:1.326969921228519
Epoch 207 mean test loss - extrapolation:5.8293946048906875
Start training epoch 208
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.199554
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.341310186262874
Iteration: 2 || Loss: 4.3394922810888445
Iteration: 3 || Loss: 4.337676889779868
Iteration: 4 || Loss: 4.335861122696166
Iteration: 5 || Loss: 4.334046701279181
Iteration: 6 || Loss: 4.334046701279181
saving ADAM checkpoint...
Sum of params:81.19953
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.334046701279181
Iteration: 2 || Loss: 2.9538534623432824
Iteration: 3 || Loss: 2.6904294541660874
Iteration: 4 || Loss: 2.6757609035856795
Iteration: 5 || Loss: 2.6611627670329923
Iteration: 6 || Loss: 2.516264140997659
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.28044
Epoch 208 loss:2.516264140997659
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.28044
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.525945355425902
Iteration: 2 || Loss: 8.525482359306348
Iteration: 3 || Loss: 8.525021215350833
Iteration: 4 || Loss: 8.52456235712045
Iteration: 5 || Loss: 8.5241040921216
Iteration: 6 || Loss: 8.5241040921216
saving ADAM checkpoint...
Sum of params:81.280556
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.5241040921216
Iteration: 2 || Loss: 8.448730491378406
Iteration: 3 || Loss: 8.368505499063755
Iteration: 4 || Loss: 8.225540570920808
Iteration: 5 || Loss: 8.200776236517887
Iteration: 6 || Loss: 8.171919714548796
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.34326
Epoch 208 loss:8.171919714548796
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.34326
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.03664717854314
Iteration: 2 || Loss: 26.036190424480825
Iteration: 3 || Loss: 26.03573698498168
Iteration: 4 || Loss: 26.03528311034772
Iteration: 5 || Loss: 26.034829957397537
Iteration: 6 || Loss: 26.034829957397537
saving ADAM checkpoint...
Sum of params:81.343254
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.034829957397537
Iteration: 2 || Loss: 25.951460914589376
Iteration: 3 || Loss: 25.798412262109178
Iteration: 4 || Loss: 25.756580605998376
Iteration: 5 || Loss: 25.741418892332895
Iteration: 6 || Loss: 25.648996068745962
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.24036
Epoch 208 loss:25.648996068745962
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.913838225954429
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.97367577638072
waveform batch: 2/2
Test loss - extrapolation:20.983084885789633
Epoch 208 mean train loss:1.2530062042859453
Epoch 208 mean test loss - interpolation:1.3189730376590714
Epoch 208 mean test loss - extrapolation:5.746396721847529
Start training epoch 209
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.24036
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8729800682520086
Iteration: 2 || Loss: 3.871567481161521
Iteration: 3 || Loss: 3.870150288476223
Iteration: 4 || Loss: 3.86873401449084
Iteration: 5 || Loss: 3.8673205298561975
Iteration: 6 || Loss: 3.8673205298561975
saving ADAM checkpoint...
Sum of params:81.24036
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8673205298561975
Iteration: 2 || Loss: 3.032298283975018
Iteration: 3 || Loss: 2.645427363548903
Iteration: 4 || Loss: 2.6383518703276785
Iteration: 5 || Loss: 2.583355704946251
Iteration: 6 || Loss: 2.5101777424071594
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.337456
Epoch 209 loss:2.5101777424071594
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.337456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.516428048955598
Iteration: 2 || Loss: 8.516044573527127
Iteration: 3 || Loss: 8.515663342807624
Iteration: 4 || Loss: 8.515283183032688
Iteration: 5 || Loss: 8.514903299352138
Iteration: 6 || Loss: 8.514903299352138
saving ADAM checkpoint...
Sum of params:81.33751
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.514903299352138
Iteration: 2 || Loss: 8.464478925015204
Iteration: 3 || Loss: 8.371698065110794
Iteration: 4 || Loss: 8.177955582834
Iteration: 5 || Loss: 8.162182638272748
Iteration: 6 || Loss: 8.136542708525168
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.390495
Epoch 209 loss:8.136542708525168
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.390495
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.95214280517307
Iteration: 2 || Loss: 25.951761344526805
Iteration: 3 || Loss: 25.95138269034399
Iteration: 4 || Loss: 25.951003282768465
Iteration: 5 || Loss: 25.9506254282367
Iteration: 6 || Loss: 25.9506254282367
saving ADAM checkpoint...
Sum of params:81.39047
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.9506254282367
Iteration: 2 || Loss: 25.896683440475634
Iteration: 3 || Loss: 25.6813330221181
Iteration: 4 || Loss: 25.64760418859431
Iteration: 5 || Loss: 25.62748232620432
Iteration: 6 || Loss: 25.531564366714186
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.26219
Epoch 209 loss:25.531564366714186
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.901448288247609
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.4266226727565
waveform batch: 2/2
Test loss - extrapolation:21.108292194291142
Epoch 209 mean train loss:1.247527062677466
Epoch 209 mean test loss - interpolation:1.3169080480412683
Epoch 209 mean test loss - extrapolation:5.794576238920637
Start training epoch 210
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.26219
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.31825724613015
Iteration: 2 || Loss: 4.316445000842787
Iteration: 3 || Loss: 4.314631492698323
Iteration: 4 || Loss: 4.312817333784341
Iteration: 5 || Loss: 4.311006813529728
Iteration: 6 || Loss: 4.311006813529728
saving ADAM checkpoint...
Sum of params:81.26219
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.311006813529728
Iteration: 2 || Loss: 2.9473355487661483
Iteration: 3 || Loss: 2.670794976939069
Iteration: 4 || Loss: 2.657418235363649
Iteration: 5 || Loss: 2.640134053635278
Iteration: 6 || Loss: 2.4969897619341417
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.342125
Epoch 210 loss:2.4969897619341417
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.342125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.478524911889865
Iteration: 2 || Loss: 8.47806714936567
Iteration: 3 || Loss: 8.477609846119883
Iteration: 4 || Loss: 8.477154617199828
Iteration: 5 || Loss: 8.47669942562362
Iteration: 6 || Loss: 8.47669942562362
saving ADAM checkpoint...
Sum of params:81.342255
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.47669942562362
Iteration: 2 || Loss: 8.403352918368705
Iteration: 3 || Loss: 8.321024480061576
Iteration: 4 || Loss: 8.168609892080653
Iteration: 5 || Loss: 8.145264372148715
Iteration: 6 || Loss: 8.11636814905682
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.40701
Epoch 210 loss:8.11636814905682
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.40701
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.816173982633572
Iteration: 2 || Loss: 25.815716442732683
Iteration: 3 || Loss: 25.815259039612187
Iteration: 4 || Loss: 25.81480568347942
Iteration: 5 || Loss: 25.81435035516506
Iteration: 6 || Loss: 25.81435035516506
saving ADAM checkpoint...
Sum of params:81.406975
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.81435035516506
Iteration: 2 || Loss: 25.731168536993934
Iteration: 3 || Loss: 25.577424170738915
Iteration: 4 || Loss: 25.536861684552054
Iteration: 5 || Loss: 25.52200275969538
Iteration: 6 || Loss: 25.43092741982512
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.303894
Epoch 210 loss:25.43092741982512
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.853853423393428
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.77618822530656
waveform batch: 2/2
Test loss - extrapolation:20.824350777791665
Epoch 210 mean train loss:1.242906390717796
Epoch 210 mean test loss - interpolation:1.3089755705655712
Epoch 210 mean test loss - extrapolation:5.716711583591518
Start training epoch 211
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.303894
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8759012706210467
Iteration: 2 || Loss: 3.874463944425393
Iteration: 3 || Loss: 3.8730302466850395
Iteration: 4 || Loss: 3.8715958069779743
Iteration: 5 || Loss: 3.8701629586055155
Iteration: 6 || Loss: 3.8701629586055155
saving ADAM checkpoint...
Sum of params:81.303894
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8701629586055155
Iteration: 2 || Loss: 3.0212909928054597
Iteration: 3 || Loss: 2.6297116582925764
Iteration: 4 || Loss: 2.622601938937331
Iteration: 5 || Loss: 2.5590630949682405
Iteration: 6 || Loss: 2.4911180013232626
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.39907
Epoch 211 loss:2.4911180013232626
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.39907
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.467856553007415
Iteration: 2 || Loss: 8.467475023275503
Iteration: 3 || Loss: 8.467096226270906
Iteration: 4 || Loss: 8.466716110185553
Iteration: 5 || Loss: 8.466338974599617
Iteration: 6 || Loss: 8.466338974599617
saving ADAM checkpoint...
Sum of params:81.399124
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.466338974599617
Iteration: 2 || Loss: 8.417161532853124
Iteration: 3 || Loss: 8.322150408489046
Iteration: 4 || Loss: 8.123486207964293
Iteration: 5 || Loss: 8.108043690242857
Iteration: 6 || Loss: 8.081942053393297
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.45529
Epoch 211 loss:8.081942053393297
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.45529
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.73231185657212
Iteration: 2 || Loss: 25.73192789833762
Iteration: 3 || Loss: 25.731545233897908
Iteration: 4 || Loss: 25.73116341504845
Iteration: 5 || Loss: 25.73078208618266
Iteration: 6 || Loss: 25.73078208618266
saving ADAM checkpoint...
Sum of params:81.45526
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.73078208618266
Iteration: 2 || Loss: 25.676485509529037
Iteration: 3 || Loss: 25.464188995484452
Iteration: 4 || Loss: 25.43151114750847
Iteration: 5 || Loss: 25.411717273555052
Iteration: 6 || Loss: 25.317370558245774
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.32723
Epoch 211 loss:25.317370558245774
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.843219581542964
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:48.193391340862554
waveform batch: 2/2
Test loss - extrapolation:20.93494029689996
Epoch 211 mean train loss:1.2376010556193908
Epoch 211 mean test loss - interpolation:1.307203263590494
Epoch 211 mean test loss - extrapolation:5.7606943031468765
Start training epoch 212
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.32723
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.296047907540923
Iteration: 2 || Loss: 4.294236362662017
Iteration: 3 || Loss: 4.292424842145858
Iteration: 4 || Loss: 4.290615946735978
Iteration: 5 || Loss: 4.288808522081555
Iteration: 6 || Loss: 4.288808522081555
saving ADAM checkpoint...
Sum of params:81.32725
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.288808522081555
Iteration: 2 || Loss: 2.941762414673302
Iteration: 3 || Loss: 2.6523109930932907
Iteration: 4 || Loss: 2.6400951926958514
Iteration: 5 || Loss: 2.6193490474449157
Iteration: 6 || Loss: 2.4787953007070573
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.406624
Epoch 212 loss:2.4787953007070573
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.406624
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.433498352158242
Iteration: 2 || Loss: 8.433044282335844
Iteration: 3 || Loss: 8.432591655099921
Iteration: 4 || Loss: 8.432139006978172
Iteration: 5 || Loss: 8.43168875857009
Iteration: 6 || Loss: 8.43168875857009
saving ADAM checkpoint...
Sum of params:81.406746
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.43168875857009
Iteration: 2 || Loss: 8.360457878449017
Iteration: 3 || Loss: 8.275374965065376
Iteration: 4 || Loss: 8.114128351116225
Iteration: 5 || Loss: 8.091992337880216
Iteration: 6 || Loss: 8.06284172992163
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.47314
Epoch 212 loss:8.06284172992163
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.47314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.60234771835434
Iteration: 2 || Loss: 25.601889637259042
Iteration: 3 || Loss: 25.601430998537875
Iteration: 4 || Loss: 25.600975051979482
Iteration: 5 || Loss: 25.600516873807564
Iteration: 6 || Loss: 25.600516873807564
saving ADAM checkpoint...
Sum of params:81.47311
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.600516873807564
Iteration: 2 || Loss: 25.517680330262234
Iteration: 3 || Loss: 25.36296006954592
Iteration: 4 || Loss: 25.323808980343916
Iteration: 5 || Loss: 25.309184252028004
Iteration: 6 || Loss: 25.219237738221217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.36962
Epoch 212 loss:25.219237738221217
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.7965077197395996
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.58489626357256
waveform batch: 2/2
Test loss - extrapolation:20.66976835249701
Epoch 212 mean train loss:1.233133612718962
Epoch 212 mean test loss - interpolation:1.2994179532899333
Epoch 212 mean test loss - extrapolation:5.687888718005797
Start training epoch 213
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.36962
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.883372724784879
Iteration: 2 || Loss: 3.8819162035737933
Iteration: 3 || Loss: 3.880461197340971
Iteration: 4 || Loss: 3.879004969495974
Iteration: 5 || Loss: 3.8775535351071935
Iteration: 6 || Loss: 3.8775535351071935
saving ADAM checkpoint...
Sum of params:81.36962
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8775535351071935
Iteration: 2 || Loss: 3.0112089178684585
Iteration: 3 || Loss: 2.6149802286166675
Iteration: 4 || Loss: 2.607791984062731
Iteration: 5 || Loss: 2.534693094385505
Iteration: 6 || Loss: 2.473038176747053
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.46305
Epoch 213 loss:2.473038176747053
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.46305
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.422577773528264
Iteration: 2 || Loss: 8.422197217095288
Iteration: 3 || Loss: 8.42181949548313
Iteration: 4 || Loss: 8.4214403653381
Iteration: 5 || Loss: 8.421063212074257
Iteration: 6 || Loss: 8.421063212074257
saving ADAM checkpoint...
Sum of params:81.46312
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.421063212074257
Iteration: 2 || Loss: 8.372806180721396
Iteration: 3 || Loss: 8.275329815941987
Iteration: 4 || Loss: 8.071283306277941
Iteration: 5 || Loss: 8.056071767899832
Iteration: 6 || Loss: 8.029417582637816
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.522026
Epoch 213 loss:8.029417582637816
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.522026
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.51897456184588
Iteration: 2 || Loss: 25.518588071794284
Iteration: 3 || Loss: 25.518202660193673
Iteration: 4 || Loss: 25.517814923908805
Iteration: 5 || Loss: 25.517430493203094
Iteration: 6 || Loss: 25.517430493203094
saving ADAM checkpoint...
Sum of params:81.522
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.517430493203094
Iteration: 2 || Loss: 25.462463090836756
Iteration: 3 || Loss: 25.253490955651557
Iteration: 4 || Loss: 25.221799739770407
Iteration: 5 || Loss: 25.202372502376402
Iteration: 6 || Loss: 25.10968849230333
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.39414
Epoch 213 loss:25.10968849230333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.786771824037025
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.96310194706683
waveform batch: 2/2
Test loss - extrapolation:20.765794634204717
Epoch 213 mean train loss:1.2280049741961447
Epoch 213 mean test loss - interpolation:1.297795304006171
Epoch 213 mean test loss - extrapolation:5.727408048439297
Start training epoch 214
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.39414
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.272740080036704
Iteration: 2 || Loss: 4.270936823592475
Iteration: 3 || Loss: 4.269134284900418
Iteration: 4 || Loss: 4.2673282291274885
Iteration: 5 || Loss: 4.265523948394004
Iteration: 6 || Loss: 4.265523948394004
saving ADAM checkpoint...
Sum of params:81.39414
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.265523948394004
Iteration: 2 || Loss: 2.936878296836001
Iteration: 3 || Loss: 2.6340196387535437
Iteration: 4 || Loss: 2.622889028560791
Iteration: 5 || Loss: 2.597716478899913
Iteration: 6 || Loss: 2.4610840831542995
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.47356
Epoch 214 loss:2.4610840831542995
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.47356
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.391150952858856
Iteration: 2 || Loss: 8.39070220505227
Iteration: 3 || Loss: 8.390256026175395
Iteration: 4 || Loss: 8.38980997242171
Iteration: 5 || Loss: 8.389363506470177
Iteration: 6 || Loss: 8.389363506470177
saving ADAM checkpoint...
Sum of params:81.47368
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.389363506470177
Iteration: 2 || Loss: 8.320787975971305
Iteration: 3 || Loss: 8.231787850916229
Iteration: 4 || Loss: 8.061354229371032
Iteration: 5 || Loss: 8.040321378343588
Iteration: 6 || Loss: 8.01071747074469
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.541214
Epoch 214 loss:8.01071747074469
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.541214
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.39524331292156
Iteration: 2 || Loss: 25.39478520645645
Iteration: 3 || Loss: 25.394329285373765
Iteration: 4 || Loss: 25.393872602587503
Iteration: 5 || Loss: 25.393414039581828
Iteration: 6 || Loss: 25.393414039581828
saving ADAM checkpoint...
Sum of params:81.541176
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.393414039581828
Iteration: 2 || Loss: 25.311497818169453
Iteration: 3 || Loss: 25.155230357606854
Iteration: 4 || Loss: 25.117586493901108
Iteration: 5 || Loss: 25.103106408526546
Iteration: 6 || Loss: 25.013710709022543
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.43684
Epoch 214 loss:25.013710709022543
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.7419726651648855
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.40228494613681
waveform batch: 2/2
Test loss - extrapolation:20.51957050888099
Epoch 214 mean train loss:1.223638353893846
Epoch 214 mean test loss - interpolation:1.290328777527481
Epoch 214 mean test loss - extrapolation:5.6601546212514835
Start training epoch 215
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.43684
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.899980593227974
Iteration: 2 || Loss: 3.8984967381241318
Iteration: 3 || Loss: 3.8970133651719236
Iteration: 4 || Loss: 3.895530331559474
Iteration: 5 || Loss: 3.8940525973375144
Iteration: 6 || Loss: 3.8940525973375144
saving ADAM checkpoint...
Sum of params:81.43683
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8940525973375144
Iteration: 2 || Loss: 3.001995233178136
Iteration: 3 || Loss: 2.6013847824052805
Iteration: 4 || Loss: 2.5940965803395857
Iteration: 5 || Loss: 2.5089754339327914
Iteration: 6 || Loss: 2.455810871461624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.528885
Epoch 215 loss:2.455810871461624
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.528885
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.381102272145943
Iteration: 2 || Loss: 8.380720983818579
Iteration: 3 || Loss: 8.380340660150523
Iteration: 4 || Loss: 8.37996028197104
Iteration: 5 || Loss: 8.379581601893854
Iteration: 6 || Loss: 8.379581601893854
saving ADAM checkpoint...
Sum of params:81.52894
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.379581601893854
Iteration: 2 || Loss: 8.331591377189369
Iteration: 3 || Loss: 8.231041426863886
Iteration: 4 || Loss: 8.021193818928426
Iteration: 5 || Loss: 8.006036273201104
Iteration: 6 || Loss: 7.978824999382703
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.589966
Epoch 215 loss:7.978824999382703
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.589966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.311622064030328
Iteration: 2 || Loss: 25.31122923028568
Iteration: 3 || Loss: 25.31083854898608
Iteration: 4 || Loss: 25.310449520657347
Iteration: 5 || Loss: 25.31005941204101
Iteration: 6 || Loss: 25.31005941204101
saving ADAM checkpoint...
Sum of params:81.58994
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.31005941204101
Iteration: 2 || Loss: 25.25396664757466
Iteration: 3 || Loss: 25.048766675508418
Iteration: 4 || Loss: 25.017987724321944
Iteration: 5 || Loss: 24.999066224873243
Iteration: 6 || Loss: 24.90783752172434
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.46246
Epoch 215 loss:24.90783752172434
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.732114150154821
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.737369670256854
waveform batch: 2/2
Test loss - extrapolation:20.60080265412214
Epoch 215 mean train loss:1.218705979054092
Epoch 215 mean test loss - interpolation:1.2886856916924703
Epoch 215 mean test loss - extrapolation:5.69484769369825
Start training epoch 216
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.46246
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.250110184775547
Iteration: 2 || Loss: 4.2483078665005465
Iteration: 3 || Loss: 4.246507268778559
Iteration: 4 || Loss: 4.244707759457643
Iteration: 5 || Loss: 4.242909685900456
Iteration: 6 || Loss: 4.242909685900456
saving ADAM checkpoint...
Sum of params:81.462456
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.242909685900456
Iteration: 2 || Loss: 2.932717107760198
Iteration: 3 || Loss: 2.6164390700624267
Iteration: 4 || Loss: 2.6062690976372265
Iteration: 5 || Loss: 2.5753571359052807
Iteration: 6 || Loss: 2.443947060681886
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.54248
Epoch 216 loss:2.443947060681886
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.54248
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.353697057332141
Iteration: 2 || Loss: 8.353257077451875
Iteration: 3 || Loss: 8.352819134156105
Iteration: 4 || Loss: 8.352382015435046
Iteration: 5 || Loss: 8.351947177867972
Iteration: 6 || Loss: 8.351947177867972
saving ADAM checkpoint...
Sum of params:81.54263
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.351947177867972
Iteration: 2 || Loss: 8.287793993863641
Iteration: 3 || Loss: 8.192417489295321
Iteration: 4 || Loss: 8.009893857116136
Iteration: 5 || Loss: 7.990096180699866
Iteration: 6 || Loss: 7.959473158405068
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.61116
Epoch 216 loss:7.959473158405068
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.61116
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.194338761966073
Iteration: 2 || Loss: 25.193883707954647
Iteration: 3 || Loss: 25.19342896201535
Iteration: 4 || Loss: 25.19297463384555
Iteration: 5 || Loss: 25.19252169475742
Iteration: 6 || Loss: 25.19252169475742
saving ADAM checkpoint...
Sum of params:81.61113
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.19252169475742
Iteration: 2 || Loss: 25.112590955223457
Iteration: 3 || Loss: 24.953569122242115
Iteration: 4 || Loss: 24.91779574992681
Iteration: 5 || Loss: 24.903223597479503
Iteration: 6 || Loss: 24.813495797077696
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.50545
Epoch 216 loss:24.813495797077696
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.690207138435753
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.227836944654904
waveform batch: 2/2
Test loss - extrapolation:20.37241946109522
Epoch 216 mean train loss:1.2143764143505051
Epoch 216 mean test loss - interpolation:1.281701189739292
Epoch 216 mean test loss - extrapolation:5.633354700479177
Start training epoch 217
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.50545
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.9280939204804413
Iteration: 2 || Loss: 3.9265730041767415
Iteration: 3 || Loss: 3.9250484900203313
Iteration: 4 || Loss: 3.92353111198016
Iteration: 5 || Loss: 3.92201564627552
Iteration: 6 || Loss: 3.92201564627552
saving ADAM checkpoint...
Sum of params:81.50543
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.92201564627552
Iteration: 2 || Loss: 2.993905271138106
Iteration: 3 || Loss: 2.5891793372566836
Iteration: 4 || Loss: 2.5817707923168283
Iteration: 5 || Loss: 2.4811735282576923
Iteration: 6 || Loss: 2.4394808989104853
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.59645
Epoch 217 loss:2.4394808989104853
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.59645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.344573293804022
Iteration: 2 || Loss: 8.344185301430652
Iteration: 3 || Loss: 8.343799299740587
Iteration: 4 || Loss: 8.343415201804332
Iteration: 5 || Loss: 8.343028508390201
Iteration: 6 || Loss: 8.343028508390201
saving ADAM checkpoint...
Sum of params:81.596504
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.343028508390201
Iteration: 2 || Loss: 8.294078475508677
Iteration: 3 || Loss: 8.189441576127606
Iteration: 4 || Loss: 7.973198293424025
Iteration: 5 || Loss: 7.957805577383247
Iteration: 6 || Loss: 7.93018773285635
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.6586
Epoch 217 loss:7.93018773285635
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.6586
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.10918872047605
Iteration: 2 || Loss: 25.10878646080762
Iteration: 3 || Loss: 25.10838787054942
Iteration: 4 || Loss: 25.10798986935905
Iteration: 5 || Loss: 25.10759412407861
Iteration: 6 || Loss: 25.10759412407861
saving ADAM checkpoint...
Sum of params:81.65856
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.10759412407861
Iteration: 2 || Loss: 25.049426037127652
Iteration: 3 || Loss: 24.848864292930163
Iteration: 4 || Loss: 24.818935874374343
Iteration: 5 || Loss: 24.800818611930662
Iteration: 6 || Loss: 24.71081155150337
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.53186
Epoch 217 loss:24.71081155150337
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.678835921548877
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.516128688752154
waveform batch: 2/2
Test loss - extrapolation:20.440011235351363
Epoch 217 mean train loss:1.2096717304575932
Epoch 217 mean test loss - interpolation:1.2798059869248128
Epoch 217 mean test loss - extrapolation:5.663011660341961
Start training epoch 218
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.53186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.225781502380746
Iteration: 2 || Loss: 4.223982889350611
Iteration: 3 || Loss: 4.222187603725993
Iteration: 4 || Loss: 4.220394360764439
Iteration: 5 || Loss: 4.21860380302772
Iteration: 6 || Loss: 4.21860380302772
saving ADAM checkpoint...
Sum of params:81.531876
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.21860380302772
Iteration: 2 || Loss: 2.9288348658966865
Iteration: 3 || Loss: 2.5995460270183144
Iteration: 4 || Loss: 2.5902213197923216
Iteration: 5 || Loss: 2.551641680548549
Iteration: 6 || Loss: 2.427395533541145
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.61269
Epoch 218 loss:2.427395533541145
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.61269
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.314186558504302
Iteration: 2 || Loss: 8.313755157980573
Iteration: 3 || Loss: 8.313321638292548
Iteration: 4 || Loss: 8.312891290014432
Iteration: 5 || Loss: 8.312462041402268
Iteration: 6 || Loss: 8.312462041402268
saving ADAM checkpoint...
Sum of params:81.61282
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.312462041402268
Iteration: 2 || Loss: 8.251298976361285
Iteration: 3 || Loss: 8.150811245216056
Iteration: 4 || Loss: 7.960463967581693
Iteration: 5 || Loss: 7.941562923913281
Iteration: 6 || Loss: 7.909985340157074
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.68236
Epoch 218 loss:7.909985340157074
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.68236
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.99759567118758
Iteration: 2 || Loss: 24.997140977085117
Iteration: 3 || Loss: 24.99668899531438
Iteration: 4 || Loss: 24.99623675596511
Iteration: 5 || Loss: 24.995786634272193
Iteration: 6 || Loss: 24.995786634272193
saving ADAM checkpoint...
Sum of params:81.68235
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.995786634272193
Iteration: 2 || Loss: 24.917455301679674
Iteration: 3 || Loss: 24.756461348269653
Iteration: 4 || Loss: 24.722328661241942
Iteration: 5 || Loss: 24.70770287743205
Iteration: 6 || Loss: 24.61863630193301
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.57573
Epoch 218 loss:24.61863630193301
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.639349480578283
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.05450237926791
waveform batch: 2/2
Test loss - extrapolation:20.22965270814946
Epoch 218 mean train loss:1.2053799026079735
Epoch 218 mean test loss - interpolation:1.2732249134297138
Epoch 218 mean test loss - extrapolation:5.607012923951447
Start training epoch 219
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.57573
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.949111459378774
Iteration: 2 || Loss: 3.9475564954146654
Iteration: 3 || Loss: 3.946003793657984
Iteration: 4 || Loss: 3.9444545571293337
Iteration: 5 || Loss: 3.94290337974138
Iteration: 6 || Loss: 3.94290337974138
saving ADAM checkpoint...
Sum of params:81.57573
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.94290337974138
Iteration: 2 || Loss: 2.9837088462730588
Iteration: 3 || Loss: 2.5750344321050225
Iteration: 4 || Loss: 2.5675639494676887
Iteration: 5 || Loss: 2.453243622602718
Iteration: 6 || Loss: 2.4227799514650057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.6659
Epoch 219 loss:2.4227799514650057
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.6659
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.30836196227067
Iteration: 2 || Loss: 8.307969587259452
Iteration: 3 || Loss: 8.30757644344683
Iteration: 4 || Loss: 8.307185365444074
Iteration: 5 || Loss: 8.306794503671767
Iteration: 6 || Loss: 8.306794503671767
saving ADAM checkpoint...
Sum of params:81.66594
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.306794503671767
Iteration: 2 || Loss: 8.256915728558829
Iteration: 3 || Loss: 8.14835296757626
Iteration: 4 || Loss: 7.92644729937147
Iteration: 5 || Loss: 7.910800103248688
Iteration: 6 || Loss: 7.882745687189288
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.728294
Epoch 219 loss:7.882745687189288
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.728294
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.912793012739822
Iteration: 2 || Loss: 24.912383446701874
Iteration: 3 || Loss: 24.911976904731816
Iteration: 4 || Loss: 24.911570363459216
Iteration: 5 || Loss: 24.911165855389932
Iteration: 6 || Loss: 24.911165855389932
saving ADAM checkpoint...
Sum of params:81.72827
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.911165855389932
Iteration: 2 || Loss: 24.850729990950054
Iteration: 3 || Loss: 24.65511287932428
Iteration: 4 || Loss: 24.62572538171428
Iteration: 5 || Loss: 24.608357200568054
Iteration: 6 || Loss: 24.519730593697016
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.60219
Epoch 219 loss:24.519730593697016
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.626655756470676
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.293060631615795
waveform batch: 2/2
Test loss - extrapolation:20.28134765482884
Epoch 219 mean train loss:1.2008709045638382
Epoch 219 mean test loss - interpolation:1.2711092927451126
Epoch 219 mean test loss - extrapolation:5.631200690537053
Start training epoch 220
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.60219
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.197485769103069
Iteration: 2 || Loss: 4.19569614507184
Iteration: 3 || Loss: 4.19391001972887
Iteration: 4 || Loss: 4.192123855422522
Iteration: 5 || Loss: 4.190338080941805
Iteration: 6 || Loss: 4.190338080941805
saving ADAM checkpoint...
Sum of params:81.60219
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.190338080941805
Iteration: 2 || Loss: 2.9253826288199463
Iteration: 3 || Loss: 2.5818110440735027
Iteration: 4 || Loss: 2.5732932137683457
Iteration: 5 || Loss: 2.524234573533952
Iteration: 6 || Loss: 2.4109015718146383
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.68416
Epoch 220 loss:2.4109015718146383
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.68416
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.272534177717802
Iteration: 2 || Loss: 8.272100786807384
Iteration: 3 || Loss: 8.271671290727106
Iteration: 4 || Loss: 8.271241667430623
Iteration: 5 || Loss: 8.27081303657582
Iteration: 6 || Loss: 8.27081303657582
saving ADAM checkpoint...
Sum of params:81.68428
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.27081303657582
Iteration: 2 || Loss: 8.210676218702988
Iteration: 3 || Loss: 8.10666186890261
Iteration: 4 || Loss: 7.912676200236243
Iteration: 5 || Loss: 7.894257435395868
Iteration: 6 || Loss: 7.862320819463969
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.754
Epoch 220 loss:7.862320819463969
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.754
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.80677885425351
Iteration: 2 || Loss: 24.80632547411233
Iteration: 3 || Loss: 24.805872387806577
Iteration: 4 || Loss: 24.80542131796484
Iteration: 5 || Loss: 24.804969718560425
Iteration: 6 || Loss: 24.804969718560425
saving ADAM checkpoint...
Sum of params:81.75395
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.804969718560425
Iteration: 2 || Loss: 24.727469775643126
Iteration: 3 || Loss: 24.565254398962797
Iteration: 4 || Loss: 24.532279339839004
Iteration: 5 || Loss: 24.51777131775055
Iteration: 6 || Loss: 24.429907384328718
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.646286
Epoch 220 loss:24.429907384328718
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.589566645735853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.88765281951849
waveform batch: 2/2
Test loss - extrapolation:20.093623632214857
Epoch 220 mean train loss:1.1966596474347353
Epoch 220 mean test loss - interpolation:1.2649277742893088
Epoch 220 mean test loss - extrapolation:5.581773037644446
Start training epoch 221
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.646286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.9652774582643833
Iteration: 2 || Loss: 3.963693905124297
Iteration: 3 || Loss: 3.9621111675639
Iteration: 4 || Loss: 3.9605283815195067
Iteration: 5 || Loss: 3.9589470425539983
Iteration: 6 || Loss: 3.9589470425539983
saving ADAM checkpoint...
Sum of params:81.64629
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9589470425539983
Iteration: 2 || Loss: 2.9719686424142453
Iteration: 3 || Loss: 2.5599204336427377
Iteration: 4 || Loss: 2.5524352462851967
Iteration: 5 || Loss: 2.42573299651103
Iteration: 6 || Loss: 2.4058983981818636
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.735825
Epoch 221 loss:2.4058983981818636
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.735825
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.271853237165404
Iteration: 2 || Loss: 8.271453528030275
Iteration: 3 || Loss: 8.271054167530604
Iteration: 4 || Loss: 8.27065593272814
Iteration: 5 || Loss: 8.270259924365307
Iteration: 6 || Loss: 8.270259924365307
saving ADAM checkpoint...
Sum of params:81.73587
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.270259924365307
Iteration: 2 || Loss: 8.21949520976497
Iteration: 3 || Loss: 8.107326890663439
Iteration: 4 || Loss: 7.881072167057818
Iteration: 5 || Loss: 7.865177924431376
Iteration: 6 || Loss: 7.83659352204182
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.798294
Epoch 221 loss:7.83659352204182
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.798294
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.723016036709392
Iteration: 2 || Loss: 24.72259996087792
Iteration: 3 || Loss: 24.722183990914854
Iteration: 4 || Loss: 24.721769005740743
Iteration: 5 || Loss: 24.721353764964196
Iteration: 6 || Loss: 24.721353764964196
saving ADAM checkpoint...
Sum of params:81.798256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.721353764964196
Iteration: 2 || Loss: 24.658645988016413
Iteration: 3 || Loss: 24.4681488887248
Iteration: 4 || Loss: 24.438987296561045
Iteration: 5 || Loss: 24.42230754834209
Iteration: 6 || Loss: 24.33494813743674
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.67314
Epoch 221 loss:24.33494813743674
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.575660868870882
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:47.06963857949655
waveform batch: 2/2
Test loss - extrapolation:20.12521926871497
Epoch 221 mean train loss:1.1923255192296698
Epoch 221 mean test loss - interpolation:1.2626101448118137
Epoch 221 mean test loss - extrapolation:5.59957148735096
Start training epoch 222
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.67314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.166794278810907
Iteration: 2 || Loss: 4.1650180324884065
Iteration: 3 || Loss: 4.163238030621333
Iteration: 4 || Loss: 4.161460975223758
Iteration: 5 || Loss: 4.159688853455062
Iteration: 6 || Loss: 4.159688853455062
saving ADAM checkpoint...
Sum of params:81.67316
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.159688853455062
Iteration: 2 || Loss: 2.922686409427852
Iteration: 3 || Loss: 2.5638370482070947
Iteration: 4 || Loss: 2.5560219255895382
Iteration: 5 || Loss: 2.492416701495859
Iteration: 6 || Loss: 2.3948546548249836
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.75653
Epoch 222 loss:2.3948546548249836
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.75653
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.23261246363301
Iteration: 2 || Loss: 8.232179510655971
Iteration: 3 || Loss: 8.231746557347268
Iteration: 4 || Loss: 8.231316334202582
Iteration: 5 || Loss: 8.230884412063498
Iteration: 6 || Loss: 8.230884412063498
saving ADAM checkpoint...
Sum of params:81.75665
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.230884412063498
Iteration: 2 || Loss: 8.170703430721401
Iteration: 3 || Loss: 8.063554589826904
Iteration: 4 || Loss: 7.866645766089169
Iteration: 5 || Loss: 7.848404825386545
Iteration: 6 || Loss: 7.8165613660086555
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.82553
Epoch 222 loss:7.8165613660086555
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.82553
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.62247226656522
Iteration: 2 || Loss: 24.622017323210635
Iteration: 3 || Loss: 24.6215630286262
Iteration: 4 || Loss: 24.621110232961612
Iteration: 5 || Loss: 24.62065755194224
Iteration: 6 || Loss: 24.62065755194224
saving ADAM checkpoint...
Sum of params:81.82549
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.62065755194224
Iteration: 2 || Loss: 24.543447165548805
Iteration: 3 || Loss: 24.38039666792746
Iteration: 4 || Loss: 24.34819164315885
Iteration: 5 || Loss: 24.333908154367226
Iteration: 6 || Loss: 24.247744278559153
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.71667
Epoch 222 loss:24.247744278559153
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.540868452942786
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.72621074696084
waveform batch: 2/2
Test loss - extrapolation:19.96376738600896
Epoch 222 mean train loss:1.1882469068756136
Epoch 222 mean test loss - interpolation:1.2568114088237976
Epoch 222 mean test loss - extrapolation:5.557498177747483
Start training epoch 223
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.71667
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.975374234079643
Iteration: 2 || Loss: 3.9737608413624956
Iteration: 3 || Loss: 3.9721534261564053
Iteration: 4 || Loss: 3.9705434913534203
Iteration: 5 || Loss: 3.9689347179568575
Iteration: 6 || Loss: 3.9689347179568575
saving ADAM checkpoint...
Sum of params:81.71666
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9689347179568575
Iteration: 2 || Loss: 2.958328703850125
Iteration: 3 || Loss: 2.5437891997516955
Iteration: 4 || Loss: 2.536303097921407
Iteration: 5 || Loss: 2.399633692677125
Iteration: 6 || Loss: 2.388809546704922
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.80547
Epoch 223 loss:2.388809546704922
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.80547
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.234042254454371
Iteration: 2 || Loss: 8.23363646246905
Iteration: 3 || Loss: 8.233230732372757
Iteration: 4 || Loss: 8.23282714278186
Iteration: 5 || Loss: 8.232424932673766
Iteration: 6 || Loss: 8.232424932673766
saving ADAM checkpoint...
Sum of params:81.80557
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.232424932673766
Iteration: 2 || Loss: 8.180701821098902
Iteration: 3 || Loss: 8.065842173921254
Iteration: 4 || Loss: 7.837011530327672
Iteration: 5 || Loss: 7.820917462695811
Iteration: 6 || Loss: 7.791745822304523
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.86798
Epoch 223 loss:7.791745822304523
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.86798
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.540335116188725
Iteration: 2 || Loss: 24.539910556554393
Iteration: 3 || Loss: 24.539485145919492
Iteration: 4 || Loss: 24.539061553533298
Iteration: 5 || Loss: 24.538640697361334
Iteration: 6 || Loss: 24.538640697361334
saving ADAM checkpoint...
Sum of params:81.86795
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.538640697361334
Iteration: 2 || Loss: 24.473549290176273
Iteration: 3 || Loss: 24.28847574510052
Iteration: 4 || Loss: 24.2591891303468
Iteration: 5 || Loss: 24.243149349530455
Iteration: 6 || Loss: 24.157107064743737
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.74461
Epoch 223 loss:24.157107064743737
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.525452763974465
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.84705706468897
waveform batch: 2/2
Test loss - extrapolation:19.973070682978
Epoch 223 mean train loss:1.1840573253018338
Epoch 223 mean test loss - interpolation:1.2542421273290774
Epoch 223 mean test loss - extrapolation:5.568343978972248
Start training epoch 224
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.74461
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.130945847227983
Iteration: 2 || Loss: 4.129183987360653
Iteration: 3 || Loss: 4.127420613595602
Iteration: 4 || Loss: 4.125660630760519
Iteration: 5 || Loss: 4.123897591179919
Iteration: 6 || Loss: 4.123897591179919
saving ADAM checkpoint...
Sum of params:81.74463
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.123897591179919
Iteration: 2 || Loss: 2.9189978046748926
Iteration: 3 || Loss: 2.5448698624822956
Iteration: 4 || Loss: 2.5376028488313906
Iteration: 5 || Loss: 2.4546635322560517
Iteration: 6 || Loss: 2.378750051247391
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.82934
Epoch 224 loss:2.378750051247391
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.82934
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.19667013511777
Iteration: 2 || Loss: 8.196235476100771
Iteration: 3 || Loss: 8.195802333704409
Iteration: 4 || Loss: 8.195368363626793
Iteration: 5 || Loss: 8.194934136653464
Iteration: 6 || Loss: 8.194934136653464
saving ADAM checkpoint...
Sum of params:81.82946
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.194934136653464
Iteration: 2 || Loss: 8.134929393456327
Iteration: 3 || Loss: 8.023802483652023
Iteration: 4 || Loss: 7.821815074436623
Iteration: 5 || Loss: 7.803694941346325
Iteration: 6 || Loss: 7.7720219931974555
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.896736
Epoch 224 loss:7.7720219931974555
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.896736
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.44622098465779
Iteration: 2 || Loss: 24.44576406152399
Iteration: 3 || Loss: 24.44530873770882
Iteration: 4 || Loss: 24.44485421548188
Iteration: 5 || Loss: 24.44440014260756
Iteration: 6 || Loss: 24.44440014260756
saving ADAM checkpoint...
Sum of params:81.89669
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.44440014260756
Iteration: 2 || Loss: 24.367457508521966
Iteration: 3 || Loss: 24.203134401400636
Iteration: 4 || Loss: 24.171491555511572
Iteration: 5 || Loss: 24.157371486113274
Iteration: 6 || Loss: 24.07269125951147
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.78656
Epoch 224 loss:24.07269125951147
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.493377816239294
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.56991429941992
waveform batch: 2/2
Test loss - extrapolation:19.838960715054505
Epoch 224 mean train loss:1.1801194242743558
Epoch 224 mean test loss - interpolation:1.248896302706549
Epoch 224 mean test loss - extrapolation:5.534072917872869
Start training epoch 225
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.78656
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.982952458429256
Iteration: 2 || Loss: 3.981314689004123
Iteration: 3 || Loss: 3.979683322936212
Iteration: 4 || Loss: 3.978050937512733
Iteration: 5 || Loss: 3.976417189994278
Iteration: 6 || Loss: 3.976417189994278
saving ADAM checkpoint...
Sum of params:81.78656
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.976417189994278
Iteration: 2 || Loss: 2.943283489623903
Iteration: 3 || Loss: 2.5271675829114737
Iteration: 4 || Loss: 2.519663386874935
Iteration: 5 || Loss: 2.3763776930189806
Iteration: 6 || Loss: 2.371665984071345
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.87403
Epoch 225 loss:2.371665984071345
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.87403
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.193913265965271
Iteration: 2 || Loss: 8.193498421186183
Iteration: 3 || Loss: 8.193081802396467
Iteration: 4 || Loss: 8.192665665319698
Iteration: 5 || Loss: 8.192255128491585
Iteration: 6 || Loss: 8.192255128491585
saving ADAM checkpoint...
Sum of params:81.87416
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.192255128491585
Iteration: 2 || Loss: 8.138374898775284
Iteration: 3 || Loss: 8.023633234388575
Iteration: 4 || Loss: 7.794308183263945
Iteration: 5 || Loss: 7.777981469665606
Iteration: 6 || Loss: 7.748611288970547
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.93632
Epoch 225 loss:7.748611288970547
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.93632
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.365296429049415
Iteration: 2 || Loss: 24.364859396829118
Iteration: 3 || Loss: 24.364425744939368
Iteration: 4 || Loss: 24.363991546546185
Iteration: 5 || Loss: 24.36355688849681
Iteration: 6 || Loss: 24.36355688849681
saving ADAM checkpoint...
Sum of params:81.936264
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.36355688849681
Iteration: 2 || Loss: 24.295146501068793
Iteration: 3 || Loss: 24.116293601813187
Iteration: 4 || Loss: 24.08629923556398
Iteration: 5 || Loss: 24.07108143601434
Iteration: 6 || Loss: 23.98672879824986
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.81569
Epoch 225 loss:23.98672879824986
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.475348666296361
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.6267745520287
waveform batch: 2/2
Test loss - extrapolation:19.827517363107113
Epoch 225 mean train loss:1.17610365763075
Epoch 225 mean test loss - interpolation:1.2458914443827267
Epoch 225 mean test loss - extrapolation:5.537857659594651
Start training epoch 226
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.81569
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.082685430806749
Iteration: 2 || Loss: 4.080945991740119
Iteration: 3 || Loss: 4.079206276757068
Iteration: 4 || Loss: 4.077467217691521
Iteration: 5 || Loss: 4.075730226774579
Iteration: 6 || Loss: 4.075730226774579
saving ADAM checkpoint...
Sum of params:81.815674
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.075730226774579
Iteration: 2 || Loss: 2.9127644945503137
Iteration: 3 || Loss: 2.5241411677794483
Iteration: 4 || Loss: 2.5172003765037374
Iteration: 5 || Loss: 2.409460026703038
Iteration: 6 || Loss: 2.361892033832787
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.901566
Epoch 226 loss:2.361892033832787
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.901566
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.165574172194905
Iteration: 2 || Loss: 8.165145980556625
Iteration: 3 || Loss: 8.16471794631217
Iteration: 4 || Loss: 8.16429394480103
Iteration: 5 || Loss: 8.163864922835513
Iteration: 6 || Loss: 8.163864922835513
saving ADAM checkpoint...
Sum of params:81.9017
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.163864922835513
Iteration: 2 || Loss: 8.107014661378287
Iteration: 3 || Loss: 7.989460064355476
Iteration: 4 || Loss: 7.777128396834188
Iteration: 5 || Loss: 7.759662820998637
Iteration: 6 || Loss: 7.727193142550237
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.96781
Epoch 226 loss:7.727193142550237
waveform batch: 3/3
Using ADAM optimizer
Sum of params:81.96781
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.278459614646653
Iteration: 2 || Loss: 24.278006532399754
Iteration: 3 || Loss: 24.277552866529625
Iteration: 4 || Loss: 24.27710501038766
Iteration: 5 || Loss: 24.27665404811108
Iteration: 6 || Loss: 24.27665404811108
saving ADAM checkpoint...
Sum of params:81.967766
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.27665404811108
Iteration: 2 || Loss: 24.201935003130146
Iteration: 3 || Loss: 24.034817746878595
Iteration: 4 || Loss: 24.004019340339827
Iteration: 5 || Loss: 23.989588677076384
Iteration: 6 || Loss: 23.904951467679272
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.856125
Epoch 226 loss:23.904951467679272
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.448782097624815
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.42297294684394
waveform batch: 2/2
Test loss - extrapolation:19.71708518728805
Epoch 226 mean train loss:1.1722081601400793
Epoch 226 mean test loss - interpolation:1.2414636829374692
Epoch 226 mean test loss - extrapolation:5.511671511177666
Start training epoch 227
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.856125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.005309025746067
Iteration: 2 || Loss: 4.003637316643567
Iteration: 3 || Loss: 4.001965128082481
Iteration: 4 || Loss: 4.000292812406579
Iteration: 5 || Loss: 3.998626200012169
Iteration: 6 || Loss: 3.998626200012169
saving ADAM checkpoint...
Sum of params:81.85612
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.998626200012169
Iteration: 2 || Loss: 2.92787592207516
Iteration: 3 || Loss: 2.5111258135173244
Iteration: 4 || Loss: 2.5037019450295452
Iteration: 5 || Loss: 2.357107402301861
Iteration: 6 || Loss: 2.3525484252258133
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.94051
Epoch 227 loss:2.3525484252258133
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.94051
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.10670539822817
Iteration: 2 || Loss: 8.106363883504146
Iteration: 3 || Loss: 8.106022445238356
Iteration: 4 || Loss: 8.105683040152568
Iteration: 5 || Loss: 8.1053414529136
Iteration: 6 || Loss: 8.1053414529136
saving ADAM checkpoint...
Sum of params:81.94063
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.1053414529136
Iteration: 2 || Loss: 8.070885620761349
Iteration: 3 || Loss: 7.970964110924182
Iteration: 4 || Loss: 7.7445668164126005
Iteration: 5 || Loss: 7.73149059428227
Iteration: 6 || Loss: 7.692795164648337
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.023094
Epoch 227 loss:7.692795164648337
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.023094
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.207098605291645
Iteration: 2 || Loss: 24.206695352912828
Iteration: 3 || Loss: 24.20629395345273
Iteration: 4 || Loss: 24.205892808641405
Iteration: 5 || Loss: 24.2054940397963
Iteration: 6 || Loss: 24.2054940397963
saving ADAM checkpoint...
Sum of params:82.02309
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.2054940397963
Iteration: 2 || Loss: 24.148154795961666
Iteration: 3 || Loss: 23.957848444527226
Iteration: 4 || Loss: 23.930849459450855
Iteration: 5 || Loss: 23.909853628119492
Iteration: 6 || Loss: 23.830909824074823
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.89416
Epoch 227 loss:23.830909824074823
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.424626043777782
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.41559685025856
waveform batch: 2/2
Test loss - extrapolation:19.692141102796867
Epoch 227 mean train loss:1.1681466694465164
Epoch 227 mean test loss - interpolation:1.2374376739629638
Epoch 227 mean test loss - extrapolation:5.508978162754619
Start training epoch 228
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.89416
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.070994886832457
Iteration: 2 || Loss: 4.069243224125382
Iteration: 3 || Loss: 4.0674912060723205
Iteration: 4 || Loss: 4.065737954609791
Iteration: 5 || Loss: 4.0639864693824475
Iteration: 6 || Loss: 4.0639864693824475
saving ADAM checkpoint...
Sum of params:81.89416
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.0639864693824475
Iteration: 2 || Loss: 2.8905535510608247
Iteration: 3 || Loss: 2.483768922970476
Iteration: 4 || Loss: 2.4775738638352607
Iteration: 5 || Loss: 2.3730622935923815
Iteration: 6 || Loss: 2.3346938893466835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.98316
Epoch 228 loss:2.3346938893466835
waveform batch: 2/3
Using ADAM optimizer
Sum of params:81.98316
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.11665168127187
Iteration: 2 || Loss: 8.11622072632744
Iteration: 3 || Loss: 8.115790679027677
Iteration: 4 || Loss: 8.115361767761856
Iteration: 5 || Loss: 8.11493428446482
Iteration: 6 || Loss: 8.11493428446482
saving ADAM checkpoint...
Sum of params:81.983284
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.11493428446482
Iteration: 2 || Loss: 8.058179328108203
Iteration: 3 || Loss: 7.938170242670342
Iteration: 4 || Loss: 7.7227575132660675
Iteration: 5 || Loss: 7.705343218208896
Iteration: 6 || Loss: 7.673532732198793
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.04474
Epoch 228 loss:7.673532732198793
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.04474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.127878036292508
Iteration: 2 || Loss: 24.127419978986662
Iteration: 3 || Loss: 24.126962627871983
Iteration: 4 || Loss: 24.126506192916406
Iteration: 5 || Loss: 24.126052130223393
Iteration: 6 || Loss: 24.126052130223393
saving ADAM checkpoint...
Sum of params:82.04471
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.126052130223393
Iteration: 2 || Loss: 24.050193340052743
Iteration: 3 || Loss: 23.882490502746712
Iteration: 4 || Loss: 23.850425670299973
Iteration: 5 || Loss: 23.835865814165334
Iteration: 6 || Loss: 23.752725615041395
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.92632
Epoch 228 loss:23.752725615041395
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.395702903715948
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.26756320242337
waveform batch: 2/2
Test loss - extrapolation:19.608993421843277
Epoch 228 mean train loss:1.1641707667788577
Epoch 228 mean test loss - interpolation:1.2326171506193246
Epoch 228 mean test loss - extrapolation:5.48971305202222
Start training epoch 229
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.92632
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.976765058439145
Iteration: 2 || Loss: 3.9750904381323338
Iteration: 3 || Loss: 3.973413907302269
Iteration: 4 || Loss: 3.97174171589238
Iteration: 5 || Loss: 3.9700673957458372
Iteration: 6 || Loss: 3.9700673957458372
saving ADAM checkpoint...
Sum of params:81.926346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9700673957458372
Iteration: 2 || Loss: 2.903971841447111
Iteration: 3 || Loss: 2.4821029432981607
Iteration: 4 || Loss: 2.4744209854198056
Iteration: 5 || Loss: 2.3312594169974172
Iteration: 6 || Loss: 2.3256549859064437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.004776
Epoch 229 loss:2.3256549859064437
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.004776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.033219157471489
Iteration: 2 || Loss: 8.03284099478838
Iteration: 3 || Loss: 8.032466034095824
Iteration: 4 || Loss: 8.032089018254661
Iteration: 5 || Loss: 8.031713682608643
Iteration: 6 || Loss: 8.031713682608643
saving ADAM checkpoint...
Sum of params:82.004906
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.031713682608643
Iteration: 2 || Loss: 7.987825326536428
Iteration: 3 || Loss: 7.906315235204734
Iteration: 4 || Loss: 7.692438643901896
Iteration: 5 || Loss: 7.680401955390562
Iteration: 6 || Loss: 7.64537815475301
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.083466
Epoch 229 loss:7.64537815475301
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.083466
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.051288604203037
Iteration: 2 || Loss: 24.050853024609953
Iteration: 3 || Loss: 24.0504187001049
Iteration: 4 || Loss: 24.049984627135892
Iteration: 5 || Loss: 24.04955320126676
Iteration: 6 || Loss: 24.04955320126676
saving ADAM checkpoint...
Sum of params:82.08345
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.04955320126676
Iteration: 2 || Loss: 23.98190691361041
Iteration: 3 || Loss: 23.808542363013654
Iteration: 4 || Loss: 23.779078214248866
Iteration: 5 || Loss: 23.761938743549113
Iteration: 6 || Loss: 23.680363642479882
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.96123
Epoch 229 loss:23.680363642479882
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.375908467076063
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.23814471782235
waveform batch: 2/2
Test loss - extrapolation:19.571432523679828
Epoch 229 mean train loss:1.160392992522046
Epoch 229 mean test loss - interpolation:1.2293180778460104
Epoch 229 mean test loss - extrapolation:5.484131436791849
Start training epoch 230
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.96123
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.018442802282351
Iteration: 2 || Loss: 4.016712519480514
Iteration: 3 || Loss: 4.014984893183489
Iteration: 4 || Loss: 4.013255380911381
Iteration: 5 || Loss: 4.011525097545848
Iteration: 6 || Loss: 4.011525097545848
saving ADAM checkpoint...
Sum of params:81.961205
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.011525097545848
Iteration: 2 || Loss: 2.8801328965698447
Iteration: 3 || Loss: 2.461993832264145
Iteration: 4 || Loss: 2.455588502230102
Iteration: 5 || Loss: 2.3297328956415266
Iteration: 6 || Loss: 2.3145465159012324
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.05003
Epoch 230 loss:2.3145465159012324
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.05003
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.072181108443363
Iteration: 2 || Loss: 8.071761714720456
Iteration: 3 || Loss: 8.071344022133593
Iteration: 4 || Loss: 8.070928330122287
Iteration: 5 || Loss: 8.070513726410562
Iteration: 6 || Loss: 8.070513726410562
saving ADAM checkpoint...
Sum of params:82.050186
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.070513726410562
Iteration: 2 || Loss: 8.018129385377888
Iteration: 3 || Loss: 7.894800460238308
Iteration: 4 || Loss: 7.675590196778397
Iteration: 5 || Loss: 7.659306045966785
Iteration: 6 || Loss: 7.62511287305604
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.113556
Epoch 230 loss:7.62511287305604
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.113556
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.97950306090412
Iteration: 2 || Loss: 23.979051114659097
Iteration: 3 || Loss: 23.97860048897263
Iteration: 4 || Loss: 23.978151998646197
Iteration: 5 || Loss: 23.9777052943689
Iteration: 6 || Loss: 23.9777052943689
saving ADAM checkpoint...
Sum of params:82.113525
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.9777052943689
Iteration: 2 || Loss: 23.90505264775748
Iteration: 3 || Loss: 23.734378283286613
Iteration: 4 || Loss: 23.70383093341891
Iteration: 5 || Loss: 23.68852425782012
Iteration: 6 || Loss: 23.602428760426115
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:81.99416
Epoch 230 loss:23.602428760426115
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.356402884974901
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.140364440570345
waveform batch: 2/2
Test loss - extrapolation:19.49495773295566
Epoch 230 mean train loss:1.1566237292890824
Epoch 230 mean test loss - interpolation:1.2260671474958167
Epoch 230 mean test loss - extrapolation:5.469610181127167
Start training epoch 231
waveform batch: 1/3
Using ADAM optimizer
Sum of params:81.99416
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.0217781214537665
Iteration: 2 || Loss: 4.020048180475323
Iteration: 3 || Loss: 4.0183238420067875
Iteration: 4 || Loss: 4.016597294115107
Iteration: 5 || Loss: 4.014871233814918
Iteration: 6 || Loss: 4.014871233814918
saving ADAM checkpoint...
Sum of params:81.994156
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.014871233814918
Iteration: 2 || Loss: 2.8910226978867875
Iteration: 3 || Loss: 2.4654164412347246
Iteration: 4 || Loss: 2.4581600113867488
Iteration: 5 || Loss: 2.317851176313452
Iteration: 6 || Loss: 2.312399886439472
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.08424
Epoch 231 loss:2.312399886439472
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.08424
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.064181840056053
Iteration: 2 || Loss: 8.063771082692089
Iteration: 3 || Loss: 8.063360597106353
Iteration: 4 || Loss: 8.062951982427402
Iteration: 5 || Loss: 8.062544428934263
Iteration: 6 || Loss: 8.062544428934263
saving ADAM checkpoint...
Sum of params:82.08439
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.062544428934263
Iteration: 2 || Loss: 8.01252375994087
Iteration: 3 || Loss: 7.88547686572587
Iteration: 4 || Loss: 7.656479037274433
Iteration: 5 || Loss: 7.640722601434962
Iteration: 6 || Loss: 7.606492061760601
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.14897
Epoch 231 loss:7.606492061760601
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.14897
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.898870012977543
Iteration: 2 || Loss: 23.89842200122077
Iteration: 3 || Loss: 23.897976968918083
Iteration: 4 || Loss: 23.897533881006304
Iteration: 5 || Loss: 23.89709034364981
Iteration: 6 || Loss: 23.89709034364981
saving ADAM checkpoint...
Sum of params:82.148926
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.89709034364981
Iteration: 2 || Loss: 23.826182878621115
Iteration: 3 || Loss: 23.65220920090738
Iteration: 4 || Loss: 23.62388391175107
Iteration: 5 || Loss: 23.608092813286245
Iteration: 6 || Loss: 23.52578852880979
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.027176
Epoch 231 loss:23.52578852880979
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.335429808806007
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:46.08095365687111
waveform batch: 2/2
Test loss - extrapolation:19.44875264799814
Epoch 231 mean train loss:1.153264844034823
Epoch 231 mean test loss - interpolation:1.2225716348010012
Epoch 231 mean test loss - extrapolation:5.460808858739104
Start training epoch 232
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.027176
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.016290459727934
Iteration: 2 || Loss: 4.014553289035444
Iteration: 3 || Loss: 4.012817382089293
Iteration: 4 || Loss: 4.011081186563171
Iteration: 5 || Loss: 4.009344863342385
Iteration: 6 || Loss: 4.009344863342385
saving ADAM checkpoint...
Sum of params:82.02718
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.009344863342385
Iteration: 2 || Loss: 2.8779100062871614
Iteration: 3 || Loss: 2.4535982630649897
Iteration: 4 || Loss: 2.4466630388243122
Iteration: 5 || Loss: 2.3128909828813127
Iteration: 6 || Loss: 2.303054955871249
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.11653
Epoch 232 loss:2.303054955871249
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.11653
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.038853358752283
Iteration: 2 || Loss: 8.038440834714303
Iteration: 3 || Loss: 8.03803113639854
Iteration: 4 || Loss: 8.037620054797088
Iteration: 5 || Loss: 8.037210482521203
Iteration: 6 || Loss: 8.037210482521203
saving ADAM checkpoint...
Sum of params:82.11666
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.037210482521203
Iteration: 2 || Loss: 7.987034982527366
Iteration: 3 || Loss: 7.862201821585574
Iteration: 4 || Loss: 7.6364890381374755
Iteration: 5 || Loss: 7.620924089711873
Iteration: 6 || Loss: 7.585196959773109
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.18103
Epoch 232 loss:7.585196959773109
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.18103
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.822120152422972
Iteration: 2 || Loss: 23.821669838941034
Iteration: 3 || Loss: 23.82122222936548
Iteration: 4 || Loss: 23.820774490894046
Iteration: 5 || Loss: 23.820328156723104
Iteration: 6 || Loss: 23.820328156723104
saving ADAM checkpoint...
Sum of params:82.18101
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.820328156723104
Iteration: 2 || Loss: 23.74849099977429
Iteration: 3 || Loss: 23.57945069433805
Iteration: 4 || Loss: 23.549968310836963
Iteration: 5 || Loss: 23.53442979432934
Iteration: 6 || Loss: 23.452651235957234
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.06286
Epoch 232 loss:23.452651235957234
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.310538836348882
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.96876425687352
waveform batch: 2/2
Test loss - extrapolation:19.381436112274084
Epoch 232 mean train loss:1.1496863155724688
Epoch 232 mean test loss - interpolation:1.2184231393914804
Epoch 232 mean test loss - extrapolation:5.4458500307623
Start training epoch 233
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.06286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.978427516126241
Iteration: 2 || Loss: 3.9767083058630166
Iteration: 3 || Loss: 3.97499165858143
Iteration: 4 || Loss: 3.9732782025239937
Iteration: 5 || Loss: 3.9715649679163216
Iteration: 6 || Loss: 3.9715649679163216
saving ADAM checkpoint...
Sum of params:82.06285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9715649679163216
Iteration: 2 || Loss: 2.872720490626812
Iteration: 3 || Loss: 2.4407622579173407
Iteration: 4 || Loss: 2.433276582698137
Iteration: 5 || Loss: 2.2960525332964807
Iteration: 6 || Loss: 2.2932884566922223
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.153114
Epoch 233 loss:2.2932884566922223
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.153114
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.007549292432438
Iteration: 2 || Loss: 8.00717425074238
Iteration: 3 || Loss: 8.006799822740282
Iteration: 4 || Loss: 8.006425972857835
Iteration: 5 || Loss: 8.00605377466357
Iteration: 6 || Loss: 8.00605377466357
saving ADAM checkpoint...
Sum of params:82.15325
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.00605377466357
Iteration: 2 || Loss: 7.96585625349705
Iteration: 3 || Loss: 7.8408899456151975
Iteration: 4 || Loss: 7.611382232049578
Iteration: 5 || Loss: 7.5976421761309405
Iteration: 6 || Loss: 7.55633091705123
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.22426
Epoch 233 loss:7.55633091705123
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.22426
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.75832622877509
Iteration: 2 || Loss: 23.757899238415067
Iteration: 3 || Loss: 23.757472750766457
Iteration: 4 || Loss: 23.75704759317544
Iteration: 5 || Loss: 23.756623403725023
Iteration: 6 || Loss: 23.756623403725023
saving ADAM checkpoint...
Sum of params:82.22423
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.756623403725023
Iteration: 2 || Loss: 23.69293684745818
Iteration: 3 || Loss: 23.51179564339697
Iteration: 4 || Loss: 23.484461786397997
Iteration: 5 || Loss: 23.46542910098712
Iteration: 6 || Loss: 23.382604449970852
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.10042
Epoch 233 loss:23.382604449970852
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.293325118212195
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.92572792071449
waveform batch: 2/2
Test loss - extrapolation:19.33085388400716
Epoch 233 mean train loss:1.1459387525418727
Epoch 233 mean test loss - interpolation:1.2155541863686992
Epoch 233 mean test loss - extrapolation:5.438048483726804
Start training epoch 234
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.10042
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.037221915995697
Iteration: 2 || Loss: 4.035445509195257
Iteration: 3 || Loss: 4.033674025199885
Iteration: 4 || Loss: 4.031901710563396
Iteration: 5 || Loss: 4.030128408241122
Iteration: 6 || Loss: 4.030128408241122
saving ADAM checkpoint...
Sum of params:82.100426
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.030128408241122
Iteration: 2 || Loss: 2.8605036289404207
Iteration: 3 || Loss: 2.4282253188836695
Iteration: 4 || Loss: 2.4217976291957264
Iteration: 5 || Loss: 2.297130129146808
Iteration: 6 || Loss: 2.2828867230302037
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.19108
Epoch 234 loss:2.2828867230302037
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.19108
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.9950021368274955
Iteration: 2 || Loss: 7.994578223936768
Iteration: 3 || Loss: 7.994157602273091
Iteration: 4 || Loss: 7.993736314809522
Iteration: 5 || Loss: 7.9933179998150665
Iteration: 6 || Loss: 7.9933179998150665
saving ADAM checkpoint...
Sum of params:82.191216
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.9933179998150665
Iteration: 2 || Loss: 7.940694821700763
Iteration: 3 || Loss: 7.816293820401867
Iteration: 4 || Loss: 7.591065316548415
Iteration: 5 || Loss: 7.57512677780376
Iteration: 6 || Loss: 7.5400162698390485
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.25157
Epoch 234 loss:7.5400162698390485
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.25157
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.680389602333268
Iteration: 2 || Loss: 23.679927201604706
Iteration: 3 || Loss: 23.67946593537942
Iteration: 4 || Loss: 23.679005812197694
Iteration: 5 || Loss: 23.67854461035711
Iteration: 6 || Loss: 23.67854461035711
saving ADAM checkpoint...
Sum of params:82.251526
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.67854461035711
Iteration: 2 || Loss: 23.602467043742962
Iteration: 3 || Loss: 23.43763587145624
Iteration: 4 || Loss: 23.407142613874793
Iteration: 5 || Loss: 23.392173083234105
Iteration: 6 || Loss: 23.314800300072086
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.13105
Epoch 234 loss:23.314800300072086
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.259319795486587
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.79401884711261
waveform batch: 2/2
Test loss - extrapolation:19.278088120904325
Epoch 234 mean train loss:1.142679423894529
Epoch 234 mean test loss - interpolation:1.209886632581098
Epoch 234 mean test loss - extrapolation:5.422675580668078
Start training epoch 235
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.13105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.902894961127262
Iteration: 2 || Loss: 3.901209501847584
Iteration: 3 || Loss: 3.899526716707637
Iteration: 4 || Loss: 3.8978442636645947
Iteration: 5 || Loss: 3.896164589597645
Iteration: 6 || Loss: 3.896164589597645
saving ADAM checkpoint...
Sum of params:82.13104
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.896164589597645
Iteration: 2 || Loss: 2.8516195856684403
Iteration: 3 || Loss: 2.4104848601720654
Iteration: 4 || Loss: 2.401353481829454
Iteration: 5 || Loss: 2.273954258259899
Iteration: 6 || Loss: 2.270418692620089
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.21773
Epoch 235 loss:2.270418692620089
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.21773
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.956417275013591
Iteration: 2 || Loss: 7.955998930070121
Iteration: 3 || Loss: 7.955583725236039
Iteration: 4 || Loss: 7.955169321131191
Iteration: 5 || Loss: 7.954754890756079
Iteration: 6 || Loss: 7.954754890756079
saving ADAM checkpoint...
Sum of params:82.21785
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.954754890756079
Iteration: 2 || Loss: 7.903567869332178
Iteration: 3 || Loss: 7.789947370737814
Iteration: 4 || Loss: 7.565895672102329
Iteration: 5 || Loss: 7.551429811527836
Iteration: 6 || Loss: 7.516371461621142
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.280075
Epoch 235 loss:7.516371461621142
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.280075
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.61530955672711
Iteration: 2 || Loss: 23.61485404904741
Iteration: 3 || Loss: 23.614400374829934
Iteration: 4 || Loss: 23.613947999470295
Iteration: 5 || Loss: 23.6134983721433
Iteration: 6 || Loss: 23.6134983721433
saving ADAM checkpoint...
Sum of params:82.28005
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.6134983721433
Iteration: 2 || Loss: 23.54118085730974
Iteration: 3 || Loss: 23.376514260962267
Iteration: 4 || Loss: 23.34375612365622
Iteration: 5 || Loss: 23.328596032805617
Iteration: 6 || Loss: 23.248053366440065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.16342
Epoch 235 loss:23.248053366440065
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.240987806788435
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.746075065340506
waveform batch: 2/2
Test loss - extrapolation:19.22960608083993
Epoch 235 mean train loss:1.1391325351959067
Epoch 235 mean test loss - interpolation:1.2068313011314058
Epoch 235 mean test loss - extrapolation:5.414640095515036
Start training epoch 236
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.16342
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.929110214219507
Iteration: 2 || Loss: 3.9273964745385492
Iteration: 3 || Loss: 3.925682410020509
Iteration: 4 || Loss: 3.923967223647377
Iteration: 5 || Loss: 3.9222567608838386
Iteration: 6 || Loss: 3.9222567608838386
saving ADAM checkpoint...
Sum of params:82.163414
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9222567608838386
Iteration: 2 || Loss: 2.8410184187261476
Iteration: 3 || Loss: 2.4024499133919175
Iteration: 4 || Loss: 2.3941396952539953
Iteration: 5 || Loss: 2.263871866106361
Iteration: 6 || Loss: 2.2341643489550873
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.235016
Epoch 236 loss:2.2341643489550873
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.235016
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.7486083086531075
Iteration: 2 || Loss: 7.748376197302465
Iteration: 3 || Loss: 7.748146901525587
Iteration: 4 || Loss: 7.747919394298503
Iteration: 5 || Loss: 7.747691575765937
Iteration: 6 || Loss: 7.747691575765937
saving ADAM checkpoint...
Sum of params:82.235146
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.747691575765937
Iteration: 2 || Loss: 7.733833196203711
Iteration: 3 || Loss: 7.690835577702388
Iteration: 4 || Loss: 7.522843827275014
Iteration: 5 || Loss: 7.488761393709259
Iteration: 6 || Loss: 7.444162557029813
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.3046
Epoch 236 loss:7.444162557029813
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.3046
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.6622721916363
Iteration: 2 || Loss: 23.661864528975123
Iteration: 3 || Loss: 23.661460319931322
Iteration: 4 || Loss: 23.66105489655699
Iteration: 5 || Loss: 23.66065098111066
Iteration: 6 || Loss: 23.66065098111066
saving ADAM checkpoint...
Sum of params:82.304596
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.66065098111066
Iteration: 2 || Loss: 23.607108053664156
Iteration: 3 || Loss: 23.398043436294703
Iteration: 4 || Loss: 23.33332961380289
Iteration: 5 || Loss: 23.313877208188558
Iteration: 6 || Loss: 23.21421348602412
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.243256
Epoch 236 loss:23.21421348602412
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.215192268715933
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.54266033564969
waveform batch: 2/2
Test loss - extrapolation:19.124529118009114
Epoch 236 mean train loss:1.134225530758932
Epoch 236 mean test loss - interpolation:1.2025320447859889
Epoch 236 mean test loss - extrapolation:5.3889324544715675
Start training epoch 237
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.243256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.899774129099394
Iteration: 2 || Loss: 3.898054205572524
Iteration: 3 || Loss: 3.896336374288109
Iteration: 4 || Loss: 3.8946179166936976
Iteration: 5 || Loss: 3.8928993818953157
Iteration: 6 || Loss: 3.8928993818953157
saving ADAM checkpoint...
Sum of params:82.24327
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8928993818953157
Iteration: 2 || Loss: 2.8163913034231527
Iteration: 3 || Loss: 2.330329162975154
Iteration: 4 || Loss: 2.324527949818641
Iteration: 5 || Loss: 2.231946827558505
Iteration: 6 || Loss: 2.2177421033083635
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.337395
Epoch 237 loss:2.2177421033083635
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.337395
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.8597001618013556
Iteration: 2 || Loss: 7.859284907243983
Iteration: 3 || Loss: 7.858872636902487
Iteration: 4 || Loss: 7.8584589050494635
Iteration: 5 || Loss: 7.858048503930597
Iteration: 6 || Loss: 7.858048503930597
saving ADAM checkpoint...
Sum of params:82.33752
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.858048503930597
Iteration: 2 || Loss: 7.808745952897444
Iteration: 3 || Loss: 7.697779175585188
Iteration: 4 || Loss: 7.487355858501322
Iteration: 5 || Loss: 7.472859358368751
Iteration: 6 || Loss: 7.437549990217886
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.38328
Epoch 237 loss:7.437549990217886
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.38328
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.521615654878605
Iteration: 2 || Loss: 23.521155037127258
Iteration: 3 || Loss: 23.520694877734417
Iteration: 4 || Loss: 23.52023764553785
Iteration: 5 || Loss: 23.51978065062409
Iteration: 6 || Loss: 23.51978065062409
saving ADAM checkpoint...
Sum of params:82.38323
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.51978065062409
Iteration: 2 || Loss: 23.44635613822694
Iteration: 3 || Loss: 23.274467163773405
Iteration: 4 || Loss: 23.242355059986497
Iteration: 5 || Loss: 23.224449823737384
Iteration: 6 || Loss: 23.141291828965127
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.2346
Epoch 237 loss:23.141291828965127
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.187091809898521
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.68815773693006
waveform batch: 2/2
Test loss - extrapolation:19.168701465192086
Epoch 237 mean train loss:1.1309166869824614
Epoch 237 mean test loss - interpolation:1.1978486349830868
Epoch 237 mean test loss - extrapolation:5.404738266843512
Start training epoch 238
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.2346
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.964919958068517
Iteration: 2 || Loss: 3.9631314484466262
Iteration: 3 || Loss: 3.961347661118162
Iteration: 4 || Loss: 3.9595640383310187
Iteration: 5 || Loss: 3.9577814847758557
Iteration: 6 || Loss: 3.9577814847758557
saving ADAM checkpoint...
Sum of params:82.2346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9577814847758557
Iteration: 2 || Loss: 2.801020150596861
Iteration: 3 || Loss: 2.35904430887514
Iteration: 4 || Loss: 2.3515953722303045
Iteration: 5 || Loss: 2.2272889509192595
Iteration: 6 || Loss: 2.2200928357425487
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.32723
Epoch 238 loss:2.2200928357425487
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.32723
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.855412457820075
Iteration: 2 || Loss: 7.8549815768384965
Iteration: 3 || Loss: 7.854553396801372
Iteration: 4 || Loss: 7.854125057906667
Iteration: 5 || Loss: 7.85369801877855
Iteration: 6 || Loss: 7.85369801877855
saving ADAM checkpoint...
Sum of params:82.327354
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.85369801877855
Iteration: 2 || Loss: 7.800497259841193
Iteration: 3 || Loss: 7.680505946664435
Iteration: 4 || Loss: 7.473454289341196
Iteration: 5 || Loss: 7.458592696179266
Iteration: 6 || Loss: 7.420295122846779
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.39081
Epoch 238 loss:7.420295122846779
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.39081
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.44982384846963
Iteration: 2 || Loss: 23.449371010066116
Iteration: 3 || Loss: 23.448919079275402
Iteration: 4 || Loss: 23.4484674121873
Iteration: 5 || Loss: 23.448018853178908
Iteration: 6 || Loss: 23.448018853178908
saving ADAM checkpoint...
Sum of params:82.3908
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.448018853178908
Iteration: 2 || Loss: 23.377589218826273
Iteration: 3 || Loss: 23.21461921971169
Iteration: 4 || Loss: 23.177108944098595
Iteration: 5 || Loss: 23.161170857114854
Iteration: 6 || Loss: 23.06988003301317
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.267334
Epoch 238 loss:23.06988003301317
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.175043840429895
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.58884079661178
waveform batch: 2/2
Test loss - extrapolation:19.091333025935565
Epoch 238 mean train loss:1.1279402755725
Epoch 238 mean test loss - interpolation:1.1958406400716493
Epoch 238 mean test loss - extrapolation:5.390014485212279
Start training epoch 239
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.267334
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.9770082032446608
Iteration: 2 || Loss: 3.975222348716249
Iteration: 3 || Loss: 3.973437045596696
Iteration: 4 || Loss: 3.971651652524768
Iteration: 5 || Loss: 3.969871788482088
Iteration: 6 || Loss: 3.969871788482088
saving ADAM checkpoint...
Sum of params:82.26734
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.969871788482088
Iteration: 2 || Loss: 2.8140907187772255
Iteration: 3 || Loss: 2.3604482719711233
Iteration: 4 || Loss: 2.3526910100731557
Iteration: 5 || Loss: 2.2238984716333525
Iteration: 6 || Loss: 2.2215792227729407
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.36258
Epoch 239 loss:2.2215792227729407
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.36258
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.854425877420285
Iteration: 2 || Loss: 7.854024132605522
Iteration: 3 || Loss: 7.853623435350402
Iteration: 4 || Loss: 7.85322290214504
Iteration: 5 || Loss: 7.852824296218804
Iteration: 6 || Loss: 7.852824296218804
saving ADAM checkpoint...
Sum of params:82.3627
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.852824296218804
Iteration: 2 || Loss: 7.807156347541276
Iteration: 3 || Loss: 7.681136277504279
Iteration: 4 || Loss: 7.460953043930343
Iteration: 5 || Loss: 7.4472143209270305
Iteration: 6 || Loss: 7.405924255817461
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.424355
Epoch 239 loss:7.405924255817461
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.424355
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.374188957506874
Iteration: 2 || Loss: 23.373738830320107
Iteration: 3 || Loss: 23.373290431370098
Iteration: 4 || Loss: 23.372844105640578
Iteration: 5 || Loss: 23.372396452792426
Iteration: 6 || Loss: 23.372396452792426
saving ADAM checkpoint...
Sum of params:82.42436
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.372396452792426
Iteration: 2 || Loss: 23.302845588850605
Iteration: 3 || Loss: 23.134319720726012
Iteration: 4 || Loss: 23.104020895034857
Iteration: 5 || Loss: 23.086696893701294
Iteration: 6 || Loss: 23.00273728813396
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.29644
Epoch 239 loss:23.00273728813396
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.158521913671885
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.50896453974159
waveform batch: 2/2
Test loss - extrapolation:19.043161784913327
Epoch 239 mean train loss:1.1251807160939435
Epoch 239 mean test loss - interpolation:1.1930869856119808
Epoch 239 mean test loss - extrapolation:5.37934386038791
Start training epoch 240
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.29644
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.94696654516945
Iteration: 2 || Loss: 3.945197679676921
Iteration: 3 || Loss: 3.943429699859067
Iteration: 4 || Loss: 3.9416648956750415
Iteration: 5 || Loss: 3.9399000015482204
Iteration: 6 || Loss: 3.9399000015482204
saving ADAM checkpoint...
Sum of params:82.296455
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9399000015482204
Iteration: 2 || Loss: 2.8078309703566138
Iteration: 3 || Loss: 2.3466192451925263
Iteration: 4 || Loss: 2.338639346331901
Iteration: 5 || Loss: 2.2145647626568845
Iteration: 6 || Loss: 2.1989460769880185
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.39673
Epoch 240 loss:2.1989460769880185
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.39673
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.740843092443272
Iteration: 2 || Loss: 7.740648447004916
Iteration: 3 || Loss: 7.740454869959862
Iteration: 4 || Loss: 7.740261762228054
Iteration: 5 || Loss: 7.740069846015457
Iteration: 6 || Loss: 7.740069846015457
saving ADAM checkpoint...
Sum of params:82.396835
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.740069846015457
Iteration: 2 || Loss: 7.731551460882178
Iteration: 3 || Loss: 7.632085012825092
Iteration: 4 || Loss: 7.426332046533662
Iteration: 5 || Loss: 7.394960712638816
Iteration: 6 || Loss: 7.3327071062444364
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.46238
Epoch 240 loss:7.3327071062444364
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.46238
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.524103857362178
Iteration: 2 || Loss: 23.52371512554118
Iteration: 3 || Loss: 23.52332663637213
Iteration: 4 || Loss: 23.522941505332053
Iteration: 5 || Loss: 23.52255564948967
Iteration: 6 || Loss: 23.52255564948967
saving ADAM checkpoint...
Sum of params:82.46238
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.52255564948967
Iteration: 2 || Loss: 23.477135037855923
Iteration: 3 || Loss: 23.2032791657291
Iteration: 4 || Loss: 23.112059450319446
Iteration: 5 || Loss: 23.088918480840768
Iteration: 6 || Loss: 22.97562006845193
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.38824
Epoch 240 loss:22.97562006845193
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.134233664552709
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.356048703496185
waveform batch: 2/2
Test loss - extrapolation:18.96318939246872
Epoch 240 mean train loss:1.1209404569546342
Epoch 240 mean test loss - interpolation:1.189038944092118
Epoch 240 mean test loss - extrapolation:5.359936507997076
Start training epoch 241
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.38824
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.909198981577531
Iteration: 2 || Loss: 3.907427675831322
Iteration: 3 || Loss: 3.9056593517404363
Iteration: 4 || Loss: 3.903892618368467
Iteration: 5 || Loss: 3.9021275141910055
Iteration: 6 || Loss: 3.9021275141910055
saving ADAM checkpoint...
Sum of params:82.388245
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.9021275141910055
Iteration: 2 || Loss: 2.7820176719923024
Iteration: 3 || Loss: 2.2810611892711976
Iteration: 4 || Loss: 2.2748535950803315
Iteration: 5 || Loss: 2.1761743577028034
Iteration: 6 || Loss: 2.1704638070787152
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.485794
Epoch 241 loss:2.1704638070787152
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.485794
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.724411350036489
Iteration: 2 || Loss: 7.723998551515676
Iteration: 3 || Loss: 7.7235873548553435
Iteration: 4 || Loss: 7.7231760760830825
Iteration: 5 || Loss: 7.722767325351394
Iteration: 6 || Loss: 7.722767325351394
saving ADAM checkpoint...
Sum of params:82.48591
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.722767325351394
Iteration: 2 || Loss: 7.675222978217043
Iteration: 3 || Loss: 7.567103317481348
Iteration: 4 || Loss: 7.376561105173521
Iteration: 5 || Loss: 7.363177936560027
Iteration: 6 || Loss: 7.3178327124989995
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.52381
Epoch 241 loss:7.3178327124989995
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.52381
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.2974051871107
Iteration: 2 || Loss: 23.29695504723393
Iteration: 3 || Loss: 23.296506151936093
Iteration: 4 || Loss: 23.296059465190254
Iteration: 5 || Loss: 23.295613769321825
Iteration: 6 || Loss: 23.295613769321825
saving ADAM checkpoint...
Sum of params:82.52379
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.295613769321825
Iteration: 2 || Loss: 23.2280716232949
Iteration: 3 || Loss: 23.059408374302013
Iteration: 4 || Loss: 23.017506140377037
Iteration: 5 || Loss: 22.999667752534627
Iteration: 6 || Loss: 22.901264634956267
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.38194
Epoch 241 loss:22.901264634956267
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.10681050207858
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.4723827473343
waveform batch: 2/2
Test loss - extrapolation:18.977063995428907
Epoch 241 mean train loss:1.1168814191218615
Epoch 241 mean test loss - interpolation:1.1844684170130966
Epoch 241 mean test loss - extrapolation:5.3707872285636
Start training epoch 242
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.38194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.0267735265540585
Iteration: 2 || Loss: 4.024912507295462
Iteration: 3 || Loss: 4.023048700149262
Iteration: 4 || Loss: 4.021188469708662
Iteration: 5 || Loss: 4.019331228871674
Iteration: 6 || Loss: 4.019331228871674
saving ADAM checkpoint...
Sum of params:82.38194
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.019331228871674
Iteration: 2 || Loss: 2.783720497156866
Iteration: 3 || Loss: 2.315332038843397
Iteration: 4 || Loss: 2.308678202573873
Iteration: 5 || Loss: 2.1875864411006294
Iteration: 6 || Loss: 2.178347554735025
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.48236
Epoch 242 loss:2.178347554735025
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.48236
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.74791508499917
Iteration: 2 || Loss: 7.747478598826516
Iteration: 3 || Loss: 7.747041297554526
Iteration: 4 || Loss: 7.74660619164485
Iteration: 5 || Loss: 7.746172744249002
Iteration: 6 || Loss: 7.746172744249002
saving ADAM checkpoint...
Sum of params:82.48248
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.746172744249002
Iteration: 2 || Loss: 7.692353648670699
Iteration: 3 || Loss: 7.571715766285067
Iteration: 4 || Loss: 7.368216484822694
Iteration: 5 || Loss: 7.353340012660322
Iteration: 6 || Loss: 7.3111353157159
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.53502
Epoch 242 loss:7.3111353157159
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.53502
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.209743058027932
Iteration: 2 || Loss: 23.209283844911248
Iteration: 3 || Loss: 23.208825134339587
Iteration: 4 || Loss: 23.208365695360207
Iteration: 5 || Loss: 23.2079092616036
Iteration: 6 || Loss: 23.2079092616036
saving ADAM checkpoint...
Sum of params:82.53499
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.2079092616036
Iteration: 2 || Loss: 23.13606915080725
Iteration: 3 || Loss: 22.977660870950857
Iteration: 4 || Loss: 22.937485144218503
Iteration: 5 || Loss: 22.921588023583116
Iteration: 6 || Loss: 22.833289886033516
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.40594
Epoch 242 loss:22.833289886033516
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.0802110614728555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.329625805129226
waveform batch: 2/2
Test loss - extrapolation:18.9226138512383
Epoch 242 mean train loss:1.1145783709132566
Epoch 242 mean test loss - interpolation:1.1800351769121427
Epoch 242 mean test loss - extrapolation:5.354353304697294
Start training epoch 243
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.40594
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8865042163756756
Iteration: 2 || Loss: 3.884741858213166
Iteration: 3 || Loss: 3.882977424395867
Iteration: 4 || Loss: 3.881221708979
Iteration: 5 || Loss: 3.8794641891568222
Iteration: 6 || Loss: 3.8794641891568222
saving ADAM checkpoint...
Sum of params:82.405945
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8794641891568222
Iteration: 2 || Loss: 2.778587493823608
Iteration: 3 || Loss: 2.300021068594101
Iteration: 4 || Loss: 2.287882284774173
Iteration: 5 || Loss: 2.177954332376084
Iteration: 6 || Loss: 2.1721735304869645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.50254
Epoch 243 loss:2.1721735304869645
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.50254
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.721307868426117
Iteration: 2 || Loss: 7.720889059724325
Iteration: 3 || Loss: 7.72047421851044
Iteration: 4 || Loss: 7.720058951037212
Iteration: 5 || Loss: 7.7196472901780195
Iteration: 6 || Loss: 7.7196472901780195
saving ADAM checkpoint...
Sum of params:82.50266
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.7196472901780195
Iteration: 2 || Loss: 7.671200241331551
Iteration: 3 || Loss: 7.558299026208888
Iteration: 4 || Loss: 7.350622163139655
Iteration: 5 || Loss: 7.337779778329002
Iteration: 6 || Loss: 7.293409835495739
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.55516
Epoch 243 loss:7.293409835495739
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.55516
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.144738449494678
Iteration: 2 || Loss: 23.144286587934477
Iteration: 3 || Loss: 23.143837355668403
Iteration: 4 || Loss: 23.143388224249144
Iteration: 5 || Loss: 23.14294002001602
Iteration: 6 || Loss: 23.14294002001602
saving ADAM checkpoint...
Sum of params:82.55514
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.14294002001602
Iteration: 2 || Loss: 23.074354020788082
Iteration: 3 || Loss: 22.91596118071638
Iteration: 4 || Loss: 22.875031676981333
Iteration: 5 || Loss: 22.858731481425416
Iteration: 6 || Loss: 22.77119666131344
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.43634
Epoch 243 loss:22.77119666131344
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.0631815997175105
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.25200093941634
waveform batch: 2/2
Test loss - extrapolation:18.871624656282833
Epoch 243 mean train loss:1.111613104389522
Epoch 243 mean test loss - interpolation:1.1771969332862517
Epoch 243 mean test loss - extrapolation:5.343635466308264
Start training epoch 244
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.43634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8658188895571146
Iteration: 2 || Loss: 3.8640669346812313
Iteration: 3 || Loss: 3.862313727674928
Iteration: 4 || Loss: 3.8605623818591104
Iteration: 5 || Loss: 3.8588087275098424
Iteration: 6 || Loss: 3.8588087275098424
saving ADAM checkpoint...
Sum of params:82.43634
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8588087275098424
Iteration: 2 || Loss: 2.7703782309976055
Iteration: 3 || Loss: 2.2882723941963157
Iteration: 4 || Loss: 2.275274985079158
Iteration: 5 || Loss: 2.1715836440804144
Iteration: 6 || Loss: 2.164660731288258
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.5329
Epoch 244 loss:2.164660731288258
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.5329
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.698244882552089
Iteration: 2 || Loss: 7.69783778954636
Iteration: 3 || Loss: 7.6974315802732995
Iteration: 4 || Loss: 7.69702641235943
Iteration: 5 || Loss: 7.696620739699192
Iteration: 6 || Loss: 7.696620739699192
saving ADAM checkpoint...
Sum of params:82.53302
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.696620739699192
Iteration: 2 || Loss: 7.650816574203573
Iteration: 3 || Loss: 7.539447584448417
Iteration: 4 || Loss: 7.332561507406847
Iteration: 5 || Loss: 7.320313197751368
Iteration: 6 || Loss: 7.271759337394341
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.58307
Epoch 244 loss:7.271759337394341
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.58307
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.088851323549658
Iteration: 2 || Loss: 23.088403707760747
Iteration: 3 || Loss: 23.08795558121308
Iteration: 4 || Loss: 23.087509339114284
Iteration: 5 || Loss: 23.08706241618259
Iteration: 6 || Loss: 23.08706241618259
saving ADAM checkpoint...
Sum of params:82.58304
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.08706241618259
Iteration: 2 || Loss: 23.01966840248482
Iteration: 3 || Loss: 22.86046821893535
Iteration: 4 || Loss: 22.817536633152148
Iteration: 5 || Loss: 22.800994536762296
Iteration: 6 || Loss: 22.712451815226792
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.46989
Epoch 244 loss:22.712451815226792
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.045324697595784
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.181382165637224
waveform batch: 2/2
Test loss - extrapolation:18.82152930684955
Epoch 244 mean train loss:1.108581789100324
Epoch 244 mean test loss - interpolation:1.1742207829326305
Epoch 244 mean test loss - extrapolation:5.333575956040565
Start training epoch 245
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.46989
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.858053074871228
Iteration: 2 || Loss: 3.8562953313933717
Iteration: 3 || Loss: 3.8545370975317463
Iteration: 4 || Loss: 3.852784348172797
Iteration: 5 || Loss: 3.8510301195742187
Iteration: 6 || Loss: 3.8510301195742187
saving ADAM checkpoint...
Sum of params:82.46989
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8510301195742187
Iteration: 2 || Loss: 2.7638185941310645
Iteration: 3 || Loss: 2.2772439323371247
Iteration: 4 || Loss: 2.263165463784129
Iteration: 5 || Loss: 2.163060194600135
Iteration: 6 || Loss: 2.155805569162733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.56734
Epoch 245 loss:2.155805569162733
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.56734
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.6738456430941255
Iteration: 2 || Loss: 7.673440563180615
Iteration: 3 || Loss: 7.673037207554189
Iteration: 4 || Loss: 7.672634165036128
Iteration: 5 || Loss: 7.672232175165699
Iteration: 6 || Loss: 7.672232175165699
saving ADAM checkpoint...
Sum of params:82.56744
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.672232175165699
Iteration: 2 || Loss: 7.627458062051356
Iteration: 3 || Loss: 7.51690078859148
Iteration: 4 || Loss: 7.311533861887638
Iteration: 5 || Loss: 7.299544216608857
Iteration: 6 || Loss: 7.248339891272008
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.61445
Epoch 245 loss:7.248339891272008
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.61445
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.036548076749494
Iteration: 2 || Loss: 23.03609921199681
Iteration: 3 || Loss: 23.035652714813484
Iteration: 4 || Loss: 23.035206516597775
Iteration: 5 || Loss: 23.034760640462537
Iteration: 6 || Loss: 23.034760640462537
saving ADAM checkpoint...
Sum of params:82.61442
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.034760640462537
Iteration: 2 || Loss: 22.968017906351594
Iteration: 3 || Loss: 22.807755938744506
Iteration: 4 || Loss: 22.761990798800014
Iteration: 5 || Loss: 22.745324374291005
Iteration: 6 || Loss: 22.655981719952386
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.50411
Epoch 245 loss:22.655981719952386
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.026033227806758
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.119495882438095
waveform batch: 2/2
Test loss - extrapolation:18.77700706186167
Epoch 245 mean train loss:1.1055216269099009
Epoch 245 mean test loss - interpolation:1.171005537967793
Epoch 245 mean test loss - extrapolation:5.324708578691648
Start training epoch 246
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.50411
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8502524752645138
Iteration: 2 || Loss: 3.8484900644762545
Iteration: 3 || Loss: 3.8467303255538843
Iteration: 4 || Loss: 3.8449700238725506
Iteration: 5 || Loss: 3.8432121491600837
Iteration: 6 || Loss: 3.8432121491600837
saving ADAM checkpoint...
Sum of params:82.50412
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8432121491600837
Iteration: 2 || Loss: 2.7557423816522135
Iteration: 3 || Loss: 2.265790732835247
Iteration: 4 || Loss: 2.250059167113505
Iteration: 5 || Loss: 2.1528889121110675
Iteration: 6 || Loss: 2.1460291839490546
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.602455
Epoch 246 loss:2.1460291839490546
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.602455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.647037404262285
Iteration: 2 || Loss: 7.646631745485039
Iteration: 3 || Loss: 7.64622775372035
Iteration: 4 || Loss: 7.645824656599382
Iteration: 5 || Loss: 7.6454248858787475
Iteration: 6 || Loss: 7.6454248858787475
saving ADAM checkpoint...
Sum of params:82.60256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.6454248858787475
Iteration: 2 || Loss: 7.601037401627895
Iteration: 3 || Loss: 7.491114312907234
Iteration: 4 || Loss: 7.2888395056657584
Iteration: 5 || Loss: 7.277024346851034
Iteration: 6 || Loss: 7.223375440189454
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.64661
Epoch 246 loss:7.223375440189454
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.64661
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.988710262259538
Iteration: 2 || Loss: 22.988263418249527
Iteration: 3 || Loss: 22.987817943967446
Iteration: 4 || Loss: 22.987372946322754
Iteration: 5 || Loss: 22.986927372711197
Iteration: 6 || Loss: 22.986927372711197
saving ADAM checkpoint...
Sum of params:82.64657
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.986927372711197
Iteration: 2 || Loss: 22.921674550497773
Iteration: 3 || Loss: 22.759889029376303
Iteration: 4 || Loss: 22.709274283338107
Iteration: 5 || Loss: 22.69234968005964
Iteration: 6 || Loss: 22.6012903365912
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.539345
Epoch 246 loss:22.6012903365912
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:7.006294532668152
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.062470277919864
waveform batch: 2/2
Test loss - extrapolation:18.734213068667742
Epoch 246 mean train loss:1.1024377572665416
Epoch 246 mean test loss - interpolation:1.167715755444692
Epoch 246 mean test loss - extrapolation:5.3163902788823005
Start training epoch 247
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.539345
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.846340714509374
Iteration: 2 || Loss: 3.8445716297094368
Iteration: 3 || Loss: 3.8428039966264222
Iteration: 4 || Loss: 3.8410354780442257
Iteration: 5 || Loss: 3.8392703824713603
Iteration: 6 || Loss: 3.8392703824713603
saving ADAM checkpoint...
Sum of params:82.53936
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8392703824713603
Iteration: 2 || Loss: 2.747101964482268
Iteration: 3 || Loss: 2.2544549691563387
Iteration: 4 || Loss: 2.2369359003430493
Iteration: 5 || Loss: 2.14222300880309
Iteration: 6 || Loss: 2.1360365879785914
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.63865
Epoch 247 loss:2.1360365879785914
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.63865
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.619494782021321
Iteration: 2 || Loss: 7.619088611276875
Iteration: 3 || Loss: 7.61868381500439
Iteration: 4 || Loss: 7.61828077109974
Iteration: 5 || Loss: 7.617877972759339
Iteration: 6 || Loss: 7.617877972759339
saving ADAM checkpoint...
Sum of params:82.63878
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.617877972759339
Iteration: 2 || Loss: 7.573473392154713
Iteration: 3 || Loss: 7.4640556061157985
Iteration: 4 || Loss: 7.265375006697779
Iteration: 5 || Loss: 7.253647243003266
Iteration: 6 || Loss: 7.197993719820054
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.67953
Epoch 247 loss:7.197993719820054
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.67953
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.94357617721228
Iteration: 2 || Loss: 22.943131280066222
Iteration: 3 || Loss: 22.942685226830676
Iteration: 4 || Loss: 22.942239947455302
Iteration: 5 || Loss: 22.94179715228349
Iteration: 6 || Loss: 22.94179715228349
saving ADAM checkpoint...
Sum of params:82.6795
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.94179715228349
Iteration: 2 || Loss: 22.877646415298994
Iteration: 3 || Loss: 22.71409632743359
Iteration: 4 || Loss: 22.657841574180146
Iteration: 5 || Loss: 22.640657181132617
Iteration: 6 || Loss: 22.54871423827547
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.57482
Epoch 247 loss:22.54871423827547
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.985617440463088
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:45.00373560165737
waveform batch: 2/2
Test loss - extrapolation:18.693875886753172
Epoch 247 mean train loss:1.0994049843473832
Epoch 247 mean test loss - interpolation:1.1642695734105146
Epoch 247 mean test loss - extrapolation:5.308134290700878
Start training epoch 248
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.57482
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8331676091900504
Iteration: 2 || Loss: 3.8313925740237322
Iteration: 3 || Loss: 3.8296187147846976
Iteration: 4 || Loss: 3.827847538600172
Iteration: 5 || Loss: 3.8260778908093296
Iteration: 6 || Loss: 3.8260778908093296
saving ADAM checkpoint...
Sum of params:82.574814
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8260778908093296
Iteration: 2 || Loss: 2.7380956795741436
Iteration: 3 || Loss: 2.2419711643176257
Iteration: 4 || Loss: 2.221559954719402
Iteration: 5 || Loss: 2.131077678957436
Iteration: 6 || Loss: 2.125396853133431
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.675354
Epoch 248 loss:2.125396853133431
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.675354
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.590834953621865
Iteration: 2 || Loss: 7.590428162380063
Iteration: 3 || Loss: 7.590023509524716
Iteration: 4 || Loss: 7.589618430698679
Iteration: 5 || Loss: 7.589215952001704
Iteration: 6 || Loss: 7.589215952001704
saving ADAM checkpoint...
Sum of params:82.67548
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.589215952001704
Iteration: 2 || Loss: 7.544969037497945
Iteration: 3 || Loss: 7.4352896656309735
Iteration: 4 || Loss: 7.240722077384133
Iteration: 5 || Loss: 7.229135132157595
Iteration: 6 || Loss: 7.1714096828733265
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.71258
Epoch 248 loss:7.1714096828733265
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.71258
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.904190955706984
Iteration: 2 || Loss: 22.903748894370317
Iteration: 3 || Loss: 22.9033077702247
Iteration: 4 || Loss: 22.90286791558187
Iteration: 5 || Loss: 22.902429243233797
Iteration: 6 || Loss: 22.902429243233797
saving ADAM checkpoint...
Sum of params:82.71254
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.902429243233797
Iteration: 2 || Loss: 22.840509576793245
Iteration: 3 || Loss: 22.673467941332657
Iteration: 4 || Loss: 22.609461583770926
Iteration: 5 || Loss: 22.591905028703316
Iteration: 6 || Loss: 22.49895159682249
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.61133
Epoch 248 loss:22.49895159682249
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.964867770983299
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.944821597149804
waveform batch: 2/2
Test loss - extrapolation:18.655210465027196
Epoch 248 mean train loss:1.096405452856181
Epoch 248 mean test loss - interpolation:1.1608112951638832
Epoch 248 mean test loss - extrapolation:5.300002671848083
Start training epoch 249
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.61133
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.8161966101247096
Iteration: 2 || Loss: 3.8144257674613735
Iteration: 3 || Loss: 3.8126556390343036
Iteration: 4 || Loss: 3.8108853251448482
Iteration: 5 || Loss: 3.8091174409564643
Iteration: 6 || Loss: 3.8091174409564643
saving ADAM checkpoint...
Sum of params:82.61134
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.8091174409564643
Iteration: 2 || Loss: 2.7290490469107427
Iteration: 3 || Loss: 2.2282558088007356
Iteration: 4 || Loss: 2.2051212982199413
Iteration: 5 || Loss: 2.118461668552209
Iteration: 6 || Loss: 2.1140264029791886
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.71369
Epoch 249 loss:2.1140264029791886
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.71369
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.561138176991267
Iteration: 2 || Loss: 7.560730391274023
Iteration: 3 || Loss: 7.560325596226512
Iteration: 4 || Loss: 7.559920398363621
Iteration: 5 || Loss: 7.559517066754818
Iteration: 6 || Loss: 7.559517066754818
saving ADAM checkpoint...
Sum of params:82.71381
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.559517066754818
Iteration: 2 || Loss: 7.515648361319057
Iteration: 3 || Loss: 7.404204304058458
Iteration: 4 || Loss: 7.2147249078870255
Iteration: 5 || Loss: 7.203261409245497
Iteration: 6 || Loss: 7.143577424910143
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.74636
Epoch 249 loss:7.143577424910143
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.74636
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.8724076588606
Iteration: 2 || Loss: 22.87197117149839
Iteration: 3 || Loss: 22.871535774205196
Iteration: 4 || Loss: 22.871101567619494
Iteration: 5 || Loss: 22.870670461365567
Iteration: 6 || Loss: 22.870670461365567
saving ADAM checkpoint...
Sum of params:82.74632
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.870670461365567
Iteration: 2 || Loss: 22.811737271473056
Iteration: 3 || Loss: 22.63868693488871
Iteration: 4 || Loss: 22.5645342706389
Iteration: 5 || Loss: 22.546768864067165
Iteration: 6 || Loss: 22.453088766203336
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.64877
Epoch 249 loss:22.453088766203336
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.943454352173502
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.88452390983407
waveform batch: 2/2
Test loss - extrapolation:18.619700688569797
Epoch 249 mean train loss:1.0934721584169886
Epoch 249 mean test loss - interpolation:1.157242392028917
Epoch 249 mean test loss - extrapolation:5.292018716533655
Start training epoch 250
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.64877
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.7890253328465344
Iteration: 2 || Loss: 3.7872656756903886
Iteration: 3 || Loss: 3.785505263659891
Iteration: 4 || Loss: 3.783744406449461
Iteration: 5 || Loss: 3.781986915894465
Iteration: 6 || Loss: 3.781986915894465
saving ADAM checkpoint...
Sum of params:82.64877
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.781986915894465
Iteration: 2 || Loss: 2.72043828583912
Iteration: 3 || Loss: 2.212934395049222
Iteration: 4 || Loss: 2.1907430774039542
Iteration: 5 || Loss: 2.1033051512679988
Iteration: 6 || Loss: 2.1014714876407607
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.75449
Epoch 250 loss:2.1014714876407607
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.75449
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.532657049600325
Iteration: 2 || Loss: 7.532248644672974
Iteration: 3 || Loss: 7.5318410742088036
Iteration: 4 || Loss: 7.531432567578752
Iteration: 5 || Loss: 7.53102977667602
Iteration: 6 || Loss: 7.53102977667602
saving ADAM checkpoint...
Sum of params:82.754654
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.53102977667602
Iteration: 2 || Loss: 7.487140840974355
Iteration: 3 || Loss: 7.3690451875450345
Iteration: 4 || Loss: 7.186853105609252
Iteration: 5 || Loss: 7.1753036028336235
Iteration: 6 || Loss: 7.1144607070454295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.78096
Epoch 250 loss:7.1144607070454295
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.78096
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.850763338191665
Iteration: 2 || Loss: 22.850334943246686
Iteration: 3 || Loss: 22.84990911799804
Iteration: 4 || Loss: 22.84948319337289
Iteration: 5 || Loss: 22.84905924982937
Iteration: 6 || Loss: 22.84905924982937
saving ADAM checkpoint...
Sum of params:82.78093
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.84905924982937
Iteration: 2 || Loss: 22.793896707894326
Iteration: 3 || Loss: 22.61109008800783
Iteration: 4 || Loss: 22.523605117112094
Iteration: 5 || Loss: 22.50547967285271
Iteration: 6 || Loss: 22.411458050276316
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.68649
Epoch 250 loss:22.411458050276316
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.921766674603363
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.82702530383428
waveform batch: 2/2
Test loss - extrapolation:18.588051323526884
Epoch 250 mean train loss:1.0905996636193966
Epoch 250 mean test loss - interpolation:1.1536277791005605
Epoch 250 mean test loss - extrapolation:5.284589718946764
Start training epoch 251
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.68649
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.758070740526514
Iteration: 2 || Loss: 3.7563233044719646
Iteration: 3 || Loss: 3.754576384868557
Iteration: 4 || Loss: 3.7528290077379975
Iteration: 5 || Loss: 3.751085089301364
Iteration: 6 || Loss: 3.751085089301364
saving ADAM checkpoint...
Sum of params:82.6865
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.751085089301364
Iteration: 2 || Loss: 2.7123616133740307
Iteration: 3 || Loss: 2.196961296645183
Iteration: 4 || Loss: 2.180559324391678
Iteration: 5 || Loss: 2.090586985892345
Iteration: 6 || Loss: 2.088718337419469
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.78903
Epoch 251 loss:2.088718337419469
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.78903
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.47259350273012
Iteration: 2 || Loss: 7.472207748274204
Iteration: 3 || Loss: 7.471823520197434
Iteration: 4 || Loss: 7.471438489854925
Iteration: 5 || Loss: 7.471057137025943
Iteration: 6 || Loss: 7.471057137025943
saving ADAM checkpoint...
Sum of params:82.78914
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.471057137025943
Iteration: 2 || Loss: 7.432366115656722
Iteration: 3 || Loss: 7.332687522539604
Iteration: 4 || Loss: 7.155167063182926
Iteration: 5 || Loss: 7.145468477229628
Iteration: 6 || Loss: 7.08264938189315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.8197
Epoch 251 loss:7.08264938189315
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.8197
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.838400880522904
Iteration: 2 || Loss: 22.837990540024354
Iteration: 3 || Loss: 22.837584141122658
Iteration: 4 || Loss: 22.83717878074359
Iteration: 5 || Loss: 22.836771610010636
Iteration: 6 || Loss: 22.836771610010636
saving ADAM checkpoint...
Sum of params:82.81969
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.836771610010636
Iteration: 2 || Loss: 22.7880599537613
Iteration: 3 || Loss: 22.59620616603228
Iteration: 4 || Loss: 22.491593754792895
Iteration: 5 || Loss: 22.47125711380972
Iteration: 6 || Loss: 22.380648716978644
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.727646
Epoch 251 loss:22.380648716978644
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.893484985280944
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.75539433381359
waveform batch: 2/2
Test loss - extrapolation:18.567437866488376
Epoch 251 mean train loss:1.088000566768664
Epoch 251 mean test loss - interpolation:1.1489141642134906
Epoch 251 mean test loss - extrapolation:5.2769026833584975
Start training epoch 252
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.727646
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.6787083871391393
Iteration: 2 || Loss: 3.6770136775756614
Iteration: 3 || Loss: 3.6753207579037785
Iteration: 4 || Loss: 3.673626637269323
Iteration: 5 || Loss: 3.6719352133453422
Iteration: 6 || Loss: 3.6719352133453422
saving ADAM checkpoint...
Sum of params:82.72765
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.6719352133453422
Iteration: 2 || Loss: 2.6995527873119336
Iteration: 3 || Loss: 2.175165643100544
Iteration: 4 || Loss: 2.166803878105111
Iteration: 5 || Loss: 2.0827910534533296
Iteration: 6 || Loss: 2.0722073048257417
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.83302
Epoch 252 loss:2.0722073048257417
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.83302
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.434288568951955
Iteration: 2 || Loss: 7.4339032177635875
Iteration: 3 || Loss: 7.433518646220687
Iteration: 4 || Loss: 7.433137868390389
Iteration: 5 || Loss: 7.432756779838773
Iteration: 6 || Loss: 7.432756779838773
saving ADAM checkpoint...
Sum of params:82.83314
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.432756779838773
Iteration: 2 || Loss: 7.39470488909898
Iteration: 3 || Loss: 7.289048791936255
Iteration: 4 || Loss: 7.118737785468951
Iteration: 5 || Loss: 7.109489642898334
Iteration: 6 || Loss: 7.049687189629215
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.859634
Epoch 252 loss:7.049687189629215
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.859634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.8431080749653
Iteration: 2 || Loss: 22.842718853911677
Iteration: 3 || Loss: 22.84233129693966
Iteration: 4 || Loss: 22.841943611800062
Iteration: 5 || Loss: 22.841557451026482
Iteration: 6 || Loss: 22.841557451026482
saving ADAM checkpoint...
Sum of params:82.8596
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.841557451026482
Iteration: 2 || Loss: 22.79930993776074
Iteration: 3 || Loss: 22.59344285070464
Iteration: 4 || Loss: 22.46530863186592
Iteration: 5 || Loss: 22.44311393821103
Iteration: 6 || Loss: 22.355429156066386
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.76784
Epoch 252 loss:22.355429156066386
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.866511844173356
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.70403974474838
waveform batch: 2/2
Test loss - extrapolation:18.552870779790794
Epoch 252 mean train loss:1.0854249534662532
Epoch 252 mean test loss - interpolation:1.1444186406955594
Epoch 252 mean test loss - extrapolation:5.271409210378264
Start training epoch 253
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.76784
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.6191425004760367
Iteration: 2 || Loss: 3.6174823563252083
Iteration: 3 || Loss: 3.6158258434651587
Iteration: 4 || Loss: 3.6141714767008026
Iteration: 5 || Loss: 3.6125150978407103
Iteration: 6 || Loss: 3.6125150978407103
saving ADAM checkpoint...
Sum of params:82.767845
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.6125150978407103
Iteration: 2 || Loss: 2.689181763139468
Iteration: 3 || Loss: 2.1570197448167443
Iteration: 4 || Loss: 2.1508083030003884
Iteration: 5 || Loss: 2.0740657388073656
Iteration: 6 || Loss: 2.056869620786066
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.87416
Epoch 253 loss:2.056869620786066
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.87416
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.391436166772474
Iteration: 2 || Loss: 7.3910530143119075
Iteration: 3 || Loss: 7.390672883261904
Iteration: 4 || Loss: 7.390293574029079
Iteration: 5 || Loss: 7.389915586936704
Iteration: 6 || Loss: 7.389915586936704
saving ADAM checkpoint...
Sum of params:82.87428
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.389915586936704
Iteration: 2 || Loss: 7.352746417266813
Iteration: 3 || Loss: 7.246661105855745
Iteration: 4 || Loss: 7.08210598634107
Iteration: 5 || Loss: 7.073503168073397
Iteration: 6 || Loss: 7.017770856320804
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.90025
Epoch 253 loss:7.017770856320804
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.90025
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.842745974130917
Iteration: 2 || Loss: 22.8423756964512
Iteration: 3 || Loss: 22.842008443119813
Iteration: 4 || Loss: 22.841641066044314
Iteration: 5 || Loss: 22.841272288975837
Iteration: 6 || Loss: 22.841272288975837
saving ADAM checkpoint...
Sum of params:82.90024
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.841272288975837
Iteration: 2 || Loss: 22.804345111537334
Iteration: 3 || Loss: 22.58912589692177
Iteration: 4 || Loss: 22.442693803962353
Iteration: 5 || Loss: 22.417876080085186
Iteration: 6 || Loss: 22.3332131794427
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.80544
Epoch 253 loss:22.3332131794427
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.840174861832618
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.66375088042393
waveform batch: 2/2
Test loss - extrapolation:18.542989271295273
Epoch 253 mean train loss:1.0830294364327437
Epoch 253 mean test loss - interpolation:1.1400291436387697
Epoch 253 mean test loss - extrapolation:5.2672283459766005
Start training epoch 254
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.80544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.5688297601879375
Iteration: 2 || Loss: 3.567201238717705
Iteration: 3 || Loss: 3.5655755828739677
Iteration: 4 || Loss: 3.5639476922736937
Iteration: 5 || Loss: 3.5623243604334434
Iteration: 6 || Loss: 3.5623243604334434
saving ADAM checkpoint...
Sum of params:82.80545
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.5623243604334434
Iteration: 2 || Loss: 2.67816635464121
Iteration: 3 || Loss: 2.1405258284129633
Iteration: 4 || Loss: 2.1350441583699062
Iteration: 5 || Loss: 2.0640185064200436
Iteration: 6 || Loss: 2.042568512189886
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.912445
Epoch 254 loss:2.042568512189886
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.912445
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.352233281446091
Iteration: 2 || Loss: 7.35185474308043
Iteration: 3 || Loss: 7.351476362246213
Iteration: 4 || Loss: 7.351099204282885
Iteration: 5 || Loss: 7.350722981511567
Iteration: 6 || Loss: 7.350722981511567
saving ADAM checkpoint...
Sum of params:82.91256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.350722981511567
Iteration: 2 || Loss: 7.314170953543308
Iteration: 3 || Loss: 7.207570435246947
Iteration: 4 || Loss: 7.047070795860676
Iteration: 5 || Loss: 7.038992525727635
Iteration: 6 || Loss: 6.987888015877804
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.93789
Epoch 254 loss:6.987888015877804
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.93789
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.836355543841442
Iteration: 2 || Loss: 22.836000799133867
Iteration: 3 || Loss: 22.835646197550574
Iteration: 4 || Loss: 22.835292749956707
Iteration: 5 || Loss: 22.834939170260423
Iteration: 6 || Loss: 22.834939170260423
saving ADAM checkpoint...
Sum of params:82.93786
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.834939170260423
Iteration: 2 || Loss: 22.801702788333014
Iteration: 3 || Loss: 22.57989048171481
Iteration: 4 || Loss: 22.42005590361559
Iteration: 5 || Loss: 22.392895437899497
Iteration: 6 || Loss: 22.311780212241104
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.83989
Epoch 254 loss:22.311780212241104
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.814968931499468
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.62680733002281
waveform batch: 2/2
Test loss - extrapolation:18.53429863235916
Epoch 254 mean train loss:1.080766784148579
Epoch 254 mean test loss - interpolation:1.1358281552499114
Epoch 254 mean test loss - extrapolation:5.263425496865164
Start training epoch 255
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.83989
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.5216442020764798
Iteration: 2 || Loss: 3.520045275959165
Iteration: 3 || Loss: 3.518449350329614
Iteration: 4 || Loss: 3.5168501479562644
Iteration: 5 || Loss: 3.515252246687272
Iteration: 6 || Loss: 3.515252246687272
saving ADAM checkpoint...
Sum of params:82.83989
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.515252246687272
Iteration: 2 || Loss: 2.666770548854776
Iteration: 3 || Loss: 2.12515003141307
Iteration: 4 || Loss: 2.1199546901720274
Iteration: 5 || Loss: 2.0539358565605728
Iteration: 6 || Loss: 2.0295311499322364
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.9472
Epoch 255 loss:2.0295311499322364
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.9472
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.316751266163249
Iteration: 2 || Loss: 7.31637519607031
Iteration: 3 || Loss: 7.315999021967028
Iteration: 4 || Loss: 7.315623586906392
Iteration: 5 || Loss: 7.315250697687394
Iteration: 6 || Loss: 7.315250697687394
saving ADAM checkpoint...
Sum of params:82.9473
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.315250697687394
Iteration: 2 || Loss: 7.279424689440698
Iteration: 3 || Loss: 7.172300597372033
Iteration: 4 || Loss: 7.014798839631521
Iteration: 5 || Loss: 7.007159790953959
Iteration: 6 || Loss: 6.960356322821984
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.97127
Epoch 255 loss:6.960356322821984
waveform batch: 3/3
Using ADAM optimizer
Sum of params:82.97127
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.82319672393056
Iteration: 2 || Loss: 22.822849846538823
Iteration: 3 || Loss: 22.82250350797863
Iteration: 4 || Loss: 22.82215963392178
Iteration: 5 || Loss: 22.821815835779372
Iteration: 6 || Loss: 22.821815835779372
saving ADAM checkpoint...
Sum of params:82.97122
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.821815835779372
Iteration: 2 || Loss: 22.79069294171602
Iteration: 3 || Loss: 22.564456831055093
Iteration: 4 || Loss: 22.396436629216193
Iteration: 5 || Loss: 22.367584472255036
Iteration: 6 || Loss: 22.289876799218945
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.87073
Epoch 255 loss:22.289876799218945
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.791787912297426
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.593205546559794
waveform batch: 2/2
Test loss - extrapolation:18.525324443410298
Epoch 255 mean train loss:1.0786125611025228
Epoch 255 mean test loss - interpolation:1.131964652049571
Epoch 255 mean test loss - extrapolation:5.259877499164174
Start training epoch 256
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.87073
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.4807770282917216
Iteration: 2 || Loss: 3.4792053952711033
Iteration: 3 || Loss: 3.4776309144630635
Iteration: 4 || Loss: 3.476057957782398
Iteration: 5 || Loss: 3.4744856487101683
Iteration: 6 || Loss: 3.4744856487101683
saving ADAM checkpoint...
Sum of params:82.87073
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.4744856487101683
Iteration: 2 || Loss: 2.6557628499302166
Iteration: 3 || Loss: 2.1110074230888216
Iteration: 4 || Loss: 2.1059025122645654
Iteration: 5 || Loss: 2.044176601855007
Iteration: 6 || Loss: 2.0178068662330446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.97818
Epoch 256 loss:2.0178068662330446
waveform batch: 2/3
Using ADAM optimizer
Sum of params:82.97818
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.28460324177423
Iteration: 2 || Loss: 7.2842293000280325
Iteration: 3 || Loss: 7.2838560872148985
Iteration: 4 || Loss: 7.2834846651877605
Iteration: 5 || Loss: 7.283113582037234
Iteration: 6 || Loss: 7.283113582037234
saving ADAM checkpoint...
Sum of params:82.9783
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.283113582037234
Iteration: 2 || Loss: 7.248055451943929
Iteration: 3 || Loss: 7.140606021533443
Iteration: 4 || Loss: 6.985647136382055
Iteration: 5 || Loss: 6.978372034480774
Iteration: 6 || Loss: 6.935238130500197
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.00042
Epoch 256 loss:6.935238130500197
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.00042
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.804338824569353
Iteration: 2 || Loss: 22.80399793578134
Iteration: 3 || Loss: 22.80365732017142
Iteration: 4 || Loss: 22.803318338292225
Iteration: 5 || Loss: 22.802979673223632
Iteration: 6 || Loss: 22.802979673223632
saving ADAM checkpoint...
Sum of params:83.00039
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.802979673223632
Iteration: 2 || Loss: 22.77300988984166
Iteration: 3 || Loss: 22.54365510255166
Iteration: 4 || Loss: 22.37133458539006
Iteration: 5 || Loss: 22.341392472430663
Iteration: 6 || Loss: 22.266926957714986
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.89796
Epoch 256 loss:22.266926957714986
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.770414945944383
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.56218903512864
waveform batch: 2/2
Test loss - extrapolation:18.515845395559605
Epoch 256 mean train loss:1.0765507570499389
Epoch 256 mean test loss - interpolation:1.1284024909907304
Epoch 256 mean test loss - extrapolation:5.256502869224021
Start training epoch 257
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.89796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.44495964993169
Iteration: 2 || Loss: 3.4434037818752055
Iteration: 3 || Loss: 3.4418513123017678
Iteration: 4 || Loss: 3.440298903565847
Iteration: 5 || Loss: 3.438749510393841
Iteration: 6 || Loss: 3.438749510393841
saving ADAM checkpoint...
Sum of params:82.89795
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.438749510393841
Iteration: 2 || Loss: 2.644850710218742
Iteration: 3 || Loss: 2.0980265013600663
Iteration: 4 || Loss: 2.0929282103239264
Iteration: 5 || Loss: 2.034740045685485
Iteration: 6 || Loss: 2.0072188939607636
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.00544
Epoch 257 loss:2.0072188939607636
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.00544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.255429613429585
Iteration: 2 || Loss: 7.255059648604608
Iteration: 3 || Loss: 7.254690205215883
Iteration: 4 || Loss: 7.254321320186535
Iteration: 5 || Loss: 7.253954586389506
Iteration: 6 || Loss: 7.253954586389506
saving ADAM checkpoint...
Sum of params:83.00555
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.253954586389506
Iteration: 2 || Loss: 7.219717440023933
Iteration: 3 || Loss: 7.112156442817141
Iteration: 4 || Loss: 6.959457538075297
Iteration: 5 || Loss: 6.952474764936403
Iteration: 6 || Loss: 6.912425193595533
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.02571
Epoch 257 loss:6.912425193595533
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.02571
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.780647645542174
Iteration: 2 || Loss: 22.78030754242451
Iteration: 3 || Loss: 22.779969230828897
Iteration: 4 || Loss: 22.77963142184147
Iteration: 5 || Loss: 22.779295454943206
Iteration: 6 || Loss: 22.779295454943206
saving ADAM checkpoint...
Sum of params:83.02567
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.779295454943206
Iteration: 2 || Loss: 22.749729327981584
Iteration: 3 || Loss: 22.51872826692366
Iteration: 4 || Loss: 22.344835134574442
Iteration: 5 || Loss: 22.31431162438445
Iteration: 6 || Loss: 22.24296249506881
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.92188
Epoch 257 loss:22.24296249506881
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.750692334120559
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.53284193274186
waveform batch: 2/2
Test loss - extrapolation:18.505706111796105
Epoch 257 mean train loss:1.0745726407801761
Epoch 257 mean test loss - interpolation:1.1251153890200931
Epoch 257 mean test loss - extrapolation:5.25321233704483
Start training epoch 258
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.92188
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.4129261516323965
Iteration: 2 || Loss: 3.4113902999626893
Iteration: 3 || Loss: 3.409857813150082
Iteration: 4 || Loss: 3.408323804904555
Iteration: 5 || Loss: 3.4067893884892504
Iteration: 6 || Loss: 3.4067893884892504
saving ADAM checkpoint...
Sum of params:82.9219
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.4067893884892504
Iteration: 2 || Loss: 2.634057672515809
Iteration: 3 || Loss: 2.086053437250367
Iteration: 4 || Loss: 2.080932651370279
Iteration: 5 || Loss: 2.025580516524305
Iteration: 6 || Loss: 1.9975799930609697
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.02928
Epoch 258 loss:1.9975799930609697
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.02928
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.228814898999988
Iteration: 2 || Loss: 7.228447099218659
Iteration: 3 || Loss: 7.228078621568162
Iteration: 4 || Loss: 7.227715627052655
Iteration: 5 || Loss: 7.227352412932113
Iteration: 6 || Loss: 7.227352412932113
saving ADAM checkpoint...
Sum of params:83.02942
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.227352412932113
Iteration: 2 || Loss: 7.193973625232917
Iteration: 3 || Loss: 7.086514412552246
Iteration: 4 || Loss: 6.935868432109716
Iteration: 5 || Loss: 6.92914350645232
Iteration: 6 || Loss: 6.891637291210552
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.0478
Epoch 258 loss:6.891637291210552
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.0478
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.755226942579753
Iteration: 2 || Loss: 22.754886956623523
Iteration: 3 || Loss: 22.754550252795788
Iteration: 4 || Loss: 22.75421270417094
Iteration: 5 || Loss: 22.753876585878487
Iteration: 6 || Loss: 22.753876585878487
saving ADAM checkpoint...
Sum of params:83.04777
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.753876585878487
Iteration: 2 || Loss: 22.724456623237447
Iteration: 3 || Loss: 22.492053278049664
Iteration: 4 || Loss: 22.317668802634387
Iteration: 5 || Loss: 22.286802925016143
Iteration: 6 || Loss: 22.218301388038217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.94334
Epoch 258 loss:22.218301388038217
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.732285425439764
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.50456451131603
waveform batch: 2/2
Test loss - extrapolation:18.49498005749305
Epoch 258 mean train loss:1.072673057665853
Epoch 258 mean test loss - interpolation:1.1220475709066273
Epoch 258 mean test loss - extrapolation:5.249962047400757
Start training epoch 259
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.94334
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.383454336849198
Iteration: 2 || Loss: 3.3819373740011565
Iteration: 3 || Loss: 3.3804137801563536
Iteration: 4 || Loss: 3.378898918334257
Iteration: 5 || Loss: 3.3773844146838092
Iteration: 6 || Loss: 3.3773844146838092
saving ADAM checkpoint...
Sum of params:82.94333
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.3773844146838092
Iteration: 2 || Loss: 2.623264337671283
Iteration: 3 || Loss: 2.0749435312338833
Iteration: 4 || Loss: 2.0697884394761648
Iteration: 5 || Loss: 2.0167112228296293
Iteration: 6 || Loss: 1.9886612864694022
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.05053
Epoch 259 loss:1.9886612864694022
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.05053
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.204306386834982
Iteration: 2 || Loss: 7.203943503262818
Iteration: 3 || Loss: 7.203580524418627
Iteration: 4 || Loss: 7.203218075533847
Iteration: 5 || Loss: 7.202861268252149
Iteration: 6 || Loss: 7.202861268252149
saving ADAM checkpoint...
Sum of params:83.050644
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.202861268252149
Iteration: 2 || Loss: 7.170319919242828
Iteration: 3 || Loss: 7.0631031059844815
Iteration: 4 || Loss: 6.9143651417534
Iteration: 5 || Loss: 6.907850779316802
Iteration: 6 || Loss: 6.87251489129684
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.06745
Epoch 259 loss:6.87251489129684
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.06745
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.72796189140339
Iteration: 2 || Loss: 22.727620355091794
Iteration: 3 || Loss: 22.727283680611468
Iteration: 4 || Loss: 22.726946403096235
Iteration: 5 || Loss: 22.72660957216561
Iteration: 6 || Loss: 22.72660957216561
saving ADAM checkpoint...
Sum of params:83.0674
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.72660957216561
Iteration: 2 || Loss: 22.69712796237444
Iteration: 3 || Loss: 22.464162187130885
Iteration: 4 || Loss: 22.290121660832007
Iteration: 5 || Loss: 22.25904096875294
Iteration: 6 || Loss: 22.193164135718018
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.96273
Epoch 259 loss:22.193164135718018
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.714998516686037
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.477187675799925
waveform batch: 2/2
Test loss - extrapolation:18.483748199384845
Epoch 259 mean train loss:1.0708393211546297
Epoch 259 mean test loss - interpolation:1.1191664194476727
Epoch 259 mean test loss - extrapolation:5.246744656265398
Start training epoch 260
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.96273
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.3563130159025367
Iteration: 2 || Loss: 3.3548071453836323
Iteration: 3 || Loss: 3.3533036335840967
Iteration: 4 || Loss: 3.3518005179931274
Iteration: 5 || Loss: 3.350297645513238
Iteration: 6 || Loss: 3.350297645513238
saving ADAM checkpoint...
Sum of params:82.96274
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.350297645513238
Iteration: 2 || Loss: 2.612446327049013
Iteration: 3 || Loss: 2.0645684201940595
Iteration: 4 || Loss: 2.059385082771435
Iteration: 5 || Loss: 2.0080951875876907
Iteration: 6 || Loss: 1.9803515597728876
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.069626
Epoch 260 loss:1.9803515597728876
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.069626
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.181629309323403
Iteration: 2 || Loss: 7.1812696705249515
Iteration: 3 || Loss: 7.180909681396244
Iteration: 4 || Loss: 7.180552250241794
Iteration: 5 || Loss: 7.180196047259493
Iteration: 6 || Loss: 7.180196047259493
saving ADAM checkpoint...
Sum of params:83.06974
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.180196047259493
Iteration: 2 || Loss: 7.148446793572177
Iteration: 3 || Loss: 7.04157833298807
Iteration: 4 || Loss: 6.894604012167088
Iteration: 5 || Loss: 6.888269098235995
Iteration: 6 || Loss: 6.854772306907231
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.08529
Epoch 260 loss:6.854772306907231
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.08529
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.700226044399702
Iteration: 2 || Loss: 22.69988570072872
Iteration: 3 || Loss: 22.699546293851395
Iteration: 4 || Loss: 22.69920592303921
Iteration: 5 || Loss: 22.6988686706915
Iteration: 6 || Loss: 22.6988686706915
saving ADAM checkpoint...
Sum of params:83.08526
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.6988686706915
Iteration: 2 || Loss: 22.66909138972805
Iteration: 3 || Loss: 22.435845056380877
Iteration: 4 || Loss: 22.262556700186686
Iteration: 5 || Loss: 22.23141151815483
Iteration: 6 || Loss: 22.167891774906323
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.980576
Epoch 260 loss:22.167891774906323
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.69874838624092
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.45023582753654
waveform batch: 2/2
Test loss - extrapolation:18.472097200961432
Epoch 260 mean train loss:1.0690695048822911
Epoch 260 mean test loss - interpolation:1.1164580643734867
Epoch 260 mean test loss - extrapolation:5.243527752374831
Start training epoch 261
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.980576
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.330727608710879
Iteration: 2 || Loss: 3.3292350738724847
Iteration: 3 || Loss: 3.327745628424295
Iteration: 4 || Loss: 3.326254967411802
Iteration: 5 || Loss: 3.3247657286563825
Iteration: 6 || Loss: 3.3247657286563825
saving ADAM checkpoint...
Sum of params:82.98058
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.3247657286563825
Iteration: 2 || Loss: 2.601981188045546
Iteration: 3 || Loss: 2.0547941572319925
Iteration: 4 || Loss: 2.049585671215844
Iteration: 5 || Loss: 1.99969689039408
Iteration: 6 || Loss: 1.9725239036796416
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.0871
Epoch 261 loss:1.9725239036796416
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.0871
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.160393300859215
Iteration: 2 || Loss: 7.160035527940235
Iteration: 3 || Loss: 7.159679006956276
Iteration: 4 || Loss: 7.159324752706685
Iteration: 5 || Loss: 7.158973246997659
Iteration: 6 || Loss: 7.158973246997659
saving ADAM checkpoint...
Sum of params:83.08722
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.158973246997659
Iteration: 2 || Loss: 7.127968741067248
Iteration: 3 || Loss: 7.02155163062078
Iteration: 4 || Loss: 6.876256740686075
Iteration: 5 || Loss: 6.87007035755681
Iteration: 6 || Loss: 6.838182873033776
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.10182
Epoch 261 loss:6.838182873033776
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.10182
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.67187543988899
Iteration: 2 || Loss: 22.671532017925216
Iteration: 3 || Loss: 22.67119285874841
Iteration: 4 || Loss: 22.670854568548304
Iteration: 5 || Loss: 22.67051511805766
Iteration: 6 || Loss: 22.67051511805766
saving ADAM checkpoint...
Sum of params:83.10177
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.67051511805766
Iteration: 2 || Loss: 22.64060519022539
Iteration: 3 || Loss: 22.40735552210913
Iteration: 4 || Loss: 22.23510897538502
Iteration: 5 || Loss: 22.203790604072555
Iteration: 6 || Loss: 22.142374503072475
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:82.997086
Epoch 261 loss:22.142374503072475
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.683516509935799
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.42455531059885
waveform batch: 2/2
Test loss - extrapolation:18.45995030364341
Epoch 261 mean train loss:1.0673476303374445
Epoch 261 mean test loss - interpolation:1.1139194183226333
Epoch 261 mean test loss - extrapolation:5.2403754678535215
Start training epoch 262
waveform batch: 1/3
Using ADAM optimizer
Sum of params:82.997086
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.3086529926131547
Iteration: 2 || Loss: 3.3071720110980904
Iteration: 3 || Loss: 3.305690527238882
Iteration: 4 || Loss: 3.3042081218993564
Iteration: 5 || Loss: 3.3027294990214324
Iteration: 6 || Loss: 3.3027294990214324
saving ADAM checkpoint...
Sum of params:82.997086
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.3027294990214324
Iteration: 2 || Loss: 2.591478179303331
Iteration: 3 || Loss: 2.045651195753688
Iteration: 4 || Loss: 2.040448230997085
Iteration: 5 || Loss: 1.9916943973225296
Iteration: 6 || Loss: 1.9651973358383932
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.103226
Epoch 262 loss:1.9651973358383932
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.103226
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.1406668384805805
Iteration: 2 || Loss: 7.140313754737533
Iteration: 3 || Loss: 7.139960860613098
Iteration: 4 || Loss: 7.139609291899599
Iteration: 5 || Loss: 7.139260136987227
Iteration: 6 || Loss: 7.139260136987227
saving ADAM checkpoint...
Sum of params:83.10334
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.139260136987227
Iteration: 2 || Loss: 7.108904313299393
Iteration: 3 || Loss: 7.002980895879891
Iteration: 4 || Loss: 6.859251730374148
Iteration: 5 || Loss: 6.85318155364063
Iteration: 6 || Loss: 6.822666706230081
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.1172
Epoch 262 loss:6.822666706230081
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.1172
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.643143715848257
Iteration: 2 || Loss: 22.642802511924142
Iteration: 3 || Loss: 22.642460119263674
Iteration: 4 || Loss: 22.642120787144332
Iteration: 5 || Loss: 22.641780008307048
Iteration: 6 || Loss: 22.641780008307048
saving ADAM checkpoint...
Sum of params:83.117165
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.641780008307048
Iteration: 2 || Loss: 22.61163337610493
Iteration: 3 || Loss: 22.378700744469054
Iteration: 4 || Loss: 22.20767878862676
Iteration: 5 || Loss: 22.176255464500784
Iteration: 6 || Loss: 22.116802289308087
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.01262
Epoch 262 loss:22.116802289308087
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.6690383591341655
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.398895423246124
waveform batch: 2/2
Test loss - extrapolation:18.447425622014922
Epoch 262 mean train loss:1.0656781493578125
Epoch 262 mean test loss - interpolation:1.1115063931890277
Epoch 262 mean test loss - extrapolation:5.2371934204384205
Start training epoch 263
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.01262
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.2875680342393485
Iteration: 2 || Loss: 3.286094239419908
Iteration: 3 || Loss: 3.284619207733381
Iteration: 4 || Loss: 3.283149246412536
Iteration: 5 || Loss: 3.2816822998925135
Iteration: 6 || Loss: 3.2816822998925135
saving ADAM checkpoint...
Sum of params:83.01263
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.2816822998925135
Iteration: 2 || Loss: 2.581136044385115
Iteration: 3 || Loss: 2.0369560569294647
Iteration: 4 || Loss: 2.031769525373816
Iteration: 5 || Loss: 1.9839554079440407
Iteration: 6 || Loss: 1.9582450175338941
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.11832
Epoch 263 loss:1.9582450175338941
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.11832
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.122166345555944
Iteration: 2 || Loss: 7.121813628659869
Iteration: 3 || Loss: 7.121463911266838
Iteration: 4 || Loss: 7.121115962150944
Iteration: 5 || Loss: 7.12076987007086
Iteration: 6 || Loss: 7.12076987007086
saving ADAM checkpoint...
Sum of params:83.11842
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.12076987007086
Iteration: 2 || Loss: 7.091013535463953
Iteration: 3 || Loss: 6.985613078886188
Iteration: 4 || Loss: 6.843330899691419
Iteration: 5 || Loss: 6.8373533313387
Iteration: 6 || Loss: 6.808026410848757
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.13173
Epoch 263 loss:6.808026410848757
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.13173
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.614787862201137
Iteration: 2 || Loss: 22.61444368061377
Iteration: 3 || Loss: 22.61409985896672
Iteration: 4 || Loss: 22.613758753752325
Iteration: 5 || Loss: 22.61341693455414
Iteration: 6 || Loss: 22.61341693455414
saving ADAM checkpoint...
Sum of params:83.1317
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.61341693455414
Iteration: 2 || Loss: 22.582920635265666
Iteration: 3 || Loss: 22.35030889256962
Iteration: 4 || Loss: 22.180530225725068
Iteration: 5 || Loss: 22.14908130028712
Iteration: 6 || Loss: 22.09134306260521
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.027405
Epoch 263 loss:22.09134306260521
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.65540892724168
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.373469547730366
waveform batch: 2/2
Test loss - extrapolation:18.434495421907847
Epoch 263 mean train loss:1.0640556721030296
Epoch 263 mean test loss - interpolation:1.1092348212069467
Epoch 263 mean test loss - extrapolation:5.233997080803184
Start training epoch 264
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.027405
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.268003415132988
Iteration: 2 || Loss: 3.2665347722175126
Iteration: 3 || Loss: 3.2650725102601346
Iteration: 4 || Loss: 3.2636113836257996
Iteration: 5 || Loss: 3.2621504304854305
Iteration: 6 || Loss: 3.2621504304854305
saving ADAM checkpoint...
Sum of params:83.027405
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.2621504304854305
Iteration: 2 || Loss: 2.57137114939327
Iteration: 3 || Loss: 2.0287029888585772
Iteration: 4 || Loss: 2.023539268310852
Iteration: 5 || Loss: 1.9765581328940944
Iteration: 6 || Loss: 1.9516289989270719
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.13265
Epoch 264 loss:1.9516289989270719
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.13265
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.1045921928203555
Iteration: 2 || Loss: 7.104243879333675
Iteration: 3 || Loss: 7.103897407879013
Iteration: 4 || Loss: 7.1035519513773435
Iteration: 5 || Loss: 7.103207670944697
Iteration: 6 || Loss: 7.103207670944697
saving ADAM checkpoint...
Sum of params:83.13277
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.103207670944697
Iteration: 2 || Loss: 7.073996601964277
Iteration: 3 || Loss: 6.969164414047483
Iteration: 4 || Loss: 6.828304841209156
Iteration: 5 || Loss: 6.822407223273169
Iteration: 6 || Loss: 6.794135482546396
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.14571
Epoch 264 loss:6.794135482546396
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.14571
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.586427834556407
Iteration: 2 || Loss: 22.586081900190184
Iteration: 3 || Loss: 22.585736805608413
Iteration: 4 || Loss: 22.585393247392556
Iteration: 5 || Loss: 22.585051122542307
Iteration: 6 || Loss: 22.585051122542307
saving ADAM checkpoint...
Sum of params:83.14566
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.585051122542307
Iteration: 2 || Loss: 22.554305522778677
Iteration: 3 || Loss: 22.322262794855757
Iteration: 4 || Loss: 22.153702566208576
Iteration: 5 || Loss: 22.122152154569786
Iteration: 6 || Loss: 22.066007780333226
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.041626
Epoch 264 loss:22.066007780333226
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.642368229554571
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.348361492099166
waveform batch: 2/2
Test loss - extrapolation:18.42128339633385
Epoch 264 mean train loss:1.0624749055795413
Epoch 264 mean test loss - interpolation:1.1070613715924285
Epoch 264 mean test loss - extrapolation:5.2308037407027514
Start training epoch 265
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.041626
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.2499443709606943
Iteration: 2 || Loss: 3.248488220323839
Iteration: 3 || Loss: 3.2470323635528775
Iteration: 4 || Loss: 3.245577453223337
Iteration: 5 || Loss: 3.2441228567706384
Iteration: 6 || Loss: 3.2441228567706384
saving ADAM checkpoint...
Sum of params:83.041626
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.2441228567706384
Iteration: 2 || Loss: 2.561582750372692
Iteration: 3 || Loss: 2.02083508999629
Iteration: 4 || Loss: 2.015711250391537
Iteration: 5 || Loss: 1.9694338792918915
Iteration: 6 || Loss: 1.9453156656160344
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.14642
Epoch 265 loss:1.9453156656160344
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.14642
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.0880257052987865
Iteration: 2 || Loss: 7.087680039444306
Iteration: 3 || Loss: 7.087335035665726
Iteration: 4 || Loss: 7.086991422601171
Iteration: 5 || Loss: 7.08664966116211
Iteration: 6 || Loss: 7.08664966116211
saving ADAM checkpoint...
Sum of params:83.14654
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.08664966116211
Iteration: 2 || Loss: 7.057904905881389
Iteration: 3 || Loss: 6.953637537415793
Iteration: 4 || Loss: 6.814091451449811
Iteration: 5 || Loss: 6.808249409690267
Iteration: 6 || Loss: 6.780917501474617
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.15921
Epoch 265 loss:6.780917501474617
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.15921
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.558138560312965
Iteration: 2 || Loss: 22.55779016456135
Iteration: 3 || Loss: 22.557445335141505
Iteration: 4 || Loss: 22.557099628085638
Iteration: 5 || Loss: 22.556757736551013
Iteration: 6 || Loss: 22.556757736551013
saving ADAM checkpoint...
Sum of params:83.15916
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.556757736551013
Iteration: 2 || Loss: 22.525824836996904
Iteration: 3 || Loss: 22.294410588557746
Iteration: 4 || Loss: 22.12710904901362
Iteration: 5 || Loss: 22.095434661251545
Iteration: 6 || Loss: 22.04071485052998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.05536
Epoch 265 loss:22.04071485052998
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.63001801845526
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.323691900173266
waveform batch: 2/2
Test loss - extrapolation:18.40769741492734
Epoch 265 mean train loss:1.0609292419869183
Epoch 265 mean test loss - interpolation:1.1050030030758766
Epoch 265 mean test loss - extrapolation:5.227615776258384
Start training epoch 266
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.05536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.233815324955553
Iteration: 2 || Loss: 3.232364123350861
Iteration: 3 || Loss: 3.2309125059647332
Iteration: 4 || Loss: 3.229459205231337
Iteration: 5 || Loss: 3.22800984453131
Iteration: 6 || Loss: 3.22800984453131
saving ADAM checkpoint...
Sum of params:83.05537
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.22800984453131
Iteration: 2 || Loss: 2.552101066396112
Iteration: 3 || Loss: 2.0133659265630772
Iteration: 4 || Loss: 2.008296084472034
Iteration: 5 || Loss: 1.9625831090174397
Iteration: 6 || Loss: 1.9393114034113332
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.159706
Epoch 266 loss:1.9393114034113332
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.159706
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.072316668809916
Iteration: 2 || Loss: 7.071973487392803
Iteration: 3 || Loss: 7.071629660944497
Iteration: 4 || Loss: 7.071288826501595
Iteration: 5 || Loss: 7.070948791022871
Iteration: 6 || Loss: 7.070948791022871
saving ADAM checkpoint...
Sum of params:83.15982
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.070948791022871
Iteration: 2 || Loss: 7.0426116935557275
Iteration: 3 || Loss: 6.938927094019594
Iteration: 4 || Loss: 6.800632946448278
Iteration: 5 || Loss: 6.794836042220691
Iteration: 6 || Loss: 6.768334216211983
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.17235
Epoch 266 loss:6.768334216211983
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.17235
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.530116540746153
Iteration: 2 || Loss: 22.52976876024647
Iteration: 3 || Loss: 22.529421163645925
Iteration: 4 || Loss: 22.529077035346273
Iteration: 5 || Loss: 22.528733214129097
Iteration: 6 || Loss: 22.528733214129097
saving ADAM checkpoint...
Sum of params:83.172295
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.528733214129097
Iteration: 2 || Loss: 22.497577158872865
Iteration: 3 || Loss: 22.266793934821905
Iteration: 4 || Loss: 22.100701770987413
Iteration: 5 || Loss: 22.068936362541017
Iteration: 6 || Loss: 22.01555382009823
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.06875
Epoch 266 loss:22.01555382009823
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.618137344612994
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.29886143558973
waveform batch: 2/2
Test loss - extrapolation:18.393858945826704
Epoch 266 mean train loss:1.0594206703352258
Epoch 266 mean test loss - interpolation:1.1030228907688324
Epoch 266 mean test loss - extrapolation:5.224393365118036
Start training epoch 267
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.06875
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.2182196897728095
Iteration: 2 || Loss: 3.216769296059487
Iteration: 3 || Loss: 3.2153237607208354
Iteration: 4 || Loss: 3.2138752168544378
Iteration: 5 || Loss: 3.2124337002195342
Iteration: 6 || Loss: 3.2124337002195342
saving ADAM checkpoint...
Sum of params:83.068756
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.2124337002195342
Iteration: 2 || Loss: 2.5428480772307833
Iteration: 3 || Loss: 2.006188757958835
Iteration: 4 || Loss: 2.0011784577799068
Iteration: 5 || Loss: 1.9559622828840064
Iteration: 6 || Loss: 1.933543755241373
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.17261
Epoch 267 loss:1.933543755241373
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.17261
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.057344171851981
Iteration: 2 || Loss: 7.057001616571852
Iteration: 3 || Loss: 7.056661093205194
Iteration: 4 || Loss: 7.056320764524542
Iteration: 5 || Loss: 7.055983428162055
Iteration: 6 || Loss: 7.055983428162055
saving ADAM checkpoint...
Sum of params:83.172745
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.055983428162055
Iteration: 2 || Loss: 7.028021806874928
Iteration: 3 || Loss: 6.924919703215282
Iteration: 4 || Loss: 6.7877999802642
Iteration: 5 || Loss: 6.782036717764075
Iteration: 6 || Loss: 6.756275269988301
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.185196
Epoch 267 loss:6.756275269988301
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.185196
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.50240313786445
Iteration: 2 || Loss: 22.5020562666181
Iteration: 3 || Loss: 22.501709963344954
Iteration: 4 || Loss: 22.50136122208499
Iteration: 5 || Loss: 22.50101676721105
Iteration: 6 || Loss: 22.50101676721105
saving ADAM checkpoint...
Sum of params:83.18516
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.50101676721105
Iteration: 2 || Loss: 22.469643796766064
Iteration: 3 || Loss: 22.239536597127646
Iteration: 4 || Loss: 22.074594345128318
Iteration: 5 || Loss: 22.042749416631974
Iteration: 6 || Loss: 21.99058031242314
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.08192
Epoch 267 loss:21.99058031242314
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.606789296706178
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.27415350238024
waveform batch: 2/2
Test loss - extrapolation:18.379757973962178
Epoch 267 mean train loss:1.0579448047466489
Epoch 267 mean test loss - interpolation:1.1011315494510296
Epoch 267 mean test loss - extrapolation:5.221159289695201
Start training epoch 268
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.08192
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.203749597764649
Iteration: 2 || Loss: 3.2023026705965956
Iteration: 3 || Loss: 3.200862148657895
Iteration: 4 || Loss: 3.199418484308194
Iteration: 5 || Loss: 3.197981142758868
Iteration: 6 || Loss: 3.197981142758868
saving ADAM checkpoint...
Sum of params:83.08191
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.197981142758868
Iteration: 2 || Loss: 2.5339631515235848
Iteration: 3 || Loss: 1.9993164508391166
Iteration: 4 || Loss: 1.994369650293088
Iteration: 5 || Loss: 1.9495773551969415
Iteration: 6 || Loss: 1.9280050910741928
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.18532
Epoch 268 loss:1.9280050910741928
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.18532
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.043006791707719
Iteration: 2 || Loss: 7.042664985717155
Iteration: 3 || Loss: 7.042325878808073
Iteration: 4 || Loss: 7.041987257186427
Iteration: 5 || Loss: 7.041649895264401
Iteration: 6 || Loss: 7.041649895264401
saving ADAM checkpoint...
Sum of params:83.185425
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.041649895264401
Iteration: 2 || Loss: 7.0140238928508305
Iteration: 3 || Loss: 6.911510329164305
Iteration: 4 || Loss: 6.775512359178077
Iteration: 5 || Loss: 6.7697740800861945
Iteration: 6 || Loss: 6.744689767369605
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.19791
Epoch 268 loss:6.744689767369605
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.19791
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.475118967615117
Iteration: 2 || Loss: 22.474769093118308
Iteration: 3 || Loss: 22.47442204572671
Iteration: 4 || Loss: 22.474073082731564
Iteration: 5 || Loss: 22.473725616330054
Iteration: 6 || Loss: 22.473725616330054
saving ADAM checkpoint...
Sum of params:83.19786
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.473725616330054
Iteration: 2 || Loss: 22.442113144410314
Iteration: 3 || Loss: 22.212640555480746
Iteration: 4 || Loss: 22.048776751740437
Iteration: 5 || Loss: 22.016867929069733
Iteration: 6 || Loss: 21.96581339418906
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.094894
Epoch 268 loss:21.96581339418906
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.5958668661218365
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.249432308144684
waveform batch: 2/2
Test loss - extrapolation:18.36545271386644
Epoch 268 mean train loss:1.0565002845735467
Epoch 268 mean test loss - interpolation:1.0993111443536394
Epoch 268 mean test loss - extrapolation:5.217907085167593
Start training epoch 269
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.094894
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.1899997586012336
Iteration: 2 || Loss: 3.1885612200637965
Iteration: 3 || Loss: 3.187122682770315
Iteration: 4 || Loss: 3.1856875303384538
Iteration: 5 || Loss: 3.1842485871140696
Iteration: 6 || Loss: 3.1842485871140696
saving ADAM checkpoint...
Sum of params:83.09491
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.1842485871140696
Iteration: 2 || Loss: 2.525379764642373
Iteration: 3 || Loss: 1.9927056772170029
Iteration: 4 || Loss: 1.9878235403119222
Iteration: 5 || Loss: 1.9435272529579222
Iteration: 6 || Loss: 1.9226661294392946
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.197845
Epoch 269 loss:1.9226661294392946
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.197845
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.02926072774249
Iteration: 2 || Loss: 7.028921270657002
Iteration: 3 || Loss: 7.02858405567087
Iteration: 4 || Loss: 7.028248290069347
Iteration: 5 || Loss: 7.0279120820172905
Iteration: 6 || Loss: 7.0279120820172905
saving ADAM checkpoint...
Sum of params:83.19796
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.0279120820172905
Iteration: 2 || Loss: 7.000574140210537
Iteration: 3 || Loss: 6.8986245322857105
Iteration: 4 || Loss: 6.763704331938546
Iteration: 5 || Loss: 6.757983924135233
Iteration: 6 || Loss: 6.733519027706865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.21051
Epoch 269 loss:6.733519027706865
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.21051
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.448127254848433
Iteration: 2 || Loss: 22.44777513009917
Iteration: 3 || Loss: 22.44742549883291
Iteration: 4 || Loss: 22.447077126482057
Iteration: 5 || Loss: 22.446729232108126
Iteration: 6 || Loss: 22.446729232108126
saving ADAM checkpoint...
Sum of params:83.21046
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.446729232108126
Iteration: 2 || Loss: 22.414854007012213
Iteration: 3 || Loss: 22.186145081104684
Iteration: 4 || Loss: 22.023282482715086
Iteration: 5 || Loss: 21.991318517155246
Iteration: 6 || Loss: 21.941308178346773
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.10781
Epoch 269 loss:21.941308178346773
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.585305783285208
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.2246674908615
waveform batch: 2/2
Test loss - extrapolation:18.35101057086548
Epoch 269 mean train loss:1.0550859770859633
Epoch 269 mean test loss - interpolation:1.097550963880868
Epoch 269 mean test loss - extrapolation:5.214639838477248
Start training epoch 270
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.10781
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.1767830082884947
Iteration: 2 || Loss: 3.1753462402389023
Iteration: 3 || Loss: 3.1739127263715443
Iteration: 4 || Loss: 3.1724792761319005
Iteration: 5 || Loss: 3.1710480366786618
Iteration: 6 || Loss: 3.1710480366786618
saving ADAM checkpoint...
Sum of params:83.1078
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.1710480366786618
Iteration: 2 || Loss: 2.5170743372285345
Iteration: 3 || Loss: 1.9863255917546792
Iteration: 4 || Loss: 1.9815094622230727
Iteration: 5 || Loss: 1.9375969628023497
Iteration: 6 || Loss: 1.917502689141427
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.21029
Epoch 270 loss:1.917502689141427
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.21029
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.016022819100654
Iteration: 2 || Loss: 7.015684845294425
Iteration: 3 || Loss: 7.015348632848439
Iteration: 4 || Loss: 7.015012165946593
Iteration: 5 || Loss: 7.01467739057244
Iteration: 6 || Loss: 7.01467739057244
saving ADAM checkpoint...
Sum of params:83.21041
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.01467739057244
Iteration: 2 || Loss: 6.987598381379363
Iteration: 3 || Loss: 6.88620907210254
Iteration: 4 || Loss: 6.752302697013522
Iteration: 5 || Loss: 6.746596753303573
Iteration: 6 || Loss: 6.722710167998952
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.22308
Epoch 270 loss:6.722710167998952
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.22308
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.421603668147046
Iteration: 2 || Loss: 22.42125048492687
Iteration: 3 || Loss: 22.420899362877268
Iteration: 4 || Loss: 22.420549439319828
Iteration: 5 || Loss: 22.420200249208964
Iteration: 6 || Loss: 22.420200249208964
saving ADAM checkpoint...
Sum of params:83.22305
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.420200249208964
Iteration: 2 || Loss: 22.388123700289825
Iteration: 3 || Loss: 22.15998709318249
Iteration: 4 || Loss: 21.998098363547673
Iteration: 5 || Loss: 21.966060766314136
Iteration: 6 || Loss: 21.916995955567902
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.12062
Epoch 270 loss:21.916995955567902
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.575180484875361
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.20019365919176
waveform batch: 2/2
Test loss - extrapolation:18.336378536870292
Epoch 270 mean train loss:1.0536968556106303
Epoch 270 mean test loss - interpolation:1.0958634141458934
Epoch 270 mean test loss - extrapolation:5.211381016338504
Start training epoch 271
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.12062
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.164842982309204
Iteration: 2 || Loss: 3.1634104041467266
Iteration: 3 || Loss: 3.16198038159103
Iteration: 4 || Loss: 3.1605514377391892
Iteration: 5 || Loss: 3.1591231621322864
Iteration: 6 || Loss: 3.1591231621322864
saving ADAM checkpoint...
Sum of params:83.12063
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.1591231621322864
Iteration: 2 || Loss: 2.5090283267129547
Iteration: 3 || Loss: 1.9801974090435244
Iteration: 4 || Loss: 1.9754520280927539
Iteration: 5 || Loss: 1.9318582739522774
Iteration: 6 || Loss: 1.9125260373251125
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.222664
Epoch 271 loss:1.9125260373251125
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.222664
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.003309388414279
Iteration: 2 || Loss: 7.002972847232968
Iteration: 3 || Loss: 7.002637083368393
Iteration: 4 || Loss: 7.002301690436102
Iteration: 5 || Loss: 7.0019687237065105
Iteration: 6 || Loss: 7.0019687237065105
saving ADAM checkpoint...
Sum of params:83.222786
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.0019687237065105
Iteration: 2 || Loss: 6.975096193261965
Iteration: 3 || Loss: 6.874252962862325
Iteration: 4 || Loss: 6.74131456865045
Iteration: 5 || Loss: 6.735619333638118
Iteration: 6 || Loss: 6.712274863228109
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.23561
Epoch 271 loss:6.712274863228109
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.23561
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.395148601953807
Iteration: 2 || Loss: 22.39479510639327
Iteration: 3 || Loss: 22.39444392300362
Iteration: 4 || Loss: 22.394092641464724
Iteration: 5 || Loss: 22.393741734636993
Iteration: 6 || Loss: 22.393741734636993
saving ADAM checkpoint...
Sum of params:83.23558
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.393741734636993
Iteration: 2 || Loss: 22.36149155123206
Iteration: 3 || Loss: 22.134044904172693
Iteration: 4 || Loss: 21.973117852472953
Iteration: 5 || Loss: 21.940993207887587
Iteration: 6 || Loss: 21.892819266359314
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.13338
Epoch 271 loss:21.892819266359314
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.565330819295652
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.175725313368936
waveform batch: 2/2
Test loss - extrapolation:18.32161484783016
Epoch 271 mean train loss:1.0523317298935357
Epoch 271 mean test loss - interpolation:1.094221803215942
Epoch 271 mean test loss - extrapolation:5.2081116800999245
Start training epoch 272
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.13338
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.1534499849162585
Iteration: 2 || Loss: 3.152020842606378
Iteration: 3 || Loss: 3.150591666217808
Iteration: 4 || Loss: 3.1491631187056095
Iteration: 5 || Loss: 3.147738569392442
Iteration: 6 || Loss: 3.147738569392442
saving ADAM checkpoint...
Sum of params:83.133385
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.147738569392442
Iteration: 2 || Loss: 2.5011256463045166
Iteration: 3 || Loss: 1.9742858576360605
Iteration: 4 || Loss: 1.9696139228718599
Iteration: 5 || Loss: 1.9263128780398011
Iteration: 6 || Loss: 1.90772275973984
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.23497
Epoch 272 loss:1.90772275973984
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.23497
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.991063691358984
Iteration: 2 || Loss: 6.990728003071659
Iteration: 3 || Loss: 6.990393569752792
Iteration: 4 || Loss: 6.990059056671798
Iteration: 5 || Loss: 6.989727672956468
Iteration: 6 || Loss: 6.989727672956468
saving ADAM checkpoint...
Sum of params:83.235085
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.989727672956468
Iteration: 2 || Loss: 6.963047400676742
Iteration: 3 || Loss: 6.862745782283678
Iteration: 4 || Loss: 6.730724266937127
Iteration: 5 || Loss: 6.725030577612891
Iteration: 6 || Loss: 6.702176203943099
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.24812
Epoch 272 loss:6.702176203943099
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.24812
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.369019376262337
Iteration: 2 || Loss: 22.368664075214326
Iteration: 3 || Loss: 22.368310165700766
Iteration: 4 || Loss: 22.367959080515565
Iteration: 5 || Loss: 22.367607693069118
Iteration: 6 || Loss: 22.367607693069118
saving ADAM checkpoint...
Sum of params:83.24809
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.367607693069118
Iteration: 2 || Loss: 22.33519093773117
Iteration: 3 || Loss: 22.10834732182128
Iteration: 4 || Loss: 21.948340889294943
Iteration: 5 || Loss: 21.91614813923176
Iteration: 6 || Loss: 21.86881076622952
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.14607
Epoch 272 loss:21.86881076622952
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.555802720522769
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.151279846339186
waveform batch: 2/2
Test loss - extrapolation:18.30670443665548
Epoch 272 mean train loss:1.0509899906866365
Epoch 272 mean test loss - interpolation:1.092633786753795
Epoch 272 mean test loss - extrapolation:5.204832023582889
Start training epoch 273
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.14607
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.1426630136793934
Iteration: 2 || Loss: 3.1412328782083767
Iteration: 3 || Loss: 3.139806865377993
Iteration: 4 || Loss: 3.1383820909648286
Iteration: 5 || Loss: 3.136959857956376
Iteration: 6 || Loss: 3.136959857956376
saving ADAM checkpoint...
Sum of params:83.14607
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.136959857956376
Iteration: 2 || Loss: 2.4934635734937003
Iteration: 3 || Loss: 1.9685778807488326
Iteration: 4 || Loss: 1.963977598457395
Iteration: 5 || Loss: 1.9209527999450058
Iteration: 6 || Loss: 1.9030749489514274
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.24722
Epoch 273 loss:1.9030749489514274
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.24722
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.9792420564888875
Iteration: 2 || Loss: 6.978906554846741
Iteration: 3 || Loss: 6.9785718667544
Iteration: 4 || Loss: 6.97823885447897
Iteration: 5 || Loss: 6.977907696240342
Iteration: 6 || Loss: 6.977907696240342
saving ADAM checkpoint...
Sum of params:83.24734
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.977907696240342
Iteration: 2 || Loss: 6.95139695969551
Iteration: 3 || Loss: 6.851627461642827
Iteration: 4 || Loss: 6.7204816968415
Iteration: 5 || Loss: 6.714786348963126
Iteration: 6 || Loss: 6.692387744607528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.26059
Epoch 273 loss:6.692387744607528
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.26059
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.343062852235438
Iteration: 2 || Loss: 22.342705628193453
Iteration: 3 || Loss: 22.342352556831877
Iteration: 4 || Loss: 22.34200000600347
Iteration: 5 || Loss: 22.3416464318808
Iteration: 6 || Loss: 22.3416464318808
saving ADAM checkpoint...
Sum of params:83.260544
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.3416464318808
Iteration: 2 || Loss: 22.309037124220886
Iteration: 3 || Loss: 22.08292622615995
Iteration: 4 || Loss: 21.923791430365682
Iteration: 5 || Loss: 21.891539295457648
Iteration: 6 || Loss: 21.845005173082242
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.15878
Epoch 273 loss:21.845005173082242
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.5465121477202475
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.126743057125815
waveform batch: 2/2
Test loss - extrapolation:18.291711257219276
Epoch 273 mean train loss:1.0496713057462483
Epoch 273 mean test loss - interpolation:1.0910853579533746
Epoch 273 mean test loss - extrapolation:5.201537859528758
Start training epoch 274
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.15878
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.132126076882193
Iteration: 2 || Loss: 3.130694723882495
Iteration: 3 || Loss: 3.1292691723556785
Iteration: 4 || Loss: 3.127845347747574
Iteration: 5 || Loss: 3.1264193539144864
Iteration: 6 || Loss: 3.1264193539144864
saving ADAM checkpoint...
Sum of params:83.15879
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.1264193539144864
Iteration: 2 || Loss: 2.486018044275898
Iteration: 3 || Loss: 1.9630433360004034
Iteration: 4 || Loss: 1.9585141131543828
Iteration: 5 || Loss: 1.9158079064328772
Iteration: 6 || Loss: 1.8985615681089736
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.25949
Epoch 274 loss:1.8985615681089736
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.25949
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.967817448783881
Iteration: 2 || Loss: 6.967481688836778
Iteration: 3 || Loss: 6.967146326087926
Iteration: 4 || Loss: 6.966812669770006
Iteration: 5 || Loss: 6.966481791320087
Iteration: 6 || Loss: 6.966481791320087
saving ADAM checkpoint...
Sum of params:83.25961
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.966481791320087
Iteration: 2 || Loss: 6.940118028449404
Iteration: 3 || Loss: 6.840851485703173
Iteration: 4 || Loss: 6.71053403605147
Iteration: 5 || Loss: 6.704836700104525
Iteration: 6 || Loss: 6.682869365044916
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.27309
Epoch 274 loss:6.682869365044916
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.27309
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.317497656477464
Iteration: 2 || Loss: 22.31714117172264
Iteration: 3 || Loss: 22.31678561263584
Iteration: 4 || Loss: 22.316431333913314
Iteration: 5 || Loss: 22.31607828786687
Iteration: 6 || Loss: 22.31607828786687
saving ADAM checkpoint...
Sum of params:83.27304
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.31607828786687
Iteration: 2 || Loss: 22.283286093804527
Iteration: 3 || Loss: 22.057817220964818
Iteration: 4 || Loss: 21.899500763962816
Iteration: 5 || Loss: 21.86720098646319
Iteration: 6 || Loss: 21.821415737404976
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.171486
Epoch 274 loss:21.821415737404976
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.537522776732204
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.10226976808783
waveform batch: 2/2
Test loss - extrapolation:18.27662076264762
Epoch 274 mean train loss:1.0483740231227194
Epoch 274 mean test loss - interpolation:1.0895871294553674
Epoch 274 mean test loss - extrapolation:5.198240877561287
Start training epoch 275
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.171486
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.122206435981659
Iteration: 2 || Loss: 3.1207815132154404
Iteration: 3 || Loss: 3.119355998294199
Iteration: 4 || Loss: 3.1179290889321027
Iteration: 5 || Loss: 3.1165071578740036
Iteration: 6 || Loss: 3.1165071578740036
saving ADAM checkpoint...
Sum of params:83.17148
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.1165071578740036
Iteration: 2 || Loss: 2.4788806660337244
Iteration: 3 || Loss: 1.9576823388317028
Iteration: 4 || Loss: 1.953221837501828
Iteration: 5 || Loss: 1.91081559777646
Iteration: 6 || Loss: 1.8941775212994576
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.27178
Epoch 275 loss:1.8941775212994576
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.27178
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.956724407837329
Iteration: 2 || Loss: 6.956390707633836
Iteration: 3 || Loss: 6.956055903271135
Iteration: 4 || Loss: 6.955723864209607
Iteration: 5 || Loss: 6.955392263426863
Iteration: 6 || Loss: 6.955392263426863
saving ADAM checkpoint...
Sum of params:83.2719
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.955392263426863
Iteration: 2 || Loss: 6.929161484501828
Iteration: 3 || Loss: 6.830387936148592
Iteration: 4 || Loss: 6.700860023404521
Iteration: 5 || Loss: 6.695159044304912
Iteration: 6 || Loss: 6.673599954778244
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.28563
Epoch 275 loss:6.673599954778244
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.28563
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.29226314120802
Iteration: 2 || Loss: 22.29190424451104
Iteration: 3 || Loss: 22.29154824492486
Iteration: 4 || Loss: 22.291192315421895
Iteration: 5 || Loss: 22.290839148001947
Iteration: 6 || Loss: 22.290839148001947
saving ADAM checkpoint...
Sum of params:83.28558
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.290839148001947
Iteration: 2 || Loss: 22.257880358796772
Iteration: 3 || Loss: 22.03295919737367
Iteration: 4 || Loss: 21.87544828514354
Iteration: 5 || Loss: 21.843094439284986
Iteration: 6 || Loss: 21.797997742988173
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.18419
Epoch 275 loss:21.797997742988173
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.528746358719616
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.077869497412294
waveform batch: 2/2
Test loss - extrapolation:18.261447140295903
Epoch 275 mean train loss:1.047095697209168
Epoch 275 mean test loss - interpolation:1.088124393119936
Epoch 275 mean test loss - extrapolation:5.1949430531423495
Start training epoch 276
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.18419
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.1127696210907114
Iteration: 2 || Loss: 3.1113432895991755
Iteration: 3 || Loss: 3.1099192955595334
Iteration: 4 || Loss: 3.1084956269491646
Iteration: 5 || Loss: 3.107077158840482
Iteration: 6 || Loss: 3.107077158840482
saving ADAM checkpoint...
Sum of params:83.1842
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.107077158840482
Iteration: 2 || Loss: 2.471857440490762
Iteration: 3 || Loss: 1.9524958474880374
Iteration: 4 || Loss: 1.9481039785005365
Iteration: 5 || Loss: 1.9059579108708469
Iteration: 6 || Loss: 1.889925299771206
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.28407
Epoch 276 loss:1.889925299771206
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.28407
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.9459783760261775
Iteration: 2 || Loss: 6.9456434917468615
Iteration: 3 || Loss: 6.945311012982082
Iteration: 4 || Loss: 6.944979796316647
Iteration: 5 || Loss: 6.9446485621225
Iteration: 6 || Loss: 6.9446485621225
saving ADAM checkpoint...
Sum of params:83.28419
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.9446485621225
Iteration: 2 || Loss: 6.918519939778638
Iteration: 3 || Loss: 6.820227592652725
Iteration: 4 || Loss: 6.69146266562983
Iteration: 5 || Loss: 6.6857564006568815
Iteration: 6 || Loss: 6.664581189717547
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.29817
Epoch 276 loss:6.664581189717547
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.29817
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.26723198054346
Iteration: 2 || Loss: 22.266873309818127
Iteration: 3 || Loss: 22.266516033603125
Iteration: 4 || Loss: 22.266158161687237
Iteration: 5 || Loss: 22.265803570543593
Iteration: 6 || Loss: 22.265803570543593
saving ADAM checkpoint...
Sum of params:83.29813
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.265803570543593
Iteration: 2 || Loss: 22.23264713919625
Iteration: 3 || Loss: 22.00835769245455
Iteration: 4 || Loss: 21.851601439054132
Iteration: 5 || Loss: 21.819212579972422
Iteration: 6 || Loss: 21.774785870337386
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.19693
Epoch 276 loss:21.774785870337386
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.520174272571112
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.053391609814874
waveform batch: 2/2
Test loss - extrapolation:18.246227884418598
Epoch 276 mean train loss:1.0458376675802117
Epoch 276 mean test loss - interpolation:1.0866957120951855
Epoch 276 mean test loss - extrapolation:5.191634957852789
Start training epoch 277
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.19693
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.1035143598989823
Iteration: 2 || Loss: 3.1020905831135948
Iteration: 3 || Loss: 3.1006692873927415
Iteration: 4 || Loss: 3.0992462628886654
Iteration: 5 || Loss: 3.0978250923961137
Iteration: 6 || Loss: 3.0978250923961137
saving ADAM checkpoint...
Sum of params:83.196915
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0978250923961137
Iteration: 2 || Loss: 2.465095255157723
Iteration: 3 || Loss: 1.9474463030620126
Iteration: 4 || Loss: 1.9431223836330254
Iteration: 5 || Loss: 1.9012193031253344
Iteration: 6 || Loss: 1.88578291411711
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.29641
Epoch 277 loss:1.88578291411711
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.29641
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.935528026233291
Iteration: 2 || Loss: 6.935192595549293
Iteration: 3 || Loss: 6.934859257892546
Iteration: 4 || Loss: 6.934527144004241
Iteration: 5 || Loss: 6.93419805894056
Iteration: 6 || Loss: 6.93419805894056
saving ADAM checkpoint...
Sum of params:83.29651
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.93419805894056
Iteration: 2 || Loss: 6.908165942737279
Iteration: 3 || Loss: 6.8103434399157665
Iteration: 4 || Loss: 6.682297992836045
Iteration: 5 || Loss: 6.676584667306697
Iteration: 6 || Loss: 6.655779757805107
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.31077
Epoch 277 loss:6.655779757805107
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.31077
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.242461248336408
Iteration: 2 || Loss: 22.242101021825487
Iteration: 3 || Loss: 22.24174394779724
Iteration: 4 || Loss: 22.241386269037925
Iteration: 5 || Loss: 22.241029321540353
Iteration: 6 || Loss: 22.241029321540353
saving ADAM checkpoint...
Sum of params:83.31074
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.241029321540353
Iteration: 2 || Loss: 22.207707682728635
Iteration: 3 || Loss: 21.984005094662546
Iteration: 4 || Loss: 21.827988127058212
Iteration: 5 || Loss: 21.795554578128055
Iteration: 6 || Loss: 21.751761884590525
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.20967
Epoch 277 loss:21.751761884590525
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.511842412544654
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.02903848918462
waveform batch: 2/2
Test loss - extrapolation:18.23094464929346
Epoch 277 mean train loss:1.0445973985004395
Epoch 277 mean test loss - interpolation:1.0853070687574424
Epoch 277 mean test loss - extrapolation:5.188331928206506
Start training epoch 278
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.20967
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0948650303469343
Iteration: 2 || Loss: 3.0934422634636207
Iteration: 3 || Loss: 3.0920229116088147
Iteration: 4 || Loss: 3.0905996459608644
Iteration: 5 || Loss: 3.089181192893377
Iteration: 6 || Loss: 3.089181192893377
saving ADAM checkpoint...
Sum of params:83.20967
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.089181192893377
Iteration: 2 || Loss: 2.4585095760364157
Iteration: 3 || Loss: 1.9425496927492139
Iteration: 4 || Loss: 1.938290294736861
Iteration: 5 || Loss: 1.8966029527063366
Iteration: 6 || Loss: 1.881753419579494
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.30875
Epoch 278 loss:1.881753419579494
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.30875
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.925382790075185
Iteration: 2 || Loss: 6.925047873591465
Iteration: 3 || Loss: 6.924715755324572
Iteration: 4 || Loss: 6.924385536404877
Iteration: 5 || Loss: 6.924054125885413
Iteration: 6 || Loss: 6.924054125885413
saving ADAM checkpoint...
Sum of params:83.30886
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.924054125885413
Iteration: 2 || Loss: 6.898092413363485
Iteration: 3 || Loss: 6.800727143697806
Iteration: 4 || Loss: 6.673373069802678
Iteration: 5 || Loss: 6.6676506973004885
Iteration: 6 || Loss: 6.647193335134885
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.323395
Epoch 278 loss:6.647193335134885
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.323395
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.21790515176751
Iteration: 2 || Loss: 22.217544291228215
Iteration: 3 || Loss: 22.21718535190567
Iteration: 4 || Loss: 22.216826264921078
Iteration: 5 || Loss: 22.216469733673613
Iteration: 6 || Loss: 22.216469733673613
saving ADAM checkpoint...
Sum of params:83.32334
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.216469733673613
Iteration: 2 || Loss: 22.1830115225269
Iteration: 3 || Loss: 21.95983695856637
Iteration: 4 || Loss: 21.80455981880046
Iteration: 5 || Loss: 21.772081175256105
Iteration: 6 || Loss: 21.728870510748457
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.22241
Epoch 278 loss:21.728870510748457
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.503715492384075
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:44.00481686428579
waveform batch: 2/2
Test loss - extrapolation:18.215587591110832
Epoch 278 mean train loss:1.0433730091538909
Epoch 278 mean test loss - interpolation:1.0839525820640126
Epoch 278 mean test loss - extrapolation:5.1850337046163855
Start training epoch 279
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.22241
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0867541362002213
Iteration: 2 || Loss: 3.0853336744009234
Iteration: 3 || Loss: 3.083912904802758
Iteration: 4 || Loss: 3.0824936314504265
Iteration: 5 || Loss: 3.081073921630734
Iteration: 6 || Loss: 3.081073921630734
saving ADAM checkpoint...
Sum of params:83.22242
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.081073921630734
Iteration: 2 || Loss: 2.452059276346545
Iteration: 3 || Loss: 1.9378091918956413
Iteration: 4 || Loss: 1.9336146105447853
Iteration: 5 || Loss: 1.8921894548799387
Iteration: 6 || Loss: 1.8778435293227795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.3211
Epoch 279 loss:1.8778435293227795
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.3211
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.915521507793162
Iteration: 2 || Loss: 6.915186888626907
Iteration: 3 || Loss: 6.91485567765766
Iteration: 4 || Loss: 6.914523730861549
Iteration: 5 || Loss: 6.91419305912624
Iteration: 6 || Loss: 6.91419305912624
saving ADAM checkpoint...
Sum of params:83.321205
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.91419305912624
Iteration: 2 || Loss: 6.888292209396623
Iteration: 3 || Loss: 6.791372912020445
Iteration: 4 || Loss: 6.664683929149875
Iteration: 5 || Loss: 6.658952230236784
Iteration: 6 || Loss: 6.638822985248467
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.33604
Epoch 279 loss:6.638822985248467
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.33604
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.19362013921835
Iteration: 2 || Loss: 22.193257864706634
Iteration: 3 || Loss: 22.19289902788364
Iteration: 4 || Loss: 22.192538766591927
Iteration: 5 || Loss: 22.19218216466423
Iteration: 6 || Loss: 22.19218216466423
saving ADAM checkpoint...
Sum of params:83.336
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.19218216466423
Iteration: 2 || Loss: 22.15853046869582
Iteration: 3 || Loss: 21.935901617164465
Iteration: 4 || Loss: 21.78130666104344
Iteration: 5 || Loss: 21.748810068800964
Iteration: 6 || Loss: 21.706183928198236
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.23522
Epoch 279 loss:21.706183928198236
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.4957278110448735
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.98041392852218
waveform batch: 2/2
Test loss - extrapolation:18.20022486883479
Epoch 279 mean train loss:1.0421672566472235
Epoch 279 mean test loss - interpolation:1.0826213018408122
Epoch 279 mean test loss - extrapolation:5.181719899779748
Start training epoch 280
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.23522
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.078514987801906
Iteration: 2 || Loss: 3.077094521590816
Iteration: 3 || Loss: 3.07567306683622
Iteration: 4 || Loss: 3.074254616695768
Iteration: 5 || Loss: 3.0728375716101297
Iteration: 6 || Loss: 3.0728375716101297
saving ADAM checkpoint...
Sum of params:83.235214
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0728375716101297
Iteration: 2 || Loss: 2.44584360038159
Iteration: 3 || Loss: 1.9331694568141298
Iteration: 4 || Loss: 1.9290345926595385
Iteration: 5 || Loss: 1.8878309397042903
Iteration: 6 || Loss: 1.8740167385641733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.333496
Epoch 280 loss:1.8740167385641733
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.333496
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.90589886571292
Iteration: 2 || Loss: 6.9055648870365065
Iteration: 3 || Loss: 6.9052321158304375
Iteration: 4 || Loss: 6.904901095127451
Iteration: 5 || Loss: 6.904570367746189
Iteration: 6 || Loss: 6.904570367746189
saving ADAM checkpoint...
Sum of params:83.333534
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.904570367746189
Iteration: 2 || Loss: 6.878726444872118
Iteration: 3 || Loss: 6.78223688982745
Iteration: 4 || Loss: 6.656177797884523
Iteration: 5 || Loss: 6.650436008660769
Iteration: 6 || Loss: 6.630624891781543
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.348694
Epoch 280 loss:6.630624891781543
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.348694
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.169600587175584
Iteration: 2 || Loss: 22.169237434670332
Iteration: 3 || Loss: 22.168876181833333
Iteration: 4 || Loss: 22.16851614681237
Iteration: 5 || Loss: 22.168159209843672
Iteration: 6 || Loss: 22.168159209843672
saving ADAM checkpoint...
Sum of params:83.34863
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.168159209843672
Iteration: 2 || Loss: 22.13435967620634
Iteration: 3 || Loss: 21.91219645174734
Iteration: 4 || Loss: 21.75828165551679
Iteration: 5 || Loss: 21.725750306587646
Iteration: 6 || Loss: 21.683661473809227
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.24798
Epoch 280 loss:21.683661473809227
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.487941688389483
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.956205346102465
waveform batch: 2/2
Test loss - extrapolation:18.18481097192467
Epoch 280 mean train loss:1.0409759691087912
Epoch 280 mean test loss - interpolation:1.0813236147315806
Epoch 280 mean test loss - extrapolation:5.1784180265022615
Start training epoch 281
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.24798
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0709058403763585
Iteration: 2 || Loss: 3.0694857316718434
Iteration: 3 || Loss: 3.068065804589223
Iteration: 4 || Loss: 3.0666476651981895
Iteration: 5 || Loss: 3.065228562157916
Iteration: 6 || Loss: 3.065228562157916
saving ADAM checkpoint...
Sum of params:83.247955
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.065228562157916
Iteration: 2 || Loss: 2.4397475628259455
Iteration: 3 || Loss: 1.9286748134531302
Iteration: 4 || Loss: 1.9246005197328546
Iteration: 5 || Loss: 1.8836360635512148
Iteration: 6 || Loss: 1.870294590126933
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.345856
Epoch 281 loss:1.870294590126933
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.345856
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.896516351095719
Iteration: 2 || Loss: 6.896183431482649
Iteration: 3 || Loss: 6.895851497853826
Iteration: 4 || Loss: 6.895520775501997
Iteration: 5 || Loss: 6.895190202460811
Iteration: 6 || Loss: 6.895190202460811
saving ADAM checkpoint...
Sum of params:83.345894
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.895190202460811
Iteration: 2 || Loss: 6.86939308878667
Iteration: 3 || Loss: 6.773328850194354
Iteration: 4 || Loss: 6.647879791111214
Iteration: 5 || Loss: 6.642127187025423
Iteration: 6 || Loss: 6.622616085891212
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.36134
Epoch 281 loss:6.622616085891212
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.36134
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.14563534496839
Iteration: 2 || Loss: 22.14527134497694
Iteration: 3 || Loss: 22.144909689261265
Iteration: 4 || Loss: 22.14455013661673
Iteration: 5 || Loss: 22.144190961895497
Iteration: 6 || Loss: 22.144190961895497
saving ADAM checkpoint...
Sum of params:83.36128
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.144190961895497
Iteration: 2 || Loss: 22.110241609207925
Iteration: 3 || Loss: 21.888663893190355
Iteration: 4 || Loss: 21.7354159664656
Iteration: 5 || Loss: 21.70285812822664
Iteration: 6 || Loss: 21.66129309699857
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.260735
Epoch 281 loss:21.66129309699857
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.480310916272868
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.932004374376945
waveform batch: 2/2
Test loss - extrapolation:18.169367697343358
Epoch 281 mean train loss:1.0398001301040247
Epoch 281 mean test loss - interpolation:1.0800518193788113
Epoch 281 mean test loss - extrapolation:5.175114339310025
Start training epoch 282
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.260735
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0635269984159
Iteration: 2 || Loss: 3.062105250393007
Iteration: 3 || Loss: 3.060686434532231
Iteration: 4 || Loss: 3.0592711491638402
Iteration: 5 || Loss: 3.0578544280498616
Iteration: 6 || Loss: 3.0578544280498616
saving ADAM checkpoint...
Sum of params:83.260735
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0578544280498616
Iteration: 2 || Loss: 2.433798470122391
Iteration: 3 || Loss: 1.924290959868874
Iteration: 4 || Loss: 1.9202751173634882
Iteration: 5 || Loss: 1.8795369234273955
Iteration: 6 || Loss: 1.8666625162068065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.35825
Epoch 282 loss:1.8666625162068065
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.35825
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.887383369177861
Iteration: 2 || Loss: 6.887048323578831
Iteration: 3 || Loss: 6.886716146554972
Iteration: 4 || Loss: 6.886385791522176
Iteration: 5 || Loss: 6.886056121523856
Iteration: 6 || Loss: 6.886056121523856
saving ADAM checkpoint...
Sum of params:83.358284
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.886056121523856
Iteration: 2 || Loss: 6.860291769774086
Iteration: 3 || Loss: 6.764637781433501
Iteration: 4 || Loss: 6.6397651039988315
Iteration: 5 || Loss: 6.634001812556441
Iteration: 6 || Loss: 6.614778502428136
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.37403
Epoch 282 loss:6.614778502428136
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.37403
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.122054707679133
Iteration: 2 || Loss: 22.121690395065446
Iteration: 3 || Loss: 22.1213270630516
Iteration: 4 || Loss: 22.120965357034425
Iteration: 5 || Loss: 22.12060523004207
Iteration: 6 || Loss: 22.12060523004207
saving ADAM checkpoint...
Sum of params:83.373985
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.12060523004207
Iteration: 2 || Loss: 22.086469541806803
Iteration: 3 || Loss: 21.865385162945447
Iteration: 4 || Loss: 21.712750465587003
Iteration: 5 || Loss: 21.680181563690784
Iteration: 6 || Loss: 21.639128773021977
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.273544
Epoch 282 loss:21.639128773021977
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.47277080755062
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.90769593676589
waveform batch: 2/2
Test loss - extrapolation:18.15395018727092
Epoch 282 mean train loss:1.038640337643342
Epoch 282 mean test loss - interpolation:1.07879513459177
Epoch 282 mean test loss - extrapolation:5.171803843669735
Start training epoch 283
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.273544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.056015876696358
Iteration: 2 || Loss: 3.0545967107369836
Iteration: 3 || Loss: 3.0531795081533226
Iteration: 4 || Loss: 3.0517582205043117
Iteration: 5 || Loss: 3.0503441424439273
Iteration: 6 || Loss: 3.0503441424439273
saving ADAM checkpoint...
Sum of params:83.27355
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0503441424439273
Iteration: 2 || Loss: 2.4280000107444812
Iteration: 3 || Loss: 1.9200026691188679
Iteration: 4 || Loss: 1.916042007448773
Iteration: 5 || Loss: 1.8755353197654223
Iteration: 6 || Loss: 1.863104868401344
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.3707
Epoch 283 loss:1.863104868401344
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.3707
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.8784586627469295
Iteration: 2 || Loss: 6.878124199720509
Iteration: 3 || Loss: 6.877791985440277
Iteration: 4 || Loss: 6.877461219753217
Iteration: 5 || Loss: 6.877131833009216
Iteration: 6 || Loss: 6.877131833009216
saving ADAM checkpoint...
Sum of params:83.370735
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.877131833009216
Iteration: 2 || Loss: 6.851385049323537
Iteration: 3 || Loss: 6.756125923476576
Iteration: 4 || Loss: 6.631806647694084
Iteration: 5 || Loss: 6.626033176911286
Iteration: 6 || Loss: 6.607089335224043
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.38677
Epoch 283 loss:6.607089335224043
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.38677
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.098627807665743
Iteration: 2 || Loss: 22.098262996295258
Iteration: 3 || Loss: 22.097898806277037
Iteration: 4 || Loss: 22.097535797304943
Iteration: 5 || Loss: 22.09717406181014
Iteration: 6 || Loss: 22.09717406181014
saving ADAM checkpoint...
Sum of params:83.386734
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.09717406181014
Iteration: 2 || Loss: 22.06288316824429
Iteration: 3 || Loss: 21.84229471556908
Iteration: 4 || Loss: 21.690285787585566
Iteration: 5 || Loss: 21.6576994572876
Iteration: 6 || Loss: 21.617123307385693
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.28639
Epoch 283 loss:21.617123307385693
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.46545535557686
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.88359446782728
waveform batch: 2/2
Test loss - extrapolation:18.138483864430885
Epoch 283 mean train loss:1.037493707276244
Epoch 283 mean test loss - interpolation:1.0775758925961434
Epoch 283 mean test loss - extrapolation:5.16850652768818
Start training epoch 284
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.28639
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.049173337788405
Iteration: 2 || Loss: 3.0477530964678996
Iteration: 3 || Loss: 3.0463349239529185
Iteration: 4 || Loss: 3.0449195533839073
Iteration: 5 || Loss: 3.043504293421097
Iteration: 6 || Loss: 3.043504293421097
saving ADAM checkpoint...
Sum of params:83.2864
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.043504293421097
Iteration: 2 || Loss: 2.42239977358247
Iteration: 3 || Loss: 1.9158344147441724
Iteration: 4 || Loss: 1.911927145171708
Iteration: 5 || Loss: 1.8716315312039544
Iteration: 6 || Loss: 1.8596366499454435
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.383156
Epoch 284 loss:1.8596366499454435
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.383156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.8697364835854
Iteration: 2 || Loss: 6.869402028375347
Iteration: 3 || Loss: 6.869068741820053
Iteration: 4 || Loss: 6.86873871076562
Iteration: 5 || Loss: 6.868409241148296
Iteration: 6 || Loss: 6.868409241148296
saving ADAM checkpoint...
Sum of params:83.383194
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.868409241148296
Iteration: 2 || Loss: 6.842675696808661
Iteration: 3 || Loss: 6.747801551010017
Iteration: 4 || Loss: 6.624021555169275
Iteration: 5 || Loss: 6.6182383073750035
Iteration: 6 || Loss: 6.599567110293723
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.39952
Epoch 284 loss:6.599567110293723
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.39952
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.075353558168434
Iteration: 2 || Loss: 22.074988558635244
Iteration: 3 || Loss: 22.07462454959363
Iteration: 4 || Loss: 22.074259932510753
Iteration: 5 || Loss: 22.073897492348177
Iteration: 6 || Loss: 22.073897492348177
saving ADAM checkpoint...
Sum of params:83.399506
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.073897492348177
Iteration: 2 || Loss: 22.039458565339846
Iteration: 3 || Loss: 21.819354837766063
Iteration: 4 || Loss: 21.6679732955192
Iteration: 5 || Loss: 21.635368990153932
Iteration: 6 || Loss: 21.595249998687123
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.29921
Epoch 284 loss:21.595249998687123
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.458249205925174
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.85953002541853
waveform batch: 2/2
Test loss - extrapolation:18.123014494911317
Epoch 284 mean train loss:1.0363604744457342
Epoch 284 mean test loss - interpolation:1.0763748676541958
Epoch 284 mean test loss - extrapolation:5.16521204336082
Start training epoch 285
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.29921
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0425171020906983
Iteration: 2 || Loss: 3.0410964007809613
Iteration: 3 || Loss: 3.039676305313649
Iteration: 4 || Loss: 3.038260294783268
Iteration: 5 || Loss: 3.036845883544887
Iteration: 6 || Loss: 3.036845883544887
saving ADAM checkpoint...
Sum of params:83.29922
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.036845883544887
Iteration: 2 || Loss: 2.416889944224105
Iteration: 3 || Loss: 1.9117721151975693
Iteration: 4 || Loss: 1.907917642531631
Iteration: 5 || Loss: 1.8678050461824836
Iteration: 6 || Loss: 1.8562526457937827
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.39565
Epoch 285 loss:1.8562526457937827
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.39565
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.861212998510906
Iteration: 2 || Loss: 6.860877806486203
Iteration: 3 || Loss: 6.860546699485247
Iteration: 4 || Loss: 6.860215016607072
Iteration: 5 || Loss: 6.859885020830708
Iteration: 6 || Loss: 6.859885020830708
saving ADAM checkpoint...
Sum of params:83.39567
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.859885020830708
Iteration: 2 || Loss: 6.834161757798381
Iteration: 3 || Loss: 6.739678795660177
Iteration: 4 || Loss: 6.616411435470111
Iteration: 5 || Loss: 6.610613596793079
Iteration: 6 || Loss: 6.5921923972819005
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.41234
Epoch 285 loss:6.5921923972819005
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.41234
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.05240125707304
Iteration: 2 || Loss: 22.05203487028077
Iteration: 3 || Loss: 22.05166879711626
Iteration: 4 || Loss: 22.051303169847966
Iteration: 5 || Loss: 22.050940952605362
Iteration: 6 || Loss: 22.050940952605362
saving ADAM checkpoint...
Sum of params:83.41229
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.050940952605362
Iteration: 2 || Loss: 22.016321239995975
Iteration: 3 || Loss: 21.796631652349376
Iteration: 4 || Loss: 21.64583337938829
Iteration: 5 || Loss: 21.61323544093906
Iteration: 6 || Loss: 21.57356376718156
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.31209
Epoch 285 loss:21.57356376718156
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.451172542189544
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.83542285636277
waveform batch: 2/2
Test loss - extrapolation:18.10755411667078
Epoch 285 mean train loss:1.0352416831123188
Epoch 285 mean test loss - interpolation:1.0751954236982573
Epoch 285 mean test loss - extrapolation:5.161914747752796
Start training epoch 286
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.31209
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.035878092121071
Iteration: 2 || Loss: 3.0344590328903935
Iteration: 3 || Loss: 3.0330408885618905
Iteration: 4 || Loss: 3.031625414722782
Iteration: 5 || Loss: 3.0302092282011017
Iteration: 6 || Loss: 3.0302092282011017
saving ADAM checkpoint...
Sum of params:83.3121
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0302092282011017
Iteration: 2 || Loss: 2.4115835682520324
Iteration: 3 || Loss: 1.9077898320616211
Iteration: 4 || Loss: 1.9039856135160775
Iteration: 5 || Loss: 1.8640986105933752
Iteration: 6 || Loss: 1.8529338594132272
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.40816
Epoch 286 loss:1.8529338594132272
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.40816
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.852854483762101
Iteration: 2 || Loss: 6.852520530630952
Iteration: 3 || Loss: 6.852188162236744
Iteration: 4 || Loss: 6.851855965732048
Iteration: 5 || Loss: 6.851526003882004
Iteration: 6 || Loss: 6.851526003882004
saving ADAM checkpoint...
Sum of params:83.40821
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.851526003882004
Iteration: 2 || Loss: 6.825813840451269
Iteration: 3 || Loss: 6.731698455302687
Iteration: 4 || Loss: 6.60892935968623
Iteration: 5 || Loss: 6.603120446424131
Iteration: 6 || Loss: 6.584949487026998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.42517
Epoch 286 loss:6.584949487026998
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.42517
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.029568588172097
Iteration: 2 || Loss: 22.02920066786415
Iteration: 3 || Loss: 22.02883625842002
Iteration: 4 || Loss: 22.028469771149332
Iteration: 5 || Loss: 22.028105250091553
Iteration: 6 || Loss: 22.028105250091553
saving ADAM checkpoint...
Sum of params:83.425125
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.028105250091553
Iteration: 2 || Loss: 21.99334629104295
Iteration: 3 || Loss: 21.774090032891422
Iteration: 4 || Loss: 21.623879699775383
Iteration: 5 || Loss: 21.59126972475129
Iteration: 6 || Loss: 21.55201780648102
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.325
Epoch 286 loss:21.55201780648102
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.444228817490078
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.81146427878806
waveform batch: 2/2
Test loss - extrapolation:18.09208378947508
Epoch 286 mean train loss:1.0341345225145258
Epoch 286 mean test loss - interpolation:1.0740381362483464
Epoch 286 mean test loss - extrapolation:5.158629005688595
Start training epoch 287
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.325
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0296572303540827
Iteration: 2 || Loss: 3.0282384714890007
Iteration: 3 || Loss: 3.0268191512512757
Iteration: 4 || Loss: 3.025402124994628
Iteration: 5 || Loss: 3.0239884064679114
Iteration: 6 || Loss: 3.0239884064679114
saving ADAM checkpoint...
Sum of params:83.325
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0239884064679114
Iteration: 2 || Loss: 2.4063262219555757
Iteration: 3 || Loss: 1.903915752071175
Iteration: 4 || Loss: 1.900159432534866
Iteration: 5 || Loss: 1.8604642702998635
Iteration: 6 || Loss: 1.8496926959928157
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.42072
Epoch 287 loss:1.8496926959928157
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.42072
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.84469751911571
Iteration: 2 || Loss: 6.844363488222658
Iteration: 3 || Loss: 6.844031918178104
Iteration: 4 || Loss: 6.843700131725577
Iteration: 5 || Loss: 6.843370451420758
Iteration: 6 || Loss: 6.843370451420758
saving ADAM checkpoint...
Sum of params:83.42078
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.843370451420758
Iteration: 2 || Loss: 6.817647618403742
Iteration: 3 || Loss: 6.723896376213512
Iteration: 4 || Loss: 6.601600979821877
Iteration: 5 || Loss: 6.595780956310505
Iteration: 6 || Loss: 6.577848907016381
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.43801
Epoch 287 loss:6.577848907016381
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.43801
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.006944995729484
Iteration: 2 || Loss: 22.006575030558132
Iteration: 3 || Loss: 22.00620859145779
Iteration: 4 || Loss: 22.005841765692207
Iteration: 5 || Loss: 22.00547817479553
Iteration: 6 || Loss: 22.00547817479553
saving ADAM checkpoint...
Sum of params:83.43799
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.00547817479553
Iteration: 2 || Loss: 21.970562605479383
Iteration: 3 || Loss: 21.751703691624293
Iteration: 4 || Loss: 21.602072326795867
Iteration: 5 || Loss: 21.569461499025163
Iteration: 6 || Loss: 21.530617817632645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.33789
Epoch 287 loss:21.530617817632645
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.437390080863476
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.78751456273113
waveform batch: 2/2
Test loss - extrapolation:18.076620406099877
Epoch 287 mean train loss:1.0330399800221324
Epoch 287 mean test loss - interpolation:1.0728983468105793
Epoch 287 mean test loss - extrapolation:5.1553445807359175
Start training epoch 288
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.33789
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0235342278792325
Iteration: 2 || Loss: 3.022113816069526
Iteration: 3 || Loss: 3.02069686313216
Iteration: 4 || Loss: 3.019281111633093
Iteration: 5 || Loss: 3.0178659139103368
Iteration: 6 || Loss: 3.0178659139103368
saving ADAM checkpoint...
Sum of params:83.33791
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0178659139103368
Iteration: 2 || Loss: 2.4012094952465963
Iteration: 3 || Loss: 1.9001276507730267
Iteration: 4 || Loss: 1.8964184775571329
Iteration: 5 || Loss: 1.8569351157029752
Iteration: 6 || Loss: 1.846522267838654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.4333
Epoch 288 loss:1.846522267838654
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.4333
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.836706045652427
Iteration: 2 || Loss: 6.836372308218365
Iteration: 3 || Loss: 6.83603927775038
Iteration: 4 || Loss: 6.835708416737135
Iteration: 5 || Loss: 6.835376975369193
Iteration: 6 || Loss: 6.835376975369193
saving ADAM checkpoint...
Sum of params:83.43333
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.835376975369193
Iteration: 2 || Loss: 6.809649406341201
Iteration: 3 || Loss: 6.716252784539467
Iteration: 4 || Loss: 6.594415005523334
Iteration: 5 || Loss: 6.588582690066669
Iteration: 6 || Loss: 6.570879622851651
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.450905
Epoch 288 loss:6.570879622851651
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.450905
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.984541955635876
Iteration: 2 || Loss: 21.984171941507732
Iteration: 3 || Loss: 21.983802891639368
Iteration: 4 || Loss: 21.983435881074445
Iteration: 5 || Loss: 21.983068024630374
Iteration: 6 || Loss: 21.983068024630374
saving ADAM checkpoint...
Sum of params:83.45085
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.983068024630374
Iteration: 2 || Loss: 21.947980344051363
Iteration: 3 || Loss: 21.72952144319175
Iteration: 4 || Loss: 21.58043127900951
Iteration: 5 || Loss: 21.547829875977264
Iteration: 6 || Loss: 21.509390993542755
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.35083
Epoch 288 loss:21.509390993542755
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.4306643653111895
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.763562130216734
waveform batch: 2/2
Test loss - extrapolation:18.06118103650231
Epoch 288 mean train loss:1.0319583753183814
Epoch 288 mean test loss - interpolation:1.0717773942185316
Epoch 288 mean test loss - extrapolation:5.1520619305599205
Start training epoch 289
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.35083
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0174866249087646
Iteration: 2 || Loss: 3.016070055731357
Iteration: 3 || Loss: 3.0146497885710386
Iteration: 4 || Loss: 3.0132329534179387
Iteration: 5 || Loss: 3.0118189154973596
Iteration: 6 || Loss: 3.0118189154973596
saving ADAM checkpoint...
Sum of params:83.35083
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0118189154973596
Iteration: 2 || Loss: 2.396265082144573
Iteration: 3 || Loss: 1.8964114569325348
Iteration: 4 || Loss: 1.8927477569451048
Iteration: 5 || Loss: 1.8534484877495987
Iteration: 6 || Loss: 1.8434095870733844
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.44588
Epoch 289 loss:1.8434095870733844
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.44588
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.828872039341372
Iteration: 2 || Loss: 6.828536690715982
Iteration: 3 || Loss: 6.828204380646895
Iteration: 4 || Loss: 6.827872515509433
Iteration: 5 || Loss: 6.827542193418602
Iteration: 6 || Loss: 6.827542193418602
saving ADAM checkpoint...
Sum of params:83.44592
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.827542193418602
Iteration: 2 || Loss: 6.801801246844947
Iteration: 3 || Loss: 6.708747548020318
Iteration: 4 || Loss: 6.58734762185624
Iteration: 5 || Loss: 6.581505711095149
Iteration: 6 || Loss: 6.56402682638714
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.463806
Epoch 289 loss:6.56402682638714
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.463806
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.962197121111537
Iteration: 2 || Loss: 21.9618277420028
Iteration: 3 || Loss: 21.96145804278963
Iteration: 4 || Loss: 21.9610887315632
Iteration: 5 || Loss: 21.960721991907352
Iteration: 6 || Loss: 21.960721991907352
saving ADAM checkpoint...
Sum of params:83.463745
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.960721991907352
Iteration: 2 || Loss: 21.925508325043825
Iteration: 3 || Loss: 21.70746397391901
Iteration: 4 || Loss: 21.558950634716876
Iteration: 5 || Loss: 21.526337771586956
Iteration: 6 || Loss: 21.488275252530457
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.363785
Epoch 289 loss:21.488275252530457
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.424055669005491
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.739787636915366
waveform batch: 2/2
Test loss - extrapolation:18.045723505753706
Epoch 289 mean train loss:1.0308866091721027
Epoch 289 mean test loss - interpolation:1.0706759448342484
Epoch 289 mean test loss - extrapolation:5.148792595222423
Start training epoch 290
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.363785
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0118697999054023
Iteration: 2 || Loss: 3.0104474973215565
Iteration: 3 || Loss: 3.009027524422881
Iteration: 4 || Loss: 3.0076066623455073
Iteration: 5 || Loss: 3.006188468651887
Iteration: 6 || Loss: 3.006188468651887
saving ADAM checkpoint...
Sum of params:83.36378
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.006188468651887
Iteration: 2 || Loss: 2.391309497363177
Iteration: 3 || Loss: 1.8927993224605193
Iteration: 4 || Loss: 1.8891796315803606
Iteration: 5 || Loss: 1.850064327448366
Iteration: 6 || Loss: 1.8403740143117997
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.45848
Epoch 290 loss:1.8403740143117997
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.45848
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.821205887747137
Iteration: 2 || Loss: 6.820872957968162
Iteration: 3 || Loss: 6.82053804848772
Iteration: 4 || Loss: 6.820203957249931
Iteration: 5 || Loss: 6.8198728731953
Iteration: 6 || Loss: 6.8198728731953
saving ADAM checkpoint...
Sum of params:83.45852
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.8198728731953
Iteration: 2 || Loss: 6.794116021546983
Iteration: 3 || Loss: 6.701407853296775
Iteration: 4 || Loss: 6.580430528448246
Iteration: 5 || Loss: 6.574572998814968
Iteration: 6 || Loss: 6.557306404826945
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.4767
Epoch 290 loss:6.557306404826945
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.4767
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.940167140382734
Iteration: 2 || Loss: 21.939795619435934
Iteration: 3 || Loss: 21.939425416870165
Iteration: 4 || Loss: 21.939056043428003
Iteration: 5 || Loss: 21.938688540824646
Iteration: 6 || Loss: 21.938688540824646
saving ADAM checkpoint...
Sum of params:83.476654
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.938688540824646
Iteration: 2 || Loss: 21.903272173929786
Iteration: 3 || Loss: 21.685618660989427
Iteration: 4 || Loss: 21.537612763153188
Iteration: 5 || Loss: 21.505025544863404
Iteration: 6 || Loss: 21.46735360020235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.37677
Epoch 290 loss:21.46735360020235
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.417522512322984
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.71584323561359
waveform batch: 2/2
Test loss - extrapolation:18.030315706722426
Epoch 290 mean train loss:1.029828759287624
Epoch 290 mean test loss - interpolation:1.069587085387164
Epoch 290 mean test loss - extrapolation:5.145513245194668
Start training epoch 291
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.37677
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.005943666609201
Iteration: 2 || Loss: 3.004520972809689
Iteration: 3 || Loss: 3.0030998229586574
Iteration: 4 || Loss: 3.001681295427567
Iteration: 5 || Loss: 3.0002621604161392
Iteration: 6 || Loss: 3.0002621604161392
saving ADAM checkpoint...
Sum of params:83.37677
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 3.0002621604161392
Iteration: 2 || Loss: 2.38659409480003
Iteration: 3 || Loss: 1.889228916465128
Iteration: 4 || Loss: 1.885650229235988
Iteration: 5 || Loss: 1.8467425407611973
Iteration: 6 || Loss: 1.837379424757521
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.47112
Epoch 291 loss:1.837379424757521
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.47112
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.813669233940955
Iteration: 2 || Loss: 6.81333264055745
Iteration: 3 || Loss: 6.8129989571703184
Iteration: 4 || Loss: 6.812665978126794
Iteration: 5 || Loss: 6.812333290562668
Iteration: 6 || Loss: 6.812333290562668
saving ADAM checkpoint...
Sum of params:83.471176
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.812333290562668
Iteration: 2 || Loss: 6.786551494671551
Iteration: 3 || Loss: 6.694167943258655
Iteration: 4 || Loss: 6.5736015718045016
Iteration: 5 || Loss: 6.567735892161273
Iteration: 6 || Loss: 6.550681573627633
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.48965
Epoch 291 loss:6.550681573627633
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.48965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.918237280107896
Iteration: 2 || Loss: 21.917865256115935
Iteration: 3 || Loss: 21.917493262294446
Iteration: 4 || Loss: 21.917122381149206
Iteration: 5 || Loss: 21.916753989039922
Iteration: 6 || Loss: 21.916753989039922
saving ADAM checkpoint...
Sum of params:83.48959
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.916753989039922
Iteration: 2 || Loss: 21.881206875868667
Iteration: 3 || Loss: 21.6639276210922
Iteration: 4 || Loss: 21.516467353176658
Iteration: 5 || Loss: 21.483881824325138
Iteration: 6 || Loss: 21.446557362460332
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.38971
Epoch 291 loss:21.446557362460332
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.411154339950814
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.69220068321718
waveform batch: 2/2
Test loss - extrapolation:18.014893529936842
Epoch 291 mean train loss:1.0287799434774305
Epoch 291 mean test loss - interpolation:1.0685257233251357
Epoch 291 mean test loss - extrapolation:5.1422578510961685
Start training epoch 292
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.38971
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 3.0007095794771357
Iteration: 2 || Loss: 2.9992868357492326
Iteration: 3 || Loss: 2.9978673343983604
Iteration: 4 || Loss: 2.996446634921244
Iteration: 5 || Loss: 2.995027123777663
Iteration: 6 || Loss: 2.995027123777663
saving ADAM checkpoint...
Sum of params:83.38971
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.995027123777663
Iteration: 2 || Loss: 2.381963031693119
Iteration: 3 || Loss: 1.885765626135197
Iteration: 4 || Loss: 1.882227497715252
Iteration: 5 || Loss: 1.8435029162811514
Iteration: 6 || Loss: 1.8344593536587914
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.48381
Epoch 292 loss:1.8344593536587914
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.48381
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.806283545178693
Iteration: 2 || Loss: 6.805946260405959
Iteration: 3 || Loss: 6.805612309518277
Iteration: 4 || Loss: 6.80527783073763
Iteration: 5 || Loss: 6.8049472310998524
Iteration: 6 || Loss: 6.8049472310998524
saving ADAM checkpoint...
Sum of params:83.48383
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.8049472310998524
Iteration: 2 || Loss: 6.779134442219343
Iteration: 3 || Loss: 6.687078409507117
Iteration: 4 || Loss: 6.5669109777486865
Iteration: 5 || Loss: 6.561033570974654
Iteration: 6 || Loss: 6.544182111628963
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.5026
Epoch 292 loss:6.544182111628963
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.5026
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.896473313470143
Iteration: 2 || Loss: 21.896101922230745
Iteration: 3 || Loss: 21.89572867429302
Iteration: 4 || Loss: 21.89535546724534
Iteration: 5 || Loss: 21.89498656240158
Iteration: 6 || Loss: 21.89498656240158
saving ADAM checkpoint...
Sum of params:83.50256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.89498656240158
Iteration: 2 || Loss: 21.85927859371354
Iteration: 3 || Loss: 21.642371619286443
Iteration: 4 || Loss: 21.495436407695937
Iteration: 5 || Loss: 21.462863990725356
Iteration: 6 || Loss: 21.425892877053137
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.4027
Epoch 292 loss:21.425892877053137
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.404832637689279
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.66849407922143
waveform batch: 2/2
Test loss - extrapolation:17.999508934423606
Epoch 292 mean train loss:1.0277425635289963
Epoch 292 mean test loss - interpolation:1.0674721062815464
Epoch 292 mean test loss - extrapolation:5.1390002511370865
Start training epoch 293
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.4027
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9953432184424784
Iteration: 2 || Loss: 2.9939223103901305
Iteration: 3 || Loss: 2.9924978364950086
Iteration: 4 || Loss: 2.9910792901223036
Iteration: 5 || Loss: 2.9896639924881194
Iteration: 6 || Loss: 2.9896639924881194
saving ADAM checkpoint...
Sum of params:83.40271
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9896639924881194
Iteration: 2 || Loss: 2.3773808737284723
Iteration: 3 || Loss: 1.8823604604010211
Iteration: 4 || Loss: 1.8788615987349853
Iteration: 5 || Loss: 1.8403350636450504
Iteration: 6 || Loss: 1.8315914993640965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.49645
Epoch 293 loss:1.8315914993640965
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.49645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.799032390902292
Iteration: 2 || Loss: 6.798694958338156
Iteration: 3 || Loss: 6.798359628589862
Iteration: 4 || Loss: 6.7980264738177105
Iteration: 5 || Loss: 6.797694099228963
Iteration: 6 || Loss: 6.797694099228963
saving ADAM checkpoint...
Sum of params:83.49649
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.797694099228963
Iteration: 2 || Loss: 6.771854833471751
Iteration: 3 || Loss: 6.680121069497231
Iteration: 4 || Loss: 6.560332114723387
Iteration: 5 || Loss: 6.554444942693568
Iteration: 6 || Loss: 6.537786208646249
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.51557
Epoch 293 loss:6.537786208646249
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.51557
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.874847760902654
Iteration: 2 || Loss: 21.8744736605392
Iteration: 3 || Loss: 21.874101632285395
Iteration: 4 || Loss: 21.873728572787197
Iteration: 5 || Loss: 21.87335924417678
Iteration: 6 || Loss: 21.87335924417678
saving ADAM checkpoint...
Sum of params:83.51553
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.87335924417678
Iteration: 2 || Loss: 21.837510521106005
Iteration: 3 || Loss: 21.620966256393224
Iteration: 4 || Loss: 21.474556567633776
Iteration: 5 || Loss: 21.44199155254026
Iteration: 6 || Loss: 21.40535401343094
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.41569
Epoch 293 loss:21.40535401343094
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.398612208630261
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.64490693681082
waveform batch: 2/2
Test loss - extrapolation:17.98414377547524
Epoch 293 mean train loss:1.0267148869462512
Epoch 293 mean test loss - interpolation:1.0664353681050434
Epoch 293 mean test loss - extrapolation:5.135754226023838
Start training epoch 294
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.41569
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.990229605707832
Iteration: 2 || Loss: 2.988806275538749
Iteration: 3 || Loss: 2.9873848650444605
Iteration: 4 || Loss: 2.9859635990932616
Iteration: 5 || Loss: 2.98454379324217
Iteration: 6 || Loss: 2.98454379324217
saving ADAM checkpoint...
Sum of params:83.41569
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.98454379324217
Iteration: 2 || Loss: 2.372892427873719
Iteration: 3 || Loss: 1.8790339231358337
Iteration: 4 || Loss: 1.8755735700187202
Iteration: 5 || Loss: 1.8372349709569888
Iteration: 6 || Loss: 1.8287823954920583
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.509155
Epoch 294 loss:1.8287823954920583
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.509155
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.791928820178208
Iteration: 2 || Loss: 6.791591506394231
Iteration: 3 || Loss: 6.791257283598
Iteration: 4 || Loss: 6.790921871750462
Iteration: 5 || Loss: 6.790590457755243
Iteration: 6 || Loss: 6.790590457755243
saving ADAM checkpoint...
Sum of params:83.5092
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.790590457755243
Iteration: 2 || Loss: 6.764717455862841
Iteration: 3 || Loss: 6.673294033723645
Iteration: 4 || Loss: 6.553870411550738
Iteration: 5 || Loss: 6.54797019921154
Iteration: 6 || Loss: 6.531500023633572
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.52855
Epoch 294 loss:6.531500023633572
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.52855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.853436834878085
Iteration: 2 || Loss: 21.85306292438333
Iteration: 3 || Loss: 21.852689070786525
Iteration: 4 || Loss: 21.852315732284627
Iteration: 5 || Loss: 21.851944144763365
Iteration: 6 || Loss: 21.851944144763365
saving ADAM checkpoint...
Sum of params:83.52853
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.851944144763365
Iteration: 2 || Loss: 21.815936617273117
Iteration: 3 || Loss: 21.59971118085472
Iteration: 4 || Loss: 21.453809219750706
Iteration: 5 || Loss: 21.42126471672126
Iteration: 6 || Loss: 21.3849597466175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.4287
Epoch 294 loss:21.3849597466175
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.392494309482313
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.62133022371535
waveform batch: 2/2
Test loss - extrapolation:17.96878965121418
Epoch 294 mean train loss:1.0256980057152802
Epoch 294 mean test loss - interpolation:1.065415718247052
Epoch 294 mean test loss - extrapolation:5.132509989577461
Start training epoch 295
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.4287
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.985192257721454
Iteration: 2 || Loss: 2.9837693480469145
Iteration: 3 || Loss: 2.9823473235548046
Iteration: 4 || Loss: 2.980927421089587
Iteration: 5 || Loss: 2.9795074052919106
Iteration: 6 || Loss: 2.9795074052919106
saving ADAM checkpoint...
Sum of params:83.428696
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9795074052919106
Iteration: 2 || Loss: 2.36854361953932
Iteration: 3 || Loss: 1.8757679407867207
Iteration: 4 || Loss: 1.8723426189170014
Iteration: 5 || Loss: 1.834178560352284
Iteration: 6 || Loss: 1.8260231256441384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.52185
Epoch 295 loss:1.8260231256441384
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.52185
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.784940670941833
Iteration: 2 || Loss: 6.7846035261065065
Iteration: 3 || Loss: 6.7842683041645335
Iteration: 4 || Loss: 6.783934875742838
Iteration: 5 || Loss: 6.783600185531857
Iteration: 6 || Loss: 6.783600185531857
saving ADAM checkpoint...
Sum of params:83.52187
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.783600185531857
Iteration: 2 || Loss: 6.757686273745886
Iteration: 3 || Loss: 6.666573118391237
Iteration: 4 || Loss: 6.547508199933039
Iteration: 5 || Loss: 6.54159594624561
Iteration: 6 || Loss: 6.525313154559047
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.541534
Epoch 295 loss:6.525313154559047
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.541534
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.83218806591337
Iteration: 2 || Loss: 21.8318112708527
Iteration: 3 || Loss: 21.831436278595493
Iteration: 4 || Loss: 21.83106123978274
Iteration: 5 || Loss: 21.830688043976934
Iteration: 6 || Loss: 21.830688043976934
saving ADAM checkpoint...
Sum of params:83.5415
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.830688043976934
Iteration: 2 || Loss: 21.794490050772723
Iteration: 3 || Loss: 21.578653495837013
Iteration: 4 || Loss: 21.433221211758706
Iteration: 5 || Loss: 21.400705864719157
Iteration: 6 || Loss: 21.364737836451653
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.44174
Epoch 295 loss:21.364737836451653
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.386421655937902
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.59769116346139
waveform batch: 2/2
Test loss - extrapolation:17.953490624330318
Epoch 295 mean train loss:1.0246922109191323
Epoch 295 mean test loss - interpolation:1.0644036093229836
Epoch 295 mean test loss - extrapolation:5.129265148982642
Start training epoch 296
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.44174
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9799912546326777
Iteration: 2 || Loss: 2.978566375055925
Iteration: 3 || Loss: 2.9771423187884687
Iteration: 4 || Loss: 2.975722535886478
Iteration: 5 || Loss: 2.974305339834305
Iteration: 6 || Loss: 2.974305339834305
saving ADAM checkpoint...
Sum of params:83.44172
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.974305339834305
Iteration: 2 || Loss: 2.364300802200988
Iteration: 3 || Loss: 1.8725485982575445
Iteration: 4 || Loss: 1.8691576143220934
Iteration: 5 || Loss: 1.8312231559297478
Iteration: 6 || Loss: 1.8233039285210313
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.53456
Epoch 296 loss:1.8233039285210313
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.53456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.778061662072917
Iteration: 2 || Loss: 6.777724120608445
Iteration: 3 || Loss: 6.777387161788149
Iteration: 4 || Loss: 6.777052450711502
Iteration: 5 || Loss: 6.776719198485627
Iteration: 6 || Loss: 6.776719198485627
saving ADAM checkpoint...
Sum of params:83.5346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.776719198485627
Iteration: 2 || Loss: 6.750771278661656
Iteration: 3 || Loss: 6.659951106260278
Iteration: 4 || Loss: 6.541225726546662
Iteration: 5 || Loss: 6.535303322565745
Iteration: 6 || Loss: 6.519198642532801
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.55457
Epoch 296 loss:6.519198642532801
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.55457
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.81119545828676
Iteration: 2 || Loss: 21.810818130577815
Iteration: 3 || Loss: 21.810441891516607
Iteration: 4 || Loss: 21.81006761367573
Iteration: 5 || Loss: 21.809693436575674
Iteration: 6 || Loss: 21.809693436575674
saving ADAM checkpoint...
Sum of params:83.55453
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.809693436575674
Iteration: 2 || Loss: 21.773344970625622
Iteration: 3 || Loss: 21.55776115052514
Iteration: 4 || Loss: 21.412818026305953
Iteration: 5 || Loss: 21.380318077836346
Iteration: 6 || Loss: 21.344650971295554
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.45474
Epoch 296 loss:21.344650971295554
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.3804896383864245
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.57430838138898
waveform batch: 2/2
Test loss - extrapolation:17.93820475152805
Epoch 296 mean train loss:1.0236949497361858
Epoch 296 mean test loss - interpolation:1.0634149397310708
Epoch 296 mean test loss - extrapolation:5.12604276107642
Start training epoch 297
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.45474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.975325811092531
Iteration: 2 || Loss: 2.9739010654738567
Iteration: 3 || Loss: 2.972478517851216
Iteration: 4 || Loss: 2.9710608008748816
Iteration: 5 || Loss: 2.969638484289379
Iteration: 6 || Loss: 2.969638484289379
saving ADAM checkpoint...
Sum of params:83.45475
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.969638484289379
Iteration: 2 || Loss: 2.3601541919281446
Iteration: 3 || Loss: 1.8694127529283426
Iteration: 4 || Loss: 1.866056077806992
Iteration: 5 || Loss: 1.8283095990307627
Iteration: 6 || Loss: 1.8206420019702998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.54729
Epoch 297 loss:1.8206420019702998
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.54729
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.771295623911575
Iteration: 2 || Loss: 6.770957191451738
Iteration: 3 || Loss: 6.7706209784447
Iteration: 4 || Loss: 6.770285701735454
Iteration: 5 || Loss: 6.769951822988967
Iteration: 6 || Loss: 6.769951822988967
saving ADAM checkpoint...
Sum of params:83.54733
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.769951822988967
Iteration: 2 || Loss: 6.743960524951356
Iteration: 3 || Loss: 6.653435618755696
Iteration: 4 || Loss: 6.535049933353106
Iteration: 5 || Loss: 6.529117109677449
Iteration: 6 || Loss: 6.51319108302082
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.56756
Epoch 297 loss:6.51319108302082
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.56756
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.79019878711417
Iteration: 2 || Loss: 21.789820847447952
Iteration: 3 || Loss: 21.789445830286613
Iteration: 4 || Loss: 21.789067997588518
Iteration: 5 || Loss: 21.7886945393509
Iteration: 6 || Loss: 21.7886945393509
saving ADAM checkpoint...
Sum of params:83.56755
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.7886945393509
Iteration: 2 || Loss: 21.75220942383719
Iteration: 3 || Loss: 21.53694259186567
Iteration: 4 || Loss: 21.392512804259525
Iteration: 5 || Loss: 21.36002938313593
Iteration: 6 || Loss: 21.324658365037326
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.46775
Epoch 297 loss:21.324658365037326
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.374624959243836
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.55098613240269
waveform batch: 2/2
Test loss - extrapolation:17.922939309310543
Epoch 297 mean train loss:1.0227066017251187
Epoch 297 mean test loss - interpolation:1.062437493207306
Epoch 297 mean test loss - extrapolation:5.1228271201427695
Start training epoch 298
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.46775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9707889871050948
Iteration: 2 || Loss: 2.9693635221878805
Iteration: 3 || Loss: 2.967940787769027
Iteration: 4 || Loss: 2.966520399467744
Iteration: 5 || Loss: 2.9650969398014793
Iteration: 6 || Loss: 2.9650969398014793
saving ADAM checkpoint...
Sum of params:83.467766
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9650969398014793
Iteration: 2 || Loss: 2.3560184712075842
Iteration: 3 || Loss: 1.8663386540176428
Iteration: 4 || Loss: 1.8630145021901425
Iteration: 5 || Loss: 1.8254417810595762
Iteration: 6 || Loss: 1.8180332872686022
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.56002
Epoch 298 loss:1.8180332872686022
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.56002
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.764668010223911
Iteration: 2 || Loss: 6.764328856849396
Iteration: 3 || Loss: 6.7639922141590185
Iteration: 4 || Loss: 6.763657231096506
Iteration: 5 || Loss: 6.763321454192249
Iteration: 6 || Loss: 6.763321454192249
saving ADAM checkpoint...
Sum of params:83.56006
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.763321454192249
Iteration: 2 || Loss: 6.737287383712646
Iteration: 3 || Loss: 6.647053669906278
Iteration: 4 || Loss: 6.5289851796134295
Iteration: 5 || Loss: 6.523041565130185
Iteration: 6 || Loss: 6.507283963918748
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.58057
Epoch 298 loss:6.507283963918748
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.58057
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.769385980664588
Iteration: 2 || Loss: 21.76900823659325
Iteration: 3 || Loss: 21.76863039267461
Iteration: 4 || Loss: 21.76825436069874
Iteration: 5 || Loss: 21.767879259836036
Iteration: 6 || Loss: 21.767879259836036
saving ADAM checkpoint...
Sum of params:83.58054
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.767879259836036
Iteration: 2 || Loss: 21.731219072455513
Iteration: 3 || Loss: 21.516300210072817
Iteration: 4 || Loss: 21.372333186283203
Iteration: 5 || Loss: 21.339879605992138
Iteration: 6 || Loss: 21.304815125101488
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.480804
Epoch 298 loss:21.304815125101488
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.368812338353553
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.52759831524866
waveform batch: 2/2
Test loss - extrapolation:17.90772615826201
Epoch 298 mean train loss:1.0217287026306496
Epoch 298 mean test loss - interpolation:1.0614687230589255
Epoch 298 mean test loss - extrapolation:5.119610372792557
Start training epoch 299
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.480804
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9660650915918576
Iteration: 2 || Loss: 2.9646375534827207
Iteration: 3 || Loss: 2.9632144864080376
Iteration: 4 || Loss: 2.961793860151995
Iteration: 5 || Loss: 2.960375665002958
Iteration: 6 || Loss: 2.960375665002958
saving ADAM checkpoint...
Sum of params:83.480804
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.960375665002958
Iteration: 2 || Loss: 2.3520330479881344
Iteration: 3 || Loss: 1.8633050130947244
Iteration: 4 || Loss: 1.8600118167234136
Iteration: 5 || Loss: 1.82261005626122
Iteration: 6 || Loss: 1.8154614249050829
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.572754
Epoch 299 loss:1.8154614249050829
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.572754
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.758119723689493
Iteration: 2 || Loss: 6.757780132517418
Iteration: 3 || Loss: 6.7574429667546925
Iteration: 4 || Loss: 6.757107534564606
Iteration: 5 || Loss: 6.756773260289432
Iteration: 6 || Loss: 6.756773260289432
saving ADAM checkpoint...
Sum of params:83.572784
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.756773260289432
Iteration: 2 || Loss: 6.730698238152076
Iteration: 3 || Loss: 6.640756719134945
Iteration: 4 || Loss: 6.522997805877182
Iteration: 5 || Loss: 6.517043623786778
Iteration: 6 || Loss: 6.501449239249852
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.593605
Epoch 299 loss:6.501449239249852
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.593605
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.748778346318602
Iteration: 2 || Loss: 21.748396853736637
Iteration: 3 || Loss: 21.748018113702923
Iteration: 4 || Loss: 21.747641766579502
Iteration: 5 || Loss: 21.747266002586763
Iteration: 6 || Loss: 21.747266002586763
saving ADAM checkpoint...
Sum of params:83.59357
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.747266002586763
Iteration: 2 || Loss: 21.710457546645994
Iteration: 3 || Loss: 21.495798175255256
Iteration: 4 || Loss: 21.3523134595385
Iteration: 5 || Loss: 21.319884090025862
Iteration: 6 || Loss: 21.28509937296235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.493805
Epoch 299 loss:21.28509937296235
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.363114988242145
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.50438957983446
waveform batch: 2/2
Test loss - extrapolation:17.892523176305545
Epoch 299 mean train loss:1.0207589667971477
Epoch 299 mean test loss - interpolation:1.0605191647070242
Epoch 299 mean test loss - extrapolation:5.116409396345
Start training epoch 300
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.493805
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9617195745123124
Iteration: 2 || Loss: 2.9602968065700255
Iteration: 3 || Loss: 2.9588725367959343
Iteration: 4 || Loss: 2.957449538827838
Iteration: 5 || Loss: 2.956028532766186
Iteration: 6 || Loss: 2.956028532766186
saving ADAM checkpoint...
Sum of params:83.49382
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.956028532766186
Iteration: 2 || Loss: 2.3481161607498016
Iteration: 3 || Loss: 1.860343634526089
Iteration: 4 || Loss: 1.8570812332114246
Iteration: 5 || Loss: 1.8198608985097617
Iteration: 6 || Loss: 1.8129398478549572
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.5855
Epoch 300 loss:1.8129398478549572
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.5855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.7516960141177265
Iteration: 2 || Loss: 6.751355843302752
Iteration: 3 || Loss: 6.751019646290634
Iteration: 4 || Loss: 6.750682706433619
Iteration: 5 || Loss: 6.750347277769985
Iteration: 6 || Loss: 6.750347277769985
saving ADAM checkpoint...
Sum of params:83.585556
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.750347277769985
Iteration: 2 || Loss: 6.7242171017794465
Iteration: 3 || Loss: 6.634557545744158
Iteration: 4 || Loss: 6.517105020250466
Iteration: 5 || Loss: 6.511140715120303
Iteration: 6 || Loss: 6.495707933653229
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.60664
Epoch 300 loss:6.495707933653229
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.60664
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.728265142390686
Iteration: 2 || Loss: 21.72788494933725
Iteration: 3 || Loss: 21.727505531029994
Iteration: 4 || Loss: 21.727127816286757
Iteration: 5 || Loss: 21.726750989744122
Iteration: 6 || Loss: 21.726750989744122
saving ADAM checkpoint...
Sum of params:83.606606
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.726750989744122
Iteration: 2 || Loss: 21.689796670332864
Iteration: 3 || Loss: 21.475416873741253
Iteration: 4 || Loss: 21.33240727058381
Iteration: 5 || Loss: 21.30000095158357
Iteration: 6 || Loss: 21.265496463382583
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.50682
Epoch 300 loss:21.265496463382583
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.357466159485566
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.48121939389244
waveform batch: 2/2
Test loss - extrapolation:17.87736517123726
Epoch 300 mean train loss:1.0197980774100264
Epoch 300 mean test loss - interpolation:1.0595776932475942
Epoch 300 mean test loss - extrapolation:5.113215380427475
Start training epoch 301
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.50682
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9574035822664775
Iteration: 2 || Loss: 2.9559762073770175
Iteration: 3 || Loss: 2.954551321688081
Iteration: 4 || Loss: 2.9531288573514796
Iteration: 5 || Loss: 2.9517069358606496
Iteration: 6 || Loss: 2.9517069358606496
saving ADAM checkpoint...
Sum of params:83.50683
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9517069358606496
Iteration: 2 || Loss: 2.3442502719738334
Iteration: 3 || Loss: 1.8574325427008875
Iteration: 4 || Loss: 1.854199539089386
Iteration: 5 || Loss: 1.8171560325784426
Iteration: 6 || Loss: 1.8104611324758424
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.59821
Epoch 301 loss:1.8104611324758424
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.59821
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.745381475815653
Iteration: 2 || Loss: 6.745040559107585
Iteration: 3 || Loss: 6.744703468003992
Iteration: 4 || Loss: 6.744366037830136
Iteration: 5 || Loss: 6.744029483459736
Iteration: 6 || Loss: 6.744029483459736
saving ADAM checkpoint...
Sum of params:83.59828
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.744029483459736
Iteration: 2 || Loss: 6.7178497853045185
Iteration: 3 || Loss: 6.628468910728387
Iteration: 4 || Loss: 6.5113044544041285
Iteration: 5 || Loss: 6.5053301895834466
Iteration: 6 || Loss: 6.490053570861284
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.61966
Epoch 301 loss:6.490053570861284
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.61966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.70787837858193
Iteration: 2 || Loss: 21.707496865364185
Iteration: 3 || Loss: 21.70711749110084
Iteration: 4 || Loss: 21.706739328362136
Iteration: 5 || Loss: 21.706361309781087
Iteration: 6 || Loss: 21.706361309781087
saving ADAM checkpoint...
Sum of params:83.61962
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.706361309781087
Iteration: 2 || Loss: 21.669249714728966
Iteration: 3 || Loss: 21.455162050100846
Iteration: 4 || Loss: 21.312617717241746
Iteration: 5 || Loss: 21.280241386643155
Iteration: 6 || Loss: 21.246013125768428
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.51984
Epoch 301 loss:21.246013125768428
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.351878896516792
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.45807653187564
waveform batch: 2/2
Test loss - extrapolation:17.862238431420394
Epoch 301 mean train loss:1.0188457872105365
Epoch 301 mean test loss - interpolation:1.0586464827527986
Epoch 301 mean test loss - extrapolation:5.110026246941336
Start training epoch 302
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.51984
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.953102218956287
Iteration: 2 || Loss: 2.9516747570137634
Iteration: 3 || Loss: 2.950249218476188
Iteration: 4 || Loss: 2.94882507476278
Iteration: 5 || Loss: 2.9474049976524195
Iteration: 6 || Loss: 2.9474049976524195
saving ADAM checkpoint...
Sum of params:83.51983
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9474049976524195
Iteration: 2 || Loss: 2.340442441825581
Iteration: 3 || Loss: 1.854570034301698
Iteration: 4 || Loss: 1.8513654240544453
Iteration: 5 || Loss: 1.814530734485856
Iteration: 6 || Loss: 1.8080247920473111
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.61097
Epoch 302 loss:1.8080247920473111
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.61097
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.739165442592863
Iteration: 2 || Loss: 6.738825111949902
Iteration: 3 || Loss: 6.738487050370418
Iteration: 4 || Loss: 6.7381488903823685
Iteration: 5 || Loss: 6.737812249836883
Iteration: 6 || Loss: 6.737812249836883
saving ADAM checkpoint...
Sum of params:83.611
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.737812249836883
Iteration: 2 || Loss: 6.711577833121217
Iteration: 3 || Loss: 6.622465412963153
Iteration: 4 || Loss: 6.505587824803292
Iteration: 5 || Loss: 6.499601804432136
Iteration: 6 || Loss: 6.484475058541973
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.632675
Epoch 302 loss:6.484475058541973
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.632675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.68772776440994
Iteration: 2 || Loss: 21.68734517678941
Iteration: 3 || Loss: 21.68696469227733
Iteration: 4 || Loss: 21.686586136818615
Iteration: 5 || Loss: 21.68620707447716
Iteration: 6 || Loss: 21.68620707447716
saving ADAM checkpoint...
Sum of params:83.63262
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.68620707447716
Iteration: 2 || Loss: 21.64891544344188
Iteration: 3 || Loss: 21.43508330941521
Iteration: 4 || Loss: 21.292975282200704
Iteration: 5 || Loss: 21.260634289657183
Iteration: 6 || Loss: 21.226680984986462
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.53286
Epoch 302 loss:21.226680984986462
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.346354762980237
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.43494600879854
waveform batch: 2/2
Test loss - extrapolation:17.84716056728238
Epoch 302 mean train loss:1.0179027874336464
Epoch 302 mean test loss - interpolation:1.0577257938300395
Epoch 302 mean test loss - extrapolation:5.10684221467341
Start training epoch 303
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.53286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9487827405884626
Iteration: 2 || Loss: 2.9473566933855015
Iteration: 3 || Loss: 2.9459319334618845
Iteration: 4 || Loss: 2.9445092223453697
Iteration: 5 || Loss: 2.943085443553974
Iteration: 6 || Loss: 2.943085443553974
saving ADAM checkpoint...
Sum of params:83.532875
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.943085443553974
Iteration: 2 || Loss: 2.336754657794502
Iteration: 3 || Loss: 1.8517530413384427
Iteration: 4 || Loss: 1.848574346228594
Iteration: 5 || Loss: 1.8119226063676719
Iteration: 6 || Loss: 1.8056218243787114
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.62372
Epoch 303 loss:1.8056218243787114
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.62372
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.73302053741684
Iteration: 2 || Loss: 6.732679587203005
Iteration: 3 || Loss: 6.732341036284066
Iteration: 4 || Loss: 6.7320029915740385
Iteration: 5 || Loss: 6.731667061170858
Iteration: 6 || Loss: 6.731667061170858
saving ADAM checkpoint...
Sum of params:83.623764
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.731667061170858
Iteration: 2 || Loss: 6.7053808961581485
Iteration: 3 || Loss: 6.616540402610333
Iteration: 4 || Loss: 6.4999363829058465
Iteration: 5 || Loss: 6.49394152655213
Iteration: 6 || Loss: 6.478964786239502
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.64568
Epoch 303 loss:6.478964786239502
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.64568
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.667676150399224
Iteration: 2 || Loss: 21.667293405694764
Iteration: 3 || Loss: 21.666913291211504
Iteration: 4 || Loss: 21.666530622093177
Iteration: 5 || Loss: 21.666152071378193
Iteration: 6 || Loss: 21.666152071378193
saving ADAM checkpoint...
Sum of params:83.64564
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.666152071378193
Iteration: 2 || Loss: 21.628700511579588
Iteration: 3 || Loss: 21.415141502268522
Iteration: 4 || Loss: 21.27347977782403
Iteration: 5 || Loss: 21.241167887679204
Iteration: 6 || Loss: 21.207478434327555
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.54588
Epoch 303 loss:21.207478434327555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.340918711276236
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.41195139230019
waveform batch: 2/2
Test loss - extrapolation:17.83211325398533
Epoch 303 mean train loss:1.0169677601705438
Epoch 303 mean test loss - interpolation:1.0568197852127061
Epoch 303 mean test loss - extrapolation:5.103672053857127
Start training epoch 304
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.54588
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9447124375259928
Iteration: 2 || Loss: 2.943283804645098
Iteration: 3 || Loss: 2.941859763161134
Iteration: 4 || Loss: 2.9404385425290664
Iteration: 5 || Loss: 2.9390171726695598
Iteration: 6 || Loss: 2.9390171726695598
saving ADAM checkpoint...
Sum of params:83.54589
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9390171726695598
Iteration: 2 || Loss: 2.333127328889053
Iteration: 3 || Loss: 1.8489893025957491
Iteration: 4 || Loss: 1.845836429257211
Iteration: 5 || Loss: 1.8093514682313856
Iteration: 6 || Loss: 1.8032599500600797
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.636475
Epoch 304 loss:1.8032599500600797
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.636475
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.726977413604134
Iteration: 2 || Loss: 6.726635648188044
Iteration: 3 || Loss: 6.726295024352055
Iteration: 4 || Loss: 6.725957216689869
Iteration: 5 || Loss: 6.725620588676855
Iteration: 6 || Loss: 6.725620588676855
saving ADAM checkpoint...
Sum of params:83.63652
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.725620588676855
Iteration: 2 || Loss: 6.699280861291506
Iteration: 3 || Loss: 6.6107012487064996
Iteration: 4 || Loss: 6.4943631023860044
Iteration: 5 || Loss: 6.488356533453604
Iteration: 6 || Loss: 6.473527684931234
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.65871
Epoch 304 loss:6.473527684931234
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.65871
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.647801333078707
Iteration: 2 || Loss: 21.647416522580485
Iteration: 3 || Loss: 21.647034563722933
Iteration: 4 || Loss: 21.646652855038774
Iteration: 5 || Loss: 21.646270984011288
Iteration: 6 || Loss: 21.646270984011288
saving ADAM checkpoint...
Sum of params:83.658676
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.646270984011288
Iteration: 2 || Loss: 21.60867649401176
Iteration: 3 || Loss: 21.39531952368515
Iteration: 4 || Loss: 21.254110734039994
Iteration: 5 || Loss: 21.221826833109724
Iteration: 6 || Loss: 21.188383516604283
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.55888
Epoch 304 loss:21.188383516604283
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.335568955045574
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.389085837398724
waveform batch: 2/2
Test loss - extrapolation:17.817096910587967
Epoch 304 mean train loss:1.0160403845377792
Epoch 304 mean test loss - interpolation:1.0559281591742622
Epoch 304 mean test loss - extrapolation:5.100515228998891
Start training epoch 305
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.55888
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9408654283703477
Iteration: 2 || Loss: 2.9394360908260913
Iteration: 3 || Loss: 2.9380109133777146
Iteration: 4 || Loss: 2.9365866445273605
Iteration: 5 || Loss: 2.9351647886568544
Iteration: 6 || Loss: 2.9351647886568544
saving ADAM checkpoint...
Sum of params:83.55889
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9351647886568544
Iteration: 2 || Loss: 2.329582517179669
Iteration: 3 || Loss: 1.8462794937757587
Iteration: 4 || Loss: 1.84315201952656
Iteration: 5 || Loss: 1.8068106813489548
Iteration: 6 || Loss: 1.8009396982177568
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.649216
Epoch 305 loss:1.8009396982177568
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.649216
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.721018369477442
Iteration: 2 || Loss: 6.720676812323987
Iteration: 3 || Loss: 6.720335627067146
Iteration: 4 || Loss: 6.719996913561862
Iteration: 5 || Loss: 6.719660453789385
Iteration: 6 || Loss: 6.719660453789385
saving ADAM checkpoint...
Sum of params:83.649254
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.719660453789385
Iteration: 2 || Loss: 6.693259179150812
Iteration: 3 || Loss: 6.604951599679377
Iteration: 4 || Loss: 6.488875564454794
Iteration: 5 || Loss: 6.482859574608232
Iteration: 6 || Loss: 6.468172366183117
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.67173
Epoch 305 loss:6.468172366183117
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.67173
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.628002633505428
Iteration: 2 || Loss: 21.627620444876946
Iteration: 3 || Loss: 21.627236155723956
Iteration: 4 || Loss: 21.62685343068166
Iteration: 5 || Loss: 21.6264732850106
Iteration: 6 || Loss: 21.6264732850106
saving ADAM checkpoint...
Sum of params:83.67169
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.6264732850106
Iteration: 2 || Loss: 21.588704366782846
Iteration: 3 || Loss: 21.375600012642543
Iteration: 4 || Loss: 21.234843805171785
Iteration: 5 || Loss: 21.202595380662206
Iteration: 6 || Loss: 21.169403588077667
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.571884
Epoch 305 loss:21.169403588077667
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.330245481216737
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.366191993489124
waveform batch: 2/2
Test loss - extrapolation:17.802131018705378
Epoch 305 mean train loss:1.0151212293958118
Epoch 305 mean test loss - interpolation:1.055040913536123
Epoch 305 mean test loss - extrapolation:5.097360251016209
Start training epoch 306
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.571884
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9368830340033134
Iteration: 2 || Loss: 2.935455739610172
Iteration: 3 || Loss: 2.934029618317948
Iteration: 4 || Loss: 2.932604868657772
Iteration: 5 || Loss: 2.931179844572594
Iteration: 6 || Loss: 2.931179844572594
saving ADAM checkpoint...
Sum of params:83.57189
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.931179844572594
Iteration: 2 || Loss: 2.3260917269225585
Iteration: 3 || Loss: 1.8436068971427415
Iteration: 4 || Loss: 1.8405044110013105
Iteration: 5 || Loss: 1.8043649147221215
Iteration: 6 || Loss: 1.7986549840562163
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.66194
Epoch 306 loss:1.7986549840562163
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.66194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.7151554247396765
Iteration: 2 || Loss: 6.714814017658135
Iteration: 3 || Loss: 6.7144729608424365
Iteration: 4 || Loss: 6.714134244989582
Iteration: 5 || Loss: 6.71379734753729
Iteration: 6 || Loss: 6.71379734753729
saving ADAM checkpoint...
Sum of params:83.66199
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.71379734753729
Iteration: 2 || Loss: 6.6873371630244565
Iteration: 3 || Loss: 6.5992849478794575
Iteration: 4 || Loss: 6.48346071218492
Iteration: 5 || Loss: 6.477435488665254
Iteration: 6 || Loss: 6.4628829201662885
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.684746
Epoch 306 loss:6.4628829201662885
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.684746
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.60836410738953
Iteration: 2 || Loss: 21.60797943534546
Iteration: 3 || Loss: 21.607594594248315
Iteration: 4 || Loss: 21.607212364575044
Iteration: 5 || Loss: 21.606829921432773
Iteration: 6 || Loss: 21.606829921432773
saving ADAM checkpoint...
Sum of params:83.68471
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.606829921432773
Iteration: 2 || Loss: 21.568905921923054
Iteration: 3 || Loss: 21.356038215536795
Iteration: 4 || Loss: 21.215710866321533
Iteration: 5 || Loss: 21.18349744710733
Iteration: 6 || Loss: 21.15055309676445
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.58489
Epoch 306 loss:21.15055309676445
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.325015676940074
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.34342346786031
waveform batch: 2/2
Test loss - extrapolation:17.787188412137564
Epoch 306 mean train loss:1.0142100345167915
Epoch 306 mean test loss - interpolation:1.0541692794900124
Epoch 306 mean test loss - extrapolation:5.09421765666649
Start training epoch 307
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.58489
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.933138039674741
Iteration: 2 || Loss: 2.9317096832799394
Iteration: 3 || Loss: 2.9302826113291243
Iteration: 4 || Loss: 2.9288586099454457
Iteration: 5 || Loss: 2.927434646972133
Iteration: 6 || Loss: 2.927434646972133
saving ADAM checkpoint...
Sum of params:83.5849
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.927434646972133
Iteration: 2 || Loss: 2.322667020269786
Iteration: 3 || Loss: 1.84098414023217
Iteration: 4 || Loss: 1.8379046611892018
Iteration: 5 || Loss: 1.8019346902888322
Iteration: 6 || Loss: 1.7964054536690877
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.674675
Epoch 307 loss:1.7964054536690877
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.674675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.709371957601078
Iteration: 2 || Loss: 6.709029999412935
Iteration: 3 || Loss: 6.708688279381949
Iteration: 4 || Loss: 6.708348220657506
Iteration: 5 || Loss: 6.708010993810382
Iteration: 6 || Loss: 6.708010993810382
saving ADAM checkpoint...
Sum of params:83.67473
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.708010993810382
Iteration: 2 || Loss: 6.681489611595134
Iteration: 3 || Loss: 6.593691740650679
Iteration: 4 || Loss: 6.478117891563426
Iteration: 5 || Loss: 6.4720839077298224
Iteration: 6 || Loss: 6.457664738493663
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.69771
Epoch 307 loss:6.457664738493663
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.69771
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.588815312534
Iteration: 2 || Loss: 21.588427606601602
Iteration: 3 || Loss: 21.588041921258345
Iteration: 4 || Loss: 21.58765743797248
Iteration: 5 || Loss: 21.587276524114177
Iteration: 6 || Loss: 21.587276524114177
saving ADAM checkpoint...
Sum of params:83.69768
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.587276524114177
Iteration: 2 || Loss: 21.54918625893908
Iteration: 3 || Loss: 21.3365870621152
Iteration: 4 || Loss: 21.196695315263817
Iteration: 5 || Loss: 21.16451938068214
Iteration: 6 || Loss: 21.13181401279198
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.59788
Epoch 307 loss:21.13181401279198
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.319828557723799
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.3206964735816
waveform batch: 2/2
Test loss - extrapolation:17.772302611158892
Epoch 307 mean train loss:1.0133063518949907
Epoch 307 mean test loss - interpolation:1.053304759620633
Epoch 307 mean test loss - extrapolation:5.091083257061707
Start training epoch 308
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.59788
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.929384231618249
Iteration: 2 || Loss: 2.9279554194280446
Iteration: 3 || Loss: 2.9265299474973427
Iteration: 4 || Loss: 2.9251042123400337
Iteration: 5 || Loss: 2.923680916910209
Iteration: 6 || Loss: 2.923680916910209
saving ADAM checkpoint...
Sum of params:83.59787
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.923680916910209
Iteration: 2 || Loss: 2.3193094154246143
Iteration: 3 || Loss: 1.8383997143194328
Iteration: 4 || Loss: 1.8353430876107577
Iteration: 5 || Loss: 1.7995600272665853
Iteration: 6 || Loss: 1.7941909632105506
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.68741
Epoch 308 loss:1.7941909632105506
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.68741
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.70367378039123
Iteration: 2 || Loss: 6.70332967998964
Iteration: 3 || Loss: 6.702987773494958
Iteration: 4 || Loss: 6.7026492393350585
Iteration: 5 || Loss: 6.702309580926576
Iteration: 6 || Loss: 6.702309580926576
saving ADAM checkpoint...
Sum of params:83.687454
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.702309580926576
Iteration: 2 || Loss: 6.6757266184787625
Iteration: 3 || Loss: 6.5881777403918385
Iteration: 4 || Loss: 6.472840088664569
Iteration: 5 || Loss: 6.4667961616474905
Iteration: 6 || Loss: 6.452511812132147
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.71072
Epoch 308 loss:6.452511812132147
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.71072
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.56946235962376
Iteration: 2 || Loss: 21.569073905502176
Iteration: 3 || Loss: 21.56868849821992
Iteration: 4 || Loss: 21.568303109614554
Iteration: 5 || Loss: 21.56792058878249
Iteration: 6 || Loss: 21.56792058878249
saving ADAM checkpoint...
Sum of params:83.71069
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.56792058878249
Iteration: 2 || Loss: 21.529682749071224
Iteration: 3 || Loss: 21.317263811936012
Iteration: 4 || Loss: 21.1778041404301
Iteration: 5 || Loss: 21.14565816735613
Iteration: 6 || Loss: 21.11317918872974
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.610825
Epoch 308 loss:21.11317918872974
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.314686069168352
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.29807226129157
waveform batch: 2/2
Test loss - extrapolation:17.757460447092186
Epoch 308 mean train loss:1.0124097228990496
Epoch 308 mean test loss - interpolation:1.0524476781947254
Epoch 308 mean test loss - extrapolation:5.08796105903198
Start training epoch 309
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.610825
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.925747328774283
Iteration: 2 || Loss: 2.9243212835125934
Iteration: 3 || Loss: 2.9228981243265704
Iteration: 4 || Loss: 2.9214705679797293
Iteration: 5 || Loss: 2.9200442895872047
Iteration: 6 || Loss: 2.9200442895872047
saving ADAM checkpoint...
Sum of params:83.610825
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9200442895872047
Iteration: 2 || Loss: 2.315961452921261
Iteration: 3 || Loss: 1.8358675061251442
Iteration: 4 || Loss: 1.8328332700240424
Iteration: 5 || Loss: 1.7972096725991056
Iteration: 6 || Loss: 1.7920133265167786
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.70012
Epoch 309 loss:1.7920133265167786
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.70012
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.698058879202695
Iteration: 2 || Loss: 6.697714764525819
Iteration: 3 || Loss: 6.697370584224663
Iteration: 4 || Loss: 6.69703080340293
Iteration: 5 || Loss: 6.69669203288626
Iteration: 6 || Loss: 6.69669203288626
saving ADAM checkpoint...
Sum of params:83.70016
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.69669203288626
Iteration: 2 || Loss: 6.670048201339147
Iteration: 3 || Loss: 6.5827521799526885
Iteration: 4 || Loss: 6.467642469436728
Iteration: 5 || Loss: 6.461588933483834
Iteration: 6 || Loss: 6.447431071076703
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.72367
Epoch 309 loss:6.447431071076703
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.72367
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.550179273400516
Iteration: 2 || Loss: 21.54979228534521
Iteration: 3 || Loss: 21.549404967592583
Iteration: 4 || Loss: 21.549018333557253
Iteration: 5 || Loss: 21.548634564102038
Iteration: 6 || Loss: 21.548634564102038
saving ADAM checkpoint...
Sum of params:83.72363
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.548634564102038
Iteration: 2 || Loss: 21.51023907869688
Iteration: 3 || Loss: 21.298037301227634
Iteration: 4 || Loss: 21.15900707671722
Iteration: 5 || Loss: 21.126899470157145
Iteration: 6 || Loss: 21.094653269187333
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.623764
Epoch 309 loss:21.094653269187333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.309615857253256
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.27548788911561
waveform batch: 2/2
Test loss - extrapolation:17.74264880132757
Epoch 309 mean train loss:1.0115206091993385
Epoch 309 mean test loss - interpolation:1.0516026428755427
Epoch 309 mean test loss - extrapolation:5.084844724203598
Start training epoch 310
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.623764
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.922158823072866
Iteration: 2 || Loss: 2.9207310976642393
Iteration: 3 || Loss: 2.9193039698475016
Iteration: 4 || Loss: 2.9178769197395873
Iteration: 5 || Loss: 2.9164506062443327
Iteration: 6 || Loss: 2.9164506062443327
saving ADAM checkpoint...
Sum of params:83.623764
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9164506062443327
Iteration: 2 || Loss: 2.3126988450907335
Iteration: 3 || Loss: 1.8333699608459908
Iteration: 4 || Loss: 1.8303557499977168
Iteration: 5 || Loss: 1.7949072826855113
Iteration: 6 || Loss: 1.7898670651817519
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.712814
Epoch 310 loss:1.7898670651817519
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.712814
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.69251817288191
Iteration: 2 || Loss: 6.692173328096375
Iteration: 3 || Loss: 6.691831632850374
Iteration: 4 || Loss: 6.691490040901356
Iteration: 5 || Loss: 6.691151057111365
Iteration: 6 || Loss: 6.691151057111365
saving ADAM checkpoint...
Sum of params:83.712845
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.691151057111365
Iteration: 2 || Loss: 6.6644390832704765
Iteration: 3 || Loss: 6.577390280943817
Iteration: 4 || Loss: 6.462508328384881
Iteration: 5 || Loss: 6.45644184812053
Iteration: 6 || Loss: 6.442408461840279
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.73664
Epoch 310 loss:6.442408461840279
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.73664
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.53118595612203
Iteration: 2 || Loss: 21.53079699537153
Iteration: 3 || Loss: 21.5304102723481
Iteration: 4 || Loss: 21.530022604752357
Iteration: 5 || Loss: 21.529636482312544
Iteration: 6 || Loss: 21.529636482312544
saving ADAM checkpoint...
Sum of params:83.7366
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.529636482312544
Iteration: 2 || Loss: 21.491051390843264
Iteration: 3 || Loss: 21.279013311724018
Iteration: 4 || Loss: 21.14036081339062
Iteration: 5 || Loss: 21.10829779886666
Iteration: 6 || Loss: 21.076285301452
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.63672
Epoch 310 loss:21.076285301452
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.304603387930264
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.25290393502708
waveform batch: 2/2
Test loss - extrapolation:17.72789974802596
Epoch 310 mean train loss:1.01064002856807
Epoch 310 mean test loss - interpolation:1.0507672313217107
Epoch 310 mean test loss - extrapolation:5.08173364025442
Start training epoch 311
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.63672
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.918507098524452
Iteration: 2 || Loss: 2.917077924246329
Iteration: 3 || Loss: 2.9156506522036225
Iteration: 4 || Loss: 2.9142242442628716
Iteration: 5 || Loss: 2.9127996688250763
Iteration: 6 || Loss: 2.9127996688250763
saving ADAM checkpoint...
Sum of params:83.63673
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9127996688250763
Iteration: 2 || Loss: 2.3096202461455446
Iteration: 3 || Loss: 1.8308999420817804
Iteration: 4 || Loss: 1.827905406338923
Iteration: 5 || Loss: 1.7926274257051986
Iteration: 6 || Loss: 1.7877433458433987
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.725525
Epoch 311 loss:1.7877433458433987
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.725525
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.687018710756853
Iteration: 2 || Loss: 6.686673080763509
Iteration: 3 || Loss: 6.686331212923446
Iteration: 4 || Loss: 6.68598824738543
Iteration: 5 || Loss: 6.6856475998190446
Iteration: 6 || Loss: 6.6856475998190446
saving ADAM checkpoint...
Sum of params:83.72556
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.6856475998190446
Iteration: 2 || Loss: 6.658877110658204
Iteration: 3 || Loss: 6.572074228233333
Iteration: 4 || Loss: 6.45741639479949
Iteration: 5 || Loss: 6.451346708747494
Iteration: 6 || Loss: 6.4374384996239815
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.749596
Epoch 311 loss:6.4374384996239815
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.749596
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.51205951127514
Iteration: 2 || Loss: 21.51167154615084
Iteration: 3 || Loss: 21.511281075287698
Iteration: 4 || Loss: 21.510892967620887
Iteration: 5 || Loss: 21.510507983540705
Iteration: 6 || Loss: 21.510507983540705
saving ADAM checkpoint...
Sum of params:83.749565
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.510507983540705
Iteration: 2 || Loss: 21.471787749572137
Iteration: 3 || Loss: 21.260023518373817
Iteration: 4 || Loss: 21.12182328226766
Iteration: 5 || Loss: 21.08979008474971
Iteration: 6 || Loss: 21.05798296539468
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.64965
Epoch 311 loss:21.05798296539468
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.299635252824347
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.23053741347022
waveform batch: 2/2
Test loss - extrapolation:17.71317671803376
Epoch 311 mean train loss:1.0097643038228297
Epoch 311 mean test loss - interpolation:1.0499392088040578
Epoch 311 mean test loss - extrapolation:5.078642844291998
Start training epoch 312
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.64965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9151868232161733
Iteration: 2 || Loss: 2.9137564958234434
Iteration: 3 || Loss: 2.9123257446516972
Iteration: 4 || Loss: 2.910898884715215
Iteration: 5 || Loss: 2.909473681264231
Iteration: 6 || Loss: 2.909473681264231
saving ADAM checkpoint...
Sum of params:83.649635
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.909473681264231
Iteration: 2 || Loss: 2.306398375640056
Iteration: 3 || Loss: 1.8284907885411248
Iteration: 4 || Loss: 1.8255169654483328
Iteration: 5 || Loss: 1.7904046890268694
Iteration: 6 || Loss: 1.78566554295951
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.73822
Epoch 312 loss:1.78566554295951
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.73822
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.681636443076281
Iteration: 2 || Loss: 6.681291330926085
Iteration: 3 || Loss: 6.680948198511721
Iteration: 4 || Loss: 6.680606066653605
Iteration: 5 || Loss: 6.680264394204809
Iteration: 6 || Loss: 6.680264394204809
saving ADAM checkpoint...
Sum of params:83.73825
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.680264394204809
Iteration: 2 || Loss: 6.6534224019399355
Iteration: 3 || Loss: 6.566863029016473
Iteration: 4 || Loss: 6.452417612164275
Iteration: 5 || Loss: 6.446334790304216
Iteration: 6 || Loss: 6.432543762839143
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.76253
Epoch 312 loss:6.432543762839143
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.76253
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.493259241077407
Iteration: 2 || Loss: 21.492870036351583
Iteration: 3 || Loss: 21.49248060216951
Iteration: 4 || Loss: 21.492091829270244
Iteration: 5 || Loss: 21.491704448920203
Iteration: 6 || Loss: 21.491704448920203
saving ADAM checkpoint...
Sum of params:83.76251
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.491704448920203
Iteration: 2 || Loss: 21.452800000284483
Iteration: 3 || Loss: 21.24119869513167
Iteration: 4 || Loss: 21.10338837219594
Iteration: 5 || Loss: 21.07140298644035
Iteration: 6 || Loss: 21.039822049642332
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.66255
Epoch 312 loss:21.039822049642332
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.294739400364186
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.20811261975338
waveform batch: 2/2
Test loss - extrapolation:17.698505272401633
Epoch 312 mean train loss:1.008897632946241
Epoch 312 mean test loss - interpolation:1.049123233394031
Epoch 312 mean test loss - extrapolation:5.075551491012917
Start training epoch 313
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.66255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.9117283082671275
Iteration: 2 || Loss: 2.910296807709834
Iteration: 3 || Loss: 2.908870574344661
Iteration: 4 || Loss: 2.907441794366884
Iteration: 5 || Loss: 2.9060176279149776
Iteration: 6 || Loss: 2.9060176279149776
saving ADAM checkpoint...
Sum of params:83.66258
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.9060176279149776
Iteration: 2 || Loss: 2.3033868659202525
Iteration: 3 || Loss: 1.8260969632773065
Iteration: 4 || Loss: 1.8231416831421756
Iteration: 5 || Loss: 1.7881958995146483
Iteration: 6 || Loss: 1.7836060812482584
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.7509
Epoch 313 loss:1.7836060812482584
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.7509
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.676286114509745
Iteration: 2 || Loss: 6.675941622310663
Iteration: 3 || Loss: 6.675595471301317
Iteration: 4 || Loss: 6.6752540338037285
Iteration: 5 || Loss: 6.674913076573499
Iteration: 6 || Loss: 6.674913076573499
saving ADAM checkpoint...
Sum of params:83.75094
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.674913076573499
Iteration: 2 || Loss: 6.648010432792393
Iteration: 3 || Loss: 6.561690478402422
Iteration: 4 || Loss: 6.447457206686663
Iteration: 5 || Loss: 6.4413668674944935
Iteration: 6 || Loss: 6.427697293163614
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.77546
Epoch 313 loss:6.427697293163614
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.77546
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.474479880085276
Iteration: 2 || Loss: 21.474087611497907
Iteration: 3 || Loss: 21.47369722852152
Iteration: 4 || Loss: 21.473309099118215
Iteration: 5 || Loss: 21.47292146139435
Iteration: 6 || Loss: 21.47292146139435
saving ADAM checkpoint...
Sum of params:83.775444
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.47292146139435
Iteration: 2 || Loss: 21.43386790606402
Iteration: 3 || Loss: 21.222464024458997
Iteration: 4 || Loss: 21.085080196031978
Iteration: 5 || Loss: 21.053127121087442
Iteration: 6 || Loss: 21.021753392605216
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.675446
Epoch 313 loss:21.021753392605216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.289883440825788
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.18585744435349
waveform batch: 2/2
Test loss - extrapolation:17.68387561733357
Epoch 313 mean train loss:1.0080364402419686
Epoch 313 mean test loss - interpolation:1.048313906804298
Epoch 313 mean test loss - extrapolation:5.072477755140588
Start training epoch 314
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.675446
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.90849289481742
Iteration: 2 || Loss: 2.9070617424773486
Iteration: 3 || Loss: 2.9056315730953304
Iteration: 4 || Loss: 2.9042041122364197
Iteration: 5 || Loss: 2.902776422696774
Iteration: 6 || Loss: 2.902776422696774
saving ADAM checkpoint...
Sum of params:83.67544
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.902776422696774
Iteration: 2 || Loss: 2.3003196899589975
Iteration: 3 || Loss: 1.8237557649542266
Iteration: 4 || Loss: 1.8208195259256454
Iteration: 5 || Loss: 1.7860467991399103
Iteration: 6 || Loss: 1.7815837467703486
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.763565
Epoch 314 loss:1.7815837467703486
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.763565
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.671045078890926
Iteration: 2 || Loss: 6.670699615643956
Iteration: 3 || Loss: 6.670354692188002
Iteration: 4 || Loss: 6.670011128130509
Iteration: 5 || Loss: 6.669669374174724
Iteration: 6 || Loss: 6.669669374174724
saving ADAM checkpoint...
Sum of params:83.76359
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.669669374174724
Iteration: 2 || Loss: 6.642694454413158
Iteration: 3 || Loss: 6.556604640613922
Iteration: 4 || Loss: 6.442573563777979
Iteration: 5 || Loss: 6.4364750486618085
Iteration: 6 || Loss: 6.422916293328526
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.78836
Epoch 314 loss:6.422916293328526
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.78836
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.4558680069214
Iteration: 2 || Loss: 21.45547520126813
Iteration: 3 || Loss: 21.45508394200998
Iteration: 4 || Loss: 21.454695059111035
Iteration: 5 || Loss: 21.454305737361633
Iteration: 6 || Loss: 21.454305737361633
saving ADAM checkpoint...
Sum of params:83.78834
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.454305737361633
Iteration: 2 || Loss: 21.41508687887426
Iteration: 3 || Loss: 21.203867814216288
Iteration: 4 || Loss: 21.06687167830289
Iteration: 5 || Loss: 21.03496154273024
Iteration: 6 || Loss: 21.00380183043879
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.68833
Epoch 314 loss:21.00380183043879
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.285077179911461
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.16361404157106
waveform batch: 2/2
Test loss - extrapolation:17.669292577472863
Epoch 314 mean train loss:1.0071828231219884
Epoch 314 mean test loss - interpolation:1.0475128633185768
Epoch 314 mean test loss - extrapolation:5.069408884920327
Start training epoch 315
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.68833
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.905210825425375
Iteration: 2 || Loss: 2.903779154234567
Iteration: 3 || Loss: 2.9023523612277002
Iteration: 4 || Loss: 2.9009252084895243
Iteration: 5 || Loss: 2.8994971240727963
Iteration: 6 || Loss: 2.8994971240727963
saving ADAM checkpoint...
Sum of params:83.68833
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8994971240727963
Iteration: 2 || Loss: 2.297330916119533
Iteration: 3 || Loss: 1.8214433813104443
Iteration: 4 || Loss: 1.8185246638853558
Iteration: 5 || Loss: 1.7838912416234416
Iteration: 6 || Loss: 1.7795853983724843
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.77619
Epoch 315 loss:1.7795853983724843
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.77619
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.665830458120932
Iteration: 2 || Loss: 6.665483847287293
Iteration: 3 || Loss: 6.665138016541916
Iteration: 4 || Loss: 6.664793841398241
Iteration: 5 || Loss: 6.664451712977466
Iteration: 6 || Loss: 6.664451712977466
saving ADAM checkpoint...
Sum of params:83.77623
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.664451712977466
Iteration: 2 || Loss: 6.637411856505299
Iteration: 3 || Loss: 6.551569005013739
Iteration: 4 || Loss: 6.437737240379339
Iteration: 5 || Loss: 6.431628201330692
Iteration: 6 || Loss: 6.418184006856381
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.801254
Epoch 315 loss:6.418184006856381
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.801254
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.43737093933748
Iteration: 2 || Loss: 21.43697678097454
Iteration: 3 || Loss: 21.43658435666648
Iteration: 4 || Loss: 21.436194656704338
Iteration: 5 || Loss: 21.435802306325083
Iteration: 6 || Loss: 21.435802306325083
saving ADAM checkpoint...
Sum of params:83.801216
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.435802306325083
Iteration: 2 || Loss: 21.396431344119573
Iteration: 3 || Loss: 21.185375718009464
Iteration: 4 || Loss: 21.048783543547646
Iteration: 5 || Loss: 21.016911129047287
Iteration: 6 || Loss: 20.985945851345676
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.70118
Epoch 315 loss:20.985945851345676
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.280318576452549
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.14148602832905
waveform batch: 2/2
Test loss - extrapolation:17.654764233533143
Epoch 315 mean train loss:1.006335008847398
Epoch 315 mean test loss - interpolation:1.0467197627420914
Epoch 315 mean test loss - extrapolation:5.066354188488516
Start training epoch 316
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.70118
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.902056699323544
Iteration: 2 || Loss: 2.9006249346502258
Iteration: 3 || Loss: 2.899194796011024
Iteration: 4 || Loss: 2.897764676272533
Iteration: 5 || Loss: 2.896338177117266
Iteration: 6 || Loss: 2.896338177117266
saving ADAM checkpoint...
Sum of params:83.70119
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.896338177117266
Iteration: 2 || Loss: 2.2944132652647973
Iteration: 3 || Loss: 1.8191715133263104
Iteration: 4 || Loss: 1.8162697798533916
Iteration: 5 || Loss: 1.7818077219050006
Iteration: 6 || Loss: 1.77762004403615
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.78883
Epoch 316 loss:1.77762004403615
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.78883
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.660712138234094
Iteration: 2 || Loss: 6.66036594020453
Iteration: 3 || Loss: 6.6600202967817514
Iteration: 4 || Loss: 6.659674991380731
Iteration: 5 || Loss: 6.65933171245935
Iteration: 6 || Loss: 6.65933171245935
saving ADAM checkpoint...
Sum of params:83.78887
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.65933171245935
Iteration: 2 || Loss: 6.632222882206249
Iteration: 3 || Loss: 6.54660483924547
Iteration: 4 || Loss: 6.4329646701017005
Iteration: 5 || Loss: 6.426847018061923
Iteration: 6 || Loss: 6.413512469716099
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.81413
Epoch 316 loss:6.413512469716099
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.81413
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.419010427840877
Iteration: 2 || Loss: 21.418616823041564
Iteration: 3 || Loss: 21.418224179262708
Iteration: 4 || Loss: 21.417831838247764
Iteration: 5 || Loss: 21.417441992892623
Iteration: 6 || Loss: 21.417441992892623
saving ADAM checkpoint...
Sum of params:83.8141
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.417441992892623
Iteration: 2 || Loss: 21.377907425845446
Iteration: 3 || Loss: 21.166991085097337
Iteration: 4 || Loss: 21.030792807339374
Iteration: 5 || Loss: 20.99896254622392
Iteration: 6 || Loss: 20.968200180701437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.71402
Epoch 316 loss:20.968200180701437
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.275633532072329
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.119417764731075
waveform batch: 2/2
Test loss - extrapolation:17.64026352771534
Epoch 316 mean train loss:1.0054942308432306
Epoch 316 mean test loss - interpolation:1.045938922012055
Epoch 316 mean test loss - extrapolation:5.063306774370535
Start training epoch 317
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.71402
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8989884566709194
Iteration: 2 || Loss: 2.89755329599015
Iteration: 3 || Loss: 2.8961230967527185
Iteration: 4 || Loss: 2.8946936957033746
Iteration: 5 || Loss: 2.8932644451196277
Iteration: 6 || Loss: 2.8932644451196277
saving ADAM checkpoint...
Sum of params:83.71404
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8932644451196277
Iteration: 2 || Loss: 2.291566050605344
Iteration: 3 || Loss: 1.816927464054294
Iteration: 4 || Loss: 1.8140435798226617
Iteration: 5 || Loss: 1.77973258795237
Iteration: 6 || Loss: 1.7756785532693788
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.80146
Epoch 317 loss:1.7756785532693788
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.80146
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.655632848567711
Iteration: 2 || Loss: 6.655284280150344
Iteration: 3 || Loss: 6.654938247310268
Iteration: 4 || Loss: 6.65459419559522
Iteration: 5 || Loss: 6.654252094016437
Iteration: 6 || Loss: 6.654252094016437
saving ADAM checkpoint...
Sum of params:83.8015
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.654252094016437
Iteration: 2 || Loss: 6.627074167658542
Iteration: 3 || Loss: 6.541694971856927
Iteration: 4 || Loss: 6.428244519511619
Iteration: 5 || Loss: 6.422116256868531
Iteration: 6 || Loss: 6.408889051848606
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.82701
Epoch 317 loss:6.408889051848606
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.82701
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.400752618740544
Iteration: 2 || Loss: 21.40035875183245
Iteration: 3 || Loss: 21.39996515857116
Iteration: 4 || Loss: 21.39957108569934
Iteration: 5 || Loss: 21.399180335916125
Iteration: 6 || Loss: 21.399180335916125
saving ADAM checkpoint...
Sum of params:83.82697
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.399180335916125
Iteration: 2 || Loss: 21.359484508941364
Iteration: 3 || Loss: 21.148736607396167
Iteration: 4 || Loss: 21.012919532610624
Iteration: 5 || Loss: 20.981127215090325
Iteration: 6 || Loss: 20.950563484828987
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.72685
Epoch 317 loss:20.950563484828987
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.27096199406403
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.09740385717934
waveform batch: 2/2
Test loss - extrapolation:17.625822160012383
Epoch 317 mean train loss:1.0046596927567921
Epoch 317 mean test loss - interpolation:1.045160332344005
Epoch 317 mean test loss - extrapolation:5.060268834765977
Start training epoch 318
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.72685
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8958865667068854
Iteration: 2 || Loss: 2.894455118640644
Iteration: 3 || Loss: 2.8930238142553377
Iteration: 4 || Loss: 2.891592113759866
Iteration: 5 || Loss: 2.890165943813977
Iteration: 6 || Loss: 2.890165943813977
saving ADAM checkpoint...
Sum of params:83.72685
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.890165943813977
Iteration: 2 || Loss: 2.2887048095774114
Iteration: 3 || Loss: 1.814718706965009
Iteration: 4 || Loss: 1.811849427165255
Iteration: 5 || Loss: 1.7777148617877327
Iteration: 6 || Loss: 1.773764018088961
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.81407
Epoch 318 loss:1.773764018088961
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.81407
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.650633242357996
Iteration: 2 || Loss: 6.650286702933143
Iteration: 3 || Loss: 6.649938355883023
Iteration: 4 || Loss: 6.649592638634494
Iteration: 5 || Loss: 6.649249313339392
Iteration: 6 || Loss: 6.649249313339392
saving ADAM checkpoint...
Sum of params:83.81411
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.649249313339392
Iteration: 2 || Loss: 6.622002387671345
Iteration: 3 || Loss: 6.536838746966807
Iteration: 4 || Loss: 6.423576488026828
Iteration: 5 || Loss: 6.41743964571767
Iteration: 6 || Loss: 6.4043197652363295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.83984
Epoch 318 loss:6.4043197652363295
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.83984
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.382591834798188
Iteration: 2 || Loss: 21.382197703852178
Iteration: 3 || Loss: 21.381802777352362
Iteration: 4 || Loss: 21.381409049818572
Iteration: 5 || Loss: 21.381016958798956
Iteration: 6 || Loss: 21.381016958798956
saving ADAM checkpoint...
Sum of params:83.839836
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.381016958798956
Iteration: 2 || Loss: 21.341156262009214
Iteration: 3 || Loss: 21.130564656779438
Iteration: 4 || Loss: 20.99514703117777
Iteration: 5 || Loss: 20.963399253500004
Iteration: 6 || Loss: 20.93302320672896
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.739685
Epoch 318 loss:20.93302320672896
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.266358412974615
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.07547957214502
waveform batch: 2/2
Test loss - extrapolation:17.611423897789358
Epoch 318 mean train loss:1.003831275519112
Epoch 318 mean test loss - interpolation:1.0443930688291025
Epoch 318 mean test loss - extrapolation:5.057241955827865
Start training epoch 319
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.739685
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.892906923242057
Iteration: 2 || Loss: 2.891475779554032
Iteration: 3 || Loss: 2.8900440229916695
Iteration: 4 || Loss: 2.888612765700107
Iteration: 5 || Loss: 2.8871831602956632
Iteration: 6 || Loss: 2.8871831602956632
saving ADAM checkpoint...
Sum of params:83.73968
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8871831602956632
Iteration: 2 || Loss: 2.2859607800275854
Iteration: 3 || Loss: 1.8125410965676045
Iteration: 4 || Loss: 1.8096875716620653
Iteration: 5 || Loss: 1.7757193197704608
Iteration: 6 || Loss: 1.7718761208624167
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.82667
Epoch 319 loss:1.7718761208624167
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.82667
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.645698075967153
Iteration: 2 || Loss: 6.645348470414625
Iteration: 3 || Loss: 6.6450010813358515
Iteration: 4 || Loss: 6.644656606417084
Iteration: 5 || Loss: 6.644312313295699
Iteration: 6 || Loss: 6.644312313295699
saving ADAM checkpoint...
Sum of params:83.82671
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.644312313295699
Iteration: 2 || Loss: 6.616987600649661
Iteration: 3 || Loss: 6.5320455817915555
Iteration: 4 || Loss: 6.418965162512503
Iteration: 5 || Loss: 6.4128198969218895
Iteration: 6 || Loss: 6.39980298664112
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85271
Epoch 319 loss:6.39980298664112
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.85271
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.364525398899282
Iteration: 2 || Loss: 21.364128888979884
Iteration: 3 || Loss: 21.3637347270325
Iteration: 4 || Loss: 21.3633414665145
Iteration: 5 || Loss: 21.362946899539356
Iteration: 6 || Loss: 21.362946899539356
saving ADAM checkpoint...
Sum of params:83.85269
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.362946899539356
Iteration: 2 || Loss: 21.322935384381218
Iteration: 3 || Loss: 21.112502856817883
Iteration: 4 || Loss: 20.977473553400316
Iteration: 5 || Loss: 20.945762966128555
Iteration: 6 || Loss: 20.915574213751487
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.75251
Epoch 319 loss:20.915574213751487
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.261777402092884
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.05363351502004
waveform batch: 2/2
Test loss - extrapolation:17.597076776429542
Epoch 319 mean train loss:1.0030087352156905
Epoch 319 mean test loss - interpolation:1.0436295670154807
Epoch 319 mean test loss - extrapolation:5.054225857620799
Start training epoch 320
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.75251
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8899684587401873
Iteration: 2 || Loss: 2.888533615663033
Iteration: 3 || Loss: 2.8871024493324837
Iteration: 4 || Loss: 2.885671722363032
Iteration: 5 || Loss: 2.884243614419284
Iteration: 6 || Loss: 2.884243614419284
saving ADAM checkpoint...
Sum of params:83.75251
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.884243614419284
Iteration: 2 || Loss: 2.2831624930241676
Iteration: 3 || Loss: 1.8103984797489654
Iteration: 4 || Loss: 1.807560208651481
Iteration: 5 || Loss: 1.7737490759704195
Iteration: 6 || Loss: 1.7700152588544302
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.83927
Epoch 320 loss:1.7700152588544302
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.83927
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.6408212343484205
Iteration: 2 || Loss: 6.64047049551664
Iteration: 3 || Loss: 6.640123386240879
Iteration: 4 || Loss: 6.639777000822511
Iteration: 5 || Loss: 6.639431555257691
Iteration: 6 || Loss: 6.639431555257691
saving ADAM checkpoint...
Sum of params:83.83931
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.639431555257691
Iteration: 2 || Loss: 6.612035481480029
Iteration: 3 || Loss: 6.527321740917308
Iteration: 4 || Loss: 6.414412673084686
Iteration: 5 || Loss: 6.408258710360859
Iteration: 6 || Loss: 6.395336903881527
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.86552
Epoch 320 loss:6.395336903881527
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.86552
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.346620177459595
Iteration: 2 || Loss: 21.34622207107964
Iteration: 3 || Loss: 21.345826501533963
Iteration: 4 || Loss: 21.345430725195495
Iteration: 5 || Loss: 21.345037440477878
Iteration: 6 || Loss: 21.345037440477878
saving ADAM checkpoint...
Sum of params:83.86553
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.345037440477878
Iteration: 2 || Loss: 21.304848351070856
Iteration: 3 || Loss: 21.094562210287727
Iteration: 4 || Loss: 20.9599039933868
Iteration: 5 || Loss: 20.928239052813446
Iteration: 6 || Loss: 20.898244379660902
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.76533
Epoch 320 loss:20.898244379660902
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.257257071720404
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.03179659940316
waveform batch: 2/2
Test loss - extrapolation:17.582767580041367
Epoch 320 mean train loss:1.0021929842205812
Epoch 320 mean test loss - interpolation:1.0428761786200673
Epoch 320 mean test loss - extrapolation:5.051213681620378
Start training epoch 321
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.76533
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8869930645837343
Iteration: 2 || Loss: 2.8855581022054393
Iteration: 3 || Loss: 2.8841252382673015
Iteration: 4 || Loss: 2.8826961330036687
Iteration: 5 || Loss: 2.8812663059901933
Iteration: 6 || Loss: 2.8812663059901933
saving ADAM checkpoint...
Sum of params:83.76532
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8812663059901933
Iteration: 2 || Loss: 2.2804963758762433
Iteration: 3 || Loss: 1.8082771443208467
Iteration: 4 || Loss: 1.805452530015259
Iteration: 5 || Loss: 1.7717820573490837
Iteration: 6 || Loss: 1.7681738868652275
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85187
Epoch 321 loss:1.7681738868652275
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.85187
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.635951751125478
Iteration: 2 || Loss: 6.635602434875538
Iteration: 3 || Loss: 6.6352540817170365
Iteration: 4 || Loss: 6.6349077550511595
Iteration: 5 || Loss: 6.634562978942478
Iteration: 6 || Loss: 6.634562978942478
saving ADAM checkpoint...
Sum of params:83.85191
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.634562978942478
Iteration: 2 || Loss: 6.607108185640966
Iteration: 3 || Loss: 6.522627695786516
Iteration: 4 || Loss: 6.4098964733683665
Iteration: 5 || Loss: 6.403732387259045
Iteration: 6 || Loss: 6.390910452570267
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.87833
Epoch 321 loss:6.390910452570267
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.87833
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.328852325727805
Iteration: 2 || Loss: 21.328451978458585
Iteration: 3 || Loss: 21.328054798540787
Iteration: 4 || Loss: 21.327661576959315
Iteration: 5 || Loss: 21.327265776809654
Iteration: 6 || Loss: 21.327265776809654
saving ADAM checkpoint...
Sum of params:83.87836
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.327265776809654
Iteration: 2 || Loss: 21.28691697618435
Iteration: 3 || Loss: 21.076763441549396
Iteration: 4 || Loss: 20.942464992468903
Iteration: 5 || Loss: 20.91084176700389
Iteration: 6 || Loss: 20.881027762531122
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.77812
Epoch 321 loss:20.881027762531122
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.252783734139507
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:43.010082175700674
waveform batch: 2/2
Test loss - extrapolation:17.568515404571922
Epoch 321 mean train loss:1.0013831759298832
Epoch 321 mean test loss - interpolation:1.0421306223565845
Epoch 321 mean test loss - extrapolation:5.048216465022716
Start training epoch 322
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.77812
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8841567293931156
Iteration: 2 || Loss: 2.882724392752531
Iteration: 3 || Loss: 2.881290657118894
Iteration: 4 || Loss: 2.8798575265765587
Iteration: 5 || Loss: 2.8784261831677624
Iteration: 6 || Loss: 2.8784261831677624
saving ADAM checkpoint...
Sum of params:83.778145
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8784261831677624
Iteration: 2 || Loss: 2.2778793120937144
Iteration: 3 || Loss: 1.8061893481481914
Iteration: 4 || Loss: 1.8033795933936352
Iteration: 5 || Loss: 1.769861411586646
Iteration: 6 || Loss: 1.7663563282901908
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.86448
Epoch 322 loss:1.7663563282901908
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.86448
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.6311758693851
Iteration: 2 || Loss: 6.630824871296861
Iteration: 3 || Loss: 6.630476364440859
Iteration: 4 || Loss: 6.630130066502613
Iteration: 5 || Loss: 6.629783204649385
Iteration: 6 || Loss: 6.629783204649385
saving ADAM checkpoint...
Sum of params:83.86451
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.629783204649385
Iteration: 2 || Loss: 6.602249957924866
Iteration: 3 || Loss: 6.517987185599834
Iteration: 4 || Loss: 6.405424782635117
Iteration: 5 || Loss: 6.399251845145623
Iteration: 6 || Loss: 6.386529439119957
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.891174
Epoch 322 loss:6.386529439119957
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.891174
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.3111623280502
Iteration: 2 || Loss: 21.310764433730075
Iteration: 3 || Loss: 21.31036766339611
Iteration: 4 || Loss: 21.309969076838623
Iteration: 5 || Loss: 21.309572149085266
Iteration: 6 || Loss: 21.309572149085266
saving ADAM checkpoint...
Sum of params:83.891205
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.309572149085266
Iteration: 2 || Loss: 21.269067173243332
Iteration: 3 || Loss: 21.05902847147803
Iteration: 4 || Loss: 20.92511855158909
Iteration: 5 || Loss: 20.89353566135513
Iteration: 6 || Loss: 20.863895146062895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.7909
Epoch 322 loss:20.863895146062895
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.248345995633063
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.98847542577097
waveform batch: 2/2
Test loss - extrapolation:17.554307478355152
Epoch 322 mean train loss:1.0005786521887257
Epoch 322 mean test loss - interpolation:1.041390999272177
Epoch 322 mean test loss - extrapolation:5.045231908677177
Start training epoch 323
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.7909
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.881417307527568
Iteration: 2 || Loss: 2.879984650926798
Iteration: 3 || Loss: 2.8785500578124683
Iteration: 4 || Loss: 2.877115307168974
Iteration: 5 || Loss: 2.8756870291913503
Iteration: 6 || Loss: 2.8756870291913503
saving ADAM checkpoint...
Sum of params:83.79091
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8756870291913503
Iteration: 2 || Loss: 2.2752516294995595
Iteration: 3 || Loss: 1.8041352559000015
Iteration: 4 || Loss: 1.8013378604400894
Iteration: 5 || Loss: 1.7679915774506554
Iteration: 6 || Loss: 1.7645673779818376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.877045
Epoch 323 loss:1.7645673779818376
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.877045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.626461949493266
Iteration: 2 || Loss: 6.626111085035795
Iteration: 3 || Loss: 6.625762502452745
Iteration: 4 || Loss: 6.625414714521479
Iteration: 5 || Loss: 6.625067603983135
Iteration: 6 || Loss: 6.625067603983135
saving ADAM checkpoint...
Sum of params:83.87708
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.625067603983135
Iteration: 2 || Loss: 6.597454148916439
Iteration: 3 || Loss: 6.513404924819964
Iteration: 4 || Loss: 6.401012609283517
Iteration: 5 || Loss: 6.394830294632128
Iteration: 6 || Loss: 6.382201308890106
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.90398
Epoch 323 loss:6.382201308890106
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.90398
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.29361365498668
Iteration: 2 || Loss: 21.293213815767963
Iteration: 3 || Loss: 21.292815700201228
Iteration: 4 || Loss: 21.292416375722
Iteration: 5 || Loss: 21.29202105806492
Iteration: 6 || Loss: 21.29202105806492
saving ADAM checkpoint...
Sum of params:83.904015
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.29202105806492
Iteration: 2 || Loss: 21.251339184860285
Iteration: 3 || Loss: 21.04142609454465
Iteration: 4 || Loss: 20.90786887844091
Iteration: 5 || Loss: 20.87633089954449
Iteration: 6 || Loss: 20.846874458861894
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.80369
Epoch 323 loss:20.846874458861894
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.243949632620744
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.966872051125726
waveform batch: 2/2
Test loss - extrapolation:17.540152362427154
Epoch 323 mean train loss:0.999780798128753
Epoch 323 mean test loss - interpolation:1.0406582721034574
Epoch 323 mean test loss - extrapolation:5.042252034462741
Start training epoch 324
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.80369
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8786131107368464
Iteration: 2 || Loss: 2.8771778680185487
Iteration: 3 || Loss: 2.8757430885955637
Iteration: 4 || Loss: 2.8743112643460953
Iteration: 5 || Loss: 2.8728768537069014
Iteration: 6 || Loss: 2.8728768537069014
saving ADAM checkpoint...
Sum of params:83.80368
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8728768537069014
Iteration: 2 || Loss: 2.27271071563548
Iteration: 3 || Loss: 1.8021003158659479
Iteration: 4 || Loss: 1.7993164012352751
Iteration: 5 || Loss: 1.766113642199027
Iteration: 6 || Loss: 1.7627949165142696
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.88962
Epoch 324 loss:1.7627949165142696
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.88962
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.621770717713022
Iteration: 2 || Loss: 6.621419968669188
Iteration: 3 || Loss: 6.621070432747028
Iteration: 4 || Loss: 6.620721722657772
Iteration: 5 || Loss: 6.6203739625618665
Iteration: 6 || Loss: 6.6203739625618665
saving ADAM checkpoint...
Sum of params:83.88965
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.6203739625618665
Iteration: 2 || Loss: 6.5926944307273185
Iteration: 3 || Loss: 6.50886523678142
Iteration: 4 || Loss: 6.396635049004264
Iteration: 5 || Loss: 6.390444499909107
Iteration: 6 || Loss: 6.377909770020885
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.91677
Epoch 324 loss:6.377909770020885
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.91677
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.276148315619917
Iteration: 2 || Loss: 21.2757462164104
Iteration: 3 || Loss: 21.275345026797964
Iteration: 4 || Loss: 21.274948301616963
Iteration: 5 || Loss: 21.274551995785252
Iteration: 6 || Loss: 21.274551995785252
saving ADAM checkpoint...
Sum of params:83.91681
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.274551995785252
Iteration: 2 || Loss: 21.233724992433974
Iteration: 3 || Loss: 21.023898779907015
Iteration: 4 || Loss: 20.890726600747218
Iteration: 5 || Loss: 20.859225909632006
Iteration: 6 || Loss: 20.829931556810013
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.81643
Epoch 324 loss:20.829931556810013
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.23959102673112
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.945430571920994
waveform batch: 2/2
Test loss - extrapolation:17.526044891788686
Epoch 324 mean train loss:0.9989874566670748
Epoch 324 mean test loss - interpolation:1.03993183778852
Epoch 324 mean test loss - extrapolation:5.039289621975807
Start training epoch 325
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.81643
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8759903361730617
Iteration: 2 || Loss: 2.8745533895163677
Iteration: 3 || Loss: 2.8731213333236316
Iteration: 4 || Loss: 2.8716845919298
Iteration: 5 || Loss: 2.870251840254243
Iteration: 6 || Loss: 2.870251840254243
saving ADAM checkpoint...
Sum of params:83.81644
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.870251840254243
Iteration: 2 || Loss: 2.270143743869203
Iteration: 3 || Loss: 1.8001062486734658
Iteration: 4 || Loss: 1.7973345674258037
Iteration: 5 || Loss: 1.7642806054484101
Iteration: 6 || Loss: 1.7610520698680558
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.90217
Epoch 325 loss:1.7610520698680558
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.90217
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.6171576619103485
Iteration: 2 || Loss: 6.616804874466005
Iteration: 3 || Loss: 6.61645458815898
Iteration: 4 || Loss: 6.616106257550096
Iteration: 5 || Loss: 6.6157577074002365
Iteration: 6 || Loss: 6.6157577074002365
saving ADAM checkpoint...
Sum of params:83.90221
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.6157577074002365
Iteration: 2 || Loss: 6.58800018607682
Iteration: 3 || Loss: 6.504390415702115
Iteration: 4 || Loss: 6.392317267854237
Iteration: 5 || Loss: 6.3861177588847235
Iteration: 6 || Loss: 6.373674075120785
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.929535
Epoch 325 loss:6.373674075120785
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.929535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.25878417550698
Iteration: 2 || Loss: 21.258383213821972
Iteration: 3 || Loss: 21.257981091037063
Iteration: 4 || Loss: 21.257582777894797
Iteration: 5 || Loss: 21.25718526458856
Iteration: 6 || Loss: 21.25718526458856
saving ADAM checkpoint...
Sum of params:83.92957
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.25718526458856
Iteration: 2 || Loss: 21.216182759056668
Iteration: 3 || Loss: 21.006505937392813
Iteration: 4 || Loss: 20.873669977573375
Iteration: 5 || Loss: 20.84221367737791
Iteration: 6 || Loss: 20.813094977114986
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.82917
Epoch 325 loss:20.813094977114986
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.235249553722942
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.92395643000865
waveform batch: 2/2
Test loss - extrapolation:17.51200118592287
Epoch 325 mean train loss:0.9982007283484078
Epoch 325 mean test loss - interpolation:1.0392082589538236
Epoch 325 mean test loss - extrapolation:5.036329801327627
Start training epoch 326
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.82917
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8732064991664914
Iteration: 2 || Loss: 2.871767481449502
Iteration: 3 || Loss: 2.870330555676074
Iteration: 4 || Loss: 2.868897691780655
Iteration: 5 || Loss: 2.86746515274746
Iteration: 6 || Loss: 2.86746515274746
saving ADAM checkpoint...
Sum of params:83.829155
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.86746515274746
Iteration: 2 || Loss: 2.2676591101397916
Iteration: 3 || Loss: 1.7981246542931162
Iteration: 4 || Loss: 1.795365232244958
Iteration: 5 || Loss: 1.7624585493473377
Iteration: 6 || Loss: 1.7593248691715355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.91468
Epoch 326 loss:1.7593248691715355
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.91468
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.61257515763266
Iteration: 2 || Loss: 6.612222874279052
Iteration: 3 || Loss: 6.611871703417896
Iteration: 4 || Loss: 6.611521936698304
Iteration: 5 || Loss: 6.611174220784306
Iteration: 6 || Loss: 6.611174220784306
saving ADAM checkpoint...
Sum of params:83.91474
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.611174220784306
Iteration: 2 || Loss: 6.583347541085858
Iteration: 3 || Loss: 6.499952543562664
Iteration: 4 || Loss: 6.388035600070642
Iteration: 5 || Loss: 6.381827209480387
Iteration: 6 || Loss: 6.369470916500012
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.94227
Epoch 326 loss:6.369470916500012
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.94227
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.241529106001583
Iteration: 2 || Loss: 21.24112680716996
Iteration: 3 || Loss: 21.240724371124564
Iteration: 4 || Loss: 21.24032320632173
Iteration: 5 || Loss: 21.23992495639436
Iteration: 6 || Loss: 21.23992495639436
saving ADAM checkpoint...
Sum of params:83.94231
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.23992495639436
Iteration: 2 || Loss: 21.198774087546212
Iteration: 3 || Loss: 20.989193879745574
Iteration: 4 || Loss: 20.856723805208475
Iteration: 5 || Loss: 20.825306727353087
Iteration: 6 || Loss: 20.796353028478812
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.84188
Epoch 326 loss:20.796353028478812
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.230989423826207
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.902654913229405
waveform batch: 2/2
Test loss - extrapolation:17.49798131796753
Epoch 326 mean train loss:0.9974189246258746
Epoch 326 mean test loss - interpolation:1.0384982373043679
Epoch 326 mean test loss - extrapolation:5.033386352599744
Start training epoch 327
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.84188
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8707025048389156
Iteration: 2 || Loss: 2.869265542384517
Iteration: 3 || Loss: 2.867826478043924
Iteration: 4 || Loss: 2.8663906520756224
Iteration: 5 || Loss: 2.864958391904254
Iteration: 6 || Loss: 2.864958391904254
saving ADAM checkpoint...
Sum of params:83.841866
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.864958391904254
Iteration: 2 || Loss: 2.2651926348886375
Iteration: 3 || Loss: 1.7961790016544785
Iteration: 4 || Loss: 1.793432423990268
Iteration: 5 || Loss: 1.7606660778612027
Iteration: 6 || Loss: 1.7576224269296057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.9272
Epoch 327 loss:1.7576224269296057
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.9272
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.608042035440281
Iteration: 2 || Loss: 6.607689787828414
Iteration: 3 || Loss: 6.607338706069089
Iteration: 4 || Loss: 6.606987327141199
Iteration: 5 || Loss: 6.606638010050947
Iteration: 6 || Loss: 6.606638010050947
saving ADAM checkpoint...
Sum of params:83.927246
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.606638010050947
Iteration: 2 || Loss: 6.5787404048600076
Iteration: 3 || Loss: 6.4955657758901335
Iteration: 4 || Loss: 6.383800185651954
Iteration: 5 || Loss: 6.377583502828945
Iteration: 6 || Loss: 6.365316098289038
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.95502
Epoch 327 loss:6.365316098289038
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.95502
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.224364682810904
Iteration: 2 || Loss: 21.223961733473313
Iteration: 3 || Loss: 21.223559489751565
Iteration: 4 || Loss: 21.22315798542498
Iteration: 5 || Loss: 21.22275816464241
Iteration: 6 || Loss: 21.22275816464241
saving ADAM checkpoint...
Sum of params:83.955055
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.22275816464241
Iteration: 2 || Loss: 21.181450174112488
Iteration: 3 || Loss: 20.971977812706257
Iteration: 4 || Loss: 20.839863195521684
Iteration: 5 || Loss: 20.808486260314663
Iteration: 6 || Loss: 20.779696396069482
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.85455
Epoch 327 loss:20.779696396069482
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.226747879781629
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.881387294685084
waveform batch: 2/2
Test loss - extrapolation:17.484015523798725
Epoch 327 mean train loss:0.996642583492694
Epoch 327 mean test loss - interpolation:1.0377913132969383
Epoch 327 mean test loss - extrapolation:5.03045023487365
Start training epoch 328
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.85455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.868145498458122
Iteration: 2 || Loss: 2.866705686169564
Iteration: 3 || Loss: 2.8652671008511965
Iteration: 4 || Loss: 2.8638288174616546
Iteration: 5 || Loss: 2.8623926665435575
Iteration: 6 || Loss: 2.8623926665435575
saving ADAM checkpoint...
Sum of params:83.854546
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8623926665435575
Iteration: 2 || Loss: 2.2627635868273215
Iteration: 3 || Loss: 1.794256881263641
Iteration: 4 || Loss: 1.7915201398240879
Iteration: 5 || Loss: 1.7589153409496894
Iteration: 6 || Loss: 1.7559408756446164
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.93969
Epoch 328 loss:1.7559408756446164
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.93969
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.603588099655103
Iteration: 2 || Loss: 6.603232855387256
Iteration: 3 || Loss: 6.602880775125934
Iteration: 4 || Loss: 6.602528429330612
Iteration: 5 || Loss: 6.602179378105511
Iteration: 6 || Loss: 6.602179378105511
saving ADAM checkpoint...
Sum of params:83.939735
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.602179378105511
Iteration: 2 || Loss: 6.574196340052779
Iteration: 3 || Loss: 6.4912203400798765
Iteration: 4 || Loss: 6.379611671824393
Iteration: 5 || Loss: 6.373386614900849
Iteration: 6 || Loss: 6.361204690211334
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.967705
Epoch 328 loss:6.361204690211334
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.967705
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.2072998820914
Iteration: 2 || Loss: 21.206894418167966
Iteration: 3 || Loss: 21.206490779412956
Iteration: 4 || Loss: 21.206090222085187
Iteration: 5 || Loss: 21.205688734528206
Iteration: 6 || Loss: 21.205688734528206
saving ADAM checkpoint...
Sum of params:83.96776
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.205688734528206
Iteration: 2 || Loss: 21.164209406731107
Iteration: 3 || Loss: 20.95485584089335
Iteration: 4 || Loss: 20.823093971517032
Iteration: 5 || Loss: 20.791763403237855
Iteration: 6 || Loss: 20.763141002758754
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.86721
Epoch 328 loss:20.763141002758754
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.222561307212256
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.86018388742913
waveform batch: 2/2
Test loss - extrapolation:17.470091879525334
Epoch 328 mean train loss:0.9958719506418865
Epoch 328 mean test loss - interpolation:1.0370935512020427
Epoch 328 mean test loss - extrapolation:5.027522980579539
Start training epoch 329
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.86721
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8656555057565
Iteration: 2 || Loss: 2.864213338277398
Iteration: 3 || Loss: 2.8627738418863724
Iteration: 4 || Loss: 2.861335497302573
Iteration: 5 || Loss: 2.859897228390573
Iteration: 6 || Loss: 2.859897228390573
saving ADAM checkpoint...
Sum of params:83.8672
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.859897228390573
Iteration: 2 || Loss: 2.260388281773097
Iteration: 3 || Loss: 1.7923548013581185
Iteration: 4 || Loss: 1.7896295631341954
Iteration: 5 || Loss: 1.7571651575801215
Iteration: 6 || Loss: 1.7542785407743349
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.95217
Epoch 329 loss:1.7542785407743349
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.95217
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.599151695533104
Iteration: 2 || Loss: 6.598796456981732
Iteration: 3 || Loss: 6.598443789394845
Iteration: 4 || Loss: 6.598092507942766
Iteration: 5 || Loss: 6.597741730567653
Iteration: 6 || Loss: 6.597741730567653
saving ADAM checkpoint...
Sum of params:83.95221
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.597741730567653
Iteration: 2 || Loss: 6.569682316322009
Iteration: 3 || Loss: 6.486922478315085
Iteration: 4 || Loss: 6.375464523592261
Iteration: 5 || Loss: 6.369233841546124
Iteration: 6 || Loss: 6.357132722468739
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.98038
Epoch 329 loss:6.357132722468739
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.98038
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.190324710143532
Iteration: 2 || Loss: 21.189919044452513
Iteration: 3 || Loss: 21.18951636100911
Iteration: 4 || Loss: 21.189113137796696
Iteration: 5 || Loss: 21.18871034777885
Iteration: 6 || Loss: 21.18871034777885
saving ADAM checkpoint...
Sum of params:83.980415
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.18871034777885
Iteration: 2 || Loss: 21.14707048471341
Iteration: 3 || Loss: 20.937851353899312
Iteration: 4 || Loss: 20.806429275884753
Iteration: 5 || Loss: 20.775138290145254
Iteration: 6 || Loss: 20.746674828050192
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.87985
Epoch 329 loss:20.746674828050192
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.218367655506979
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.83903060488385
waveform batch: 2/2
Test loss - extrapolation:17.456236817534407
Epoch 329 mean train loss:0.9951064169411471
Epoch 329 mean test loss - interpolation:1.0363946092511631
Epoch 329 mean test loss - extrapolation:5.0246056185348555
Start training epoch 330
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.87985
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.863091133572473
Iteration: 2 || Loss: 2.8616488042494637
Iteration: 3 || Loss: 2.860208553678476
Iteration: 4 || Loss: 2.8587672746309862
Iteration: 5 || Loss: 2.8573343658564156
Iteration: 6 || Loss: 2.8573343658564156
saving ADAM checkpoint...
Sum of params:83.87986
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8573343658564156
Iteration: 2 || Loss: 2.257999906732795
Iteration: 3 || Loss: 1.790481807320669
Iteration: 4 || Loss: 1.7877681378914898
Iteration: 5 || Loss: 1.7554456334190112
Iteration: 6 || Loss: 1.7526368038023843
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.96459
Epoch 330 loss:1.7526368038023843
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.96459
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.59476114526151
Iteration: 2 || Loss: 6.594406775739266
Iteration: 3 || Loss: 6.59405157370473
Iteration: 4 || Loss: 6.593699006151199
Iteration: 5 || Loss: 6.593350307060322
Iteration: 6 || Loss: 6.593350307060322
saving ADAM checkpoint...
Sum of params:83.964645
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.593350307060322
Iteration: 2 || Loss: 6.565221643174137
Iteration: 3 || Loss: 6.482669486909678
Iteration: 4 || Loss: 6.371354476223063
Iteration: 5 || Loss: 6.365112203000347
Iteration: 6 || Loss: 6.353097566632363
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.99304
Epoch 330 loss:6.353097566632363
waveform batch: 3/3
Using ADAM optimizer
Sum of params:83.99304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.17351437766179
Iteration: 2 || Loss: 21.17310798857968
Iteration: 3 || Loss: 21.17270395968233
Iteration: 4 || Loss: 21.17229806285019
Iteration: 5 || Loss: 21.171896147768784
Iteration: 6 || Loss: 21.171896147768784
saving ADAM checkpoint...
Sum of params:83.99309
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.171896147768784
Iteration: 2 || Loss: 21.130112240602386
Iteration: 3 || Loss: 20.92095251962231
Iteration: 4 || Loss: 20.789863718434734
Iteration: 5 || Loss: 20.758610079854446
Iteration: 6 || Loss: 20.73029973958526
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.89246
Epoch 330 loss:20.73029973958526
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.214246585828735
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.81800939024204
waveform batch: 2/2
Test loss - extrapolation:17.44241797802751
Epoch 330 mean train loss:0.9943460037937935
Epoch 330 mean test loss - interpolation:1.0357077643047892
Epoch 330 mean test loss - extrapolation:5.021702280689129
Start training epoch 331
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.89246
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8607183162905585
Iteration: 2 || Loss: 2.8592767732840585
Iteration: 3 || Loss: 2.85783673301556
Iteration: 4 || Loss: 2.8563957645066385
Iteration: 5 || Loss: 2.8549564068012305
Iteration: 6 || Loss: 2.8549564068012305
saving ADAM checkpoint...
Sum of params:83.89246
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8549564068012305
Iteration: 2 || Loss: 2.255682665457914
Iteration: 3 || Loss: 1.7886363867960946
Iteration: 4 || Loss: 1.7859311820532873
Iteration: 5 || Loss: 1.7537559942157854
Iteration: 6 || Loss: 1.7510147965328615
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.97704
Epoch 331 loss:1.7510147965328615
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.97704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.590433638750182
Iteration: 2 || Loss: 6.5900774735889796
Iteration: 3 || Loss: 6.5897231979251485
Iteration: 4 || Loss: 6.589371126208366
Iteration: 5 || Loss: 6.589019326285496
Iteration: 6 || Loss: 6.589019326285496
saving ADAM checkpoint...
Sum of params:83.97708
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.589019326285496
Iteration: 2 || Loss: 6.560812174268631
Iteration: 3 || Loss: 6.478459459897701
Iteration: 4 || Loss: 6.367287422288984
Iteration: 5 || Loss: 6.361038176475331
Iteration: 6 || Loss: 6.349102750389537
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.00567
Epoch 331 loss:6.349102750389537
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.00567
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.15676224793737
Iteration: 2 || Loss: 21.15635462562455
Iteration: 3 || Loss: 21.15594993450627
Iteration: 4 || Loss: 21.155545473797044
Iteration: 5 || Loss: 21.155142210321085
Iteration: 6 || Loss: 21.155142210321085
saving ADAM checkpoint...
Sum of params:84.005714
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.155142210321085
Iteration: 2 || Loss: 21.11319828352429
Iteration: 3 || Loss: 20.90413275054106
Iteration: 4 || Loss: 20.773380349273786
Iteration: 5 || Loss: 20.742168949681535
Iteration: 6 || Loss: 20.714014283657242
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.90506
Epoch 331 loss:20.714014283657242
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.210159511216316
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.79703635852215
waveform batch: 2/2
Test loss - extrapolation:17.428644962558202
Epoch 331 mean train loss:0.9935907527786083
Epoch 331 mean test loss - interpolation:1.0350265852027194
Epoch 331 mean test loss - extrapolation:5.018806776756696
Start training epoch 332
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.90506
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.858347311650543
Iteration: 2 || Loss: 2.8569045437666536
Iteration: 3 || Loss: 2.8554635838843208
Iteration: 4 || Loss: 2.854024358360967
Iteration: 5 || Loss: 2.8525852279795845
Iteration: 6 || Loss: 2.8525852279795845
saving ADAM checkpoint...
Sum of params:83.90505
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8525852279795845
Iteration: 2 || Loss: 2.253411264554795
Iteration: 3 || Loss: 1.7868099520276608
Iteration: 4 || Loss: 1.7841155377024078
Iteration: 5 || Loss: 1.7520751906640641
Iteration: 6 || Loss: 1.749411928683411
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.98942
Epoch 332 loss:1.749411928683411
waveform batch: 2/3
Using ADAM optimizer
Sum of params:83.98942
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.5861285854865175
Iteration: 2 || Loss: 6.585772306982262
Iteration: 3 || Loss: 6.585417847732728
Iteration: 4 || Loss: 6.585064596138901
Iteration: 5 || Loss: 6.584712659386844
Iteration: 6 || Loss: 6.584712659386844
saving ADAM checkpoint...
Sum of params:83.98946
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.584712659386844
Iteration: 2 || Loss: 6.556428786801129
Iteration: 3 || Loss: 6.474290199752208
Iteration: 4 || Loss: 6.363261269449187
Iteration: 5 || Loss: 6.357004732880635
Iteration: 6 || Loss: 6.3451480448629
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.018265
Epoch 332 loss:6.3451480448629
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.018265
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.14008253096883
Iteration: 2 || Loss: 21.13967552221963
Iteration: 3 || Loss: 21.139267825424053
Iteration: 4 || Loss: 21.138862945165283
Iteration: 5 || Loss: 21.138458200518876
Iteration: 6 || Loss: 21.138458200518876
saving ADAM checkpoint...
Sum of params:84.018295
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.138458200518876
Iteration: 2 || Loss: 21.096354766705076
Iteration: 3 || Loss: 20.887405005390534
Iteration: 4 || Loss: 20.756986976293884
Iteration: 5 || Loss: 20.72581861820911
Iteration: 6 || Loss: 20.69781814251471
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.91762
Epoch 332 loss:20.69781814251471
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.206103690918199
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.776114637779415
waveform batch: 2/2
Test loss - extrapolation:17.41492040362853
Epoch 332 mean train loss:0.9928406246917593
Epoch 332 mean test loss - interpolation:1.034350615153033
Epoch 332 mean test loss - extrapolation:5.015919586783995
Start training epoch 333
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.91762
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8559985023827412
Iteration: 2 || Loss: 2.8545519164660056
Iteration: 3 || Loss: 2.853110025278893
Iteration: 4 || Loss: 2.851668191456764
Iteration: 5 || Loss: 2.850229044304898
Iteration: 6 || Loss: 2.850229044304898
saving ADAM checkpoint...
Sum of params:83.91762
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.850229044304898
Iteration: 2 || Loss: 2.2511623100746254
Iteration: 3 || Loss: 1.7850049720171786
Iteration: 4 || Loss: 1.782319842742084
Iteration: 5 || Loss: 1.7504256284759292
Iteration: 6 || Loss: 1.7478268033811553
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.0018
Epoch 333 loss:1.7478268033811553
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.0018
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.581873104248238
Iteration: 2 || Loss: 6.581516575400853
Iteration: 3 || Loss: 6.581161481015932
Iteration: 4 || Loss: 6.580806691494235
Iteration: 5 || Loss: 6.580454463157391
Iteration: 6 || Loss: 6.580454463157391
saving ADAM checkpoint...
Sum of params:84.00183
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.580454463157391
Iteration: 2 || Loss: 6.552094440218987
Iteration: 3 || Loss: 6.47016053100977
Iteration: 4 || Loss: 6.359266589588785
Iteration: 5 || Loss: 6.352997470402376
Iteration: 6 || Loss: 6.341222638526686
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.030846
Epoch 333 loss:6.341222638526686
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.030846
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.123664397141553
Iteration: 2 || Loss: 21.123254993073814
Iteration: 3 || Loss: 21.122848593152437
Iteration: 4 || Loss: 21.122442322997458
Iteration: 5 || Loss: 21.122037366936706
Iteration: 6 || Loss: 21.122037366936706
saving ADAM checkpoint...
Sum of params:84.03091
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.122037366936706
Iteration: 2 || Loss: 21.07978363246792
Iteration: 3 || Loss: 20.87083701928498
Iteration: 4 || Loss: 20.740711956052245
Iteration: 5 || Loss: 20.70957864824735
Iteration: 6 || Loss: 20.68172494506568
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.93016
Epoch 333 loss:20.68172494506568
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.2020501376966255
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.75527120518501
waveform batch: 2/2
Test loss - extrapolation:17.401263769101234
Epoch 333 mean train loss:0.9920956685163284
Epoch 333 mean test loss - interpolation:1.0336750229494376
Epoch 333 mean test loss - extrapolation:5.01304458119052
Start training epoch 334
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.93016
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.85360157971016
Iteration: 2 || Loss: 2.852157640825182
Iteration: 3 || Loss: 2.850715694945596
Iteration: 4 || Loss: 2.849274792731972
Iteration: 5 || Loss: 2.847832945437021
Iteration: 6 || Loss: 2.847832945437021
saving ADAM checkpoint...
Sum of params:83.93016
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.847832945437021
Iteration: 2 || Loss: 2.2489037571580717
Iteration: 3 || Loss: 1.7832259621954107
Iteration: 4 || Loss: 1.7805507112174512
Iteration: 5 || Loss: 1.7487818371266572
Iteration: 6 || Loss: 1.7462576739035014
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.014175
Epoch 334 loss:1.7462576739035014
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.014175
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.577639717838876
Iteration: 2 || Loss: 6.5772816951886375
Iteration: 3 || Loss: 6.576926080910778
Iteration: 4 || Loss: 6.576571490514189
Iteration: 5 || Loss: 6.576219410018744
Iteration: 6 || Loss: 6.576219410018744
saving ADAM checkpoint...
Sum of params:84.014206
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.576219410018744
Iteration: 2 || Loss: 6.547790588599971
Iteration: 3 || Loss: 6.466064400281161
Iteration: 4 || Loss: 6.35530352895702
Iteration: 5 || Loss: 6.3490267205190305
Iteration: 6 || Loss: 6.3373311702326065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.043434
Epoch 334 loss:6.3373311702326065
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.043434
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.10717472432481
Iteration: 2 || Loss: 21.106764918571113
Iteration: 3 || Loss: 21.10635798877874
Iteration: 4 || Loss: 21.105951265860295
Iteration: 5 || Loss: 21.105545654582826
Iteration: 6 || Loss: 21.105545654582826
saving ADAM checkpoint...
Sum of params:84.04347
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.105545654582826
Iteration: 2 || Loss: 21.063142726377777
Iteration: 3 || Loss: 20.854301867041013
Iteration: 4 || Loss: 20.72451136389525
Iteration: 5 || Loss: 20.693413008083862
Iteration: 6 || Loss: 20.66570577850014
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.94268
Epoch 334 loss:20.66570577850014
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.198055500905426
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.73453850893439
waveform batch: 2/2
Test loss - extrapolation:17.387642097985406
Epoch 334 mean train loss:0.9913549869874568
Epoch 334 mean test loss - interpolation:1.0330092501509043
Epoch 334 mean test loss - extrapolation:5.0101817172433165
Start training epoch 335
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.94268
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.851378486361763
Iteration: 2 || Loss: 2.849932341824375
Iteration: 3 || Loss: 2.8484878961652815
Iteration: 4 || Loss: 2.8470471780905764
Iteration: 5 || Loss: 2.8456071131977496
Iteration: 6 || Loss: 2.8456071131977496
saving ADAM checkpoint...
Sum of params:83.94269
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8456071131977496
Iteration: 2 || Loss: 2.246681391616292
Iteration: 3 || Loss: 1.7814690518897396
Iteration: 4 || Loss: 1.7788030457474195
Iteration: 5 || Loss: 1.7471721162985208
Iteration: 6 || Loss: 1.74470990836943
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.026505
Epoch 335 loss:1.74470990836943
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.026505
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.573471322730932
Iteration: 2 || Loss: 6.573114596459891
Iteration: 3 || Loss: 6.572757180896933
Iteration: 4 || Loss: 6.572401804186185
Iteration: 5 || Loss: 6.57204829509746
Iteration: 6 || Loss: 6.57204829509746
saving ADAM checkpoint...
Sum of params:84.02655
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.57204829509746
Iteration: 2 || Loss: 6.54353759932154
Iteration: 3 || Loss: 6.462013769917904
Iteration: 4 || Loss: 6.351387791101479
Iteration: 5 || Loss: 6.345101581486481
Iteration: 6 || Loss: 6.333478422540142
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.05597
Epoch 335 loss:6.333478422540142
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.05597
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.090831615517505
Iteration: 2 || Loss: 21.090422336723233
Iteration: 3 || Loss: 21.09001295717373
Iteration: 4 || Loss: 21.08960421884447
Iteration: 5 || Loss: 21.089197911745387
Iteration: 6 || Loss: 21.089197911745387
saving ADAM checkpoint...
Sum of params:84.05601
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.089197911745387
Iteration: 2 || Loss: 21.046643706295676
Iteration: 3 || Loss: 20.837866254137406
Iteration: 4 || Loss: 20.708392103884425
Iteration: 5 || Loss: 20.677334790702755
Iteration: 6 || Loss: 20.649769258978477
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.95516
Epoch 335 loss:20.649769258978477
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.194089294058592
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.713854500650406
waveform batch: 2/2
Test loss - extrapolation:17.37406963804264
Epoch 335 mean train loss:0.9906192272375189
Epoch 335 mean test loss - interpolation:1.0323482156764319
Epoch 335 mean test loss - extrapolation:5.007327011557754
Start training epoch 336
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.95516
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8491329006698174
Iteration: 2 || Loss: 2.8476840859662818
Iteration: 3 || Loss: 2.8462372009703167
Iteration: 4 || Loss: 2.8447931664571398
Iteration: 5 || Loss: 2.8433513891447433
Iteration: 6 || Loss: 2.8433513891447433
saving ADAM checkpoint...
Sum of params:83.95516
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8433513891447433
Iteration: 2 || Loss: 2.244509616969181
Iteration: 3 || Loss: 1.7797331976264283
Iteration: 4 || Loss: 1.7770755144191124
Iteration: 5 || Loss: 1.7455796749913333
Iteration: 6 || Loss: 1.743179018331024
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.03881
Epoch 336 loss:1.743179018331024
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.03881
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.569344034138225
Iteration: 2 || Loss: 6.568984172711722
Iteration: 3 || Loss: 6.568626777059072
Iteration: 4 || Loss: 6.56827178754937
Iteration: 5 || Loss: 6.567916515477124
Iteration: 6 || Loss: 6.567916515477124
saving ADAM checkpoint...
Sum of params:84.03886
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.567916515477124
Iteration: 2 || Loss: 6.539323879714531
Iteration: 3 || Loss: 6.458001421889554
Iteration: 4 || Loss: 6.3475065332515745
Iteration: 5 || Loss: 6.341213427644706
Iteration: 6 || Loss: 6.32966350483382
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.06846
Epoch 336 loss:6.32966350483382
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.06846
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.074588937177186
Iteration: 2 || Loss: 21.07417794436708
Iteration: 3 || Loss: 21.073769101426652
Iteration: 4 || Loss: 21.07335967502751
Iteration: 5 || Loss: 21.072951200645175
Iteration: 6 || Loss: 21.072951200645175
saving ADAM checkpoint...
Sum of params:84.068504
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.072951200645175
Iteration: 2 || Loss: 21.03023744118312
Iteration: 3 || Loss: 20.82152762626278
Iteration: 4 || Loss: 20.692363194361388
Iteration: 5 || Loss: 20.661344033094753
Iteration: 6 || Loss: 20.633926962403994
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.967636
Epoch 336 loss:20.633926962403994
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.190154763906205
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.6932109905082
waveform batch: 2/2
Test loss - extrapolation:17.36054182227229
Epoch 336 mean train loss:0.9898886029506496
Epoch 336 mean test loss - interpolation:1.031692460651034
Epoch 336 mean test loss - extrapolation:5.004479401065041
Start training epoch 337
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.967636
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.84688325363685
Iteration: 2 || Loss: 2.8454338140370203
Iteration: 3 || Loss: 2.8439876087789524
Iteration: 4 || Loss: 2.842545794541169
Iteration: 5 || Loss: 2.841102221194306
Iteration: 6 || Loss: 2.841102221194306
saving ADAM checkpoint...
Sum of params:83.96764
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.841102221194306
Iteration: 2 || Loss: 2.2423734122098615
Iteration: 3 || Loss: 1.778014032775738
Iteration: 4 || Loss: 1.7753643701678383
Iteration: 5 || Loss: 1.7439957878129717
Iteration: 6 || Loss: 1.7416622902652186
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.05111
Epoch 337 loss:1.7416622902652186
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.05111
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.565229702136775
Iteration: 2 || Loss: 6.5648705801822835
Iteration: 3 || Loss: 6.564513620635318
Iteration: 4 || Loss: 6.564156164879434
Iteration: 5 || Loss: 6.563802388155441
Iteration: 6 || Loss: 6.563802388155441
saving ADAM checkpoint...
Sum of params:84.05114
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.563802388155441
Iteration: 2 || Loss: 6.535135462739852
Iteration: 3 || Loss: 6.454016725262745
Iteration: 4 || Loss: 6.343655163672989
Iteration: 5 || Loss: 6.3373556291558995
Iteration: 6 || Loss: 6.325880067851284
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.080956
Epoch 337 loss:6.325880067851284
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.080956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.058390992369006
Iteration: 2 || Loss: 21.057976847941447
Iteration: 3 || Loss: 21.057565915533175
Iteration: 4 || Loss: 21.057157419936324
Iteration: 5 || Loss: 21.05675023366891
Iteration: 6 || Loss: 21.05675023366891
saving ADAM checkpoint...
Sum of params:84.08098
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.05675023366891
Iteration: 2 || Loss: 21.013882827887283
Iteration: 3 || Loss: 20.80527037581207
Iteration: 4 || Loss: 20.676423206302246
Iteration: 5 || Loss: 20.645440340566942
Iteration: 6 || Loss: 20.618161630858207
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.98008
Epoch 337 loss:20.618161630858207
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.186240189232282
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.67267179411955
waveform batch: 2/2
Test loss - extrapolation:17.347074802394154
Epoch 337 mean train loss:0.9891622065163692
Epoch 337 mean test loss - interpolation:1.0310400315387136
Epoch 337 mean test loss - extrapolation:5.001645549709475
Start training epoch 338
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.98008
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8447027234524516
Iteration: 2 || Loss: 2.8432547163747386
Iteration: 3 || Loss: 2.84180603070113
Iteration: 4 || Loss: 2.8403630007906315
Iteration: 5 || Loss: 2.838917976085571
Iteration: 6 || Loss: 2.838917976085571
saving ADAM checkpoint...
Sum of params:83.98008
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.838917976085571
Iteration: 2 || Loss: 2.240223238715616
Iteration: 3 || Loss: 1.7763191005620005
Iteration: 4 || Loss: 1.7736775614084686
Iteration: 5 || Loss: 1.742449078371075
Iteration: 6 || Loss: 1.7401664751682446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.06337
Epoch 338 loss:1.7401664751682446
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.06337
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.561181827884891
Iteration: 2 || Loss: 6.560822393600854
Iteration: 3 || Loss: 6.560464773084071
Iteration: 4 || Loss: 6.560107489618425
Iteration: 5 || Loss: 6.5597517992259435
Iteration: 6 || Loss: 6.5597517992259435
saving ADAM checkpoint...
Sum of params:84.06341
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.5597517992259435
Iteration: 2 || Loss: 6.53100887331969
Iteration: 3 || Loss: 6.45008243010643
Iteration: 4 || Loss: 6.339846807951594
Iteration: 5 || Loss: 6.33353704857881
Iteration: 6 || Loss: 6.322131944709713
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.09341
Epoch 338 loss:6.322131944709713
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.09341
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.042328217857214
Iteration: 2 || Loss: 21.041916580390858
Iteration: 3 || Loss: 21.04150407814067
Iteration: 4 || Loss: 21.041094162893625
Iteration: 5 || Loss: 21.040685929263628
Iteration: 6 || Loss: 21.040685929263628
saving ADAM checkpoint...
Sum of params:84.09345
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.040685929263628
Iteration: 2 || Loss: 20.997668657268484
Iteration: 3 || Loss: 20.78909667600494
Iteration: 4 || Loss: 20.660563275067275
Iteration: 5 || Loss: 20.62962380629467
Iteration: 6 || Loss: 20.602485252524055
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:83.992485
Epoch 338 loss:20.602485252524055
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.182399594807007
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.652219735744254
waveform batch: 2/2
Test loss - extrapolation:17.333637254232446
Epoch 338 mean train loss:0.9884408162897246
Epoch 338 mean test loss - interpolation:1.0303999324678346
Epoch 338 mean test loss - extrapolation:4.998821415831391
Start training epoch 339
waveform batch: 1/3
Using ADAM optimizer
Sum of params:83.992485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8426464781358156
Iteration: 2 || Loss: 2.8411984994495723
Iteration: 3 || Loss: 2.839749210740325
Iteration: 4 || Loss: 2.838303124625753
Iteration: 5 || Loss: 2.8368555344533233
Iteration: 6 || Loss: 2.8368555344533233
saving ADAM checkpoint...
Sum of params:83.992485
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8368555344533233
Iteration: 2 || Loss: 2.238191615497641
Iteration: 3 || Loss: 1.7746425424080605
Iteration: 4 || Loss: 1.7720079009524348
Iteration: 5 || Loss: 1.7409149665446326
Iteration: 6 || Loss: 1.7386847761837785
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.0756
Epoch 339 loss:1.7386847761837785
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.0756
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.557162258009259
Iteration: 2 || Loss: 6.556803082452762
Iteration: 3 || Loss: 6.556442599768676
Iteration: 4 || Loss: 6.556084639985576
Iteration: 5 || Loss: 6.555729206016895
Iteration: 6 || Loss: 6.555729206016895
saving ADAM checkpoint...
Sum of params:84.07564
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.555729206016895
Iteration: 2 || Loss: 6.526906569066894
Iteration: 3 || Loss: 6.44617510769407
Iteration: 4 || Loss: 6.336065228846645
Iteration: 5 || Loss: 6.329746695868023
Iteration: 6 || Loss: 6.318415397744272
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.10585
Epoch 339 loss:6.318415397744272
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.10585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.02634404643995
Iteration: 2 || Loss: 21.025930902050554
Iteration: 3 || Loss: 21.02551864265526
Iteration: 4 || Loss: 21.025108163397533
Iteration: 5 || Loss: 21.02469774376064
Iteration: 6 || Loss: 21.02469774376064
saving ADAM checkpoint...
Sum of params:84.105896
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.02469774376064
Iteration: 2 || Loss: 20.98154285706223
Iteration: 3 || Loss: 20.773012483532547
Iteration: 4 || Loss: 20.64479127457042
Iteration: 5 || Loss: 20.61388401648224
Iteration: 6 || Loss: 20.58687076200526
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.00485
Epoch 339 loss:20.58687076200526
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.178539568371444
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.63185150854138
waveform batch: 2/2
Test loss - extrapolation:17.320265544674704
Epoch 339 mean train loss:0.9877231357218383
Epoch 339 mean test loss - interpolation:1.029756594728574
Epoch 339 mean test loss - extrapolation:4.996009754434674
Start training epoch 340
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.00485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.840556177000162
Iteration: 2 || Loss: 2.8391075264490495
Iteration: 3 || Loss: 2.8376574831253736
Iteration: 4 || Loss: 2.83621004861991
Iteration: 5 || Loss: 2.83476532025795
Iteration: 6 || Loss: 2.83476532025795
saving ADAM checkpoint...
Sum of params:84.00487
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.83476532025795
Iteration: 2 || Loss: 2.236063753233948
Iteration: 3 || Loss: 1.7729934842541637
Iteration: 4 || Loss: 1.770367570364437
Iteration: 5 || Loss: 1.7393983154253327
Iteration: 6 || Loss: 1.7372233686246796
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.0878
Epoch 340 loss:1.7372233686246796
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.0878
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.553193445964479
Iteration: 2 || Loss: 6.552831150623526
Iteration: 3 || Loss: 6.552471568061326
Iteration: 4 || Loss: 6.552114030043531
Iteration: 5 || Loss: 6.551757296163863
Iteration: 6 || Loss: 6.551757296163863
saving ADAM checkpoint...
Sum of params:84.087845
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.551757296163863
Iteration: 2 || Loss: 6.522856301703751
Iteration: 3 || Loss: 6.442322935465444
Iteration: 4 || Loss: 6.332330173786355
Iteration: 5 || Loss: 6.326002435141847
Iteration: 6 || Loss: 6.31473887093253
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.11823
Epoch 340 loss:6.31473887093253
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.11823
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.010416286161963
Iteration: 2 || Loss: 21.010002992283766
Iteration: 3 || Loss: 21.009589853091956
Iteration: 4 || Loss: 21.009176193772696
Iteration: 5 || Loss: 21.00876716124136
Iteration: 6 || Loss: 21.00876716124136
saving ADAM checkpoint...
Sum of params:84.11826
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.00876716124136
Iteration: 2 || Loss: 20.96546177342819
Iteration: 3 || Loss: 20.756999643664432
Iteration: 4 || Loss: 20.629080424128002
Iteration: 5 || Loss: 20.59821551557381
Iteration: 6 || Loss: 20.57134157014838
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.0172
Epoch 340 loss:20.57134157014838
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.174731380532164
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.61151201141644
waveform batch: 2/2
Test loss - extrapolation:17.306931421303496
Epoch 340 mean train loss:0.9870104761967444
Epoch 340 mean test loss - interpolation:1.0291218967553608
Epoch 340 mean test loss - extrapolation:4.993203619393328
Start training epoch 341
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.0172
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.838476389790903
Iteration: 2 || Loss: 2.8370254606419762
Iteration: 3 || Loss: 2.835573771010709
Iteration: 4 || Loss: 2.8341265495525025
Iteration: 5 || Loss: 2.8326813476399355
Iteration: 6 || Loss: 2.8326813476399355
saving ADAM checkpoint...
Sum of params:84.01722
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8326813476399355
Iteration: 2 || Loss: 2.2340256727248886
Iteration: 3 || Loss: 1.7713534487283056
Iteration: 4 || Loss: 1.768735061638074
Iteration: 5 || Loss: 1.7379003051343687
Iteration: 6 || Loss: 1.7357748556232782
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.09998
Epoch 341 loss:1.7357748556232782
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.09998
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.549245785689389
Iteration: 2 || Loss: 6.548884633894647
Iteration: 3 || Loss: 6.548523790235459
Iteration: 4 || Loss: 6.5481649636120025
Iteration: 5 || Loss: 6.5478081974256135
Iteration: 6 || Loss: 6.5478081974256135
saving ADAM checkpoint...
Sum of params:84.10004
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.5478081974256135
Iteration: 2 || Loss: 6.518833441075811
Iteration: 3 || Loss: 6.438493847056992
Iteration: 4 || Loss: 6.328625180735271
Iteration: 5 || Loss: 6.322288215831079
Iteration: 6 || Loss: 6.3110901546596905
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.130585
Epoch 341 loss:6.3110901546596905
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.130585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.994607404798515
Iteration: 2 || Loss: 20.994192546519624
Iteration: 3 || Loss: 20.9937778859401
Iteration: 4 || Loss: 20.993366760174425
Iteration: 5 || Loss: 20.99295455664406
Iteration: 6 || Loss: 20.99295455664406
saving ADAM checkpoint...
Sum of params:84.13062
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.99295455664406
Iteration: 2 || Loss: 20.949480833891133
Iteration: 3 || Loss: 20.74106950626443
Iteration: 4 || Loss: 20.61346050277089
Iteration: 5 || Loss: 20.582638641627277
Iteration: 6 || Loss: 20.555904496049514
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.029526
Epoch 341 loss:20.555904496049514
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.170953630955005
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.59121542265341
waveform batch: 2/2
Test loss - extrapolation:17.29364203684929
Epoch 341 mean train loss:0.9863023967700856
Epoch 341 mean test loss - interpolation:1.028492271825834
Epoch 341 mean test loss - extrapolation:4.990404788291891
Start training epoch 342
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.029526
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.836378449925753
Iteration: 2 || Loss: 2.8349251799278408
Iteration: 3 || Loss: 2.833474739679079
Iteration: 4 || Loss: 2.8320275008088442
Iteration: 5 || Loss: 2.8305792732599238
Iteration: 6 || Loss: 2.8305792732599238
saving ADAM checkpoint...
Sum of params:84.02954
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8305792732599238
Iteration: 2 || Loss: 2.232047375223641
Iteration: 3 || Loss: 1.7697315470720272
Iteration: 4 || Loss: 1.7671206878988952
Iteration: 5 || Loss: 1.7364059731130093
Iteration: 6 || Loss: 1.7343393927116737
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.112144
Epoch 342 loss:1.7343393927116737
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.112144
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.545309053185508
Iteration: 2 || Loss: 6.544946421250642
Iteration: 3 || Loss: 6.544584834511314
Iteration: 4 || Loss: 6.544226731003785
Iteration: 5 || Loss: 6.543868335568287
Iteration: 6 || Loss: 6.543868335568287
saving ADAM checkpoint...
Sum of params:84.112175
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.543868335568287
Iteration: 2 || Loss: 6.514821504880337
Iteration: 3 || Loss: 6.434691263953429
Iteration: 4 || Loss: 6.3249427504315285
Iteration: 5 || Loss: 6.318595104911357
Iteration: 6 || Loss: 6.30746531705211
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.14294
Epoch 342 loss:6.30746531705211
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.14294
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.978946359739325
Iteration: 2 || Loss: 20.97853040690596
Iteration: 3 || Loss: 20.978115202059858
Iteration: 4 || Loss: 20.977702382509555
Iteration: 5 || Loss: 20.977290267547268
Iteration: 6 || Loss: 20.977290267547268
saving ADAM checkpoint...
Sum of params:84.142975
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.977290267547268
Iteration: 2 || Loss: 20.93368547499589
Iteration: 3 || Loss: 20.725285779365777
Iteration: 4 || Loss: 20.597952820738957
Iteration: 5 || Loss: 20.567161557408603
Iteration: 6 || Loss: 20.540552878404373
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.041824
Epoch 342 loss:20.540552878404373
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.16721021019109
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.5710665021019
waveform batch: 2/2
Test loss - extrapolation:17.28040563873069
Epoch 342 mean train loss:0.9855985375230399
Epoch 342 mean test loss - interpolation:1.0278683683651817
Epoch 342 mean test loss - extrapolation:4.987622678402716
Start training epoch 343
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.041824
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.834449567289831
Iteration: 2 || Loss: 2.8329993775092857
Iteration: 3 || Loss: 2.83154847866903
Iteration: 4 || Loss: 2.8300978499215974
Iteration: 5 || Loss: 2.8286482944288545
Iteration: 6 || Loss: 2.8286482944288545
saving ADAM checkpoint...
Sum of params:84.04183
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8286482944288545
Iteration: 2 || Loss: 2.2300424545668807
Iteration: 3 || Loss: 1.7681380441600176
Iteration: 4 || Loss: 1.7655331367710028
Iteration: 5 || Loss: 1.7349454983759074
Iteration: 6 || Loss: 1.7329211217264995
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.12426
Epoch 343 loss:1.7329211217264995
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.12426
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.541449376314314
Iteration: 2 || Loss: 6.541085189739141
Iteration: 3 || Loss: 6.540724754738377
Iteration: 4 || Loss: 6.540364603988003
Iteration: 5 || Loss: 6.540007959193938
Iteration: 6 || Loss: 6.540007959193938
saving ADAM checkpoint...
Sum of params:84.124306
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.540007959193938
Iteration: 2 || Loss: 6.5108747656778725
Iteration: 3 || Loss: 6.430926660204753
Iteration: 4 || Loss: 6.32129529148027
Iteration: 5 || Loss: 6.314941842717955
Iteration: 6 || Loss: 6.303879787964004
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.15526
Epoch 343 loss:6.303879787964004
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.15526
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.963254179659643
Iteration: 2 || Loss: 20.96283720378679
Iteration: 3 || Loss: 20.962421908403314
Iteration: 4 || Loss: 20.962007689627608
Iteration: 5 || Loss: 20.961593737469315
Iteration: 6 || Loss: 20.961593737469315
saving ADAM checkpoint...
Sum of params:84.1553
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.961593737469315
Iteration: 2 || Loss: 20.917830452241926
Iteration: 3 || Loss: 20.7095179293515
Iteration: 4 || Loss: 20.582495623184847
Iteration: 5 || Loss: 20.551745260005852
Iteration: 6 || Loss: 20.52526501077546
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.05407
Epoch 343 loss:20.52526501077546
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.163476319034895
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.55092734470212
waveform batch: 2/2
Test loss - extrapolation:17.26721854028636
Epoch 343 mean train loss:0.9848988248436539
Epoch 343 mean test loss - interpolation:1.0272460531724825
Epoch 343 mean test loss - extrapolation:4.984845490415707
Start training epoch 344
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.05407
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8324323770958375
Iteration: 2 || Loss: 2.8309780536351847
Iteration: 3 || Loss: 2.8295277650186854
Iteration: 4 || Loss: 2.82807523047454
Iteration: 5 || Loss: 2.826627665158719
Iteration: 6 || Loss: 2.826627665158719
saving ADAM checkpoint...
Sum of params:84.05409
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.826627665158719
Iteration: 2 || Loss: 2.228074729386264
Iteration: 3 || Loss: 1.7665542086454704
Iteration: 4 || Loss: 1.7639553338009155
Iteration: 5 || Loss: 1.7334960188518056
Iteration: 6 || Loss: 1.7315189069020938
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.13638
Epoch 344 loss:1.7315189069020938
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.13638
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.537609106225536
Iteration: 2 || Loss: 6.537244309560688
Iteration: 3 || Loss: 6.5368826200995525
Iteration: 4 || Loss: 6.536522957353228
Iteration: 5 || Loss: 6.536163796448968
Iteration: 6 || Loss: 6.536163796448968
saving ADAM checkpoint...
Sum of params:84.13641
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.536163796448968
Iteration: 2 || Loss: 6.506952479799579
Iteration: 3 || Loss: 6.427199641257384
Iteration: 4 || Loss: 6.3176839254406385
Iteration: 5 || Loss: 6.31131945883857
Iteration: 6 || Loss: 6.300319798494511
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.167534
Epoch 344 loss:6.300319798494511
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.167534
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.947767013129564
Iteration: 2 || Loss: 20.94734835717265
Iteration: 3 || Loss: 20.946930972151566
Iteration: 4 || Loss: 20.946516610509452
Iteration: 5 || Loss: 20.946103687177416
Iteration: 6 || Loss: 20.946103687177416
saving ADAM checkpoint...
Sum of params:84.16759
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.946103687177416
Iteration: 2 || Loss: 20.90218832730045
Iteration: 3 || Loss: 20.693883554382385
Iteration: 4 || Loss: 20.567135298906784
Iteration: 5 || Loss: 20.53641920720549
Iteration: 6 || Loss: 20.510070502254695
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.06634
Epoch 344 loss:20.510070502254695
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.159771908224936
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.53086464015805
waveform batch: 2/2
Test loss - extrapolation:17.254082304617324
Epoch 344 mean train loss:0.9842037657810794
Epoch 344 mean test loss - interpolation:1.0266286513708227
Epoch 344 mean test loss - extrapolation:4.9820789120646145
Start training epoch 345
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.06634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8304463510149054
Iteration: 2 || Loss: 2.8289914016356237
Iteration: 3 || Loss: 2.827537993671142
Iteration: 4 || Loss: 2.8260847602361494
Iteration: 5 || Loss: 2.824638828599899
Iteration: 6 || Loss: 2.824638828599899
saving ADAM checkpoint...
Sum of params:84.06634
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.824638828599899
Iteration: 2 || Loss: 2.2261198225331196
Iteration: 3 || Loss: 1.7649889737158964
Iteration: 4 || Loss: 1.7623970363015573
Iteration: 5 || Loss: 1.732053828869439
Iteration: 6 || Loss: 1.7301277203476757
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.14844
Epoch 345 loss:1.7301277203476757
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.14844
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.533790665600872
Iteration: 2 || Loss: 6.533426985376644
Iteration: 3 || Loss: 6.533064593910489
Iteration: 4 || Loss: 6.532703959292972
Iteration: 5 || Loss: 6.532342741309863
Iteration: 6 || Loss: 6.532342741309863
saving ADAM checkpoint...
Sum of params:84.14848
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.532342741309863
Iteration: 2 || Loss: 6.503050349224077
Iteration: 3 || Loss: 6.423493683688128
Iteration: 4 || Loss: 6.314094943733411
Iteration: 5 || Loss: 6.307723486007209
Iteration: 6 || Loss: 6.296789036904311
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.1798
Epoch 345 loss:6.296789036904311
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.1798
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.93226422386697
Iteration: 2 || Loss: 20.931845290057304
Iteration: 3 || Loss: 20.931427805774195
Iteration: 4 || Loss: 20.931012839070917
Iteration: 5 || Loss: 20.930597949760923
Iteration: 6 || Loss: 20.930597949760923
saving ADAM checkpoint...
Sum of params:84.17984
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.930597949760923
Iteration: 2 || Loss: 20.886536181372392
Iteration: 3 || Loss: 20.67828311050438
Iteration: 4 || Loss: 20.551844023346934
Iteration: 5 || Loss: 20.52116611233113
Iteration: 6 || Loss: 20.494936556830492
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.07853
Epoch 345 loss:20.494936556830492
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.156106019405234
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.51090498161468
waveform batch: 2/2
Test loss - extrapolation:17.240989885756864
Epoch 345 mean train loss:0.9835121832442234
Epoch 345 mean test loss - interpolation:1.0260176699008723
Epoch 345 mean test loss - extrapolation:4.979324572280962
Start training epoch 346
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.07853
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8285640584955773
Iteration: 2 || Loss: 2.8271092442711208
Iteration: 3 || Loss: 2.8256535760226518
Iteration: 4 || Loss: 2.824200024631673
Iteration: 5 || Loss: 2.8227524647233846
Iteration: 6 || Loss: 2.8227524647233846
saving ADAM checkpoint...
Sum of params:84.07852
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8227524647233846
Iteration: 2 || Loss: 2.2242140012275153
Iteration: 3 || Loss: 1.763444460551763
Iteration: 4 || Loss: 1.760859889540377
Iteration: 5 || Loss: 1.7306318335856778
Iteration: 6 || Loss: 1.7287549757142835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.16048
Epoch 346 loss:1.7287549757142835
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.16048
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.529984976370419
Iteration: 2 || Loss: 6.529620816699242
Iteration: 3 || Loss: 6.529256263528602
Iteration: 4 || Loss: 6.5288968292312575
Iteration: 5 || Loss: 6.528537340670648
Iteration: 6 || Loss: 6.528537340670648
saving ADAM checkpoint...
Sum of params:84.16052
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.528537340670648
Iteration: 2 || Loss: 6.499185778024037
Iteration: 3 || Loss: 6.4198366373575855
Iteration: 4 || Loss: 6.310544617495981
Iteration: 5 || Loss: 6.30415918044693
Iteration: 6 || Loss: 6.293284181150901
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.192024
Epoch 346 loss:6.293284181150901
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.192024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.916953915055217
Iteration: 2 || Loss: 20.916536556532932
Iteration: 3 || Loss: 20.916117335483225
Iteration: 4 || Loss: 20.915701294364084
Iteration: 5 || Loss: 20.915285626246874
Iteration: 6 || Loss: 20.915285626246874
saving ADAM checkpoint...
Sum of params:84.19206
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.915285626246874
Iteration: 2 || Loss: 20.87108237701455
Iteration: 3 || Loss: 20.662829286706206
Iteration: 4 || Loss: 20.536647237544376
Iteration: 5 || Loss: 20.506002771888653
Iteration: 6 || Loss: 20.4798993281558
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.09072
Epoch 346 loss:20.4798993281558
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.152452333892133
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.49097650928845
waveform batch: 2/2
Test loss - extrapolation:17.227944885673878
Epoch 346 mean train loss:0.9828254650007237
Epoch 346 mean test loss - interpolation:1.0254087223153554
Epoch 346 mean test loss - extrapolation:4.9765767829135275
Start training epoch 347
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.09072
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8266210780966596
Iteration: 2 || Loss: 2.825163675212462
Iteration: 3 || Loss: 2.823710786994406
Iteration: 4 || Loss: 2.8222577130333764
Iteration: 5 || Loss: 2.820806497464473
Iteration: 6 || Loss: 2.820806497464473
saving ADAM checkpoint...
Sum of params:84.09072
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.820806497464473
Iteration: 2 || Loss: 2.222286793719402
Iteration: 3 || Loss: 1.761915509789021
Iteration: 4 || Loss: 1.7593350325383865
Iteration: 5 || Loss: 1.729240028784333
Iteration: 6 || Loss: 1.7273911658336816
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.172516
Epoch 347 loss:1.7273911658336816
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.172516
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.526273072770253
Iteration: 2 || Loss: 6.525907955210443
Iteration: 3 || Loss: 6.525542105047148
Iteration: 4 || Loss: 6.525181276600111
Iteration: 5 || Loss: 6.5248214184845885
Iteration: 6 || Loss: 6.5248214184845885
saving ADAM checkpoint...
Sum of params:84.17255
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.5248214184845885
Iteration: 2 || Loss: 6.495371891107554
Iteration: 3 || Loss: 6.416185176475037
Iteration: 4 || Loss: 6.307012131886758
Iteration: 5 || Loss: 6.300621414900192
Iteration: 6 || Loss: 6.289809950722057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.20422
Epoch 347 loss:6.289809950722057
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.20422
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.901611338000418
Iteration: 2 || Loss: 20.901190344048114
Iteration: 3 || Loss: 20.900773907654926
Iteration: 4 || Loss: 20.90035596507048
Iteration: 5 || Loss: 20.899939735127504
Iteration: 6 || Loss: 20.899939735127504
saving ADAM checkpoint...
Sum of params:84.20426
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.899939735127504
Iteration: 2 || Loss: 20.85558678489736
Iteration: 3 || Loss: 20.647380038189365
Iteration: 4 || Loss: 20.521508249929504
Iteration: 5 || Loss: 20.49090216180978
Iteration: 6 || Loss: 20.464918662137375
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.10285
Epoch 347 loss:20.464918662137375
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.148846630794256
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.47115721634694
waveform batch: 2/2
Test loss - extrapolation:17.214941873624923
Epoch 347 mean train loss:0.9821420613342452
Epoch 347 mean test loss - interpolation:1.0248077717990427
Epoch 347 mean test loss - extrapolation:4.973841590830989
Start training epoch 348
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.10285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8248118427672257
Iteration: 2 || Loss: 2.823356750898763
Iteration: 3 || Loss: 2.8219012150252505
Iteration: 4 || Loss: 2.8204494528770954
Iteration: 5 || Loss: 2.818996893322294
Iteration: 6 || Loss: 2.818996893322294
saving ADAM checkpoint...
Sum of params:84.10286
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.818996893322294
Iteration: 2 || Loss: 2.2204228814046294
Iteration: 3 || Loss: 1.7604027223609138
Iteration: 4 || Loss: 1.7578291886081088
Iteration: 5 || Loss: 1.7278478615642596
Iteration: 6 || Loss: 1.726045865259877
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.184494
Epoch 348 loss:1.726045865259877
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.184494
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.5225420083664405
Iteration: 2 || Loss: 6.522176752622276
Iteration: 3 || Loss: 6.52181167745335
Iteration: 4 || Loss: 6.521450756792032
Iteration: 5 || Loss: 6.521087859970412
Iteration: 6 || Loss: 6.521087859970412
saving ADAM checkpoint...
Sum of params:84.184525
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.521087859970412
Iteration: 2 || Loss: 6.491566835505499
Iteration: 3 || Loss: 6.412584879354934
Iteration: 4 || Loss: 6.303521186960901
Iteration: 5 || Loss: 6.2971194035851825
Iteration: 6 || Loss: 6.286367575484313
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.216385
Epoch 348 loss:6.286367575484313
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.216385
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.88640927323182
Iteration: 2 || Loss: 20.885988860268885
Iteration: 3 || Loss: 20.88556879371702
Iteration: 4 || Loss: 20.885150303557
Iteration: 5 || Loss: 20.884732486713336
Iteration: 6 || Loss: 20.884732486713336
saving ADAM checkpoint...
Sum of params:84.21642
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.884732486713336
Iteration: 2 || Loss: 20.840233360079065
Iteration: 3 || Loss: 20.63205454322293
Iteration: 4 || Loss: 20.506453487774223
Iteration: 5 || Loss: 20.475881164659086
Iteration: 6 || Loss: 20.450017447444235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.114975
Epoch 348 loss:20.450017447444235
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.145243378257356
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.45136478538997
waveform batch: 2/2
Test loss - extrapolation:17.20199412476647
Epoch 348 mean train loss:0.9814631340754629
Epoch 348 mean test loss - interpolation:1.0242072297095592
Epoch 348 mean test loss - extrapolation:4.971113242513037
Start training epoch 349
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.114975
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.822925545450175
Iteration: 2 || Loss: 2.8214690496309585
Iteration: 3 || Loss: 2.8200132355737795
Iteration: 4 || Loss: 2.818558930180011
Iteration: 5 || Loss: 2.8171048068443874
Iteration: 6 || Loss: 2.8171048068443874
saving ADAM checkpoint...
Sum of params:84.11498
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8171048068443874
Iteration: 2 || Loss: 2.2185517459927313
Iteration: 3 || Loss: 1.7589062913475026
Iteration: 4 || Loss: 1.7563381356763716
Iteration: 5 || Loss: 1.7264727466803766
Iteration: 6 || Loss: 1.724712739257442
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.196465
Epoch 349 loss:1.724712739257442
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.196465
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.518856899559217
Iteration: 2 || Loss: 6.5184906744569755
Iteration: 3 || Loss: 6.5181245320306225
Iteration: 4 || Loss: 6.517760710629375
Iteration: 5 || Loss: 6.517398878527349
Iteration: 6 || Loss: 6.517398878527349
saving ADAM checkpoint...
Sum of params:84.1965
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.517398878527349
Iteration: 2 || Loss: 6.487801899976265
Iteration: 3 || Loss: 6.409009767752837
Iteration: 4 || Loss: 6.300052289062597
Iteration: 5 || Loss: 6.293641973347212
Iteration: 6 || Loss: 6.2829503362918695
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.22853
Epoch 349 loss:6.2829503362918695
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.22853
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.871253354164175
Iteration: 2 || Loss: 20.870833258012215
Iteration: 3 || Loss: 20.87041267922027
Iteration: 4 || Loss: 20.869993496436816
Iteration: 5 || Loss: 20.869575310754378
Iteration: 6 || Loss: 20.869575310754378
saving ADAM checkpoint...
Sum of params:84.22857
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.869575310754378
Iteration: 2 || Loss: 20.824926111614197
Iteration: 3 || Loss: 20.616785493274595
Iteration: 4 || Loss: 20.49146985895089
Iteration: 5 || Loss: 20.46093420159398
Iteration: 6 || Loss: 20.435193060495216
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.127106
Epoch 349 loss:20.435193060495216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.141697670436485
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.431673321029805
waveform batch: 2/2
Test loss - extrapolation:17.189082317349175
Epoch 349 mean train loss:0.980788142622225
Epoch 349 mean test loss - interpolation:1.0236162784060807
Epoch 349 mean test loss - extrapolation:4.968396303198248
Start training epoch 350
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.127106
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.821172275532558
Iteration: 2 || Loss: 2.8197133614674423
Iteration: 3 || Loss: 2.818256934553488
Iteration: 4 || Loss: 2.81680229507336
Iteration: 5 || Loss: 2.81534928804039
Iteration: 6 || Loss: 2.81534928804039
saving ADAM checkpoint...
Sum of params:84.12709
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.81534928804039
Iteration: 2 || Loss: 2.2167455580671875
Iteration: 3 || Loss: 1.7574250932945819
Iteration: 4 || Loss: 1.754862730035264
Iteration: 5 || Loss: 1.7251176402728046
Iteration: 6 || Loss: 1.7233919905877308
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.20839
Epoch 350 loss:1.7233919905877308
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.20839
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.5152029054390805
Iteration: 2 || Loss: 6.51483625867044
Iteration: 3 || Loss: 6.514470680490005
Iteration: 4 || Loss: 6.514106199225414
Iteration: 5 || Loss: 6.513743542235657
Iteration: 6 || Loss: 6.513743542235657
saving ADAM checkpoint...
Sum of params:84.208435
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.513743542235657
Iteration: 2 || Loss: 6.484071133251364
Iteration: 3 || Loss: 6.405463137724603
Iteration: 4 || Loss: 6.296614259221671
Iteration: 5 || Loss: 6.290195268216962
Iteration: 6 || Loss: 6.279560879184543
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.24064
Epoch 350 loss:6.279560879184543
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.24064
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.85625430508832
Iteration: 2 || Loss: 20.855830847783576
Iteration: 3 || Loss: 20.85540975365117
Iteration: 4 || Loss: 20.854989373146413
Iteration: 5 || Loss: 20.854569838082014
Iteration: 6 || Loss: 20.854569838082014
saving ADAM checkpoint...
Sum of params:84.24068
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.854569838082014
Iteration: 2 || Loss: 20.80977114276466
Iteration: 3 || Loss: 20.601623180052076
Iteration: 4 || Loss: 20.47657148859096
Iteration: 5 || Loss: 20.44607223833083
Iteration: 6 || Loss: 20.420448756017898
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.139145
Epoch 350 loss:20.420448756017898
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.138136483877719
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.412001031842216
waveform batch: 2/2
Test loss - extrapolation:17.17623368937388
Epoch 350 mean train loss:0.9801172974410404
Epoch 350 mean test loss - interpolation:1.0230227473129532
Epoch 350 mean test loss - extrapolation:4.965686226768008
Start training epoch 351
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.139145
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.81928637837883
Iteration: 2 || Loss: 2.817826929431965
Iteration: 3 || Loss: 2.8163695616117757
Iteration: 4 || Loss: 2.8149144569754707
Iteration: 5 || Loss: 2.8134595592672493
Iteration: 6 || Loss: 2.8134595592672493
saving ADAM checkpoint...
Sum of params:84.13915
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8134595592672493
Iteration: 2 || Loss: 2.214914697325005
Iteration: 3 || Loss: 1.7559578674577085
Iteration: 4 || Loss: 1.7533998497237324
Iteration: 5 || Loss: 1.7237707159871012
Iteration: 6 || Loss: 1.722083055299962
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.22029
Epoch 351 loss:1.722083055299962
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.22029
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.51157485063749
Iteration: 2 || Loss: 6.511206393402676
Iteration: 3 || Loss: 6.510840881851001
Iteration: 4 || Loss: 6.510475715498765
Iteration: 5 || Loss: 6.510113772123421
Iteration: 6 || Loss: 6.510113772123421
saving ADAM checkpoint...
Sum of params:84.22034
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.510113772123421
Iteration: 2 || Loss: 6.480360650217146
Iteration: 3 || Loss: 6.401940065729752
Iteration: 4 || Loss: 6.293199058034576
Iteration: 5 || Loss: 6.286769677554997
Iteration: 6 || Loss: 6.276192922189061
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.25271
Epoch 351 loss:6.276192922189061
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.25271
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.84124316382534
Iteration: 2 || Loss: 20.840820680775686
Iteration: 3 || Loss: 20.840399863748434
Iteration: 4 || Loss: 20.83997990447457
Iteration: 5 || Loss: 20.83955923144101
Iteration: 6 || Loss: 20.83955923144101
saving ADAM checkpoint...
Sum of params:84.25276
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.83955923144101
Iteration: 2 || Loss: 20.794627130383766
Iteration: 3 || Loss: 20.586510037248065
Iteration: 4 || Loss: 20.461743386813733
Iteration: 5 || Loss: 20.431279001137156
Iteration: 6 || Loss: 20.405764882358156
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.15118
Epoch 351 loss:20.405764882358156
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.13462980127376
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.39247690459731
waveform batch: 2/2
Test loss - extrapolation:17.16341971668428
Epoch 351 mean train loss:0.9794496848223165
Epoch 351 mean test loss - interpolation:1.0224383002122932
Epoch 351 mean test loss - extrapolation:4.9629913851067995
Start training epoch 352
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.15118
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.817617263053656
Iteration: 2 || Loss: 2.816155810967889
Iteration: 3 || Loss: 2.8146957985119894
Iteration: 4 || Loss: 2.813241395007896
Iteration: 5 || Loss: 2.8117865391488555
Iteration: 6 || Loss: 2.8117865391488555
saving ADAM checkpoint...
Sum of params:84.151184
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8117865391488555
Iteration: 2 || Loss: 2.213105776202145
Iteration: 3 || Loss: 1.7545124521084936
Iteration: 4 || Loss: 1.7519603966750408
Iteration: 5 || Loss: 1.722440672665474
Iteration: 6 || Loss: 1.7207894138191695
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.23216
Epoch 352 loss:1.7207894138191695
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.23216
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.507970206888557
Iteration: 2 || Loss: 6.507600130627585
Iteration: 3 || Loss: 6.507234793764397
Iteration: 4 || Loss: 6.506869269808122
Iteration: 5 || Loss: 6.506505654625251
Iteration: 6 || Loss: 6.506505654625251
saving ADAM checkpoint...
Sum of params:84.23221
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.506505654625251
Iteration: 2 || Loss: 6.476688103943802
Iteration: 3 || Loss: 6.39846239132093
Iteration: 4 || Loss: 6.289817015112831
Iteration: 5 || Loss: 6.283377651217102
Iteration: 6 || Loss: 6.27285733744693
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.26477
Epoch 352 loss:6.27285733744693
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.26477
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.82637104180159
Iteration: 2 || Loss: 20.82594754353086
Iteration: 3 || Loss: 20.825522837408123
Iteration: 4 || Loss: 20.825103268651226
Iteration: 5 || Loss: 20.824682703025093
Iteration: 6 || Loss: 20.824682703025093
saving ADAM checkpoint...
Sum of params:84.26482
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.824682703025093
Iteration: 2 || Loss: 20.77959826491404
Iteration: 3 || Loss: 20.571502187142517
Iteration: 4 || Loss: 20.446990170200866
Iteration: 5 || Loss: 20.416560926467046
Iteration: 6 || Loss: 20.39116558941164
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.163185
Epoch 352 loss:20.39116558941164
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.131132082340608
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.372938785523004
waveform batch: 2/2
Test loss - extrapolation:17.15065552412198
Epoch 352 mean train loss:0.9787866324371635
Epoch 352 mean test loss - interpolation:1.021855347056768
Epoch 352 mean test loss - extrapolation:4.960299525803749
Start training epoch 353
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.163185
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.815801497940903
Iteration: 2 || Loss: 2.814342885610606
Iteration: 3 || Loss: 2.812881495291514
Iteration: 4 || Loss: 2.8114242117556625
Iteration: 5 || Loss: 2.8099682053824417
Iteration: 6 || Loss: 2.8099682053824417
saving ADAM checkpoint...
Sum of params:84.16317
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8099682053824417
Iteration: 2 || Loss: 2.211341314340146
Iteration: 3 || Loss: 1.7530733786942978
Iteration: 4 || Loss: 1.7505258045162888
Iteration: 5 || Loss: 1.7211186523408206
Iteration: 6 || Loss: 1.7195052256589383
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.244026
Epoch 353 loss:1.7195052256589383
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.244026
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.504383080740206
Iteration: 2 || Loss: 6.504014094230389
Iteration: 3 || Loss: 6.503647705228243
Iteration: 4 || Loss: 6.503282441512591
Iteration: 5 || Loss: 6.502919223414648
Iteration: 6 || Loss: 6.502919223414648
saving ADAM checkpoint...
Sum of params:84.244064
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.502919223414648
Iteration: 2 || Loss: 6.473026000304489
Iteration: 3 || Loss: 6.394990269752198
Iteration: 4 || Loss: 6.286449748166894
Iteration: 5 || Loss: 6.2799988417734065
Iteration: 6 || Loss: 6.269537578307067
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.27681
Epoch 353 loss:6.269537578307067
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.27681
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.811580525100204
Iteration: 2 || Loss: 20.811156753345873
Iteration: 3 || Loss: 20.81073217107773
Iteration: 4 || Loss: 20.810310041761106
Iteration: 5 || Loss: 20.809889504736805
Iteration: 6 || Loss: 20.809889504736805
saving ADAM checkpoint...
Sum of params:84.276855
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.809889504736805
Iteration: 2 || Loss: 20.764686275412668
Iteration: 3 || Loss: 20.556556000957077
Iteration: 4 || Loss: 20.43231855343336
Iteration: 5 || Loss: 20.401919566305928
Iteration: 6 || Loss: 20.37662627466836
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.17515
Epoch 353 loss:20.37662627466836
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.127667920871313
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.35356779624443
waveform batch: 2/2
Test loss - extrapolation:17.13794078394491
Epoch 353 mean train loss:0.9781265199529091
Epoch 353 mean test loss - interpolation:1.0212779868118855
Epoch 353 mean test loss - extrapolation:4.957625715015777
Start training epoch 354
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.17515
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8141892962123602
Iteration: 2 || Loss: 2.8127277548608483
Iteration: 3 || Loss: 2.811267522510137
Iteration: 4 || Loss: 2.809809128463146
Iteration: 5 || Loss: 2.8083519674470447
Iteration: 6 || Loss: 2.8083519674470447
saving ADAM checkpoint...
Sum of params:84.17514
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8083519674470447
Iteration: 2 || Loss: 2.2095594744376092
Iteration: 3 || Loss: 1.7516615495284773
Iteration: 4 || Loss: 1.749117424014551
Iteration: 5 || Loss: 1.7198285457055515
Iteration: 6 || Loss: 1.7182372972684292
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.25585
Epoch 354 loss:1.7182372972684292
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.25585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.5009052807963394
Iteration: 2 || Loss: 6.500536360625281
Iteration: 3 || Loss: 6.500168413715602
Iteration: 4 || Loss: 6.499802570964747
Iteration: 5 || Loss: 6.499436513692583
Iteration: 6 || Loss: 6.499436513692583
saving ADAM checkpoint...
Sum of params:84.25589
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.499436513692583
Iteration: 2 || Loss: 6.469435513270532
Iteration: 3 || Loss: 6.3915583524856965
Iteration: 4 || Loss: 6.283124713365819
Iteration: 5 || Loss: 6.276669723463201
Iteration: 6 || Loss: 6.266260006422576
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.28879
Epoch 354 loss:6.266260006422576
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.28879
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.796796389071254
Iteration: 2 || Loss: 20.79637226287493
Iteration: 3 || Loss: 20.79594755865089
Iteration: 4 || Loss: 20.79552466091193
Iteration: 5 || Loss: 20.795102845558215
Iteration: 6 || Loss: 20.795102845558215
saving ADAM checkpoint...
Sum of params:84.28883
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.795102845558215
Iteration: 2 || Loss: 20.74971681814453
Iteration: 3 || Loss: 20.541647102281615
Iteration: 4 || Loss: 20.417684043332017
Iteration: 5 || Loss: 20.387326638138227
Iteration: 6 || Loss: 20.362157492132006
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.18711
Epoch 354 loss:20.362157492132006
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.124210693748845
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.33410630922317
waveform batch: 2/2
Test loss - extrapolation:17.125265620170694
Epoch 354 mean train loss:0.9774708550283798
Epoch 354 mean test loss - interpolation:1.0207017822914741
Epoch 354 mean test loss - extrapolation:4.954947660782822
Start training epoch 355
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.18711
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.812346417243315
Iteration: 2 || Loss: 2.8108853633270936
Iteration: 3 || Loss: 2.8094243229209734
Iteration: 4 || Loss: 2.8079648730670037
Iteration: 5 || Loss: 2.8065077308083577
Iteration: 6 || Loss: 2.8065077308083577
saving ADAM checkpoint...
Sum of params:84.18711
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8065077308083577
Iteration: 2 || Loss: 2.20784727078305
Iteration: 3 || Loss: 1.7502459447674672
Iteration: 4 || Loss: 1.7477069121135445
Iteration: 5 || Loss: 1.718528708866615
Iteration: 6 || Loss: 1.716977102921544
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.26764
Epoch 355 loss:1.716977102921544
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.26764
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.497349522612074
Iteration: 2 || Loss: 6.496979289847588
Iteration: 3 || Loss: 6.496611389335722
Iteration: 4 || Loss: 6.496244991557035
Iteration: 5 || Loss: 6.495878408434262
Iteration: 6 || Loss: 6.495878408434262
saving ADAM checkpoint...
Sum of params:84.26768
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.495878408434262
Iteration: 2 || Loss: 6.465821575519246
Iteration: 3 || Loss: 6.388149940866561
Iteration: 4 || Loss: 6.2798169443873535
Iteration: 5 || Loss: 6.273348059396765
Iteration: 6 || Loss: 6.262994782861689
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.30076
Epoch 355 loss:6.262994782861689
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.30076
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.78215447998833
Iteration: 2 || Loss: 20.781727575075227
Iteration: 3 || Loss: 20.781303307683597
Iteration: 4 || Loss: 20.780879877797748
Iteration: 5 || Loss: 20.78045716308604
Iteration: 6 || Loss: 20.78045716308604
saving ADAM checkpoint...
Sum of params:84.3008
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.78045716308604
Iteration: 2 || Loss: 20.73495589352023
Iteration: 3 || Loss: 20.526864077312055
Iteration: 4 || Loss: 20.40315933058295
Iteration: 5 || Loss: 20.372830455145156
Iteration: 6 || Loss: 20.347761626734343
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.19902
Epoch 355 loss:20.347761626734343
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.120796951149045
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.3148668068449
waveform batch: 2/2
Test loss - extrapolation:17.112633356331102
Epoch 355 mean train loss:0.9768183969833647
Epoch 355 mean test loss - interpolation:1.0201328251915076
Epoch 355 mean test loss - extrapolation:4.952291680264667
Start training epoch 356
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.19902
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.810777036883121
Iteration: 2 || Loss: 2.8093151777371768
Iteration: 3 || Loss: 2.807851617385137
Iteration: 4 || Loss: 2.806391727064492
Iteration: 5 || Loss: 2.8049349687620504
Iteration: 6 || Loss: 2.8049349687620504
saving ADAM checkpoint...
Sum of params:84.19902
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8049349687620504
Iteration: 2 || Loss: 2.2060987788853033
Iteration: 3 || Loss: 1.7488640885786588
Iteration: 4 || Loss: 1.746329437182466
Iteration: 5 || Loss: 1.7172634552966817
Iteration: 6 || Loss: 1.7157333206991086
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.27942
Epoch 356 loss:1.7157333206991086
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.27942
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.493911011595692
Iteration: 2 || Loss: 6.49354165044738
Iteration: 3 || Loss: 6.493171798731983
Iteration: 4 || Loss: 6.492805242910325
Iteration: 5 || Loss: 6.492440524100863
Iteration: 6 || Loss: 6.492440524100863
saving ADAM checkpoint...
Sum of params:84.279465
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.492440524100863
Iteration: 2 || Loss: 6.462285684510731
Iteration: 3 || Loss: 6.384773601415469
Iteration: 4 || Loss: 6.276540556807219
Iteration: 5 || Loss: 6.27006732543676
Iteration: 6 || Loss: 6.259765506038533
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.3127
Epoch 356 loss:6.259765506038533
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.3127
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.767486809576162
Iteration: 2 || Loss: 20.767059617832984
Iteration: 3 || Loss: 20.766634492219158
Iteration: 4 || Loss: 20.76621058565459
Iteration: 5 || Loss: 20.76578684158507
Iteration: 6 || Loss: 20.76578684158507
saving ADAM checkpoint...
Sum of params:84.31274
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.76578684158507
Iteration: 2 || Loss: 20.720127262125633
Iteration: 3 || Loss: 20.512085872847752
Iteration: 4 || Loss: 20.38866455393124
Iteration: 5 || Loss: 20.35837477157378
Iteration: 6 || Loss: 20.333418809317937
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.21089
Epoch 356 loss:20.333418809317937
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.1173977595773374
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.29560499709236
waveform batch: 2/2
Test loss - extrapolation:17.100048277740044
Epoch 356 mean train loss:0.9761695736570889
Epoch 356 mean test loss - interpolation:1.0195662932628895
Epoch 356 mean test loss - extrapolation:4.9496377729027
Start training epoch 357
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.21089
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.809103434876727
Iteration: 2 || Loss: 2.807640973560616
Iteration: 3 || Loss: 2.8061813996265306
Iteration: 4 || Loss: 2.804719213088505
Iteration: 5 || Loss: 2.8032610152748716
Iteration: 6 || Loss: 2.8032610152748716
saving ADAM checkpoint...
Sum of params:84.21091
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.8032610152748716
Iteration: 2 || Loss: 2.2043969720940666
Iteration: 3 || Loss: 1.747482731945501
Iteration: 4 || Loss: 1.7449529501078205
Iteration: 5 || Loss: 1.7159958198891698
Iteration: 6 || Loss: 1.7145001559088782
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.291145
Epoch 357 loss:1.7145001559088782
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.291145
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.490442029592131
Iteration: 2 || Loss: 6.490071206826687
Iteration: 3 || Loss: 6.489702494389161
Iteration: 4 || Loss: 6.489334522433493
Iteration: 5 || Loss: 6.488968508172106
Iteration: 6 || Loss: 6.488968508172106
saving ADAM checkpoint...
Sum of params:84.29118
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.488968508172106
Iteration: 2 || Loss: 6.458744227532924
Iteration: 3 || Loss: 6.381429076967059
Iteration: 4 || Loss: 6.273293023494306
Iteration: 5 || Loss: 6.266807926901592
Iteration: 6 || Loss: 6.256557323434923
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.32459
Epoch 357 loss:6.256557323434923
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.32459
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.752995432160354
Iteration: 2 || Loss: 20.752568457549984
Iteration: 3 || Loss: 20.75214178485405
Iteration: 4 || Loss: 20.751715967315985
Iteration: 5 || Loss: 20.75129135949522
Iteration: 6 || Loss: 20.75129135949522
saving ADAM checkpoint...
Sum of params:84.324615
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.75129135949522
Iteration: 2 || Loss: 20.70549121613176
Iteration: 3 || Loss: 20.497441903948435
Iteration: 4 || Loss: 20.374264760050337
Iteration: 5 || Loss: 20.344008909687854
Iteration: 6 || Loss: 20.31915973965846
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.222755
Epoch 357 loss:20.31915973965846
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.114015998343943
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.276411392120444
waveform batch: 2/2
Test loss - extrapolation:17.087514169071166
Epoch 357 mean train loss:0.9755247316897332
Epoch 357 mean test loss - interpolation:1.019002666390657
Epoch 357 mean test loss - extrapolation:4.946993796765967
Start training epoch 358
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.222755
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.807425945670279
Iteration: 2 || Loss: 2.8059617432221398
Iteration: 3 || Loss: 2.8044994448781098
Iteration: 4 || Loss: 2.8030412375945586
Iteration: 5 || Loss: 2.801580410053046
Iteration: 6 || Loss: 2.801580410053046
saving ADAM checkpoint...
Sum of params:84.222755
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.801580410053046
Iteration: 2 || Loss: 2.202704231773956
Iteration: 3 || Loss: 1.7461181039814007
Iteration: 4 || Loss: 1.74359229276941
Iteration: 5 || Loss: 1.7147394278580683
Iteration: 6 || Loss: 1.7132755541065545
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.30286
Epoch 358 loss:1.7132755541065545
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.30286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.48698212166622
Iteration: 2 || Loss: 6.486611118941351
Iteration: 3 || Loss: 6.486241687724413
Iteration: 4 || Loss: 6.485872656698359
Iteration: 5 || Loss: 6.485506273521796
Iteration: 6 || Loss: 6.485506273521796
saving ADAM checkpoint...
Sum of params:84.30288
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.485506273521796
Iteration: 2 || Loss: 6.455225239996001
Iteration: 3 || Loss: 6.378103491572535
Iteration: 4 || Loss: 6.270060525419806
Iteration: 5 || Loss: 6.2635603023018795
Iteration: 6 || Loss: 6.253361307751085
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.336464
Epoch 358 loss:6.253361307751085
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.336464
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.738597791197332
Iteration: 2 || Loss: 20.73816988101587
Iteration: 3 || Loss: 20.73774285770673
Iteration: 4 || Loss: 20.737318717837937
Iteration: 5 || Loss: 20.736892416513488
Iteration: 6 || Loss: 20.736892416513488
saving ADAM checkpoint...
Sum of params:84.33651
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.736892416513488
Iteration: 2 || Loss: 20.69097291150372
Iteration: 3 || Loss: 20.482885447388774
Iteration: 4 || Loss: 20.359950092768884
Iteration: 5 || Loss: 20.32972049916315
Iteration: 6 || Loss: 20.304970386048208
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.23459
Epoch 358 loss:20.304970386048208
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.110659380843027
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.25733056424922
waveform batch: 2/2
Test loss - extrapolation:17.07501625468101
Epoch 358 mean train loss:0.9748830085484775
Epoch 358 mean test loss - interpolation:1.0184432301405046
Epoch 358 mean test loss - extrapolation:4.9443622349108525
Start training epoch 359
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.23459
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8058544795733025
Iteration: 2 || Loss: 2.8043887717395157
Iteration: 3 || Loss: 2.802925338756776
Iteration: 4 || Loss: 2.8014646462486974
Iteration: 5 || Loss: 2.800004572963244
Iteration: 6 || Loss: 2.800004572963244
saving ADAM checkpoint...
Sum of params:84.23458
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.800004572963244
Iteration: 2 || Loss: 2.2010060686088386
Iteration: 3 || Loss: 1.7447740730250074
Iteration: 4 || Loss: 1.742251275162974
Iteration: 5 || Loss: 1.7135080280765467
Iteration: 6 || Loss: 1.712064449161489
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.31455
Epoch 359 loss:1.712064449161489
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.31455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.483620266880479
Iteration: 2 || Loss: 6.483246965620875
Iteration: 3 || Loss: 6.482878507519546
Iteration: 4 || Loss: 6.482509092960204
Iteration: 5 || Loss: 6.482142153009628
Iteration: 6 || Loss: 6.482142153009628
saving ADAM checkpoint...
Sum of params:84.31459
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.482142153009628
Iteration: 2 || Loss: 6.451757435466272
Iteration: 3 || Loss: 6.374793258768346
Iteration: 4 || Loss: 6.266851682039914
Iteration: 5 || Loss: 6.260344728985776
Iteration: 6 || Loss: 6.250196426158452
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.34833
Epoch 359 loss:6.250196426158452
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.34833
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.72422838241562
Iteration: 2 || Loss: 20.723798036991138
Iteration: 3 || Loss: 20.723369826595665
Iteration: 4 || Loss: 20.722943303605224
Iteration: 5 || Loss: 20.722517525474792
Iteration: 6 || Loss: 20.722517525474792
saving ADAM checkpoint...
Sum of params:84.34838
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.722517525474792
Iteration: 2 || Loss: 20.676433818063938
Iteration: 3 || Loss: 20.468365848859808
Iteration: 4 || Loss: 20.34568195073012
Iteration: 5 || Loss: 20.31548916765739
Iteration: 6 || Loss: 20.29085085041145
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.2464
Epoch 359 loss:20.29085085041145
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.107315580122085
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.23822947473879
waveform batch: 2/2
Test loss - extrapolation:17.06256686003481
Epoch 359 mean train loss:0.974245231921772
Epoch 359 mean test loss - interpolation:1.0178859300203476
Epoch 359 mean test loss - extrapolation:4.9417330278978
Start training epoch 360
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.2464
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8041710909884783
Iteration: 2 || Loss: 2.8027076716627652
Iteration: 3 || Loss: 2.801243738760769
Iteration: 4 || Loss: 2.799780944711884
Iteration: 5 || Loss: 2.798319240909472
Iteration: 6 || Loss: 2.798319240909472
saving ADAM checkpoint...
Sum of params:84.2464
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.798319240909472
Iteration: 2 || Loss: 2.1993679309330543
Iteration: 3 || Loss: 1.7434333555649217
Iteration: 4 || Loss: 1.7409133429997314
Iteration: 5 || Loss: 1.7122752606147515
Iteration: 6 || Loss: 1.7108613598235407
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.32622
Epoch 360 loss:1.7108613598235407
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.32622
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.480217751947309
Iteration: 2 || Loss: 6.479845541580817
Iteration: 3 || Loss: 6.47947291553716
Iteration: 4 || Loss: 6.4791033608079065
Iteration: 5 || Loss: 6.478737134691492
Iteration: 6 || Loss: 6.478737134691492
saving ADAM checkpoint...
Sum of params:84.32625
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.478737134691492
Iteration: 2 || Loss: 6.448284867333947
Iteration: 3 || Loss: 6.371511665601319
Iteration: 4 || Loss: 6.2636653300796254
Iteration: 5 || Loss: 6.257151323522358
Iteration: 6 || Loss: 6.2470537200167735
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.36013
Epoch 360 loss:6.2470537200167735
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.36013
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.709889330807496
Iteration: 2 || Loss: 20.70945991743598
Iteration: 3 || Loss: 20.70903074380454
Iteration: 4 || Loss: 20.708603281637235
Iteration: 5 || Loss: 20.70817701192385
Iteration: 6 || Loss: 20.70817701192385
saving ADAM checkpoint...
Sum of params:84.36017
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.70817701192385
Iteration: 2 || Loss: 20.66196205114042
Iteration: 3 || Loss: 20.45390628708514
Iteration: 4 || Loss: 20.331487123561338
Iteration: 5 || Loss: 20.301327377252665
Iteration: 6 || Loss: 20.27678690694814
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.258156
Epoch 360 loss:20.27678690694814
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.104010941209674
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.2192712377964
waveform batch: 2/2
Test loss - extrapolation:17.05015887590843
Epoch 360 mean train loss:0.9736104133375328
Epoch 360 mean test loss - interpolation:1.0173351568682791
Epoch 360 mean test loss - extrapolation:4.939119176142069
Start training epoch 361
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.258156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.802655813235027
Iteration: 2 || Loss: 2.8011904080565415
Iteration: 3 || Loss: 2.799727548007713
Iteration: 4 || Loss: 2.7982613642292953
Iteration: 5 || Loss: 2.796800033115712
Iteration: 6 || Loss: 2.796800033115712
saving ADAM checkpoint...
Sum of params:84.25815
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.796800033115712
Iteration: 2 || Loss: 2.197717840411778
Iteration: 3 || Loss: 1.742111079477784
Iteration: 4 || Loss: 1.7395961870739425
Iteration: 5 || Loss: 1.7110591587137087
Iteration: 6 || Loss: 1.7096728570748563
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.33783
Epoch 361 loss:1.7096728570748563
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.33783
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4768805257813025
Iteration: 2 || Loss: 6.476505935235965
Iteration: 3 || Loss: 6.47613537791039
Iteration: 4 || Loss: 6.475764547226547
Iteration: 5 || Loss: 6.475397610686662
Iteration: 6 || Loss: 6.475397610686662
saving ADAM checkpoint...
Sum of params:84.33787
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.475397610686662
Iteration: 2 || Loss: 6.444854295370803
Iteration: 3 || Loss: 6.368262384033347
Iteration: 4 || Loss: 6.260511624323961
Iteration: 5 || Loss: 6.253988205199379
Iteration: 6 || Loss: 6.24393832155052
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.37191
Epoch 361 loss:6.24393832155052
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.37191
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.695642889494497
Iteration: 2 || Loss: 20.69521167264425
Iteration: 3 || Loss: 20.69478311751825
Iteration: 4 || Loss: 20.694354285903053
Iteration: 5 || Loss: 20.693927704897423
Iteration: 6 || Loss: 20.693927704897423
saving ADAM checkpoint...
Sum of params:84.37196
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.693927704897423
Iteration: 2 || Loss: 20.647562005403735
Iteration: 3 || Loss: 20.439498799452082
Iteration: 4 || Loss: 20.31734624714455
Iteration: 5 || Loss: 20.287223189922702
Iteration: 6 || Loss: 20.26278641620485
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.269905
Epoch 361 loss:20.26278641620485
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.100728668867495
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.200350719350894
waveform batch: 2/2
Test loss - extrapolation:17.037793205150464
Epoch 361 mean train loss:0.9729792274079389
Epoch 361 mean test loss - interpolation:1.0167881114779158
Epoch 361 mean test loss - extrapolation:4.9365119937084465
Start training epoch 362
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.269905
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.8011178128099687
Iteration: 2 || Loss: 2.7996522360970486
Iteration: 3 || Loss: 2.798183984318865
Iteration: 4 || Loss: 2.7967216717068397
Iteration: 5 || Loss: 2.7952604817100872
Iteration: 6 || Loss: 2.7952604817100872
saving ADAM checkpoint...
Sum of params:84.2699
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7952604817100872
Iteration: 2 || Loss: 2.1961132531643894
Iteration: 3 || Loss: 1.7407992514711481
Iteration: 4 || Loss: 1.7382884698305872
Iteration: 5 || Loss: 1.7098561872812272
Iteration: 6 || Loss: 1.708494116058875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.34943
Epoch 362 loss:1.708494116058875
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.34943
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.473541820450403
Iteration: 2 || Loss: 6.473167210112468
Iteration: 3 || Loss: 6.4727953130185085
Iteration: 4 || Loss: 6.472425896382074
Iteration: 5 || Loss: 6.472057254711584
Iteration: 6 || Loss: 6.472057254711584
saving ADAM checkpoint...
Sum of params:84.34946
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.472057254711584
Iteration: 2 || Loss: 6.441448052774812
Iteration: 3 || Loss: 6.3650379745496295
Iteration: 4 || Loss: 6.257382650976647
Iteration: 5 || Loss: 6.250848889923111
Iteration: 6 || Loss: 6.240844889652062
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.383675
Epoch 362 loss:6.240844889652062
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.383675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.68146457964222
Iteration: 2 || Loss: 20.681033993184784
Iteration: 3 || Loss: 20.6806025459406
Iteration: 4 || Loss: 20.68017303527291
Iteration: 5 || Loss: 20.679746964572942
Iteration: 6 || Loss: 20.679746964572942
saving ADAM checkpoint...
Sum of params:84.38371
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.679746964572942
Iteration: 2 || Loss: 20.633244968657454
Iteration: 3 || Loss: 20.42519259333367
Iteration: 4 || Loss: 20.303282103417885
Iteration: 5 || Loss: 20.27318963179548
Iteration: 6 || Loss: 20.248855215834656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.281586
Epoch 362 loss:20.248855215834656
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.0974518210080335
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.18148278723122
waveform batch: 2/2
Test loss - extrapolation:17.025477925970716
Epoch 362 mean train loss:0.9723515248808825
Epoch 362 mean test loss - interpolation:1.0162419701680057
Epoch 362 mean test loss - extrapolation:4.933913392766828
Start training epoch 363
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.281586
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.799558044228077
Iteration: 2 || Loss: 2.7980898217866446
Iteration: 3 || Loss: 2.7966263768863864
Iteration: 4 || Loss: 2.7951609763160525
Iteration: 5 || Loss: 2.7936982810739908
Iteration: 6 || Loss: 2.7936982810739908
saving ADAM checkpoint...
Sum of params:84.2816
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7936982810739908
Iteration: 2 || Loss: 2.1944810265287336
Iteration: 3 || Loss: 1.7395010759945755
Iteration: 4 || Loss: 1.7369934820539528
Iteration: 5 || Loss: 1.7086600014997244
Iteration: 6 || Loss: 1.707325781072607
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.360985
Epoch 363 loss:1.707325781072607
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.360985
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4702105550544955
Iteration: 2 || Loss: 6.469837470971715
Iteration: 3 || Loss: 6.469465685792528
Iteration: 4 || Loss: 6.469094265245294
Iteration: 5 || Loss: 6.468725365310765
Iteration: 6 || Loss: 6.468725365310765
saving ADAM checkpoint...
Sum of params:84.36102
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.468725365310765
Iteration: 2 || Loss: 6.4380584470409685
Iteration: 3 || Loss: 6.3618385602245935
Iteration: 4 || Loss: 6.254270490504738
Iteration: 5 || Loss: 6.2477215327954525
Iteration: 6 || Loss: 6.237764986394172
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.3954
Epoch 363 loss:6.237764986394172
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.3954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.667406507166778
Iteration: 2 || Loss: 20.66697389377426
Iteration: 3 || Loss: 20.666543987531334
Iteration: 4 || Loss: 20.666116107985786
Iteration: 5 || Loss: 20.665687206959085
Iteration: 6 || Loss: 20.665687206959085
saving ADAM checkpoint...
Sum of params:84.39545
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.665687206959085
Iteration: 2 || Loss: 20.61906895508681
Iteration: 3 || Loss: 20.410975150360056
Iteration: 4 || Loss: 20.289295631418536
Iteration: 5 || Loss: 20.25923038006066
Iteration: 6 || Loss: 20.234990026840762
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.293274
Epoch 363 loss:20.234990026840762
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.094203807678596
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.16272122955298
waveform batch: 2/2
Test loss - extrapolation:17.01320577117827
Epoch 363 mean train loss:0.9717269239416394
Epoch 363 mean test loss - interpolation:1.0157006346130995
Epoch 363 mean test loss - extrapolation:4.931327250060938
Start training epoch 364
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.293274
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7980969535203397
Iteration: 2 || Loss: 2.7966296077700936
Iteration: 3 || Loss: 2.7951640304563043
Iteration: 4 || Loss: 2.7936987155135546
Iteration: 5 || Loss: 2.792234660117273
Iteration: 6 || Loss: 2.792234660117273
saving ADAM checkpoint...
Sum of params:84.29328
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.792234660117273
Iteration: 2 || Loss: 2.192863687877858
Iteration: 3 || Loss: 1.73822039792029
Iteration: 4 || Loss: 1.73571586071854
Iteration: 5 || Loss: 1.7074810056055436
Iteration: 6 || Loss: 1.7061677675069344
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.3725
Epoch 364 loss:1.7061677675069344
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.3725
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.466953861911403
Iteration: 2 || Loss: 6.466579733403405
Iteration: 3 || Loss: 6.466206706899898
Iteration: 4 || Loss: 6.4658351105538
Iteration: 5 || Loss: 6.465465466435568
Iteration: 6 || Loss: 6.465465466435568
saving ADAM checkpoint...
Sum of params:84.37256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.465465466435568
Iteration: 2 || Loss: 6.434709954604931
Iteration: 3 || Loss: 6.358659770936327
Iteration: 4 || Loss: 6.251180305936352
Iteration: 5 || Loss: 6.244624677468023
Iteration: 6 || Loss: 6.234715195815198
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.4071
Epoch 364 loss:6.234715195815198
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.4071
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.65334307937702
Iteration: 2 || Loss: 20.65291056910981
Iteration: 3 || Loss: 20.65247910057892
Iteration: 4 || Loss: 20.652049583823064
Iteration: 5 || Loss: 20.651619826977672
Iteration: 6 || Loss: 20.651619826977672
saving ADAM checkpoint...
Sum of params:84.407135
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.651619826977672
Iteration: 2 || Loss: 20.604857663737988
Iteration: 3 || Loss: 20.396779254322418
Iteration: 4 || Loss: 20.275353659420556
Iteration: 5 || Loss: 20.24532067333581
Iteration: 6 || Loss: 20.22118084578096
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.30492
Epoch 364 loss:20.22118084578096
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.090986341443384
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.14399069561235
waveform batch: 2/2
Test loss - extrapolation:17.00096105964338
Epoch 364 mean train loss:0.9711056485897618
Epoch 364 mean test loss - interpolation:1.015164390240564
Epoch 364 mean test loss - extrapolation:4.928745979604645
Start training epoch 365
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.30492
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7966355286869757
Iteration: 2 || Loss: 2.795166917642232
Iteration: 3 || Loss: 2.793699058976888
Iteration: 4 || Loss: 2.7922303110647344
Iteration: 5 || Loss: 2.790765552980854
Iteration: 6 || Loss: 2.790765552980854
saving ADAM checkpoint...
Sum of params:84.304924
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.790765552980854
Iteration: 2 || Loss: 2.1912826889081125
Iteration: 3 || Loss: 1.7369462356482421
Iteration: 4 || Loss: 1.7344444860062809
Iteration: 5 || Loss: 1.7063136734192914
Iteration: 6 || Loss: 1.7050208147106545
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.38402
Epoch 365 loss:1.7050208147106545
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.38402
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.463700100325335
Iteration: 2 || Loss: 6.463324660010514
Iteration: 3 || Loss: 6.4629517082673225
Iteration: 4 || Loss: 6.462580220192146
Iteration: 5 || Loss: 6.462209844145681
Iteration: 6 || Loss: 6.462209844145681
saving ADAM checkpoint...
Sum of params:84.38405
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.462209844145681
Iteration: 2 || Loss: 6.431380467869586
Iteration: 3 || Loss: 6.35550323098038
Iteration: 4 || Loss: 6.248114673534198
Iteration: 5 || Loss: 6.241547017104381
Iteration: 6 || Loss: 6.231685437026046
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.41876
Epoch 365 loss:6.231685437026046
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.41876
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.639405843164937
Iteration: 2 || Loss: 20.638972189572012
Iteration: 3 || Loss: 20.638541542772078
Iteration: 4 || Loss: 20.638108515182136
Iteration: 5 || Loss: 20.63767866246686
Iteration: 6 || Loss: 20.63767866246686
saving ADAM checkpoint...
Sum of params:84.41881
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.63767866246686
Iteration: 2 || Loss: 20.590785237673455
Iteration: 3 || Loss: 20.382675912359527
Iteration: 4 || Loss: 20.261483933943094
Iteration: 5 || Loss: 20.23147995233943
Iteration: 6 || Loss: 20.207437069474548
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.31654
Epoch 365 loss:20.207437069474548
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.087765041779611
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.12531140921135
waveform batch: 2/2
Test loss - extrapolation:16.98877746963912
Epoch 365 mean train loss:0.9704877007314224
Epoch 365 mean test loss - interpolation:1.0146275069632684
Epoch 365 mean test loss - extrapolation:4.926174073237539
Start training epoch 366
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.31654
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.795127486143665
Iteration: 2 || Loss: 2.793656643269196
Iteration: 3 || Loss: 2.7921870320429196
Iteration: 4 || Loss: 2.7907198788035417
Iteration: 5 || Loss: 2.7892560098327506
Iteration: 6 || Loss: 2.7892560098327506
saving ADAM checkpoint...
Sum of params:84.316536
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7892560098327506
Iteration: 2 || Loss: 2.189698811484641
Iteration: 3 || Loss: 1.7356861972172835
Iteration: 4 || Loss: 1.7331875323174106
Iteration: 5 || Loss: 1.7051534346342863
Iteration: 6 || Loss: 1.7038824990038455
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.39549
Epoch 366 loss:1.7038824990038455
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.39549
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.46047272055045
Iteration: 2 || Loss: 6.460097163344376
Iteration: 3 || Loss: 6.45972279148159
Iteration: 4 || Loss: 6.459350228066432
Iteration: 5 || Loss: 6.458979155324376
Iteration: 6 || Loss: 6.458979155324376
saving ADAM checkpoint...
Sum of params:84.39554
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.458979155324376
Iteration: 2 || Loss: 6.428072988491725
Iteration: 3 || Loss: 6.352372549775056
Iteration: 4 || Loss: 6.245069069965133
Iteration: 5 || Loss: 6.238490192307372
Iteration: 6 || Loss: 6.22867593887094
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.43039
Epoch 366 loss:6.22867593887094
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.43039
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.625529196641935
Iteration: 2 || Loss: 20.625095764672608
Iteration: 3 || Loss: 20.624663591712054
Iteration: 4 || Loss: 20.6242310390611
Iteration: 5 || Loss: 20.6238007109834
Iteration: 6 || Loss: 20.6238007109834
saving ADAM checkpoint...
Sum of params:84.430435
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.6238007109834
Iteration: 2 || Loss: 20.576775708646906
Iteration: 3 || Loss: 20.368634595514187
Iteration: 4 || Loss: 20.247676820239985
Iteration: 5 || Loss: 20.217702827006793
Iteration: 6 || Loss: 20.193754445980062
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.32812
Epoch 366 loss:20.193754445980062
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.084572119445157
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.106693094243695
waveform batch: 2/2
Test loss - extrapolation:16.97662970989296
Epoch 366 mean train loss:0.9698728580639603
Epoch 366 mean test loss - interpolation:1.0140953532408596
Epoch 366 mean test loss - extrapolation:4.923610233678055
Start training epoch 367
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.32812
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7936494196506843
Iteration: 2 || Loss: 2.792180373586696
Iteration: 3 || Loss: 2.7907075688303316
Iteration: 4 || Loss: 2.7892404491667055
Iteration: 5 || Loss: 2.7877739736080485
Iteration: 6 || Loss: 2.7877739736080485
saving ADAM checkpoint...
Sum of params:84.32814
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7877739736080485
Iteration: 2 || Loss: 2.188140444987447
Iteration: 3 || Loss: 1.7344371534453475
Iteration: 4 || Loss: 1.7319416819628584
Iteration: 5 || Loss: 1.7040026490751012
Iteration: 6 || Loss: 1.7027539950939359
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.40695
Epoch 367 loss:1.7027539950939359
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.40695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.457241752683422
Iteration: 2 || Loss: 6.456865803254345
Iteration: 3 || Loss: 6.456491069404337
Iteration: 4 || Loss: 6.456117436093896
Iteration: 5 || Loss: 6.455747329962999
Iteration: 6 || Loss: 6.455747329962999
saving ADAM checkpoint...
Sum of params:84.40699
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.455747329962999
Iteration: 2 || Loss: 6.424776187219108
Iteration: 3 || Loss: 6.349259185588063
Iteration: 4 || Loss: 6.24204646329799
Iteration: 5 || Loss: 6.235455639458971
Iteration: 6 || Loss: 6.225684336745438
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.44201
Epoch 367 loss:6.225684336745438
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.44201
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.61169928470104
Iteration: 2 || Loss: 20.611264515701386
Iteration: 3 || Loss: 20.610830263783505
Iteration: 4 || Loss: 20.61039865297378
Iteration: 5 || Loss: 20.609967566481274
Iteration: 6 || Loss: 20.609967566481274
saving ADAM checkpoint...
Sum of params:84.44204
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.609967566481274
Iteration: 2 || Loss: 20.562804139103257
Iteration: 3 || Loss: 20.3546622940102
Iteration: 4 || Loss: 20.23393381847776
Iteration: 5 || Loss: 20.203991280648136
Iteration: 6 || Loss: 20.180140838107263
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.33969
Epoch 367 loss:20.180140838107263
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.081406940953431
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.08813576474373
waveform batch: 2/2
Test loss - extrapolation:16.96451967462876
Epoch 367 mean train loss:0.969261350687815
Epoch 367 mean test loss - interpolation:1.0135678234922385
Epoch 367 mean test loss - extrapolation:4.921054619947708
Start training epoch 368
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.33969
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7922084607561777
Iteration: 2 || Loss: 2.790734838784487
Iteration: 3 || Loss: 2.7892646801665095
Iteration: 4 || Loss: 2.7877974824189495
Iteration: 5 || Loss: 2.7863295919825535
Iteration: 6 || Loss: 2.7863295919825535
saving ADAM checkpoint...
Sum of params:84.33969
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7863295919825535
Iteration: 2 || Loss: 2.186604298785039
Iteration: 3 || Loss: 1.7331983824020696
Iteration: 4 || Loss: 1.7307045868220237
Iteration: 5 || Loss: 1.7028677156562964
Iteration: 6 || Loss: 1.7016346217071887
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.41837
Epoch 368 loss:1.7016346217071887
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.41837
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.454089161352504
Iteration: 2 || Loss: 6.453711551592174
Iteration: 3 || Loss: 6.453337414574248
Iteration: 4 || Loss: 6.452963507921485
Iteration: 5 || Loss: 6.452591141033633
Iteration: 6 || Loss: 6.452591141033633
saving ADAM checkpoint...
Sum of params:84.41842
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.452591141033633
Iteration: 2 || Loss: 6.421519551433064
Iteration: 3 || Loss: 6.346156884219689
Iteration: 4 || Loss: 6.239040093100859
Iteration: 5 || Loss: 6.232443107620017
Iteration: 6 || Loss: 6.2227154312766615
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.4536
Epoch 368 loss:6.2227154312766615
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.4536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.59794087678895
Iteration: 2 || Loss: 20.597505496183395
Iteration: 3 || Loss: 20.597072469055057
Iteration: 4 || Loss: 20.596638114400623
Iteration: 5 || Loss: 20.59620688209172
Iteration: 6 || Loss: 20.59620688209172
saving ADAM checkpoint...
Sum of params:84.45362
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.59620688209172
Iteration: 2 || Loss: 20.54889162006442
Iteration: 3 || Loss: 20.340739837825744
Iteration: 4 || Loss: 20.22024991565474
Iteration: 5 || Loss: 20.19034222878907
Iteration: 6 || Loss: 20.166585011865124
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.351234
Epoch 368 loss:20.166585011865124
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.078235445475033
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.069601817573954
waveform batch: 2/2
Test loss - extrapolation:16.95246517676273
Epoch 368 mean train loss:0.9686529332706543
Epoch 368 mean test loss - interpolation:1.0130392409125055
Epoch 368 mean test loss - extrapolation:4.91850558286139
Start training epoch 369
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.351234
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7906775540358337
Iteration: 2 || Loss: 2.7892051125320814
Iteration: 3 || Loss: 2.787732959115168
Iteration: 4 || Loss: 2.786263923550891
Iteration: 5 || Loss: 2.7847969899774645
Iteration: 6 || Loss: 2.7847969899774645
saving ADAM checkpoint...
Sum of params:84.351234
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7847969899774645
Iteration: 2 || Loss: 2.1850816265682074
Iteration: 3 || Loss: 1.7319690204397826
Iteration: 4 || Loss: 1.7294774810814713
Iteration: 5 || Loss: 1.7017317599138089
Iteration: 6 || Loss: 1.700523859821202
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.42976
Epoch 369 loss:1.700523859821202
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.42976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.450890806119047
Iteration: 2 || Loss: 6.4505139345216485
Iteration: 3 || Loss: 6.450136970978608
Iteration: 4 || Loss: 6.449761158641326
Iteration: 5 || Loss: 6.449389990140211
Iteration: 6 || Loss: 6.449389990140211
saving ADAM checkpoint...
Sum of params:84.42982
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.449389990140211
Iteration: 2 || Loss: 6.418254553612105
Iteration: 3 || Loss: 6.343090733600864
Iteration: 4 || Loss: 6.236057395502319
Iteration: 5 || Loss: 6.229446836348911
Iteration: 6 || Loss: 6.2197647765824176
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.46515
Epoch 369 loss:6.2197647765824176
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.46515
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.5842477242752
Iteration: 2 || Loss: 20.583811281041985
Iteration: 3 || Loss: 20.5833756449581
Iteration: 4 || Loss: 20.58294320204479
Iteration: 5 || Loss: 20.582510450035556
Iteration: 6 || Loss: 20.582510450035556
saving ADAM checkpoint...
Sum of params:84.46519
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.582510450035556
Iteration: 2 || Loss: 20.535078578612918
Iteration: 3 || Loss: 20.32689087975238
Iteration: 4 || Loss: 20.206638226949664
Iteration: 5 || Loss: 20.176758707752743
Iteration: 6 || Loss: 20.15309131391174
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.36273
Epoch 369 loss:20.15309131391174
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.075129198461818
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.05122782090969
waveform batch: 2/2
Test loss - extrapolation:16.94043575280066
Epoch 369 mean train loss:0.9680475844936332
Epoch 369 mean test loss - interpolation:1.0125215330769697
Epoch 369 mean test loss - extrapolation:4.915971964475863
Start training epoch 370
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.36273
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7893963244935516
Iteration: 2 || Loss: 2.787923728747626
Iteration: 3 || Loss: 2.7864510317309064
Iteration: 4 || Loss: 2.7849823048588003
Iteration: 5 || Loss: 2.7835132378057135
Iteration: 6 || Loss: 2.7835132378057135
saving ADAM checkpoint...
Sum of params:84.36273
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7835132378057135
Iteration: 2 || Loss: 2.1835651876212276
Iteration: 3 || Loss: 1.7307561830908125
Iteration: 4 || Loss: 1.728267220548964
Iteration: 5 || Loss: 1.7006210886290312
Iteration: 6 || Loss: 1.6994256163269235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.44113
Epoch 370 loss:1.6994256163269235
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.44113
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.447805344549735
Iteration: 2 || Loss: 6.447425703780434
Iteration: 3 || Loss: 6.447049654729495
Iteration: 4 || Loss: 6.446674607900997
Iteration: 5 || Loss: 6.446302746545182
Iteration: 6 || Loss: 6.446302746545182
saving ADAM checkpoint...
Sum of params:84.44118
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.446302746545182
Iteration: 2 || Loss: 6.415056723833771
Iteration: 3 || Loss: 6.34004026637387
Iteration: 4 || Loss: 6.233097120292951
Iteration: 5 || Loss: 6.226479456246125
Iteration: 6 || Loss: 6.216839517453271
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.47666
Epoch 370 loss:6.216839517453271
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.47666
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.570630055649275
Iteration: 2 || Loss: 20.57019266829634
Iteration: 3 || Loss: 20.569758082810466
Iteration: 4 || Loss: 20.56932261244726
Iteration: 5 || Loss: 20.568889348283143
Iteration: 6 || Loss: 20.568889348283143
saving ADAM checkpoint...
Sum of params:84.47669
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.568889348283143
Iteration: 2 || Loss: 20.521299368215487
Iteration: 3 || Loss: 20.313089623411965
Iteration: 4 || Loss: 20.193072539994787
Iteration: 5 || Loss: 20.163227251052295
Iteration: 6 || Loss: 20.13965745272128
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.37419
Epoch 370 loss:20.13965745272128
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.072018238380309
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.03282225795869
waveform batch: 2/2
Test loss - extrapolation:16.928454113662468
Epoch 370 mean train loss:0.9674456064310853
Epoch 370 mean test loss - interpolation:1.0120030397300515
Epoch 370 mean test loss - extrapolation:4.913439697635097
Start training epoch 371
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.37419
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7879591393353964
Iteration: 2 || Loss: 2.786486065493135
Iteration: 3 || Loss: 2.7850130736576175
Iteration: 4 || Loss: 2.7835403840742265
Iteration: 5 || Loss: 2.7820728824034084
Iteration: 6 || Loss: 2.7820728824034084
saving ADAM checkpoint...
Sum of params:84.37421
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7820728824034084
Iteration: 2 || Loss: 2.1820994140079613
Iteration: 3 || Loss: 1.7295476070188138
Iteration: 4 || Loss: 1.727060910021266
Iteration: 5 || Loss: 1.6995067473703336
Iteration: 6 || Loss: 1.6983337843550586
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.45247
Epoch 371 loss:1.6983337843550586
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.45247
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.444636240001499
Iteration: 2 || Loss: 6.444256988798081
Iteration: 3 || Loss: 6.443879637875493
Iteration: 4 || Loss: 6.443504563262718
Iteration: 5 || Loss: 6.443132237621152
Iteration: 6 || Loss: 6.443132237621152
saving ADAM checkpoint...
Sum of params:84.452515
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.443132237621152
Iteration: 2 || Loss: 6.411843113964168
Iteration: 3 || Loss: 6.337018285863379
Iteration: 4 || Loss: 6.230155294987101
Iteration: 5 || Loss: 6.223522664238769
Iteration: 6 || Loss: 6.213924780622534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.48815
Epoch 371 loss:6.213924780622534
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.48815
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.557095881237746
Iteration: 2 || Loss: 20.556658975349308
Iteration: 3 || Loss: 20.55622266695584
Iteration: 4 || Loss: 20.55578776577151
Iteration: 5 || Loss: 20.555355608323946
Iteration: 6 || Loss: 20.555355608323946
saving ADAM checkpoint...
Sum of params:84.488174
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.555355608323946
Iteration: 2 || Loss: 20.507648550262825
Iteration: 3 || Loss: 20.299410250228
Iteration: 4 || Loss: 20.179597566389546
Iteration: 5 || Loss: 20.149773917520047
Iteration: 6 || Loss: 20.126290411138733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.38564
Epoch 371 loss:20.126290411138733
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.068906812196746
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:42.01452656317426
waveform batch: 2/2
Test loss - extrapolation:16.916531652979227
Epoch 371 mean train loss:0.9668465164178043
Epoch 371 mean test loss - interpolation:1.0114844686994577
Epoch 371 mean test loss - extrapolation:4.910921518012791
Start training epoch 372
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.38564
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7865588549480886
Iteration: 2 || Loss: 2.7850830081593534
Iteration: 3 || Loss: 2.783606873604304
Iteration: 4 || Loss: 2.7821344803653356
Iteration: 5 || Loss: 2.7806653066087033
Iteration: 6 || Loss: 2.7806653066087033
saving ADAM checkpoint...
Sum of params:84.38563
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7806653066087033
Iteration: 2 || Loss: 2.180581261203918
Iteration: 3 || Loss: 1.7283569253268347
Iteration: 4 || Loss: 1.7258719329525896
Iteration: 5 || Loss: 1.6984075884094891
Iteration: 6 || Loss: 1.697250961662839
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.46375
Epoch 372 loss:1.697250961662839
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.46375
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.441538157301917
Iteration: 2 || Loss: 6.441158364571134
Iteration: 3 || Loss: 6.440781503231552
Iteration: 4 || Loss: 6.440405032542658
Iteration: 5 || Loss: 6.4400306617920675
Iteration: 6 || Loss: 6.4400306617920675
saving ADAM checkpoint...
Sum of params:84.46381
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.4400306617920675
Iteration: 2 || Loss: 6.408670922288059
Iteration: 3 || Loss: 6.334012907880838
Iteration: 4 || Loss: 6.227228984247624
Iteration: 5 || Loss: 6.220585591858149
Iteration: 6 || Loss: 6.211030704660376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.4996
Epoch 372 loss:6.211030704660376
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.4996
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.543593820078538
Iteration: 2 || Loss: 20.543156532531036
Iteration: 3 || Loss: 20.542718843845112
Iteration: 4 || Loss: 20.542282638908457
Iteration: 5 || Loss: 20.541848818201046
Iteration: 6 || Loss: 20.541848818201046
saving ADAM checkpoint...
Sum of params:84.49964
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.541848818201046
Iteration: 2 || Loss: 20.494026800358295
Iteration: 3 || Loss: 20.28574037106741
Iteration: 4 || Loss: 20.166163575568195
Iteration: 5 || Loss: 20.136366989256413
Iteration: 6 || Loss: 20.11296602003319
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.39704
Epoch 372 loss:20.11296602003319
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.0658334729947345
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.99631524178739
waveform batch: 2/2
Test loss - extrapolation:16.90463618097041
Epoch 372 mean train loss:0.9662499202191864
Epoch 372 mean test loss - interpolation:1.0109722454991223
Epoch 372 mean test loss - extrapolation:4.90841261856315
Start training epoch 373
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.39704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7852704302777314
Iteration: 2 || Loss: 2.7837904637014113
Iteration: 3 || Loss: 2.7823149335791473
Iteration: 4 || Loss: 2.7808429782751385
Iteration: 5 || Loss: 2.779374239996475
Iteration: 6 || Loss: 2.779374239996475
saving ADAM checkpoint...
Sum of params:84.39704
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.779374239996475
Iteration: 2 || Loss: 2.179080790409843
Iteration: 3 || Loss: 1.727177576346387
Iteration: 4 || Loss: 1.7246959120394683
Iteration: 5 || Loss: 1.6973216127429889
Iteration: 6 || Loss: 1.6961801019353384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.475044
Epoch 373 loss:1.6961801019353384
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.475044
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.438492292414613
Iteration: 2 || Loss: 6.438112691427834
Iteration: 3 || Loss: 6.437734435188224
Iteration: 4 || Loss: 6.437360364126676
Iteration: 5 || Loss: 6.436983305142368
Iteration: 6 || Loss: 6.436983305142368
saving ADAM checkpoint...
Sum of params:84.4751
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.436983305142368
Iteration: 2 || Loss: 6.405521336557244
Iteration: 3 || Loss: 6.331025859734816
Iteration: 4 || Loss: 6.224331951761519
Iteration: 5 || Loss: 6.217680141439752
Iteration: 6 || Loss: 6.208164481257272
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.51104
Epoch 373 loss:6.208164481257272
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.51104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.53012839038759
Iteration: 2 || Loss: 20.529690601073227
Iteration: 3 || Loss: 20.529253285945455
Iteration: 4 || Loss: 20.528815597651626
Iteration: 5 || Loss: 20.528380607288742
Iteration: 6 || Loss: 20.528380607288742
saving ADAM checkpoint...
Sum of params:84.51108
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.528380607288742
Iteration: 2 || Loss: 20.480411663604546
Iteration: 3 || Loss: 20.272118585156086
Iteration: 4 || Loss: 20.1527692541995
Iteration: 5 || Loss: 20.123009212985995
Iteration: 6 || Loss: 20.09969972328223
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.40844
Epoch 373 loss:20.09969972328223
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.062770277286627
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.978089239438866
waveform batch: 2/2
Test loss - extrapolation:16.892780061125883
Epoch 373 mean train loss:0.9656567002232704
Epoch 373 mean test loss - interpolation:1.0104617128811044
Epoch 373 mean test loss - extrapolation:4.905905775047063
Start training epoch 374
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.40844
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7838654391643907
Iteration: 2 || Loss: 2.782387755432664
Iteration: 3 || Loss: 2.780914110565216
Iteration: 4 || Loss: 2.779437801783989
Iteration: 5 || Loss: 2.7779648075501284
Iteration: 6 || Loss: 2.7779648075501284
saving ADAM checkpoint...
Sum of params:84.40843
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7779648075501284
Iteration: 2 || Loss: 2.177642944722335
Iteration: 3 || Loss: 1.7260010584320375
Iteration: 4 || Loss: 1.7235206532493115
Iteration: 5 || Loss: 1.6962419213204898
Iteration: 6 || Loss: 1.6951158451127066
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.486305
Epoch 374 loss:1.6951158451127066
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.486305
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.435401297433173
Iteration: 2 || Loss: 6.435020372252969
Iteration: 3 || Loss: 6.434642682037302
Iteration: 4 || Loss: 6.434264994443212
Iteration: 5 || Loss: 6.433890132798932
Iteration: 6 || Loss: 6.433890132798932
saving ADAM checkpoint...
Sum of params:84.48634
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.433890132798932
Iteration: 2 || Loss: 6.402389446124716
Iteration: 3 || Loss: 6.328068216994622
Iteration: 4 || Loss: 6.2214506082032415
Iteration: 5 || Loss: 6.214784019674787
Iteration: 6 || Loss: 6.205308859879813
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.522446
Epoch 374 loss:6.205308859879813
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.522446
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.516743465627243
Iteration: 2 || Loss: 20.51630187401952
Iteration: 3 || Loss: 20.51586380613653
Iteration: 4 || Loss: 20.515426880267444
Iteration: 5 || Loss: 20.51499123034684
Iteration: 6 || Loss: 20.51499123034684
saving ADAM checkpoint...
Sum of params:84.52247
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.51499123034684
Iteration: 2 || Loss: 20.466918583240815
Iteration: 3 || Loss: 20.258587444578414
Iteration: 4 || Loss: 20.139456758842996
Iteration: 5 || Loss: 20.109719608434418
Iteration: 6 || Loss: 20.086495446613288
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.419785
Epoch 374 loss:20.086495446613288
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.0597484774625014
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.960017216554604
waveform batch: 2/2
Test loss - extrapolation:16.88095721022558
Epoch 374 mean train loss:0.9650662121243382
Epoch 374 mean test loss - interpolation:1.0099580795770835
Epoch 374 mean test loss - extrapolation:4.903414535565015
Start training epoch 375
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.419785
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.782664147333985
Iteration: 2 || Loss: 2.781187029402823
Iteration: 3 || Loss: 2.779711584927373
Iteration: 4 || Loss: 2.7782344713019413
Iteration: 5 || Loss: 2.7767605967575557
Iteration: 6 || Loss: 2.7767605967575557
saving ADAM checkpoint...
Sum of params:84.41977
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7767605967575557
Iteration: 2 || Loss: 2.1761589004407407
Iteration: 3 || Loss: 1.7248441972571635
Iteration: 4 || Loss: 1.722365794292389
Iteration: 5 || Loss: 1.6951739392713938
Iteration: 6 || Loss: 1.6940628377128448
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.49752
Epoch 375 loss:1.6940628377128448
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.49752
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.432404905407121
Iteration: 2 || Loss: 6.432024682196204
Iteration: 3 || Loss: 6.431644433065022
Iteration: 4 || Loss: 6.431266924601711
Iteration: 5 || Loss: 6.430891229782154
Iteration: 6 || Loss: 6.430891229782154
saving ADAM checkpoint...
Sum of params:84.49756
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.430891229782154
Iteration: 2 || Loss: 6.399283677993734
Iteration: 3 || Loss: 6.325124644699816
Iteration: 4 || Loss: 6.2185905928514735
Iteration: 5 || Loss: 6.211915517287854
Iteration: 6 || Loss: 6.202479373097533
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.53378
Epoch 375 loss:6.202479373097533
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.53378
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.503442026552605
Iteration: 2 || Loss: 20.50300194155192
Iteration: 3 || Loss: 20.502562895559112
Iteration: 4 || Loss: 20.50212576836431
Iteration: 5 || Loss: 20.501688321014157
Iteration: 6 || Loss: 20.501688321014157
saving ADAM checkpoint...
Sum of params:84.533844
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.501688321014157
Iteration: 2 || Loss: 20.453454182956428
Iteration: 3 || Loss: 20.245106588353764
Iteration: 4 || Loss: 20.12618675579644
Iteration: 5 || Loss: 20.096483030528884
Iteration: 6 || Loss: 20.0733544949974
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.43111
Epoch 375 loss:20.0733544949974
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.056713152927332
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.94186443980257
waveform batch: 2/2
Test loss - extrapolation:16.869184394930368
Epoch 375 mean train loss:0.9644791967519923
Epoch 375 mean test loss - interpolation:1.0094521921545554
Epoch 375 mean test loss - extrapolation:4.900920736227746
Start training epoch 376
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.43111
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.781213118051766
Iteration: 2 || Loss: 2.779736018576795
Iteration: 3 || Loss: 2.778255666610804
Iteration: 4 || Loss: 2.7767772942681455
Iteration: 5 || Loss: 2.7753057334993727
Iteration: 6 || Loss: 2.7753057334993727
saving ADAM checkpoint...
Sum of params:84.43111
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7753057334993727
Iteration: 2 || Loss: 2.1747588954688273
Iteration: 3 || Loss: 1.723684885079445
Iteration: 4 || Loss: 1.7212082010255885
Iteration: 5 || Loss: 1.6941043013644177
Iteration: 6 || Loss: 1.6930114646206786
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.50873
Epoch 376 loss:1.6930114646206786
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.50873
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.42935068838993
Iteration: 2 || Loss: 6.428968780370033
Iteration: 3 || Loss: 6.4285893559307095
Iteration: 4 || Loss: 6.428212061456356
Iteration: 5 || Loss: 6.42783579431094
Iteration: 6 || Loss: 6.42783579431094
saving ADAM checkpoint...
Sum of params:84.50877
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.42783579431094
Iteration: 2 || Loss: 6.396156844795802
Iteration: 3 || Loss: 6.322185848159354
Iteration: 4 || Loss: 6.215740318631416
Iteration: 5 || Loss: 6.209054456332584
Iteration: 6 || Loss: 6.1996588656054055
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.54514
Epoch 376 loss:6.1996588656054055
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.54514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.490198831531615
Iteration: 2 || Loss: 20.4897583002932
Iteration: 3 || Loss: 20.489315827220587
Iteration: 4 || Loss: 20.48887846392669
Iteration: 5 || Loss: 20.48844031860337
Iteration: 6 || Loss: 20.48844031860337
saving ADAM checkpoint...
Sum of params:84.54518
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.48844031860337
Iteration: 2 || Loss: 20.440076943223485
Iteration: 3 || Loss: 20.231698010076652
Iteration: 4 || Loss: 20.11299647313334
Iteration: 5 || Loss: 20.08331970716598
Iteration: 6 || Loss: 20.06027477383565
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.4424
Epoch 376 loss:20.06027477383565
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.053714453360683
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.92385334194166
waveform batch: 2/2
Test loss - extrapolation:16.857445662669097
Epoch 376 mean train loss:0.9638946587607494
Epoch 376 mean test loss - interpolation:1.0089524088934472
Epoch 376 mean test loss - extrapolation:4.898441583717563
Start training epoch 377
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.4424
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7799424441337877
Iteration: 2 || Loss: 2.778463824787256
Iteration: 3 || Loss: 2.776985140013793
Iteration: 4 || Loss: 2.775508374170077
Iteration: 5 || Loss: 2.774032093024664
Iteration: 6 || Loss: 2.774032093024664
saving ADAM checkpoint...
Sum of params:84.4424
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.774032093024664
Iteration: 2 || Loss: 2.1733350903940583
Iteration: 3 || Loss: 1.7225431491288645
Iteration: 4 || Loss: 1.7200680442384608
Iteration: 5 || Loss: 1.6930495391814258
Iteration: 6 || Loss: 1.691971542359231
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.51989
Epoch 377 loss:1.691971542359231
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.51989
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.426325603739334
Iteration: 2 || Loss: 6.4259444302813336
Iteration: 3 || Loss: 6.425563896654247
Iteration: 4 || Loss: 6.4251861991496035
Iteration: 5 || Loss: 6.424808499541309
Iteration: 6 || Loss: 6.424808499541309
saving ADAM checkpoint...
Sum of params:84.51992
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.424808499541309
Iteration: 2 || Loss: 6.393084937371479
Iteration: 3 || Loss: 6.319281272879
Iteration: 4 || Loss: 6.212908988857951
Iteration: 5 || Loss: 6.20620629901093
Iteration: 6 || Loss: 6.196850650562192
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.55648
Epoch 377 loss:6.196850650562192
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.55648
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.477035415601925
Iteration: 2 || Loss: 20.47659448794902
Iteration: 3 || Loss: 20.476154225904107
Iteration: 4 || Loss: 20.475714907460663
Iteration: 5 || Loss: 20.47527607457439
Iteration: 6 || Loss: 20.47527607457439
saving ADAM checkpoint...
Sum of params:84.55651
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.47527607457439
Iteration: 2 || Loss: 20.426820241776895
Iteration: 3 || Loss: 20.21836536837862
Iteration: 4 || Loss: 20.099872804338748
Iteration: 5 || Loss: 20.070217100006047
Iteration: 6 || Loss: 20.047245048126115
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.45365
Epoch 377 loss:20.047245048126115
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.0507305135503895
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.90596318883184
waveform batch: 2/2
Test loss - extrapolation:16.845758429060258
Epoch 377 mean train loss:0.9633126634843979
Epoch 377 mean test loss - interpolation:1.0084550855917316
Epoch 377 mean test loss - extrapolation:4.895976801491009
Start training epoch 378
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.45365
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7787627802377584
Iteration: 2 || Loss: 2.7772817504755727
Iteration: 3 || Loss: 2.7758053834893017
Iteration: 4 || Loss: 2.774325576820565
Iteration: 5 || Loss: 2.772849647653962
Iteration: 6 || Loss: 2.772849647653962
saving ADAM checkpoint...
Sum of params:84.45366
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.772849647653962
Iteration: 2 || Loss: 2.1718973004768367
Iteration: 3 || Loss: 1.7214176819167555
Iteration: 4 || Loss: 1.718944080811712
Iteration: 5 || Loss: 1.692007838253379
Iteration: 6 || Loss: 1.6909430427258776
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.53104
Epoch 378 loss:1.6909430427258776
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.53104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.423391978739772
Iteration: 2 || Loss: 6.423009299520235
Iteration: 3 || Loss: 6.422627998865968
Iteration: 4 || Loss: 6.422249741777853
Iteration: 5 || Loss: 6.421872036947562
Iteration: 6 || Loss: 6.421872036947562
saving ADAM checkpoint...
Sum of params:84.53107
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.421872036947562
Iteration: 2 || Loss: 6.390037206114085
Iteration: 3 || Loss: 6.3163926711571285
Iteration: 4 || Loss: 6.210101877821357
Iteration: 5 || Loss: 6.203394166408136
Iteration: 6 || Loss: 6.194075969434276
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.56775
Epoch 378 loss:6.194075969434276
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.56775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.46383862261175
Iteration: 2 || Loss: 20.46339455248541
Iteration: 3 || Loss: 20.46295368890589
Iteration: 4 || Loss: 20.462514038796925
Iteration: 5 || Loss: 20.462074686552103
Iteration: 6 || Loss: 20.462074686552103
saving ADAM checkpoint...
Sum of params:84.567795
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.462074686552103
Iteration: 2 || Loss: 20.413472621719485
Iteration: 3 || Loss: 20.205033834693072
Iteration: 4 || Loss: 20.08676483279034
Iteration: 5 || Loss: 20.057140750539133
Iteration: 6 || Loss: 20.03425634018323
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.464905
Epoch 378 loss:20.03425634018323
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.047736570409501
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.88801562224803
waveform batch: 2/2
Test loss - extrapolation:16.834107854585948
Epoch 378 mean train loss:0.962733632839427
Epoch 378 mean test loss - interpolation:1.0079560950682502
Epoch 378 mean test loss - extrapolation:4.893510289736164
Start training epoch 379
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.464905
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.777404408771199
Iteration: 2 || Loss: 2.7759206029736196
Iteration: 3 || Loss: 2.774441764535095
Iteration: 4 || Loss: 2.7729646422625356
Iteration: 5 || Loss: 2.7714868984735332
Iteration: 6 || Loss: 2.7714868984735332
saving ADAM checkpoint...
Sum of params:84.464905
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7714868984735332
Iteration: 2 || Loss: 2.170456506571187
Iteration: 3 || Loss: 1.7202912950723146
Iteration: 4 || Loss: 1.7178195470988347
Iteration: 5 || Loss: 1.6909726973006391
Iteration: 6 || Loss: 1.689921419481926
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.54213
Epoch 379 loss:1.689921419481926
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.54213
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.420419978852278
Iteration: 2 || Loss: 6.4200376736845755
Iteration: 3 || Loss: 6.419654819321808
Iteration: 4 || Loss: 6.419276472854582
Iteration: 5 || Loss: 6.418898729375188
Iteration: 6 || Loss: 6.418898729375188
saving ADAM checkpoint...
Sum of params:84.54217
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.418898729375188
Iteration: 2 || Loss: 6.387005767274765
Iteration: 3 || Loss: 6.3135319505229015
Iteration: 4 || Loss: 6.207319284028231
Iteration: 5 || Loss: 6.2005981353584145
Iteration: 6 || Loss: 6.191315872221018
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.578995
Epoch 379 loss:6.191315872221018
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.578995
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.450759014452146
Iteration: 2 || Loss: 20.450317091044273
Iteration: 3 || Loss: 20.44987546098136
Iteration: 4 || Loss: 20.44943430545185
Iteration: 5 || Loss: 20.44899419964102
Iteration: 6 || Loss: 20.44899419964102
saving ADAM checkpoint...
Sum of params:84.579025
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.44899419964102
Iteration: 2 || Loss: 20.40026258382647
Iteration: 3 || Loss: 20.19178578029924
Iteration: 4 || Loss: 20.073728924855878
Iteration: 5 || Loss: 20.04413177747671
Iteration: 6 || Loss: 20.02133311149901
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.476105
Epoch 379 loss:20.02133311149901
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.044779926816439
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.87015929748921
waveform batch: 2/2
Test loss - extrapolation:16.82249093999572
Epoch 379 mean train loss:0.9621576001104123
Epoch 379 mean test loss - interpolation:1.007463321136073
Epoch 379 mean test loss - extrapolation:4.891054186457077
Start training epoch 380
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.476105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.776124481220494
Iteration: 2 || Loss: 2.7746403562170285
Iteration: 3 || Loss: 2.7731575838059137
Iteration: 4 || Loss: 2.7716801214497884
Iteration: 5 || Loss: 2.7702011032080076
Iteration: 6 || Loss: 2.7702011032080076
saving ADAM checkpoint...
Sum of params:84.47612
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7702011032080076
Iteration: 2 || Loss: 2.1690694689182033
Iteration: 3 || Loss: 1.7191765506409324
Iteration: 4 || Loss: 1.7167052273622583
Iteration: 5 || Loss: 1.6899452607700738
Iteration: 6 || Loss: 1.6889065644819317
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.55321
Epoch 380 loss:1.6889065644819317
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.55321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.417462575734594
Iteration: 2 || Loss: 6.4170799954002975
Iteration: 3 || Loss: 6.416698850781039
Iteration: 4 || Loss: 6.41631791917351
Iteration: 5 || Loss: 6.41593851853143
Iteration: 6 || Loss: 6.41593851853143
saving ADAM checkpoint...
Sum of params:84.55324
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.41593851853143
Iteration: 2 || Loss: 6.383993095333476
Iteration: 3 || Loss: 6.310683290770189
Iteration: 4 || Loss: 6.204547256191064
Iteration: 5 || Loss: 6.197813861436619
Iteration: 6 || Loss: 6.188565089308186
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.59022
Epoch 380 loss:6.188565089308186
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.59022
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.43775385016789
Iteration: 2 || Loss: 20.437310175432746
Iteration: 3 || Loss: 20.436867138043844
Iteration: 4 || Loss: 20.4364253474739
Iteration: 5 || Loss: 20.435984616553355
Iteration: 6 || Loss: 20.435984616553355
saving ADAM checkpoint...
Sum of params:84.59027
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.435984616553355
Iteration: 2 || Loss: 20.387146680313226
Iteration: 3 || Loss: 20.178623368779128
Iteration: 4 || Loss: 20.060760684993927
Iteration: 5 || Loss: 20.03118764986467
Iteration: 6 || Loss: 20.008466268286384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.48728
Epoch 380 loss:20.008466268286384
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.041837407188012
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.85239166140547
waveform batch: 2/2
Test loss - extrapolation:16.810917100534475
Epoch 380 mean train loss:0.9615840662785
Epoch 380 mean test loss - interpolation:1.006972901198002
Epoch 380 mean test loss - extrapolation:4.888609063494996
Start training epoch 381
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.48728
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7749044378912533
Iteration: 2 || Loss: 2.7734205975632236
Iteration: 3 || Loss: 2.7719382359057794
Iteration: 4 || Loss: 2.7704555880468598
Iteration: 5 || Loss: 2.768978749406345
Iteration: 6 || Loss: 2.768978749406345
saving ADAM checkpoint...
Sum of params:84.487274
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.768978749406345
Iteration: 2 || Loss: 2.1676482016242136
Iteration: 3 || Loss: 1.7180749455701685
Iteration: 4 || Loss: 1.7156051048440832
Iteration: 5 || Loss: 1.688927876896605
Iteration: 6 || Loss: 1.68789955306805
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.56428
Epoch 381 loss:1.68789955306805
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.56428
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.414582029731532
Iteration: 2 || Loss: 6.414198765365017
Iteration: 3 || Loss: 6.4138160679286464
Iteration: 4 || Loss: 6.4134364442464555
Iteration: 5 || Loss: 6.41305690322213
Iteration: 6 || Loss: 6.41305690322213
saving ADAM checkpoint...
Sum of params:84.5643
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.41305690322213
Iteration: 2 || Loss: 6.381002023141913
Iteration: 3 || Loss: 6.3078453502263265
Iteration: 4 || Loss: 6.201789416540562
Iteration: 5 || Loss: 6.195046673292308
Iteration: 6 || Loss: 6.185840393726531
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.60144
Epoch 381 loss:6.185840393726531
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.60144
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.424767669237056
Iteration: 2 || Loss: 20.424322562000388
Iteration: 3 || Loss: 20.423880628733258
Iteration: 4 || Loss: 20.423437536812703
Iteration: 5 || Loss: 20.422995988170747
Iteration: 6 || Loss: 20.422995988170747
saving ADAM checkpoint...
Sum of params:84.60149
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.422995988170747
Iteration: 2 || Loss: 20.37402590974696
Iteration: 3 || Loss: 20.1654542545549
Iteration: 4 || Loss: 20.04782004869286
Iteration: 5 || Loss: 20.018278844396306
Iteration: 6 || Loss: 19.995636353730674
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.498436
Epoch 381 loss:19.995636353730674
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.038930247002017
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.83468125433015
waveform batch: 2/2
Test loss - extrapolation:16.79937234188267
Epoch 381 mean train loss:0.9610129758801812
Epoch 381 mean test loss - interpolation:1.0064883745003361
Epoch 381 mean test loss - extrapolation:4.8861711330177355
Start training epoch 382
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.498436
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.773744733240442
Iteration: 2 || Loss: 2.772260243834018
Iteration: 3 || Loss: 2.770777307494308
Iteration: 4 || Loss: 2.769296807343212
Iteration: 5 || Loss: 2.767815250625611
Iteration: 6 || Loss: 2.767815250625611
saving ADAM checkpoint...
Sum of params:84.49842
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.767815250625611
Iteration: 2 || Loss: 2.166299106677244
Iteration: 3 || Loss: 1.716981836233751
Iteration: 4 || Loss: 1.7145137762690301
Iteration: 5 || Loss: 1.6879197966255002
Iteration: 6 || Loss: 1.686903847468579
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.57527
Epoch 382 loss:1.686903847468579
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.57527
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.411684656332884
Iteration: 2 || Loss: 6.411299821320047
Iteration: 3 || Loss: 6.410917364085714
Iteration: 4 || Loss: 6.410535842398373
Iteration: 5 || Loss: 6.410157207541089
Iteration: 6 || Loss: 6.410157207541089
saving ADAM checkpoint...
Sum of params:84.57533
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.410157207541089
Iteration: 2 || Loss: 6.378030642390369
Iteration: 3 || Loss: 6.305041201337039
Iteration: 4 || Loss: 6.199061737432446
Iteration: 5 || Loss: 6.19230515334495
Iteration: 6 || Loss: 6.1831325349845185
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.61257
Epoch 382 loss:6.1831325349845185
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.61257
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.411894642607486
Iteration: 2 || Loss: 20.411449513621324
Iteration: 3 || Loss: 20.411005649851823
Iteration: 4 || Loss: 20.410563756615097
Iteration: 5 || Loss: 20.410120968298504
Iteration: 6 || Loss: 20.410120968298504
saving ADAM checkpoint...
Sum of params:84.61264
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.410120968298504
Iteration: 2 || Loss: 20.36101775375532
Iteration: 3 || Loss: 20.15238852701533
Iteration: 4 || Loss: 20.03494812823681
Iteration: 5 || Loss: 20.00543293746246
Iteration: 6 || Loss: 19.982877901376163
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.50954
Epoch 382 loss:19.982877901376163
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.036019857410099
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.816975183535284
waveform batch: 2/2
Test loss - extrapolation:16.787874499441635
Epoch 382 mean train loss:0.9604453201320434
Epoch 382 mean test loss - interpolation:1.0060033095683498
Epoch 382 mean test loss - extrapolation:4.88373747358141
Start training epoch 383
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.50954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.772470449624427
Iteration: 2 || Loss: 2.770985704225942
Iteration: 3 || Loss: 2.769500567418411
Iteration: 4 || Loss: 2.7680169702414164
Iteration: 5 || Loss: 2.7665380415624012
Iteration: 6 || Loss: 2.7665380415624012
saving ADAM checkpoint...
Sum of params:84.50955
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7665380415624012
Iteration: 2 || Loss: 2.1649468266494445
Iteration: 3 || Loss: 1.7158925357892336
Iteration: 4 || Loss: 1.7134253910404207
Iteration: 5 || Loss: 1.6869137973139683
Iteration: 6 || Loss: 1.6859107371604167
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.58627
Epoch 383 loss:1.6859107371604167
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.58627
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.408777356557523
Iteration: 2 || Loss: 6.408391480173747
Iteration: 3 || Loss: 6.408007847346323
Iteration: 4 || Loss: 6.407625563327404
Iteration: 5 || Loss: 6.407245310370438
Iteration: 6 || Loss: 6.407245310370438
saving ADAM checkpoint...
Sum of params:84.58632
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.407245310370438
Iteration: 2 || Loss: 6.375063574676406
Iteration: 3 || Loss: 6.302243237000202
Iteration: 4 || Loss: 6.196339856314665
Iteration: 5 || Loss: 6.189571826431926
Iteration: 6 || Loss: 6.180435349035676
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.62372
Epoch 383 loss:6.180435349035676
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.62372
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.399002629610802
Iteration: 2 || Loss: 20.398556299065284
Iteration: 3 || Loss: 20.398111783885938
Iteration: 4 || Loss: 20.397668767688344
Iteration: 5 || Loss: 20.397226799260068
Iteration: 6 || Loss: 20.397226799260068
saving ADAM checkpoint...
Sum of params:84.62376
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.397226799260068
Iteration: 2 || Loss: 20.348007703022436
Iteration: 3 || Loss: 20.139362089043598
Iteration: 4 || Loss: 20.022134729662028
Iteration: 5 || Loss: 19.992645340626293
Iteration: 6 || Loss: 19.97016496594151
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.52063
Epoch 383 loss:19.97016496594151
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.033139627255758
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.79938102527778
waveform batch: 2/2
Test loss - extrapolation:16.77640920410089
Epoch 383 mean train loss:0.9598796914530208
Epoch 383 mean test loss - interpolation:1.005523271209293
Epoch 383 mean test loss - extrapolation:4.881315852448222
Start training epoch 384
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.52063
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7713322122672652
Iteration: 2 || Loss: 2.769846229909168
Iteration: 3 || Loss: 2.7683637716589993
Iteration: 4 || Loss: 2.7668800726176555
Iteration: 5 || Loss: 2.7653977498627227
Iteration: 6 || Loss: 2.7653977498627227
saving ADAM checkpoint...
Sum of params:84.520645
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7653977498627227
Iteration: 2 || Loss: 2.163591280364788
Iteration: 3 || Loss: 1.7148174519290549
Iteration: 4 || Loss: 1.7123502791680936
Iteration: 5 || Loss: 1.6859232118496088
Iteration: 6 || Loss: 1.6849288644550602
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.597244
Epoch 384 loss:1.6849288644550602
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.597244
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.405940859499364
Iteration: 2 || Loss: 6.405554977054033
Iteration: 3 || Loss: 6.405170949632514
Iteration: 4 || Loss: 6.404789709758238
Iteration: 5 || Loss: 6.404407723534385
Iteration: 6 || Loss: 6.404407723534385
saving ADAM checkpoint...
Sum of params:84.59729
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.404407723534385
Iteration: 2 || Loss: 6.372141778737163
Iteration: 3 || Loss: 6.299468096613064
Iteration: 4 || Loss: 6.193638757218403
Iteration: 5 || Loss: 6.186857220171372
Iteration: 6 || Loss: 6.1777584385013675
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.63483
Epoch 384 loss:6.1777584385013675
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.63483
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.38625837788762
Iteration: 2 || Loss: 20.385812117424603
Iteration: 3 || Loss: 20.385366831336743
Iteration: 4 || Loss: 20.384922214301557
Iteration: 5 || Loss: 20.38448070140538
Iteration: 6 || Loss: 20.38448070140538
saving ADAM checkpoint...
Sum of params:84.63487
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.38448070140538
Iteration: 2 || Loss: 20.335139835919776
Iteration: 3 || Loss: 20.126410999625904
Iteration: 4 || Loss: 20.00937415347529
Iteration: 5 || Loss: 19.979909576821356
Iteration: 6 || Loss: 19.95751003131951
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.5317
Epoch 384 loss:19.95751003131951
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.030264250964808
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.78180267355697
waveform batch: 2/2
Test loss - extrapolation:16.764984468830203
Epoch 384 mean train loss:0.959317149457791
Epoch 384 mean test loss - interpolation:1.005044041827468
Epoch 384 mean test loss - extrapolation:4.878898928532265
Start training epoch 385
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.5317
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.770136096864998
Iteration: 2 || Loss: 2.7686477677592403
Iteration: 3 || Loss: 2.76715846148692
Iteration: 4 || Loss: 2.7656769589805674
Iteration: 5 || Loss: 2.7641946925514986
Iteration: 6 || Loss: 2.7641946925514986
saving ADAM checkpoint...
Sum of params:84.531685
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7641946925514986
Iteration: 2 || Loss: 2.1622461727036697
Iteration: 3 || Loss: 1.7137499439041366
Iteration: 4 || Loss: 1.7112848280219854
Iteration: 5 || Loss: 1.684931073961752
Iteration: 6 || Loss: 1.6839512406262989
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.60816
Epoch 385 loss:1.6839512406262989
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.60816
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.4030409847648535
Iteration: 2 || Loss: 6.402655431384897
Iteration: 3 || Loss: 6.402270974034585
Iteration: 4 || Loss: 6.401887150279344
Iteration: 5 || Loss: 6.401505993387835
Iteration: 6 || Loss: 6.401505993387835
saving ADAM checkpoint...
Sum of params:84.60821
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.401505993387835
Iteration: 2 || Loss: 6.369198532832568
Iteration: 3 || Loss: 6.296710382518148
Iteration: 4 || Loss: 6.190949572977087
Iteration: 5 || Loss: 6.184151139293909
Iteration: 6 || Loss: 6.175087378513791
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.64593
Epoch 385 loss:6.175087378513791
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.64593
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.373536119867445
Iteration: 2 || Loss: 20.373088130030986
Iteration: 3 || Loss: 20.372643189478076
Iteration: 4 || Loss: 20.372198374006313
Iteration: 5 || Loss: 20.37175381225332
Iteration: 6 || Loss: 20.37175381225332
saving ADAM checkpoint...
Sum of params:84.64594
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.37175381225332
Iteration: 2 || Loss: 20.322321213762315
Iteration: 3 || Loss: 20.11353319968613
Iteration: 4 || Loss: 19.996681278895164
Iteration: 5 || Loss: 19.967237836758063
Iteration: 6 || Loss: 19.94490625103416
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.54271
Epoch 385 loss:19.94490625103416
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.02740301269068
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.764328470977794
waveform batch: 2/2
Test loss - extrapolation:16.753605009533604
Epoch 385 mean train loss:0.958756719661181
Epoch 385 mean test loss - interpolation:1.00456716878178
Epoch 385 mean test loss - extrapolation:4.876494456709284
Start training epoch 386
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.54271
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.769018404770589
Iteration: 2 || Loss: 2.7675283685222736
Iteration: 3 || Loss: 2.7660433119846255
Iteration: 4 || Loss: 2.7645570378655693
Iteration: 5 || Loss: 2.763074415669212
Iteration: 6 || Loss: 2.763074415669212
saving ADAM checkpoint...
Sum of params:84.54272
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.763074415669212
Iteration: 2 || Loss: 2.160890143124681
Iteration: 3 || Loss: 1.7126941315796718
Iteration: 4 || Loss: 1.7102291635244977
Iteration: 5 || Loss: 1.68395544499769
Iteration: 6 || Loss: 1.6829830636454228
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.61908
Epoch 386 loss:1.6829830636454228
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.61908
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.40025870276545
Iteration: 2 || Loss: 6.39987022887368
Iteration: 3 || Loss: 6.399488318022712
Iteration: 4 || Loss: 6.399103881683826
Iteration: 5 || Loss: 6.398720132599095
Iteration: 6 || Loss: 6.398720132599095
saving ADAM checkpoint...
Sum of params:84.61911
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.398720132599095
Iteration: 2 || Loss: 6.366314310507603
Iteration: 3 || Loss: 6.2939634997008085
Iteration: 4 || Loss: 6.188279712649548
Iteration: 5 || Loss: 6.181474482538181
Iteration: 6 || Loss: 6.1724427207607695
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.656975
Epoch 386 loss:6.1724427207607695
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.656975
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.36081412297388
Iteration: 2 || Loss: 20.360365847520622
Iteration: 3 || Loss: 20.35991927449381
Iteration: 4 || Loss: 20.359473244225814
Iteration: 5 || Loss: 20.359029953806964
Iteration: 6 || Loss: 20.359029953806964
saving ADAM checkpoint...
Sum of params:84.65701
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.359029953806964
Iteration: 2 || Loss: 20.30945022399428
Iteration: 3 || Loss: 20.100650186703866
Iteration: 4 || Loss: 19.984010732422764
Iteration: 5 || Loss: 19.954596344143333
Iteration: 6 || Loss: 19.932347628221358
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.55374
Epoch 386 loss:19.932347628221358
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.024551808621676
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.74682590188656
waveform batch: 2/2
Test loss - extrapolation:16.742252562393396
Epoch 386 mean train loss:0.9581990831940534
Epoch 386 mean test loss - interpolation:1.0040919681036127
Epoch 386 mean test loss - extrapolation:4.87408987202333
Start training epoch 387
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.55374
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.767795998458302
Iteration: 2 || Loss: 2.76630748790085
Iteration: 3 || Loss: 2.7648189279646482
Iteration: 4 || Loss: 2.7633318563302085
Iteration: 5 || Loss: 2.76184721389857
Iteration: 6 || Loss: 2.76184721389857
saving ADAM checkpoint...
Sum of params:84.55373
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.76184721389857
Iteration: 2 || Loss: 2.159568158891285
Iteration: 3 || Loss: 1.7116397668750145
Iteration: 4 || Loss: 1.70917564252314
Iteration: 5 || Loss: 1.6829809558331412
Iteration: 6 || Loss: 1.6820207224743824
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.62996
Epoch 387 loss:1.6820207224743824
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.62996
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.397433924039923
Iteration: 2 || Loss: 6.3970461472944375
Iteration: 3 || Loss: 6.396661295701257
Iteration: 4 || Loss: 6.3962771213804865
Iteration: 5 || Loss: 6.395893711530477
Iteration: 6 || Loss: 6.395893711530477
saving ADAM checkpoint...
Sum of params:84.630005
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.395893711530477
Iteration: 2 || Loss: 6.363403273368614
Iteration: 3 || Loss: 6.29122770976272
Iteration: 4 || Loss: 6.185628275760625
Iteration: 5 || Loss: 6.17881195990054
Iteration: 6 || Loss: 6.169814781777357
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.66797
Epoch 387 loss:6.169814781777357
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.66797
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.348170985899266
Iteration: 2 || Loss: 20.347723396657624
Iteration: 3 || Loss: 20.347276502198856
Iteration: 4 || Loss: 20.34683024870063
Iteration: 5 || Loss: 20.34638644768998
Iteration: 6 || Loss: 20.34638644768998
saving ADAM checkpoint...
Sum of params:84.66803
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.34638644768998
Iteration: 2 || Loss: 20.296676962122962
Iteration: 3 || Loss: 20.087836535019463
Iteration: 4 || Loss: 19.97140035377339
Iteration: 5 || Loss: 19.942013507811833
Iteration: 6 || Loss: 19.91984212586856
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.5647
Epoch 387 loss:19.91984212586856
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.021720637688961
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.72940795592934
waveform batch: 2/2
Test loss - extrapolation:16.73093667464542
Epoch 387 mean train loss:0.9576440562110448
Epoch 387 mean test loss - interpolation:1.0036201062814936
Epoch 387 mean test loss - extrapolation:4.87169538588123
Start training epoch 388
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.5647
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7666217559570563
Iteration: 2 || Loss: 2.7651329835368843
Iteration: 3 || Loss: 2.763642462431539
Iteration: 4 || Loss: 2.762157048350336
Iteration: 5 || Loss: 2.7606719995696998
Iteration: 6 || Loss: 2.7606719995696998
saving ADAM checkpoint...
Sum of params:84.56472
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7606719995696998
Iteration: 2 || Loss: 2.158257975519804
Iteration: 3 || Loss: 1.7105967751968068
Iteration: 4 || Loss: 1.7081328148624098
Iteration: 5 || Loss: 1.682019552384238
Iteration: 6 || Loss: 1.681066574083137
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.640816
Epoch 388 loss:1.681066574083137
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.640816
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3946688451675655
Iteration: 2 || Loss: 6.39428102769628
Iteration: 3 || Loss: 6.393894210819104
Iteration: 4 || Loss: 6.393509613066148
Iteration: 5 || Loss: 6.3931271600265385
Iteration: 6 || Loss: 6.3931271600265385
saving ADAM checkpoint...
Sum of params:84.640854
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3931271600265385
Iteration: 2 || Loss: 6.360543712951717
Iteration: 3 || Loss: 6.288514914918801
Iteration: 4 || Loss: 6.182992761036831
Iteration: 5 || Loss: 6.176167600143079
Iteration: 6 || Loss: 6.167202027308608
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.678955
Epoch 388 loss:6.167202027308608
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.678955
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.33561071040569
Iteration: 2 || Loss: 20.33516044105313
Iteration: 3 || Loss: 20.33471308019012
Iteration: 4 || Loss: 20.33426635781124
Iteration: 5 || Loss: 20.333820621836505
Iteration: 6 || Loss: 20.333820621836505
saving ADAM checkpoint...
Sum of params:84.679
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.333820621836505
Iteration: 2 || Loss: 20.28398232982797
Iteration: 3 || Loss: 20.075086099611664
Iteration: 4 || Loss: 19.958844010697177
Iteration: 5 || Loss: 19.929484213966703
Iteration: 6 || Loss: 19.907386788765546
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.57564
Epoch 388 loss:19.907386788765546
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.018880519829019
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.71202268938269
waveform batch: 2/2
Test loss - extrapolation:16.719679598732036
Epoch 388 mean train loss:0.9570915651778376
Epoch 388 mean test loss - interpolation:1.0031467533048366
Epoch 388 mean test loss - extrapolation:4.86930852400956
Start training epoch 389
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.57564
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7653947353172605
Iteration: 2 || Loss: 2.763904220824612
Iteration: 3 || Loss: 2.7624134687482855
Iteration: 4 || Loss: 2.760924664826336
Iteration: 5 || Loss: 2.759439788853145
Iteration: 6 || Loss: 2.759439788853145
saving ADAM checkpoint...
Sum of params:84.575645
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.759439788853145
Iteration: 2 || Loss: 2.156923660072086
Iteration: 3 || Loss: 1.7095611081436812
Iteration: 4 || Loss: 1.7070985089142543
Iteration: 5 || Loss: 1.6810587867784264
Iteration: 6 || Loss: 1.6801181014911977
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.65162
Epoch 389 loss:1.6801181014911977
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.65162
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3918560246945
Iteration: 2 || Loss: 6.391466873791324
Iteration: 3 || Loss: 6.391080192802972
Iteration: 4 || Loss: 6.390694507988564
Iteration: 5 || Loss: 6.3903116940010705
Iteration: 6 || Loss: 6.3903116940010705
saving ADAM checkpoint...
Sum of params:84.651665
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3903116940010705
Iteration: 2 || Loss: 6.357686997851927
Iteration: 3 || Loss: 6.285833257093216
Iteration: 4 || Loss: 6.1803739484843465
Iteration: 5 || Loss: 6.173531997484445
Iteration: 6 || Loss: 6.164600295500416
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.68992
Epoch 389 loss:6.164600295500416
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.68992
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.32306660518658
Iteration: 2 || Loss: 20.322616579614394
Iteration: 3 || Loss: 20.32216790713215
Iteration: 4 || Loss: 20.321720945646966
Iteration: 5 || Loss: 20.32127446121376
Iteration: 6 || Loss: 20.32127446121376
saving ADAM checkpoint...
Sum of params:84.68995
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.32127446121376
Iteration: 2 || Loss: 20.271345883043956
Iteration: 3 || Loss: 20.062391136416437
Iteration: 4 || Loss: 19.946347086358408
Iteration: 5 || Loss: 19.91700887430979
Iteration: 6 || Loss: 19.894977438739048
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.58654
Epoch 389 loss:19.894977438739048
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.016091753488444
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.694781203604585
waveform batch: 2/2
Test loss - extrapolation:16.708441004438903
Epoch 389 mean train loss:0.9565412357148504
Epoch 389 mean test loss - interpolation:1.0026819589147407
Epoch 389 mean test loss - extrapolation:4.866935184003624
Start training epoch 390
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.58654
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7643691281531013
Iteration: 2 || Loss: 2.7628761957829755
Iteration: 3 || Loss: 2.7613862257614628
Iteration: 4 || Loss: 2.7598991662965378
Iteration: 5 || Loss: 2.758412333175811
Iteration: 6 || Loss: 2.758412333175811
saving ADAM checkpoint...
Sum of params:84.58655
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.758412333175811
Iteration: 2 || Loss: 2.155609201318973
Iteration: 3 || Loss: 1.7085396329061722
Iteration: 4 || Loss: 1.7060765450903348
Iteration: 5 || Loss: 1.6801137138345603
Iteration: 6 || Loss: 1.679180037977048
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.6624
Epoch 390 loss:1.679180037977048
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.6624
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3891256246128965
Iteration: 2 || Loss: 6.388736018892849
Iteration: 3 || Loss: 6.388349600481592
Iteration: 4 || Loss: 6.387962068812135
Iteration: 5 || Loss: 6.3875792022866795
Iteration: 6 || Loss: 6.3875792022866795
saving ADAM checkpoint...
Sum of params:84.66246
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3875792022866795
Iteration: 2 || Loss: 6.354872339173641
Iteration: 3 || Loss: 6.283163826495321
Iteration: 4 || Loss: 6.177773376610713
Iteration: 5 || Loss: 6.170918161064714
Iteration: 6 || Loss: 6.162020016941439
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.70085
Epoch 390 loss:6.162020016941439
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.70085
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.310590510386596
Iteration: 2 || Loss: 20.31013920465478
Iteration: 3 || Loss: 20.309690161989494
Iteration: 4 || Loss: 20.309241954702436
Iteration: 5 || Loss: 20.308795974982168
Iteration: 6 || Loss: 20.308795974982168
saving ADAM checkpoint...
Sum of params:84.7009
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.308795974982168
Iteration: 2 || Loss: 20.25874087572989
Iteration: 3 || Loss: 20.04973885806054
Iteration: 4 || Loss: 19.93388196138692
Iteration: 5 || Loss: 19.904568931042153
Iteration: 6 || Loss: 19.882615067713118
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.59742
Epoch 390 loss:19.882615067713118
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.013301108262587
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.67751890637574
waveform batch: 2/2
Test loss - extrapolation:16.697238236928996
Epoch 390 mean train loss:0.9559936249183313
Epoch 390 mean test loss - interpolation:1.0022168513770977
Epoch 390 mean test loss - extrapolation:4.864563095275394
Start training epoch 391
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.59742
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7632263430921755
Iteration: 2 || Loss: 2.7617321469008154
Iteration: 3 || Loss: 2.7602403183914874
Iteration: 4 || Loss: 2.758751901408144
Iteration: 5 || Loss: 2.7572645815503676
Iteration: 6 || Loss: 2.7572645815503676
saving ADAM checkpoint...
Sum of params:84.59743
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7572645815503676
Iteration: 2 || Loss: 2.15430988801144
Iteration: 3 || Loss: 1.7075210355091697
Iteration: 4 || Loss: 1.7050581985852509
Iteration: 5 || Loss: 1.6791724060358644
Iteration: 6 || Loss: 1.6782468795506342
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.67318
Epoch 391 loss:1.6782468795506342
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.67318
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3863974863505355
Iteration: 2 || Loss: 6.386008515013447
Iteration: 3 || Loss: 6.385619845230693
Iteration: 4 || Loss: 6.38523387152605
Iteration: 5 || Loss: 6.384848114632533
Iteration: 6 || Loss: 6.384848114632533
saving ADAM checkpoint...
Sum of params:84.67322
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.384848114632533
Iteration: 2 || Loss: 6.352051814007444
Iteration: 3 || Loss: 6.280496359998022
Iteration: 4 || Loss: 6.175186554527209
Iteration: 5 || Loss: 6.168320569023431
Iteration: 6 || Loss: 6.159453592520214
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.71174
Epoch 391 loss:6.159453592520214
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.71174
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.298187394269767
Iteration: 2 || Loss: 20.297736565809668
Iteration: 3 || Loss: 20.297285822928014
Iteration: 4 || Loss: 20.296837985579437
Iteration: 5 || Loss: 20.29639219969794
Iteration: 6 || Loss: 20.29639219969794
saving ADAM checkpoint...
Sum of params:84.71179
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.29639219969794
Iteration: 2 || Loss: 20.246208211790083
Iteration: 3 || Loss: 20.03714458657845
Iteration: 4 || Loss: 19.921474115364507
Iteration: 5 || Loss: 19.89218868758326
Iteration: 6 || Loss: 19.87031281358861
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.60829
Epoch 391 loss:19.87031281358861
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.01053989526419
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.66029139539227
waveform batch: 2/2
Test loss - extrapolation:16.68606579948182
Epoch 391 mean train loss:0.955448733988257
Epoch 391 mean test loss - interpolation:1.0017566492106984
Epoch 391 mean test loss - extrapolation:4.862196432906174
Start training epoch 392
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.60829
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.762106647215885
Iteration: 2 || Loss: 2.7606107693318664
Iteration: 3 || Loss: 2.7591187338880765
Iteration: 4 || Loss: 2.757628723235509
Iteration: 5 || Loss: 2.7561410497546754
Iteration: 6 || Loss: 2.7561410497546754
saving ADAM checkpoint...
Sum of params:84.6083
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7561410497546754
Iteration: 2 || Loss: 2.1530641310782888
Iteration: 3 || Loss: 1.706508715833493
Iteration: 4 || Loss: 1.7040454376803287
Iteration: 5 || Loss: 1.6782349361632813
Iteration: 6 || Loss: 1.6773183469034998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.68391
Epoch 392 loss:1.6773183469034998
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.68391
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.383649524391265
Iteration: 2 || Loss: 6.383256900418622
Iteration: 3 || Loss: 6.382868897408964
Iteration: 4 || Loss: 6.382481951587701
Iteration: 5 || Loss: 6.382097075378405
Iteration: 6 || Loss: 6.382097075378405
saving ADAM checkpoint...
Sum of params:84.68396
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.382097075378405
Iteration: 2 || Loss: 6.34924600928829
Iteration: 3 || Loss: 6.277849858755774
Iteration: 4 || Loss: 6.1726101058540355
Iteration: 5 || Loss: 6.165729890941595
Iteration: 6 || Loss: 6.1568941414821285
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.722626
Epoch 392 loss:6.1568941414821285
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.722626
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.285818370576774
Iteration: 2 || Loss: 20.285368630569618
Iteration: 3 || Loss: 20.284917908341416
Iteration: 4 || Loss: 20.28446664262949
Iteration: 5 || Loss: 20.284018319749883
Iteration: 6 || Loss: 20.284018319749883
saving ADAM checkpoint...
Sum of params:84.72267
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.284018319749883
Iteration: 2 || Loss: 20.233735752140813
Iteration: 3 || Loss: 20.02460980184522
Iteration: 4 || Loss: 19.90912930264197
Iteration: 5 || Loss: 19.879864580447208
Iteration: 6 || Loss: 19.85805442910447
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.61911
Epoch 392 loss:19.85805442910447
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.007790687242285
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.643182969669155
waveform batch: 2/2
Test loss - extrapolation:16.674939070067158
Epoch 392 mean train loss:0.9549057557755206
Epoch 392 mean test loss - interpolation:1.0012984478737141
Epoch 392 mean test loss - extrapolation:4.859843503311359
Start training epoch 393
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.61911
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7610798461582706
Iteration: 2 || Loss: 2.75958416856923
Iteration: 3 || Loss: 2.758090066982395
Iteration: 4 || Loss: 2.756598364059881
Iteration: 5 || Loss: 2.7551104074527077
Iteration: 6 || Loss: 2.7551104074527077
saving ADAM checkpoint...
Sum of params:84.61912
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7551104074527077
Iteration: 2 || Loss: 2.1517836991567614
Iteration: 3 || Loss: 1.7055100567715644
Iteration: 4 || Loss: 1.7030472411293671
Iteration: 5 || Loss: 1.677306046804641
Iteration: 6 || Loss: 1.6763986977319392
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.69461
Epoch 393 loss:1.6763986977319392
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.69461
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.380968891357278
Iteration: 2 || Loss: 6.380577932286832
Iteration: 3 || Loss: 6.380188624827732
Iteration: 4 || Loss: 6.379801977123674
Iteration: 5 || Loss: 6.379414462316253
Iteration: 6 || Loss: 6.379414462316253
saving ADAM checkpoint...
Sum of params:84.694664
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.379414462316253
Iteration: 2 || Loss: 6.346454522397373
Iteration: 3 || Loss: 6.275218488347979
Iteration: 4 || Loss: 6.17005256280608
Iteration: 5 || Loss: 6.1631665501118835
Iteration: 6 || Loss: 6.154360298831009
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.73346
Epoch 393 loss:6.154360298831009
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.73346
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.273456892809936
Iteration: 2 || Loss: 20.273003895102566
Iteration: 3 || Loss: 20.272551745499744
Iteration: 4 || Loss: 20.27210142672282
Iteration: 5 || Loss: 20.271653790165363
Iteration: 6 || Loss: 20.271653790165363
saving ADAM checkpoint...
Sum of params:84.73351
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.271653790165363
Iteration: 2 || Loss: 20.221224695764175
Iteration: 3 || Loss: 20.01208172115718
Iteration: 4 || Loss: 19.896806470451452
Iteration: 5 || Loss: 19.867571515894866
Iteration: 6 || Loss: 19.84583800427106
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.629906
Epoch 393 loss:19.84583800427106
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.005027868486497
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.626039585301825
waveform batch: 2/2
Test loss - extrapolation:16.663848565728298
Epoch 393 mean train loss:0.9543654138218624
Epoch 393 mean test loss - interpolation:1.0008379780810828
Epoch 393 mean test loss - extrapolation:4.8574906792525105
Start training epoch 394
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.629906
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7599019288767095
Iteration: 2 || Loss: 2.758405906326788
Iteration: 3 || Loss: 2.756910470094131
Iteration: 4 || Loss: 2.7554141149420084
Iteration: 5 || Loss: 2.753922587857491
Iteration: 6 || Loss: 2.753922587857491
saving ADAM checkpoint...
Sum of params:84.629906
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.753922587857491
Iteration: 2 || Loss: 2.150505992957844
Iteration: 3 || Loss: 1.7045123574377627
Iteration: 4 || Loss: 1.7020504174422049
Iteration: 5 || Loss: 1.6763855274211834
Iteration: 6 || Loss: 1.675485742970198
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.70527
Epoch 394 loss:1.675485742970198
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.70527
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.378253301254853
Iteration: 2 || Loss: 6.377861791109114
Iteration: 3 || Loss: 6.377472479910138
Iteration: 4 || Loss: 6.377084247272018
Iteration: 5 || Loss: 6.376696400319925
Iteration: 6 || Loss: 6.376696400319925
saving ADAM checkpoint...
Sum of params:84.7053
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.376696400319925
Iteration: 2 || Loss: 6.343690249666576
Iteration: 3 || Loss: 6.272613366826629
Iteration: 4 || Loss: 6.167513997784073
Iteration: 5 || Loss: 6.160610871781944
Iteration: 6 || Loss: 6.151835862473289
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.744255
Epoch 394 loss:6.151835862473289
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.744255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.261207854764734
Iteration: 2 || Loss: 20.260755590956308
Iteration: 3 || Loss: 20.260303945189126
Iteration: 4 || Loss: 20.259853410011033
Iteration: 5 || Loss: 20.259404329085772
Iteration: 6 || Loss: 20.259404329085772
saving ADAM checkpoint...
Sum of params:84.74429
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.259404329085772
Iteration: 2 || Loss: 20.20888166693812
Iteration: 3 || Loss: 19.999667122645445
Iteration: 4 || Loss: 19.884559027910807
Iteration: 5 || Loss: 19.855342269614276
Iteration: 6 || Loss: 19.833674774737908
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.64067
Epoch 394 loss:19.833674774737908
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:6.0022940583847335
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.609014509731495
waveform batch: 2/2
Test loss - extrapolation:16.652793461091917
Epoch 394 mean train loss:0.9538274613855653
Epoch 394 mean test loss - interpolation:1.0003823430641223
Epoch 394 mean test loss - extrapolation:4.855150664235285
Start training epoch 395
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.64067
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7588407897180103
Iteration: 2 || Loss: 2.7573434528454777
Iteration: 3 || Loss: 2.755847175331929
Iteration: 4 || Loss: 2.7543522248318633
Iteration: 5 || Loss: 2.7528582181375714
Iteration: 6 || Loss: 2.7528582181375714
saving ADAM checkpoint...
Sum of params:84.64066
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7528582181375714
Iteration: 2 || Loss: 2.149214862290932
Iteration: 3 || Loss: 1.7035290832139638
Iteration: 4 || Loss: 1.701066271440701
Iteration: 5 || Loss: 1.6754719628793104
Iteration: 6 || Loss: 1.6745794194823562
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.71593
Epoch 395 loss:1.6745794194823562
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.71593
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.37559005431412
Iteration: 2 || Loss: 6.375196628843422
Iteration: 3 || Loss: 6.374806722092734
Iteration: 4 || Loss: 6.374417550779701
Iteration: 5 || Loss: 6.374030308087161
Iteration: 6 || Loss: 6.374030308087161
saving ADAM checkpoint...
Sum of params:84.71597
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.374030308087161
Iteration: 2 || Loss: 6.34094815841043
Iteration: 3 || Loss: 6.270016985868523
Iteration: 4 || Loss: 6.1649862646457185
Iteration: 5 || Loss: 6.158068201403689
Iteration: 6 || Loss: 6.149324082877638
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.75504
Epoch 395 loss:6.149324082877638
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.75504
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.249001256897593
Iteration: 2 || Loss: 20.24854595139106
Iteration: 3 || Loss: 20.248095360504838
Iteration: 4 || Loss: 20.247642847611647
Iteration: 5 || Loss: 20.247191360812025
Iteration: 6 || Loss: 20.247191360812025
saving ADAM checkpoint...
Sum of params:84.75509
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.247191360812025
Iteration: 2 || Loss: 20.196556810332947
Iteration: 3 || Loss: 19.987273929005212
Iteration: 4 || Loss: 19.87234802689611
Iteration: 5 || Loss: 19.84315531100463
Iteration: 6 || Loss: 19.8215565579375
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.6514
Epoch 395 loss:19.8215565579375
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.999568502769284
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.59201663617663
waveform batch: 2/2
Test loss - extrapolation:16.64177817271256
Epoch 395 mean train loss:0.953291726217155
Epoch 395 mean test loss - interpolation:0.9999280837948806
Epoch 395 mean test loss - extrapolation:4.852816234074099
Start training epoch 396
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.6514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.757758283942899
Iteration: 2 || Loss: 2.7562589427590787
Iteration: 3 || Loss: 2.7547638033565516
Iteration: 4 || Loss: 2.7532682235494166
Iteration: 5 || Loss: 2.7517730271372223
Iteration: 6 || Loss: 2.7517730271372223
saving ADAM checkpoint...
Sum of params:84.651405
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7517730271372223
Iteration: 2 || Loss: 2.1479570825902305
Iteration: 3 || Loss: 1.702550714862861
Iteration: 4 || Loss: 1.7000880215910659
Iteration: 5 || Loss: 1.6745632622178794
Iteration: 6 || Loss: 1.6736788004236491
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.72654
Epoch 396 loss:1.6736788004236491
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.72654
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.372928442006351
Iteration: 2 || Loss: 6.372536991068963
Iteration: 3 || Loss: 6.372145809159647
Iteration: 4 || Loss: 6.371755909803704
Iteration: 5 || Loss: 6.371368799850898
Iteration: 6 || Loss: 6.371368799850898
saving ADAM checkpoint...
Sum of params:84.72658
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.371368799850898
Iteration: 2 || Loss: 6.338208518479212
Iteration: 3 || Loss: 6.26743641623624
Iteration: 4 || Loss: 6.1624707853560485
Iteration: 5 || Loss: 6.155537544886488
Iteration: 6 || Loss: 6.146826991513933
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.765816
Epoch 396 loss:6.146826991513933
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.765816
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.23685963193996
Iteration: 2 || Loss: 20.236405128472352
Iteration: 3 || Loss: 20.23595250175597
Iteration: 4 || Loss: 20.23550027897234
Iteration: 5 || Loss: 20.23505079535828
Iteration: 6 || Loss: 20.23505079535828
saving ADAM checkpoint...
Sum of params:84.76586
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.23505079535828
Iteration: 2 || Loss: 20.184311863638694
Iteration: 3 || Loss: 19.974941138103393
Iteration: 4 || Loss: 19.860188803357715
Iteration: 5 || Loss: 19.831018198809193
Iteration: 6 || Loss: 19.809487542228315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.662125
Epoch 396 loss:19.809487542228315
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.996883061882975
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.57509469893662
waveform batch: 2/2
Test loss - extrapolation:16.630785854776526
Epoch 396 mean train loss:0.9527583908333067
Epoch 396 mean test loss - interpolation:0.9994805103138291
Epoch 396 mean test loss - extrapolation:4.850490046142762
Start training epoch 397
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.662125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.75677003423678
Iteration: 2 || Loss: 2.755270468475072
Iteration: 3 || Loss: 2.7537739573062567
Iteration: 4 || Loss: 2.7522732668099037
Iteration: 5 || Loss: 2.7507804816381913
Iteration: 6 || Loss: 2.7507804816381913
saving ADAM checkpoint...
Sum of params:84.662125
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7507804816381913
Iteration: 2 || Loss: 2.14672972651554
Iteration: 3 || Loss: 1.701579220316126
Iteration: 4 || Loss: 1.6991148598749446
Iteration: 5 || Loss: 1.6736624720325195
Iteration: 6 || Loss: 1.6727837560819192
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.73715
Epoch 397 loss:1.6727837560819192
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.73715
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.37030285394881
Iteration: 2 || Loss: 6.3699107821439105
Iteration: 3 || Loss: 6.369520317440372
Iteration: 4 || Loss: 6.369128721933156
Iteration: 5 || Loss: 6.3687396686834665
Iteration: 6 || Loss: 6.3687396686834665
saving ADAM checkpoint...
Sum of params:84.7372
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3687396686834665
Iteration: 2 || Loss: 6.335499506090471
Iteration: 3 || Loss: 6.264865586765566
Iteration: 4 || Loss: 6.159971471722666
Iteration: 5 || Loss: 6.153029807523263
Iteration: 6 || Loss: 6.144345658790945
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.776535
Epoch 397 loss:6.144345658790945
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.776535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.2247239319483
Iteration: 2 || Loss: 20.22426664419567
Iteration: 3 || Loss: 20.22381141994399
Iteration: 4 || Loss: 20.223360173194664
Iteration: 5 || Loss: 20.22290777899648
Iteration: 6 || Loss: 20.22290777899648
saving ADAM checkpoint...
Sum of params:84.77657
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.22290777899648
Iteration: 2 || Loss: 20.17202867747184
Iteration: 3 || Loss: 19.962637161607223
Iteration: 4 || Loss: 19.848068188158788
Iteration: 5 || Loss: 19.818925058394708
Iteration: 6 || Loss: 19.797468971395304
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.672806
Epoch 397 loss:19.797468971395304
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.994180250029573
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.55814916799588
waveform batch: 2/2
Test loss - extrapolation:16.619831713928598
Epoch 397 mean train loss:0.9522275305609714
Epoch 397 mean test loss - interpolation:0.9990300416715955
Epoch 397 mean test loss - extrapolation:4.848165073493706
Start training epoch 398
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.672806
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7556322200373047
Iteration: 2 || Loss: 2.754131282509634
Iteration: 3 || Loss: 2.7526324734387035
Iteration: 4 || Loss: 2.751135916370064
Iteration: 5 || Loss: 2.7496394641063158
Iteration: 6 || Loss: 2.7496394641063158
saving ADAM checkpoint...
Sum of params:84.67282
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7496394641063158
Iteration: 2 || Loss: 2.145499177509099
Iteration: 3 || Loss: 1.7006111745833712
Iteration: 4 || Loss: 1.6981477350525551
Iteration: 5 || Loss: 1.6727649593798115
Iteration: 6 || Loss: 1.6718935928203005
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.74773
Epoch 398 loss:1.6718935928203005
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.74773
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.367646501098032
Iteration: 2 || Loss: 6.367253405006993
Iteration: 3 || Loss: 6.366862146725151
Iteration: 4 || Loss: 6.366472411611815
Iteration: 5 || Loss: 6.366083498227263
Iteration: 6 || Loss: 6.366083498227263
saving ADAM checkpoint...
Sum of params:84.74776
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.366083498227263
Iteration: 2 || Loss: 6.332777419380669
Iteration: 3 || Loss: 6.26230762959621
Iteration: 4 || Loss: 6.1574844782358715
Iteration: 5 || Loss: 6.150533792989356
Iteration: 6 || Loss: 6.141876052308914
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.78725
Epoch 398 loss:6.141876052308914
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.78725
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.212618771977127
Iteration: 2 || Loss: 20.21216229972853
Iteration: 3 || Loss: 20.211706897422903
Iteration: 4 || Loss: 20.21125208173032
Iteration: 5 || Loss: 20.21079969963006
Iteration: 6 || Loss: 20.21079969963006
saving ADAM checkpoint...
Sum of params:84.787285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.21079969963006
Iteration: 2 || Loss: 20.15981878735902
Iteration: 3 || Loss: 19.950387390780854
Iteration: 4 || Loss: 19.83600534172011
Iteration: 5 || Loss: 19.806883634374778
Iteration: 6 || Loss: 19.785491983301217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.683464
Epoch 398 loss:19.785491983301217
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.99150351429009
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.541327774339386
waveform batch: 2/2
Test loss - extrapolation:16.608920613838816
Epoch 398 mean train loss:0.9516986768424287
Epoch 398 mean test loss - interpolation:0.9985839190483484
Epoch 398 mean test loss - extrapolation:4.845854032348183
Start training epoch 399
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.683464
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.754625954359146
Iteration: 2 || Loss: 2.7531232482948345
Iteration: 3 || Loss: 2.7516221663225897
Iteration: 4 || Loss: 2.7501255138836265
Iteration: 5 || Loss: 2.748628609827485
Iteration: 6 || Loss: 2.748628609827485
saving ADAM checkpoint...
Sum of params:84.683464
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.748628609827485
Iteration: 2 || Loss: 2.1442606271767346
Iteration: 3 || Loss: 1.6996558304497362
Iteration: 4 || Loss: 1.69719293298759
Iteration: 5 || Loss: 1.6718743964231033
Iteration: 6 || Loss: 1.671011639347339
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.758255
Epoch 399 loss:1.671011639347339
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.758255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.364978581510998
Iteration: 2 || Loss: 6.364585495873042
Iteration: 3 || Loss: 6.36419384802472
Iteration: 4 || Loss: 6.36380246481292
Iteration: 5 || Loss: 6.36341403151105
Iteration: 6 || Loss: 6.36341403151105
saving ADAM checkpoint...
Sum of params:84.7583
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.36341403151105
Iteration: 2 || Loss: 6.330085008873438
Iteration: 3 || Loss: 6.259784910535759
Iteration: 4 || Loss: 6.1550125730958465
Iteration: 5 || Loss: 6.148036836314662
Iteration: 6 || Loss: 6.139413091868439
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.79792
Epoch 399 loss:6.139413091868439
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.79792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.200644122143622
Iteration: 2 || Loss: 20.200187004899444
Iteration: 3 || Loss: 20.199732362832503
Iteration: 4 || Loss: 20.199276827528486
Iteration: 5 || Loss: 20.198824195866138
Iteration: 6 || Loss: 20.198824195866138
saving ADAM checkpoint...
Sum of params:84.79796
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.198824195866138
Iteration: 2 || Loss: 20.147777379917105
Iteration: 3 || Loss: 19.93823772126981
Iteration: 4 || Loss: 19.824005629870875
Iteration: 5 || Loss: 19.794896277397264
Iteration: 6 || Loss: 19.77356223410075
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.69408
Epoch 399 loss:19.77356223410075
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.988854408413756
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.52462090915719
waveform batch: 2/2
Test loss - extrapolation:16.59804223801545
Epoch 399 mean train loss:0.9511719643212596
Epoch 399 mean test loss - interpolation:0.9981424014022927
Epoch 399 mean test loss - extrapolation:4.843555262264387
Start training epoch 400
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.69408
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.753747209933852
Iteration: 2 || Loss: 2.7522441489414375
Iteration: 3 || Loss: 2.7507422498069274
Iteration: 4 || Loss: 2.7492436525399913
Iteration: 5 || Loss: 2.7477467104592987
Iteration: 6 || Loss: 2.7477467104592987
saving ADAM checkpoint...
Sum of params:84.69409
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7477467104592987
Iteration: 2 || Loss: 2.1430066979094673
Iteration: 3 || Loss: 1.6987118624986688
Iteration: 4 || Loss: 1.69624851514529
Iteration: 5 || Loss: 1.6709943296407694
Iteration: 6 || Loss: 1.6701364785260728
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.76876
Epoch 400 loss:1.6701364785260728
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.76876
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.362427809448535
Iteration: 2 || Loss: 6.362033729494862
Iteration: 3 || Loss: 6.361641003912098
Iteration: 4 || Loss: 6.36124994039223
Iteration: 5 || Loss: 6.360860749551502
Iteration: 6 || Loss: 6.360860749551502
saving ADAM checkpoint...
Sum of params:84.76881
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.360860749551502
Iteration: 2 || Loss: 6.3274249591841505
Iteration: 3 || Loss: 6.257258470888515
Iteration: 4 || Loss: 6.152553975542097
Iteration: 5 || Loss: 6.145569643472708
Iteration: 6 || Loss: 6.136972343628684
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.80856
Epoch 400 loss:6.136972343628684
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.80856
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.188657625194985
Iteration: 2 || Loss: 20.18819983101152
Iteration: 3 || Loss: 20.18774373298776
Iteration: 4 || Loss: 20.187288308653347
Iteration: 5 || Loss: 20.186834546686292
Iteration: 6 || Loss: 20.186834546686292
saving ADAM checkpoint...
Sum of params:84.8086
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.186834546686292
Iteration: 2 || Loss: 20.135651071431063
Iteration: 3 || Loss: 19.926076296937204
Iteration: 4 || Loss: 19.81201695056769
Iteration: 5 || Loss: 19.7829356034224
Iteration: 6 || Loss: 19.761672824837873
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.70467
Epoch 400 loss:19.761672824837873
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.9861840755759275
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.50780940721748
waveform batch: 2/2
Test loss - extrapolation:16.587198007214486
Epoch 400 mean train loss:0.9506476429997458
Epoch 400 mean test loss - interpolation:0.9976973459293212
Epoch 400 mean test loss - extrapolation:4.84125061786933
Start training epoch 401
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.70467
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7526084003713005
Iteration: 2 || Loss: 2.751103928843329
Iteration: 3 || Loss: 2.7496015943834786
Iteration: 4 || Loss: 2.7481028315741685
Iteration: 5 || Loss: 2.7466053809631634
Iteration: 6 || Loss: 2.7466053809631634
saving ADAM checkpoint...
Sum of params:84.7047
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7466053809631634
Iteration: 2 || Loss: 2.1418079640164165
Iteration: 3 || Loss: 1.6977639198185215
Iteration: 4 || Loss: 1.6952992394158557
Iteration: 5 || Loss: 1.6701155128522018
Iteration: 6 || Loss: 1.6692646826858333
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.77924
Epoch 401 loss:1.6692646826858333
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.77924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.359835187282368
Iteration: 2 || Loss: 6.359439922004725
Iteration: 3 || Loss: 6.359047191460074
Iteration: 4 || Loss: 6.358653746761833
Iteration: 5 || Loss: 6.35826355758471
Iteration: 6 || Loss: 6.35826355758471
saving ADAM checkpoint...
Sum of params:84.77929
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.35826355758471
Iteration: 2 || Loss: 6.324751519418623
Iteration: 3 || Loss: 6.254739254419668
Iteration: 4 || Loss: 6.150105776612052
Iteration: 5 || Loss: 6.143107804311091
Iteration: 6 || Loss: 6.1345422210195375
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.81918
Epoch 401 loss:6.1345422210195375
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.81918
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.176737008353886
Iteration: 2 || Loss: 20.176277243966574
Iteration: 3 || Loss: 20.17582152357644
Iteration: 4 || Loss: 20.175366796576785
Iteration: 5 || Loss: 20.17491229262435
Iteration: 6 || Loss: 20.17491229262435
saving ADAM checkpoint...
Sum of params:84.81922
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.17491229262435
Iteration: 2 || Loss: 20.12362515524096
Iteration: 3 || Loss: 19.9139598182949
Iteration: 4 || Loss: 19.80008642613876
Iteration: 5 || Loss: 19.771027657174436
Iteration: 6 || Loss: 19.749826271481467
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.71525
Epoch 401 loss:19.749826271481467
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.983556722893195
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.491143284884146
waveform batch: 2/2
Test loss - extrapolation:16.576378875137475
Epoch 401 mean train loss:0.9501252819029944
Epoch 401 mean test loss - interpolation:0.9972594538155325
Epoch 401 mean test loss - extrapolation:4.838960180001802
Start training epoch 402
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.71525
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7516685576550617
Iteration: 2 || Loss: 2.7501622074380316
Iteration: 3 || Loss: 2.748656646791288
Iteration: 4 || Loss: 2.7471491596798114
Iteration: 5 || Loss: 2.7456480473384843
Iteration: 6 || Loss: 2.7456480473384843
saving ADAM checkpoint...
Sum of params:84.71524
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7456480473384843
Iteration: 2 || Loss: 2.1406013968088446
Iteration: 3 || Loss: 1.6968288402146783
Iteration: 4 || Loss: 1.6943627582333045
Iteration: 5 || Loss: 1.6692481278019613
Iteration: 6 || Loss: 1.6684010463048145
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.789696
Epoch 402 loss:1.6684010463048145
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.789696
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.357312755354275
Iteration: 2 || Loss: 6.356916594275917
Iteration: 3 || Loss: 6.356523221591751
Iteration: 4 || Loss: 6.356128685162725
Iteration: 5 || Loss: 6.355736337420295
Iteration: 6 || Loss: 6.355736337420295
saving ADAM checkpoint...
Sum of params:84.789734
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.355736337420295
Iteration: 2 || Loss: 6.322115217557456
Iteration: 3 || Loss: 6.252234834018726
Iteration: 4 || Loss: 6.147679071508946
Iteration: 5 || Loss: 6.140673587222358
Iteration: 6 || Loss: 6.132134255676749
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.82975
Epoch 402 loss:6.132134255676749
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.82975
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.164803519529364
Iteration: 2 || Loss: 20.16434507578662
Iteration: 3 || Loss: 20.163887302846245
Iteration: 4 || Loss: 20.163430632582767
Iteration: 5 || Loss: 20.162976995442378
Iteration: 6 || Loss: 20.162976995442378
saving ADAM checkpoint...
Sum of params:84.8298
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.162976995442378
Iteration: 2 || Loss: 20.111558323564637
Iteration: 3 || Loss: 19.901845469552104
Iteration: 4 || Loss: 19.78817602767692
Iteration: 5 || Loss: 19.75914652972903
Iteration: 6 || Loss: 19.738013416893157
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.72579
Epoch 402 loss:19.738013416893157
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.980935265789468
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.474485259180454
waveform batch: 2/2
Test loss - extrapolation:16.565592764810027
Epoch 402 mean train loss:0.9496051282370593
Epoch 402 mean test loss - interpolation:0.9968225442982447
Epoch 402 mean test loss - extrapolation:4.8366731686658735
Start training epoch 403
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.72579
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7506706405456605
Iteration: 2 || Loss: 2.749162226118117
Iteration: 3 || Loss: 2.7476545551370704
Iteration: 4 || Loss: 2.7461501842830693
Iteration: 5 || Loss: 2.7446450388259738
Iteration: 6 || Loss: 2.7446450388259738
saving ADAM checkpoint...
Sum of params:84.72579
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7446450388259738
Iteration: 2 || Loss: 2.1394172965960636
Iteration: 3 || Loss: 1.69589893630743
Iteration: 4 || Loss: 1.6934331697698601
Iteration: 5 || Loss: 1.668386397424704
Iteration: 6 || Loss: 1.6675454131501029
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.80012
Epoch 403 loss:1.6675454131501029
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.80012
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3547616716254085
Iteration: 2 || Loss: 6.354364933396
Iteration: 3 || Loss: 6.353969811985619
Iteration: 4 || Loss: 6.3535763884085705
Iteration: 5 || Loss: 6.353184128484958
Iteration: 6 || Loss: 6.353184128484958
saving ADAM checkpoint...
Sum of params:84.800156
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.353184128484958
Iteration: 2 || Loss: 6.319495730908029
Iteration: 3 || Loss: 6.249767146698991
Iteration: 4 || Loss: 6.145272548153506
Iteration: 5 || Loss: 6.138251582215098
Iteration: 6 || Loss: 6.129737553491127
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.840324
Epoch 403 loss:6.129737553491127
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.840324
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.152988605532585
Iteration: 2 || Loss: 20.152528066779247
Iteration: 3 || Loss: 20.15207096441881
Iteration: 4 || Loss: 20.151614177067664
Iteration: 5 || Loss: 20.151158444051006
Iteration: 6 || Loss: 20.151158444051006
saving ADAM checkpoint...
Sum of params:84.840355
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.151158444051006
Iteration: 2 || Loss: 20.099630068352944
Iteration: 3 || Loss: 19.889851663557472
Iteration: 4 || Loss: 19.776335863358618
Iteration: 5 || Loss: 19.747326379859512
Iteration: 6 || Loss: 19.726260016553447
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.73629
Epoch 403 loss:19.726260016553447
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.978306771148078
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.457848745398685
waveform batch: 2/2
Test loss - extrapolation:16.55485088739818
Epoch 403 mean train loss:0.9490876890756785
Epoch 403 mean test loss - interpolation:0.996384461858013
Epoch 403 mean test loss - extrapolation:4.834391636066406
Start training epoch 404
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.73629
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7496046564977314
Iteration: 2 || Loss: 2.7480938583923593
Iteration: 3 || Loss: 2.746586068944001
Iteration: 4 || Loss: 2.745081086481915
Iteration: 5 || Loss: 2.7435753217901184
Iteration: 6 || Loss: 2.7435753217901184
saving ADAM checkpoint...
Sum of params:84.7363
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7435753217901184
Iteration: 2 || Loss: 2.1382076123692686
Iteration: 3 || Loss: 1.694974798517125
Iteration: 4 || Loss: 1.6925080714324787
Iteration: 5 || Loss: 1.6675256313407383
Iteration: 6 || Loss: 1.6666910755351108
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.81052
Epoch 404 loss:1.6666910755351108
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.81052
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.352198529562754
Iteration: 2 || Loss: 6.351800389400726
Iteration: 3 || Loss: 6.351404507519698
Iteration: 4 || Loss: 6.351011185920914
Iteration: 5 || Loss: 6.3506179726656855
Iteration: 6 || Loss: 6.3506179726656855
saving ADAM checkpoint...
Sum of params:84.810555
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3506179726656855
Iteration: 2 || Loss: 6.316867606782082
Iteration: 3 || Loss: 6.247298378231012
Iteration: 4 || Loss: 6.142867739018041
Iteration: 5 || Loss: 6.135834479466742
Iteration: 6 || Loss: 6.12734805473949
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.85082
Epoch 404 loss:6.12734805473949
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.85082
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.14119420647304
Iteration: 2 || Loss: 20.140733835734864
Iteration: 3 || Loss: 20.14027546245125
Iteration: 4 || Loss: 20.139818115410634
Iteration: 5 || Loss: 20.139361440490433
Iteration: 6 || Loss: 20.139361440490433
saving ADAM checkpoint...
Sum of params:84.850876
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.139361440490433
Iteration: 2 || Loss: 20.08772001769708
Iteration: 3 || Loss: 19.87789463471783
Iteration: 4 || Loss: 19.764542816666285
Iteration: 5 || Loss: 19.735555206111332
Iteration: 6 || Loss: 19.714554520281833
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.746765
Epoch 404 loss:19.714554520281833
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.975708984960005
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.441287508109205
waveform batch: 2/2
Test loss - extrapolation:16.544137102844903
Epoch 404 mean train loss:0.9485721948467736
Epoch 404 mean test loss - interpolation:0.9959514974933342
Epoch 404 mean test loss - extrapolation:4.832118717579509
Start training epoch 405
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.746765
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7486139491125257
Iteration: 2 || Loss: 2.747104388347561
Iteration: 3 || Loss: 2.7455963841545947
Iteration: 4 || Loss: 2.7440877389561296
Iteration: 5 || Loss: 2.7425847593036576
Iteration: 6 || Loss: 2.7425847593036576
saving ADAM checkpoint...
Sum of params:84.746765
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7425847593036576
Iteration: 2 || Loss: 2.1370391174269265
Iteration: 3 || Loss: 1.6940577643096482
Iteration: 4 || Loss: 1.6915893191120488
Iteration: 5 || Loss: 1.6666716927748142
Iteration: 6 || Loss: 1.6658420385171437
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.82088
Epoch 405 loss:1.6658420385171437
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.82088
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.349705587410584
Iteration: 2 || Loss: 6.349307943084682
Iteration: 3 || Loss: 6.348910532223778
Iteration: 4 || Loss: 6.348515461300116
Iteration: 5 || Loss: 6.348121281189781
Iteration: 6 || Loss: 6.348121281189781
saving ADAM checkpoint...
Sum of params:84.820915
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.348121281189781
Iteration: 2 || Loss: 6.314259553795759
Iteration: 3 || Loss: 6.2448312017121
Iteration: 4 || Loss: 6.140477958017887
Iteration: 5 || Loss: 6.13343757574938
Iteration: 6 || Loss: 6.124977096439332
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.86131
Epoch 405 loss:6.124977096439332
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.86131
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.129421863528915
Iteration: 2 || Loss: 20.12896138973201
Iteration: 3 || Loss: 20.128501415170074
Iteration: 4 || Loss: 20.128043854176205
Iteration: 5 || Loss: 20.1275873543919
Iteration: 6 || Loss: 20.1275873543919
saving ADAM checkpoint...
Sum of params:84.86136
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.1275873543919
Iteration: 2 || Loss: 20.07582198018213
Iteration: 3 || Loss: 19.86594810782297
Iteration: 4 || Loss: 19.75278056122173
Iteration: 5 || Loss: 19.72382110589965
Iteration: 6 || Loss: 19.702886330727928
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.75723
Epoch 405 loss:19.702886330727928
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.973125242477692
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.42476064870597
waveform batch: 2/2
Test loss - extrapolation:16.533450114181445
Epoch 405 mean train loss:0.9480588091615312
Epoch 405 mean test loss - interpolation:0.995520873746282
Epoch 405 mean test loss - extrapolation:4.829850896907285
Start training epoch 406
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.75723
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7476325106714237
Iteration: 2 || Loss: 2.7461203792313538
Iteration: 3 || Loss: 2.7446120472131743
Iteration: 4 || Loss: 2.743107268348286
Iteration: 5 || Loss: 2.7415980328260456
Iteration: 6 || Loss: 2.7415980328260456
saving ADAM checkpoint...
Sum of params:84.75723
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7415980328260456
Iteration: 2 || Loss: 2.135869888738972
Iteration: 3 || Loss: 1.6931446346292984
Iteration: 4 || Loss: 1.690675971258961
Iteration: 5 || Loss: 1.6658251018721086
Iteration: 6 || Loss: 1.6649998972299478
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.83121
Epoch 406 loss:1.6649998972299478
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.83121
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.347192160152824
Iteration: 2 || Loss: 6.346793542043939
Iteration: 3 || Loss: 6.346397123894397
Iteration: 4 || Loss: 6.346000771807341
Iteration: 5 || Loss: 6.345606644542891
Iteration: 6 || Loss: 6.345606644542891
saving ADAM checkpoint...
Sum of params:84.83126
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.345606644542891
Iteration: 2 || Loss: 6.3116727982650875
Iteration: 3 || Loss: 6.242392842145881
Iteration: 4 || Loss: 6.138102959141902
Iteration: 5 || Loss: 6.131046089757051
Iteration: 6 || Loss: 6.122613909799619
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.871796
Epoch 406 loss:6.122613909799619
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.871796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.117757231080883
Iteration: 2 || Loss: 20.117294880483655
Iteration: 3 || Loss: 20.116835477290206
Iteration: 4 || Loss: 20.11637683331575
Iteration: 5 || Loss: 20.11591891347959
Iteration: 6 || Loss: 20.11591891347959
saving ADAM checkpoint...
Sum of params:84.871826
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.11591891347959
Iteration: 2 || Loss: 20.064053475362506
Iteration: 3 || Loss: 19.854088396917096
Iteration: 4 || Loss: 19.74108185558151
Iteration: 5 || Loss: 19.712141317498137
Iteration: 6 || Loss: 19.691267734900634
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.76764
Epoch 406 loss:19.691267734900634
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.970553118231617
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.408315799744514
waveform batch: 2/2
Test loss - extrapolation:16.522806601766735
Epoch 406 mean train loss:0.9475476393769034
Epoch 406 mean test loss - interpolation:0.9950921863719362
Epoch 406 mean test loss - extrapolation:4.827593533459271
Start training epoch 407
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.76764
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.746691500226861
Iteration: 2 || Loss: 2.745176581829402
Iteration: 3 || Loss: 2.743665787239421
Iteration: 4 || Loss: 2.7421576647158763
Iteration: 5 || Loss: 2.7406514619916327
Iteration: 6 || Loss: 2.7406514619916327
saving ADAM checkpoint...
Sum of params:84.76763
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7406514619916327
Iteration: 2 || Loss: 2.1347074945379734
Iteration: 3 || Loss: 1.6922423886947036
Iteration: 4 || Loss: 1.6897713947095272
Iteration: 5 || Loss: 1.664983206761771
Iteration: 6 || Loss: 1.6641640151974273
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.84153
Epoch 407 loss:1.6641640151974273
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.84153
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.344703691636975
Iteration: 2 || Loss: 6.344305960593785
Iteration: 3 || Loss: 6.343908673497671
Iteration: 4 || Loss: 6.34351147189812
Iteration: 5 || Loss: 6.3431167217534465
Iteration: 6 || Loss: 6.3431167217534465
saving ADAM checkpoint...
Sum of params:84.84156
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3431167217534465
Iteration: 2 || Loss: 6.309097170069521
Iteration: 3 || Loss: 6.23996344326598
Iteration: 4 || Loss: 6.1357417210626535
Iteration: 5 || Loss: 6.1286759806794135
Iteration: 6 || Loss: 6.120266489674863
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.882225
Epoch 407 loss:6.120266489674863
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.882225
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.106069373994448
Iteration: 2 || Loss: 20.105607499222646
Iteration: 3 || Loss: 20.105145373879374
Iteration: 4 || Loss: 20.10468583760187
Iteration: 5 || Loss: 20.104227180573396
Iteration: 6 || Loss: 20.104227180573396
saving ADAM checkpoint...
Sum of params:84.882256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.104227180573396
Iteration: 2 || Loss: 20.052240671866002
Iteration: 3 || Loss: 19.84222902407592
Iteration: 4 || Loss: 19.729408228157936
Iteration: 5 || Loss: 19.700493223940974
Iteration: 6 || Loss: 19.679683071952482
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.77802
Epoch 407 loss:19.679683071952482
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.967988878018779
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.39188559056476
waveform batch: 2/2
Test loss - extrapolation:16.51219226509456
Epoch 407 mean train loss:0.9470383992008542
Epoch 407 mean test loss - interpolation:0.9946648130031298
Epoch 407 mean test loss - extrapolation:4.825339821304944
Start training epoch 408
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.77802
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.745714505797388
Iteration: 2 || Loss: 2.744201327373786
Iteration: 3 || Loss: 2.7426886285429632
Iteration: 4 || Loss: 2.741179746086811
Iteration: 5 || Loss: 2.739672750844743
Iteration: 6 || Loss: 2.739672750844743
saving ADAM checkpoint...
Sum of params:84.778046
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.739672750844743
Iteration: 2 || Loss: 2.1335523525124174
Iteration: 3 || Loss: 1.6913432027696558
Iteration: 4 || Loss: 1.6888723448553982
Iteration: 5 || Loss: 1.6641481013170323
Iteration: 6 || Loss: 1.663333150702071
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.8518
Epoch 408 loss:1.663333150702071
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.8518
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.342230504611387
Iteration: 2 || Loss: 6.341830984701068
Iteration: 3 || Loss: 6.341433337693808
Iteration: 4 || Loss: 6.341035674561892
Iteration: 5 || Loss: 6.340642342278351
Iteration: 6 || Loss: 6.340642342278351
saving ADAM checkpoint...
Sum of params:84.851845
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.340642342278351
Iteration: 2 || Loss: 6.3065448375278885
Iteration: 3 || Loss: 6.2375541565605594
Iteration: 4 || Loss: 6.133393724224907
Iteration: 5 || Loss: 6.126310778089075
Iteration: 6 || Loss: 6.117929912221157
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.892624
Epoch 408 loss:6.117929912221157
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.892624
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.094509127268797
Iteration: 2 || Loss: 20.094046831129372
Iteration: 3 || Loss: 20.093585691870725
Iteration: 4 || Loss: 20.093126825220708
Iteration: 5 || Loss: 20.092667793120448
Iteration: 6 || Loss: 20.092667793120448
saving ADAM checkpoint...
Sum of params:84.892654
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.092667793120448
Iteration: 2 || Loss: 20.04058279335203
Iteration: 3 || Loss: 19.83046816838759
Iteration: 4 || Loss: 19.717795914014868
Iteration: 5 || Loss: 19.68889924999098
Iteration: 6 || Loss: 19.668149479929745
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.78839
Epoch 408 loss:19.668149479929745
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.965443524960623
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.375529951615476
waveform batch: 2/2
Test loss - extrapolation:16.50161007668773
Epoch 408 mean train loss:0.9465314669949301
Epoch 408 mean test loss - interpolation:0.9942405874934371
Epoch 408 mean test loss - extrapolation:4.8230950023586
Start training epoch 409
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.78839
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7447923798436284
Iteration: 2 || Loss: 2.7432791990125573
Iteration: 3 || Loss: 2.741767621295006
Iteration: 4 || Loss: 2.7402573937951105
Iteration: 5 || Loss: 2.7387477788487726
Iteration: 6 || Loss: 2.7387477788487726
saving ADAM checkpoint...
Sum of params:84.78841
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7387477788487726
Iteration: 2 || Loss: 2.1323939817464814
Iteration: 3 || Loss: 1.6904531434579626
Iteration: 4 || Loss: 1.6879810067964631
Iteration: 5 || Loss: 1.6633167454206823
Iteration: 6 || Loss: 1.662506980744305
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.86204
Epoch 409 loss:1.662506980744305
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.86204
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.339723846726562
Iteration: 2 || Loss: 6.33932288759784
Iteration: 3 || Loss: 6.338924106745127
Iteration: 4 || Loss: 6.338527630850031
Iteration: 5 || Loss: 6.338133822630889
Iteration: 6 || Loss: 6.338133822630889
saving ADAM checkpoint...
Sum of params:84.86208
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.338133822630889
Iteration: 2 || Loss: 6.304004834170597
Iteration: 3 || Loss: 6.235163541664602
Iteration: 4 || Loss: 6.131055218403517
Iteration: 5 || Loss: 6.12395535266196
Iteration: 6 || Loss: 6.115599035173293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.903015
Epoch 409 loss:6.115599035173293
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.903015
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.082935833240526
Iteration: 2 || Loss: 20.082472544810575
Iteration: 3 || Loss: 20.082012738642607
Iteration: 4 || Loss: 20.0815518126359
Iteration: 5 || Loss: 20.08109239481248
Iteration: 6 || Loss: 20.08109239481248
saving ADAM checkpoint...
Sum of params:84.90306
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.08109239481248
Iteration: 2 || Loss: 20.02892454704159
Iteration: 3 || Loss: 19.818737846443813
Iteration: 4 || Loss: 19.70622666285047
Iteration: 5 || Loss: 19.67734716691108
Iteration: 6 || Loss: 19.656651722923648
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.798744
Epoch 409 loss:19.656651722923648
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.962897547565404
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.35924505450136
waveform batch: 2/2
Test loss - extrapolation:16.49106761004039
Epoch 409 mean train loss:0.9460261289255602
Epoch 409 mean test loss - interpolation:0.9938162579275672
Epoch 409 mean test loss - extrapolation:4.8208593887118125
Start training epoch 410
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.798744
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.743884144051925
Iteration: 2 || Loss: 2.7423717934908374
Iteration: 3 || Loss: 2.74085759370165
Iteration: 4 || Loss: 2.7393447586704793
Iteration: 5 || Loss: 2.7378339209516955
Iteration: 6 || Loss: 2.7378339209516955
saving ADAM checkpoint...
Sum of params:84.79874
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7378339209516955
Iteration: 2 || Loss: 2.131198932492731
Iteration: 3 || Loss: 1.6895724336523017
Iteration: 4 || Loss: 1.6870987264070947
Iteration: 5 || Loss: 1.662494270421005
Iteration: 6 || Loss: 1.6616888829449386
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.87229
Epoch 410 loss:1.6616888829449386
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.87229
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.337283849330696
Iteration: 2 || Loss: 6.336882358345874
Iteration: 3 || Loss: 6.336482449610104
Iteration: 4 || Loss: 6.336085454717984
Iteration: 5 || Loss: 6.335689345382906
Iteration: 6 || Loss: 6.335689345382906
saving ADAM checkpoint...
Sum of params:84.87232
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.335689345382906
Iteration: 2 || Loss: 6.301483610335961
Iteration: 3 || Loss: 6.232778246647561
Iteration: 4 || Loss: 6.128731088376828
Iteration: 5 || Loss: 6.121616220405502
Iteration: 6 || Loss: 6.113287043214911
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.91339
Epoch 410 loss:6.113287043214911
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.91339
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.071415497727934
Iteration: 2 || Loss: 20.07095144877288
Iteration: 3 || Loss: 20.07048873288791
Iteration: 4 || Loss: 20.070029228140736
Iteration: 5 || Loss: 20.069568682370434
Iteration: 6 || Loss: 20.069568682370434
saving ADAM checkpoint...
Sum of params:84.91343
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.069568682370434
Iteration: 2 || Loss: 20.017296704175763
Iteration: 3 || Loss: 19.80703891742794
Iteration: 4 || Loss: 19.694685548767378
Iteration: 5 || Loss: 19.665826706439933
Iteration: 6 || Loss: 19.64518970710344
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.809044
Epoch 410 loss:19.64518970710344
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.960351137593435
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.34296266497945
waveform batch: 2/2
Test loss - extrapolation:16.48056013872246
Epoch 410 mean train loss:0.945522952871148
Epoch 410 mean test loss - interpolation:0.9933918562655725
Epoch 410 mean test loss - extrapolation:4.818626900308493
Start training epoch 411
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.809044
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.742915485297771
Iteration: 2 || Loss: 2.7413983415694423
Iteration: 3 || Loss: 2.739883764841835
Iteration: 4 || Loss: 2.738372531479188
Iteration: 5 || Loss: 2.736861582266885
Iteration: 6 || Loss: 2.736861582266885
saving ADAM checkpoint...
Sum of params:84.80905
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.736861582266885
Iteration: 2 || Loss: 2.1300300712466766
Iteration: 3 || Loss: 1.6886943931352887
Iteration: 4 || Loss: 1.6862197016389142
Iteration: 5 || Loss: 1.6616766261674594
Iteration: 6 || Loss: 1.6608746038717315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.88248
Epoch 411 loss:1.6608746038717315
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.88248
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3348313075847065
Iteration: 2 || Loss: 6.334430996173432
Iteration: 3 || Loss: 6.334031946703991
Iteration: 4 || Loss: 6.333633865531856
Iteration: 5 || Loss: 6.333236568826728
Iteration: 6 || Loss: 6.333236568826728
saving ADAM checkpoint...
Sum of params:84.88253
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.333236568826728
Iteration: 2 || Loss: 6.2989687811414
Iteration: 3 || Loss: 6.230408323787837
Iteration: 4 || Loss: 6.1264197309995545
Iteration: 5 || Loss: 6.11929160327669
Iteration: 6 || Loss: 6.110986245633104
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.923706
Epoch 411 loss:6.110986245633104
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.923706
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.05991591885452
Iteration: 2 || Loss: 20.059452249393537
Iteration: 3 || Loss: 20.058987820158876
Iteration: 4 || Loss: 20.058527772047373
Iteration: 5 || Loss: 20.058067090757625
Iteration: 6 || Loss: 20.058067090757625
saving ADAM checkpoint...
Sum of params:84.923744
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.058067090757625
Iteration: 2 || Loss: 20.00569385739325
Iteration: 3 || Loss: 19.795383677481524
Iteration: 4 || Loss: 19.683186035910968
Iteration: 5 || Loss: 19.654349206016256
Iteration: 6 || Loss: 19.633771331447598
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.81935
Epoch 411 loss:19.633771331447598
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.9578452889106455
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.32675182863971
waveform batch: 2/2
Test loss - extrapolation:16.47006997293689
Epoch 411 mean train loss:0.9450217993431873
Epoch 411 mean test loss - interpolation:0.992974214818441
Epoch 411 mean test loss - extrapolation:4.81640181679805
Start training epoch 412
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.81935
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.742046736150015
Iteration: 2 || Loss: 2.740529887998192
Iteration: 3 || Loss: 2.7390152755642014
Iteration: 4 || Loss: 2.7375004570844683
Iteration: 5 || Loss: 2.735988608522177
Iteration: 6 || Loss: 2.735988608522177
saving ADAM checkpoint...
Sum of params:84.81932
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.735988608522177
Iteration: 2 || Loss: 2.128896325599282
Iteration: 3 || Loss: 1.687822921313823
Iteration: 4 || Loss: 1.6853482382279403
Iteration: 5 || Loss: 1.660863201061534
Iteration: 6 || Loss: 1.6600655032278253
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.89266
Epoch 412 loss:1.6600655032278253
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.89266
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.332454019520926
Iteration: 2 || Loss: 6.332052299060998
Iteration: 3 || Loss: 6.3316527406538725
Iteration: 4 || Loss: 6.331252605541828
Iteration: 5 || Loss: 6.330855456204002
Iteration: 6 || Loss: 6.330855456204002
saving ADAM checkpoint...
Sum of params:84.89269
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.330855456204002
Iteration: 2 || Loss: 6.296457646868776
Iteration: 3 || Loss: 6.228034841177461
Iteration: 4 || Loss: 6.124119483456932
Iteration: 5 || Loss: 6.116983026920071
Iteration: 6 || Loss: 6.10870420577365
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.934006
Epoch 412 loss:6.10870420577365
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.934006
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.04847620901018
Iteration: 2 || Loss: 20.048010714710767
Iteration: 3 || Loss: 20.0475475113766
Iteration: 4 || Loss: 20.04708590653234
Iteration: 5 || Loss: 20.04662514081877
Iteration: 6 || Loss: 20.04662514081877
saving ADAM checkpoint...
Sum of params:84.93404
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.04662514081877
Iteration: 2 || Loss: 19.994122116008988
Iteration: 3 || Loss: 19.783737764164975
Iteration: 4 || Loss: 19.671713944757027
Iteration: 5 || Loss: 19.6429007938762
Iteration: 6 || Loss: 19.622388770379068
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.829605
Epoch 412 loss:19.622388770379068
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.955328496659581
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.31051715304144
waveform batch: 2/2
Test loss - extrapolation:16.45961449353924
Epoch 412 mean train loss:0.944522706185536
Epoch 412 mean test loss - interpolation:0.9925547494432635
Epoch 412 mean test loss - extrapolation:4.814177637215057
Start training epoch 413
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.829605
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.741042494798109
Iteration: 2 || Loss: 2.7395222649903657
Iteration: 3 || Loss: 2.738005991172529
Iteration: 4 || Loss: 2.7364922135512635
Iteration: 5 || Loss: 2.7349779174318165
Iteration: 6 || Loss: 2.7349779174318165
saving ADAM checkpoint...
Sum of params:84.829605
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7349779174318165
Iteration: 2 || Loss: 2.127780211950724
Iteration: 3 || Loss: 1.686953884698261
Iteration: 4 || Loss: 1.6844762610201995
Iteration: 5 || Loss: 1.6600541581447152
Iteration: 6 || Loss: 1.6592606828585066
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.9028
Epoch 413 loss:1.6592606828585066
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.9028
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.329966651384812
Iteration: 2 || Loss: 6.329564786722159
Iteration: 3 || Loss: 6.329163198847264
Iteration: 4 || Loss: 6.328764505791295
Iteration: 5 || Loss: 6.3283673220800285
Iteration: 6 || Loss: 6.3283673220800285
saving ADAM checkpoint...
Sum of params:84.90285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3283673220800285
Iteration: 2 || Loss: 6.2939657147456405
Iteration: 3 || Loss: 6.2257014364658225
Iteration: 4 || Loss: 6.121835683565707
Iteration: 5 || Loss: 6.114676988388882
Iteration: 6 || Loss: 6.106420148132357
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.9443
Epoch 413 loss:6.106420148132357
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.9443
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.037105758005694
Iteration: 2 || Loss: 20.036641892803157
Iteration: 3 || Loss: 20.036177568504705
Iteration: 4 || Loss: 20.035713645497836
Iteration: 5 || Loss: 20.035251898472236
Iteration: 6 || Loss: 20.035251898472236
saving ADAM checkpoint...
Sum of params:84.944336
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.035251898472236
Iteration: 2 || Loss: 19.982679471718026
Iteration: 3 || Loss: 19.772212770571556
Iteration: 4 || Loss: 19.66031948017849
Iteration: 5 || Loss: 19.631521346581735
Iteration: 6 || Loss: 19.611064096742226
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.83985
Epoch 413 loss:19.611064096742226
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.9528574945989545
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.294430745533475
waveform batch: 2/2
Test loss - extrapolation:16.4491844044285
Epoch 413 mean train loss:0.94402568716321
Epoch 413 mean test loss - interpolation:0.9921429157664924
Epoch 413 mean test loss - extrapolation:4.811967929163498
Start training epoch 414
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.83985
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7402578792285435
Iteration: 2 || Loss: 2.7387364654035755
Iteration: 3 || Loss: 2.737220783971488
Iteration: 4 || Loss: 2.735707599562545
Iteration: 5 || Loss: 2.734191358629924
Iteration: 6 || Loss: 2.734191358629924
saving ADAM checkpoint...
Sum of params:84.839836
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.734191358629924
Iteration: 2 || Loss: 2.1266328819816027
Iteration: 3 || Loss: 1.6860968821132696
Iteration: 4 || Loss: 1.6836169073600877
Iteration: 5 || Loss: 1.6592518859092027
Iteration: 6 || Loss: 1.6584608702169732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.91293
Epoch 414 loss:1.6584608702169732
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.91293
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3276560881795705
Iteration: 2 || Loss: 6.327252011435723
Iteration: 3 || Loss: 6.326850000002219
Iteration: 4 || Loss: 6.326450031583785
Iteration: 5 || Loss: 6.326052460676368
Iteration: 6 || Loss: 6.326052460676368
saving ADAM checkpoint...
Sum of params:84.91298
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.326052460676368
Iteration: 2 || Loss: 6.291495199090805
Iteration: 3 || Loss: 6.2233436923919605
Iteration: 4 || Loss: 6.119554998112262
Iteration: 5 || Loss: 6.112390995133743
Iteration: 6 || Loss: 6.104159816252146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.954544
Epoch 414 loss:6.104159816252146
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.954544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.025716508137254
Iteration: 2 || Loss: 20.02525007780761
Iteration: 3 || Loss: 20.024786314201435
Iteration: 4 || Loss: 20.024323754913265
Iteration: 5 || Loss: 20.023861047933284
Iteration: 6 || Loss: 20.023861047933284
saving ADAM checkpoint...
Sum of params:84.954575
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.023861047933284
Iteration: 2 || Loss: 19.971152153441988
Iteration: 3 || Loss: 19.760622626087926
Iteration: 4 || Loss: 19.64892300268907
Iteration: 5 || Loss: 19.62015424609555
Iteration: 6 || Loss: 19.59975332023528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.85004
Epoch 414 loss:19.59975332023528
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.950356605463337
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.278295573134685
waveform batch: 2/2
Test loss - extrapolation:16.438798687247296
Epoch 414 mean train loss:0.9435301381622206
Epoch 414 mean test loss - interpolation:0.9917261009105561
Epoch 414 mean test loss - extrapolation:4.809757855031832
Start training epoch 415
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.85004
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7392675354895917
Iteration: 2 || Loss: 2.7377483320648413
Iteration: 3 || Loss: 2.73623045732588
Iteration: 4 || Loss: 2.73471594538616
Iteration: 5 || Loss: 2.7332043952718217
Iteration: 6 || Loss: 2.7332043952718217
saving ADAM checkpoint...
Sum of params:84.850044
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7332043952718217
Iteration: 2 || Loss: 2.1255499918709844
Iteration: 3 || Loss: 1.6852395701233516
Iteration: 4 || Loss: 1.6827590700099078
Iteration: 5 || Loss: 1.658454235051743
Iteration: 6 || Loss: 1.6576685164499545
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.92301
Epoch 415 loss:1.6576685164499545
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.92301
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.325219769656526
Iteration: 2 || Loss: 6.324817437994733
Iteration: 3 || Loss: 6.324414498716363
Iteration: 4 || Loss: 6.324015897030703
Iteration: 5 || Loss: 6.3236177110981036
Iteration: 6 || Loss: 6.3236177110981036
saving ADAM checkpoint...
Sum of params:84.92306
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3236177110981036
Iteration: 2 || Loss: 6.289028751734802
Iteration: 3 || Loss: 6.221034685762364
Iteration: 4 || Loss: 6.117295414854679
Iteration: 5 || Loss: 6.110111853835289
Iteration: 6 || Loss: 6.101903798687627
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.96475
Epoch 415 loss:6.101903798687627
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.96475
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.01442283492278
Iteration: 2 || Loss: 20.013953565491434
Iteration: 3 || Loss: 20.013489203451282
Iteration: 4 || Loss: 20.01302690614692
Iteration: 5 || Loss: 20.01256299812863
Iteration: 6 || Loss: 20.01256299812863
saving ADAM checkpoint...
Sum of params:84.9648
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.01256299812863
Iteration: 2 || Loss: 19.959785578596133
Iteration: 3 || Loss: 19.749168161678504
Iteration: 4 || Loss: 19.63759906538758
Iteration: 5 || Loss: 19.608842966994054
Iteration: 6 || Loss: 19.58849845951124
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.86023
Epoch 415 loss:19.58849845951124
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.947888073568426
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.26228477834375
waveform batch: 2/2
Test loss - extrapolation:16.428435145837465
Epoch 415 mean train loss:0.9430369232637524
Epoch 415 mean test loss - interpolation:0.991314678928071
Epoch 415 mean test loss - extrapolation:4.8075599936817675
Start training epoch 416
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.86023
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.738450515523688
Iteration: 2 || Loss: 2.7369295896795967
Iteration: 3 || Loss: 2.735410376308196
Iteration: 4 || Loss: 2.733896152660062
Iteration: 5 || Loss: 2.732382822521407
Iteration: 6 || Loss: 2.732382822521407
saving ADAM checkpoint...
Sum of params:84.860214
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.732382822521407
Iteration: 2 || Loss: 2.124377884075639
Iteration: 3 || Loss: 1.6843962273430284
Iteration: 4 || Loss: 1.6819136951666391
Iteration: 5 || Loss: 1.657662201181483
Iteration: 6 || Loss: 1.656880049503064
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.9331
Epoch 416 loss:1.656880049503064
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.9331
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3228445040605425
Iteration: 2 || Loss: 6.322438956085294
Iteration: 3 || Loss: 6.322037244959786
Iteration: 4 || Loss: 6.321635872383529
Iteration: 5 || Loss: 6.3212387769123755
Iteration: 6 || Loss: 6.3212387769123755
saving ADAM checkpoint...
Sum of params:84.93312
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3212387769123755
Iteration: 2 || Loss: 6.286584333958929
Iteration: 3 || Loss: 6.21873039326749
Iteration: 4 || Loss: 6.115042595164345
Iteration: 5 || Loss: 6.107843335344069
Iteration: 6 || Loss: 6.099661217564433
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.974976
Epoch 416 loss:6.099661217564433
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.974976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.0031738448368
Iteration: 2 || Loss: 20.002706268691313
Iteration: 3 || Loss: 20.002240601025708
Iteration: 4 || Loss: 20.001777845384655
Iteration: 5 || Loss: 20.00131452073993
Iteration: 6 || Loss: 20.00131452073993
saving ADAM checkpoint...
Sum of params:84.97501
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.00131452073993
Iteration: 2 || Loss: 19.948429639980002
Iteration: 3 || Loss: 19.7377292119273
Iteration: 4 || Loss: 19.626305692105845
Iteration: 5 || Loss: 19.597572133666297
Iteration: 6 || Loss: 19.577282850272248
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.87037
Epoch 416 loss:19.577282850272248
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.9454231700026385
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.24625649687845
waveform batch: 2/2
Test loss - extrapolation:16.41810796183333
Epoch 416 mean train loss:0.9425456592186119
Epoch 416 mean test loss - interpolation:0.9909038616671064
Epoch 416 mean test loss - extrapolation:4.805363704892648
Start training epoch 417
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.87037
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7375300063911694
Iteration: 2 || Loss: 2.736007890122023
Iteration: 3 || Loss: 2.7344902041176504
Iteration: 4 || Loss: 2.7329751475000723
Iteration: 5 || Loss: 2.731457810949792
Iteration: 6 || Loss: 2.731457810949792
saving ADAM checkpoint...
Sum of params:84.87036
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.731457810949792
Iteration: 2 || Loss: 2.123279550065499
Iteration: 3 || Loss: 1.6835511634224485
Iteration: 4 || Loss: 1.681066381693828
Iteration: 5 || Loss: 1.6568739180423373
Iteration: 6 || Loss: 1.6560946031647028
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.943146
Epoch 417 loss:1.6560946031647028
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.943146
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.320478356556081
Iteration: 2 || Loss: 6.3200748425539155
Iteration: 3 || Loss: 6.319672344269413
Iteration: 4 || Loss: 6.319269868569796
Iteration: 5 || Loss: 6.318870402100887
Iteration: 6 || Loss: 6.318870402100887
saving ADAM checkpoint...
Sum of params:84.9432
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.318870402100887
Iteration: 2 || Loss: 6.284151854384669
Iteration: 3 || Loss: 6.216426361140305
Iteration: 4 || Loss: 6.11280206371786
Iteration: 5 || Loss: 6.105592609654903
Iteration: 6 || Loss: 6.097429560239562
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.98514
Epoch 417 loss:6.097429560239562
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.98514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.991888881900532
Iteration: 2 || Loss: 19.991420162994054
Iteration: 3 || Loss: 19.9909535938726
Iteration: 4 || Loss: 19.99048933161639
Iteration: 5 || Loss: 19.99002588634631
Iteration: 6 || Loss: 19.99002588634631
saving ADAM checkpoint...
Sum of params:84.985176
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.99002588634631
Iteration: 2 || Loss: 19.93703200492209
Iteration: 3 || Loss: 19.726290117415747
Iteration: 4 || Loss: 19.61504049336746
Iteration: 5 || Loss: 19.58632886617459
Iteration: 6 || Loss: 19.566099347796715
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.8805
Epoch 417 loss:19.566099347796715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.942974877830693
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.23029364992694
waveform batch: 2/2
Test loss - extrapolation:16.40780362938633
Epoch 417 mean train loss:0.9420559831448614
Epoch 417 mean test loss - interpolation:0.9904958129717821
Epoch 417 mean test loss - extrapolation:4.803174773276106
Start training epoch 418
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.8805
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7366666892320546
Iteration: 2 || Loss: 2.735145752261111
Iteration: 3 || Loss: 2.7336265128885238
Iteration: 4 || Loss: 2.7321099877658375
Iteration: 5 || Loss: 2.7305921076427158
Iteration: 6 || Loss: 2.7305921076427158
saving ADAM checkpoint...
Sum of params:84.880516
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7305921076427158
Iteration: 2 || Loss: 2.122169836631712
Iteration: 3 || Loss: 1.6827145214632924
Iteration: 4 || Loss: 1.6802286530381088
Iteration: 5 || Loss: 1.656091972565401
Iteration: 6 || Loss: 1.6553163458596318
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.953156
Epoch 418 loss:1.6553163458596318
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.953156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.318138320772511
Iteration: 2 || Loss: 6.3177334903394975
Iteration: 3 || Loss: 6.317332811420063
Iteration: 4 || Loss: 6.316930652503911
Iteration: 5 || Loss: 6.316530109982498
Iteration: 6 || Loss: 6.316530109982498
saving ADAM checkpoint...
Sum of params:84.95321
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.316530109982498
Iteration: 2 || Loss: 6.281718047883816
Iteration: 3 || Loss: 6.214135769409603
Iteration: 4 || Loss: 6.11057451812995
Iteration: 5 || Loss: 6.103349004929852
Iteration: 6 || Loss: 6.095210262751833
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.995285
Epoch 418 loss:6.095210262751833
waveform batch: 3/3
Using ADAM optimizer
Sum of params:84.995285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.98073632259868
Iteration: 2 || Loss: 19.980269284794577
Iteration: 3 || Loss: 19.97980099529178
Iteration: 4 || Loss: 19.979336235199572
Iteration: 5 || Loss: 19.97887336037409
Iteration: 6 || Loss: 19.97887336037409
saving ADAM checkpoint...
Sum of params:84.995316
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.97887336037409
Iteration: 2 || Loss: 19.925778335443347
Iteration: 3 || Loss: 19.714944578935047
Iteration: 4 || Loss: 19.60382949604882
Iteration: 5 || Loss: 19.575135110109517
Iteration: 6 || Loss: 19.55496105525603
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.89062
Epoch 418 loss:19.55496105525603
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.940516562489877
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.21435402458378
waveform batch: 2/2
Test loss - extrapolation:16.3975459458852
Epoch 418 mean train loss:0.9415685401333619
Epoch 418 mean test loss - interpolation:0.9900860937483128
Epoch 418 mean test loss - extrapolation:4.800991664205748
Start training epoch 419
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.89062
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.735735527133375
Iteration: 2 || Loss: 2.7342126200378547
Iteration: 3 || Loss: 2.7326910948632075
Iteration: 4 || Loss: 2.7311731289589396
Iteration: 5 || Loss: 2.729656806562649
Iteration: 6 || Loss: 2.729656806562649
saving ADAM checkpoint...
Sum of params:84.89062
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.729656806562649
Iteration: 2 || Loss: 2.121062859197894
Iteration: 3 || Loss: 1.6818827129646547
Iteration: 4 || Loss: 1.6793937490975988
Iteration: 5 || Loss: 1.655313309102926
Iteration: 6 || Loss: 1.6545409692858624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.96316
Epoch 419 loss:1.6545409692858624
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.96316
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.315798742334181
Iteration: 2 || Loss: 6.315393504915107
Iteration: 3 || Loss: 6.3149886759894605
Iteration: 4 || Loss: 6.314587940042606
Iteration: 5 || Loss: 6.314187351096018
Iteration: 6 || Loss: 6.314187351096018
saving ADAM checkpoint...
Sum of params:84.963196
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.314187351096018
Iteration: 2 || Loss: 6.2793045985397145
Iteration: 3 || Loss: 6.211859542625254
Iteration: 4 || Loss: 6.108354538539498
Iteration: 5 || Loss: 6.101114830708711
Iteration: 6 || Loss: 6.092999039368296
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.0054
Epoch 419 loss:6.092999039368296
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.0054
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.96960598730372
Iteration: 2 || Loss: 19.969136924578848
Iteration: 3 || Loss: 19.968669793278195
Iteration: 4 || Loss: 19.968205311778632
Iteration: 5 || Loss: 19.967740462215545
Iteration: 6 || Loss: 19.967740462215545
saving ADAM checkpoint...
Sum of params:85.00544
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.967740462215545
Iteration: 2 || Loss: 19.91455422691665
Iteration: 3 || Loss: 19.703628544471567
Iteration: 4 || Loss: 19.592657730405524
Iteration: 5 || Loss: 19.563980087718694
Iteration: 6 || Loss: 19.543859555050048
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.90069
Epoch 419 loss:19.543859555050048
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.938081834017869
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.19847173176569
waveform batch: 2/2
Test loss - extrapolation:16.38730465754292
Epoch 419 mean train loss:0.9410827435760072
Epoch 419 mean test loss - interpolation:0.9896803056696449
Epoch 419 mean test loss - extrapolation:4.798814699109051
Start training epoch 420
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.90069
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.734862512109964
Iteration: 2 || Loss: 2.733339396535806
Iteration: 3 || Loss: 2.731816969566461
Iteration: 4 || Loss: 2.7302965355626307
Iteration: 5 || Loss: 2.7287768600045323
Iteration: 6 || Loss: 2.7287768600045323
saving ADAM checkpoint...
Sum of params:84.90067
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7287768600045323
Iteration: 2 || Loss: 2.1199463127951326
Iteration: 3 || Loss: 1.6810583727684418
Iteration: 4 || Loss: 1.6785682777397288
Iteration: 5 || Loss: 1.6545404426835935
Iteration: 6 || Loss: 1.653771195328099
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.97314
Epoch 420 loss:1.653771195328099
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.97314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3135077653922425
Iteration: 2 || Loss: 6.313101317274981
Iteration: 3 || Loss: 6.312697053868416
Iteration: 4 || Loss: 6.3122942940878
Iteration: 5 || Loss: 6.311894508353501
Iteration: 6 || Loss: 6.311894508353501
saving ADAM checkpoint...
Sum of params:84.97315
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.311894508353501
Iteration: 2 || Loss: 6.276901908649498
Iteration: 3 || Loss: 6.2095855175728465
Iteration: 4 || Loss: 6.106148141592192
Iteration: 5 || Loss: 6.098898596436549
Iteration: 6 || Loss: 6.0908036661472575
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.01548
Epoch 420 loss:6.0908036661472575
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.01548
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.958470567902435
Iteration: 2 || Loss: 19.958000937011743
Iteration: 3 || Loss: 19.95753198909207
Iteration: 4 || Loss: 19.95706630383784
Iteration: 5 || Loss: 19.956600870605236
Iteration: 6 || Loss: 19.956600870605236
saving ADAM checkpoint...
Sum of params:85.01553
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.956600870605236
Iteration: 2 || Loss: 19.90329022417896
Iteration: 3 || Loss: 19.69230842527048
Iteration: 4 || Loss: 19.581504769801317
Iteration: 5 || Loss: 19.55285537848719
Iteration: 6 || Loss: 19.53279641863709
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.910736
Epoch 420 loss:19.53279641863709
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.935686188789085
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.18261683238573
waveform batch: 2/2
Test loss - extrapolation:16.377076946519757
Epoch 420 mean train loss:0.94059900965905
Epoch 420 mean test loss - interpolation:0.9892810314648476
Epoch 420 mean test loss - extrapolation:4.7966411482421245
Start training epoch 421
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.910736
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7340340879767826
Iteration: 2 || Loss: 2.7325103414385197
Iteration: 3 || Loss: 2.730985893263902
Iteration: 4 || Loss: 2.7294649431353064
Iteration: 5 || Loss: 2.7279462106003782
Iteration: 6 || Loss: 2.7279462106003782
saving ADAM checkpoint...
Sum of params:84.91073
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7279462106003782
Iteration: 2 || Loss: 2.118920306365822
Iteration: 3 || Loss: 1.6802333261260587
Iteration: 4 || Loss: 1.677740943195518
Iteration: 5 || Loss: 1.6537703756263178
Iteration: 6 || Loss: 1.653003683974076
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.98307
Epoch 421 loss:1.653003683974076
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.98307
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.311147823026147
Iteration: 2 || Loss: 6.310740359139417
Iteration: 3 || Loss: 6.310335324274466
Iteration: 4 || Loss: 6.309932889547501
Iteration: 5 || Loss: 6.3095317302647
Iteration: 6 || Loss: 6.3095317302647
saving ADAM checkpoint...
Sum of params:84.983116
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3095317302647
Iteration: 2 || Loss: 6.274505487762551
Iteration: 3 || Loss: 6.207337992311273
Iteration: 4 || Loss: 6.103949255582112
Iteration: 5 || Loss: 6.096677592400463
Iteration: 6 || Loss: 6.088610166915472
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.02557
Epoch 421 loss:6.088610166915472
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.02557
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.94743699659382
Iteration: 2 || Loss: 19.946967985186962
Iteration: 3 || Loss: 19.946499950333465
Iteration: 4 || Loss: 19.946032538697708
Iteration: 5 || Loss: 19.945568941291317
Iteration: 6 || Loss: 19.945568941291317
saving ADAM checkpoint...
Sum of params:85.025604
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.945568941291317
Iteration: 2 || Loss: 19.892204898349856
Iteration: 3 || Loss: 19.68111710591419
Iteration: 4 || Loss: 19.57042978356294
Iteration: 5 || Loss: 19.541787780320504
Iteration: 6 || Loss: 19.521771398537105
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.92073
Epoch 421 loss:19.521771398537105
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.933255549453144
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.166876824631906
waveform batch: 2/2
Test loss - extrapolation:16.366917051708562
Epoch 421 mean train loss:0.9401167327388502
Epoch 421 mean test loss - interpolation:0.9888759249088573
Epoch 421 mean test loss - extrapolation:4.794482823028372
Start training epoch 422
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.92073
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.73321860206964
Iteration: 2 || Loss: 2.7316954797979545
Iteration: 3 || Loss: 2.7301738794860273
Iteration: 4 || Loss: 2.7286506706817026
Iteration: 5 || Loss: 2.727129866277499
Iteration: 6 || Loss: 2.727129866277499
saving ADAM checkpoint...
Sum of params:84.92073
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.727129866277499
Iteration: 2 || Loss: 2.1177523861834575
Iteration: 3 || Loss: 1.679424890888688
Iteration: 4 || Loss: 1.6769307615853126
Iteration: 5 || Loss: 1.6530097809572046
Iteration: 6 || Loss: 1.6522454272167244
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.992966
Epoch 422 loss:1.6522454272167244
waveform batch: 2/3
Using ADAM optimizer
Sum of params:84.992966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.308896846306601
Iteration: 2 || Loss: 6.308487982344778
Iteration: 3 || Loss: 6.308082857745292
Iteration: 4 || Loss: 6.307679357474179
Iteration: 5 || Loss: 6.307277642767148
Iteration: 6 || Loss: 6.307277642767148
saving ADAM checkpoint...
Sum of params:84.993004
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.307277642767148
Iteration: 2 || Loss: 6.272139927537317
Iteration: 3 || Loss: 6.205098739899268
Iteration: 4 || Loss: 6.101768619696662
Iteration: 5 || Loss: 6.094492224828686
Iteration: 6 || Loss: 6.086440473860035
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.03558
Epoch 422 loss:6.086440473860035
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.03558
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.936360474019
Iteration: 2 || Loss: 19.935890376282508
Iteration: 3 || Loss: 19.935420610791137
Iteration: 4 || Loss: 19.934953778747108
Iteration: 5 || Loss: 19.934488177337347
Iteration: 6 || Loss: 19.934488177337347
saving ADAM checkpoint...
Sum of params:85.035614
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.934488177337347
Iteration: 2 || Loss: 19.880971312825977
Iteration: 3 || Loss: 19.669855560427735
Iteration: 4 || Loss: 19.559341366086393
Iteration: 5 || Loss: 19.530731079290348
Iteration: 6 || Loss: 19.5107780639749
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.93075
Epoch 422 loss:19.5107780639749
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.930848962453756
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.15103055915018
waveform batch: 2/2
Test loss - extrapolation:16.356761434831387
Epoch 422 mean train loss:0.9396366884500572
Epoch 422 mean test loss - interpolation:0.988474827075626
Epoch 422 mean test loss - extrapolation:4.792315999498464
Start training epoch 423
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.93075
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.732229083225756
Iteration: 2 || Loss: 2.7307025199109045
Iteration: 3 || Loss: 2.7291799978763507
Iteration: 4 || Loss: 2.7276561848120724
Iteration: 5 || Loss: 2.7261358497111035
Iteration: 6 || Loss: 2.7261358497111035
saving ADAM checkpoint...
Sum of params:84.930756
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7261358497111035
Iteration: 2 || Loss: 2.1167362525693134
Iteration: 3 || Loss: 1.6786056650611438
Iteration: 4 || Loss: 1.6761078975029704
Iteration: 5 || Loss: 1.6522478919020103
Iteration: 6 || Loss: 1.6514870457748276
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.002846
Epoch 423 loss:1.6514870457748276
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.002846
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.306555737003664
Iteration: 2 || Loss: 6.306146742969503
Iteration: 3 || Loss: 6.305740371263597
Iteration: 4 || Loss: 6.305336735069467
Iteration: 5 || Loss: 6.304934789378441
Iteration: 6 || Loss: 6.304934789378441
saving ADAM checkpoint...
Sum of params:85.0029
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.304934789378441
Iteration: 2 || Loss: 6.269759250509966
Iteration: 3 || Loss: 6.2028623397734926
Iteration: 4 || Loss: 6.099594110983315
Iteration: 5 || Loss: 6.092297424672956
Iteration: 6 || Loss: 6.084268883807332
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.0456
Epoch 423 loss:6.084268883807332
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.0456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.925393217887947
Iteration: 2 || Loss: 19.92492201471677
Iteration: 3 || Loss: 19.924453694987857
Iteration: 4 || Loss: 19.923985611463003
Iteration: 5 || Loss: 19.92351904040377
Iteration: 6 || Loss: 19.92351904040377
saving ADAM checkpoint...
Sum of params:85.04563
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.92351904040377
Iteration: 2 || Loss: 19.869941077781043
Iteration: 3 || Loss: 19.6587127464005
Iteration: 4 || Loss: 19.54833469609385
Iteration: 5 || Loss: 19.51973637449533
Iteration: 6 || Loss: 19.499831363423333
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.9407
Epoch 423 loss:19.499831363423333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.928475367876369
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.13538811161999
waveform batch: 2/2
Test loss - extrapolation:16.346639180475236
Epoch 423 mean train loss:0.9391581825174309
Epoch 423 mean test loss - interpolation:0.9880792279793948
Epoch 423 mean test loss - extrapolation:4.790168941007935
Start training epoch 424
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.9407
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7315200097997523
Iteration: 2 || Loss: 2.7299931351975024
Iteration: 3 || Loss: 2.72846778457043
Iteration: 4 || Loss: 2.7269451664592914
Iteration: 5 || Loss: 2.725422409939661
Iteration: 6 || Loss: 2.725422409939661
saving ADAM checkpoint...
Sum of params:84.94071
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.725422409939661
Iteration: 2 || Loss: 2.1156439852118165
Iteration: 3 || Loss: 1.677804609518565
Iteration: 4 || Loss: 1.6753041106771767
Iteration: 5 || Loss: 1.6514943903305745
Iteration: 6 || Loss: 1.650736679224957
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.01271
Epoch 424 loss:1.650736679224957
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.01271
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.30430902214116
Iteration: 2 || Loss: 6.3039015758060115
Iteration: 3 || Loss: 6.30349585072648
Iteration: 4 || Loss: 6.303091265853396
Iteration: 5 || Loss: 6.302688675334966
Iteration: 6 || Loss: 6.302688675334966
saving ADAM checkpoint...
Sum of params:85.01276
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.302688675334966
Iteration: 2 || Loss: 6.2674211359526915
Iteration: 3 || Loss: 6.20064731778983
Iteration: 4 || Loss: 6.097431271818453
Iteration: 5 || Loss: 6.090119394417318
Iteration: 6 || Loss: 6.082113697009028
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.055595
Epoch 424 loss:6.082113697009028
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.055595
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.91446214058247
Iteration: 2 || Loss: 19.913990856366716
Iteration: 3 || Loss: 19.913521360774144
Iteration: 4 || Loss: 19.913052446614543
Iteration: 5 || Loss: 19.9125851243216
Iteration: 6 || Loss: 19.9125851243216
saving ADAM checkpoint...
Sum of params:85.05561
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.9125851243216
Iteration: 2 || Loss: 19.858906315203
Iteration: 3 || Loss: 19.647579234528326
Iteration: 4 || Loss: 19.537346168315402
Iteration: 5 || Loss: 19.50876783135453
Iteration: 6 || Loss: 19.488914936012954
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.950645
Epoch 424 loss:19.488914936012954
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.92609846013666
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.1197110897479
waveform batch: 2/2
Test loss - extrapolation:16.336545703682262
Epoch 424 mean train loss:0.9386815624912737
Epoch 424 mean test loss - interpolation:0.9876830766894433
Epoch 424 mean test loss - extrapolation:4.788021399452513
Start training epoch 425
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.950645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7306819014696027
Iteration: 2 || Loss: 2.7291533014463356
Iteration: 3 || Loss: 2.7276258837932787
Iteration: 4 || Loss: 2.726103365952941
Iteration: 5 || Loss: 2.7245809478932395
Iteration: 6 || Loss: 2.7245809478932395
saving ADAM checkpoint...
Sum of params:84.950645
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7245809478932395
Iteration: 2 || Loss: 2.11458193155119
Iteration: 3 || Loss: 1.6770052731405802
Iteration: 4 || Loss: 1.6745037717626983
Iteration: 5 || Loss: 1.6507442491721078
Iteration: 6 || Loss: 1.6499888356552388
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.022545
Epoch 425 loss:1.6499888356552388
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.022545
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.302051663484635
Iteration: 2 || Loss: 6.301642911485333
Iteration: 3 || Loss: 6.301235611437672
Iteration: 4 || Loss: 6.300831105355161
Iteration: 5 || Loss: 6.300427406273859
Iteration: 6 || Loss: 6.300427406273859
saving ADAM checkpoint...
Sum of params:85.022575
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.300427406273859
Iteration: 2 || Loss: 6.265065765535183
Iteration: 3 || Loss: 6.198430216202738
Iteration: 4 || Loss: 6.095278093848754
Iteration: 5 || Loss: 6.087954352797654
Iteration: 6 || Loss: 6.079969061524244
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.06553
Epoch 425 loss:6.079969061524244
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.06553
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.903523330827856
Iteration: 2 || Loss: 19.903052200353233
Iteration: 3 || Loss: 19.90258245034582
Iteration: 4 || Loss: 19.902113002013245
Iteration: 5 || Loss: 19.901643705735673
Iteration: 6 || Loss: 19.901643705735673
saving ADAM checkpoint...
Sum of params:85.06557
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.901643705735673
Iteration: 2 || Loss: 19.847853215890492
Iteration: 3 || Loss: 19.636471561845273
Iteration: 4 || Loss: 19.526389029493917
Iteration: 5 || Loss: 19.497834295569866
Iteration: 6 || Loss: 19.47803870491456
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.96056
Epoch 425 loss:19.47803870491456
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.923746193052994
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.10407704852522
waveform batch: 2/2
Test loss - extrapolation:16.326473425057817
Epoch 425 mean train loss:0.9382067793825531
Epoch 425 mean test loss - interpolation:0.9872910321754991
Epoch 425 mean test loss - extrapolation:4.78587920613192
Start training epoch 426
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.96056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.729889515403517
Iteration: 2 || Loss: 2.7283599716796165
Iteration: 3 || Loss: 2.726830756720139
Iteration: 4 || Loss: 2.725308374557814
Iteration: 5 || Loss: 2.723785102416269
Iteration: 6 || Loss: 2.723785102416269
saving ADAM checkpoint...
Sum of params:84.960556
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.723785102416269
Iteration: 2 || Loss: 2.11354332321822
Iteration: 3 || Loss: 1.6762076366291547
Iteration: 4 || Loss: 1.6737028420114395
Iteration: 5 || Loss: 1.6499984250730966
Iteration: 6 || Loss: 1.6492452047966397
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.03234
Epoch 426 loss:1.6492452047966397
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.03234
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.299808273492397
Iteration: 2 || Loss: 6.299398743580589
Iteration: 3 || Loss: 6.298993243431055
Iteration: 4 || Loss: 6.2985865679126904
Iteration: 5 || Loss: 6.298182121086618
Iteration: 6 || Loss: 6.298182121086618
saving ADAM checkpoint...
Sum of params:85.03238
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.298182121086618
Iteration: 2 || Loss: 6.262735412789615
Iteration: 3 || Loss: 6.196229744588948
Iteration: 4 || Loss: 6.093136494541243
Iteration: 5 || Loss: 6.085800487133874
Iteration: 6 || Loss: 6.077836363758295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.075455
Epoch 426 loss:6.077836363758295
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.075455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.89265837870598
Iteration: 2 || Loss: 19.892185034472252
Iteration: 3 || Loss: 19.89171142304159
Iteration: 4 || Loss: 19.89124292518632
Iteration: 5 || Loss: 19.890775523796663
Iteration: 6 || Loss: 19.890775523796663
saving ADAM checkpoint...
Sum of params:85.07549
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.890775523796663
Iteration: 2 || Loss: 19.836888045864068
Iteration: 3 || Loss: 19.62541417842313
Iteration: 4 || Loss: 19.515476328670136
Iteration: 5 || Loss: 19.486942374605434
Iteration: 6 || Loss: 19.467196580743714
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.97043
Epoch 426 loss:19.467196580743714
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.921380445991376
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.088479487172926
waveform batch: 2/2
Test loss - extrapolation:16.316441484169378
Epoch 426 mean train loss:0.9377337292861603
Epoch 426 mean test loss - interpolation:0.9868967409985627
Epoch 426 mean test loss - extrapolation:4.783743414278526
Start training epoch 427
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.97043
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7290433922469943
Iteration: 2 || Loss: 2.727512253453949
Iteration: 3 || Loss: 2.7259821989391595
Iteration: 4 || Loss: 2.724458202782977
Iteration: 5 || Loss: 2.722934005121064
Iteration: 6 || Loss: 2.722934005121064
saving ADAM checkpoint...
Sum of params:84.970436
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.722934005121064
Iteration: 2 || Loss: 2.112494390328217
Iteration: 3 || Loss: 1.6754172566952608
Iteration: 4 || Loss: 1.6729123474245773
Iteration: 5 || Loss: 1.649256459628777
Iteration: 6 || Loss: 1.6485057448136622
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.04211
Epoch 427 loss:1.6485057448136622
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.04211
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.297450631028484
Iteration: 2 || Loss: 6.297041264007169
Iteration: 3 || Loss: 6.296633349276614
Iteration: 4 || Loss: 6.296229010168033
Iteration: 5 || Loss: 6.295823391956448
Iteration: 6 || Loss: 6.295823391956448
saving ADAM checkpoint...
Sum of params:85.042145
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.295823391956448
Iteration: 2 || Loss: 6.2604244590470435
Iteration: 3 || Loss: 6.194072803605179
Iteration: 4 || Loss: 6.09100390541524
Iteration: 5 || Loss: 6.083634891623008
Iteration: 6 || Loss: 6.075694570957367
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.08537
Epoch 427 loss:6.075694570957367
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.08537
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.88186990361048
Iteration: 2 || Loss: 19.88139587927864
Iteration: 3 || Loss: 19.880925564212998
Iteration: 4 || Loss: 19.880455298025964
Iteration: 5 || Loss: 19.879986601815673
Iteration: 6 || Loss: 19.879986601815673
saving ADAM checkpoint...
Sum of params:85.0854
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.879986601815673
Iteration: 2 || Loss: 19.826093792115742
Iteration: 3 || Loss: 19.614495780424466
Iteration: 4 || Loss: 19.504643724906558
Iteration: 5 || Loss: 19.476109227342317
Iteration: 6 || Loss: 19.456401463499382
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.980286
Epoch 427 loss:19.456401463499382
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.919051337588654
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.07306450150728
waveform batch: 2/2
Test loss - extrapolation:16.306436111976353
Epoch 427 mean train loss:0.9372621303196693
Epoch 427 mean test loss - interpolation:0.9865085562647757
Epoch 427 mean test loss - extrapolation:4.781625051123636
Start training epoch 428
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.980286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7284493033881954
Iteration: 2 || Loss: 2.7269175585035783
Iteration: 3 || Loss: 2.7253878584563886
Iteration: 4 || Loss: 2.7238601351268
Iteration: 5 || Loss: 2.7223349203966034
Iteration: 6 || Loss: 2.7223349203966034
saving ADAM checkpoint...
Sum of params:84.980286
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7223349203966034
Iteration: 2 || Loss: 2.111360323082241
Iteration: 3 || Loss: 1.6746419636938763
Iteration: 4 || Loss: 1.67213319201689
Iteration: 5 || Loss: 1.6485222747831456
Iteration: 6 || Loss: 1.6477728840083794
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.05188
Epoch 428 loss:1.6477728840083794
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.05188
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2953246537325604
Iteration: 2 || Loss: 6.294916117641132
Iteration: 3 || Loss: 6.2945067733412
Iteration: 4 || Loss: 6.294100641291747
Iteration: 5 || Loss: 6.293695476929167
Iteration: 6 || Loss: 6.293695476929167
saving ADAM checkpoint...
Sum of params:85.05191
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.293695476929167
Iteration: 2 || Loss: 6.258142431331522
Iteration: 3 || Loss: 6.191883528814943
Iteration: 4 || Loss: 6.088877797492066
Iteration: 5 || Loss: 6.081505229746204
Iteration: 6 || Loss: 6.073584255158684
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.09523
Epoch 428 loss:6.073584255158684
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.09523
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.871023786019354
Iteration: 2 || Loss: 19.8705508926317
Iteration: 3 || Loss: 19.87007775635621
Iteration: 4 || Loss: 19.869608071654845
Iteration: 5 || Loss: 19.869139169434398
Iteration: 6 || Loss: 19.869139169434398
saving ADAM checkpoint...
Sum of params:85.09528
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.869139169434398
Iteration: 2 || Loss: 19.815096652583144
Iteration: 3 || Loss: 19.603460574848175
Iteration: 4 || Loss: 19.493771325597066
Iteration: 5 || Loss: 19.465266333803918
Iteration: 6 || Loss: 19.4456167941596
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.99013
Epoch 428 loss:19.4456167941596
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.916666278045758
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.0574565161853
waveform batch: 2/2
Test loss - extrapolation:16.296470881927213
Epoch 428 mean train loss:0.9367922045974713
Epoch 428 mean test loss - interpolation:0.9861110463409597
Epoch 428 mean test loss - extrapolation:4.779493949842709
Start training epoch 429
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.99013
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7274242917447618
Iteration: 2 || Loss: 2.7258911188677666
Iteration: 3 || Loss: 2.7243608763533746
Iteration: 4 || Loss: 2.7228355249046405
Iteration: 5 || Loss: 2.721309622388906
Iteration: 6 || Loss: 2.721309622388906
saving ADAM checkpoint...
Sum of params:84.990135
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.721309622388906
Iteration: 2 || Loss: 2.110324161988733
Iteration: 3 || Loss: 1.6738577217291328
Iteration: 4 || Loss: 1.6713451559770134
Iteration: 5 || Loss: 1.6477909868605207
Iteration: 6 || Loss: 1.6470430629981494
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.06161
Epoch 429 loss:1.6470430629981494
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.06161
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2931316362703935
Iteration: 2 || Loss: 6.292719079383342
Iteration: 3 || Loss: 6.292309607039048
Iteration: 4 || Loss: 6.291903193185193
Iteration: 5 || Loss: 6.291496512112963
Iteration: 6 || Loss: 6.291496512112963
saving ADAM checkpoint...
Sum of params:85.06165
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.291496512112963
Iteration: 2 || Loss: 6.255822622062546
Iteration: 3 || Loss: 6.189695885933759
Iteration: 4 || Loss: 6.086768665938709
Iteration: 5 || Loss: 6.079394488341353
Iteration: 6 || Loss: 6.07148761753589
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.10508
Epoch 429 loss:6.07148761753589
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.10508
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.86023157506146
Iteration: 2 || Loss: 19.85975544068541
Iteration: 3 || Loss: 19.859283458963798
Iteration: 4 || Loss: 19.858812027343546
Iteration: 5 || Loss: 19.85834296825082
Iteration: 6 || Loss: 19.85834296825082
saving ADAM checkpoint...
Sum of params:85.10512
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.85834296825082
Iteration: 2 || Loss: 19.804164920417634
Iteration: 3 || Loss: 19.59247322121102
Iteration: 4 || Loss: 19.482945129103015
Iteration: 5 || Loss: 19.454468423552267
Iteration: 6 || Loss: 19.434881395395774
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:84.999954
Epoch 429 loss:19.434881395395774
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.914345943162593
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.04192506460492
waveform batch: 2/2
Test loss - extrapolation:16.286504620750204
Epoch 429 mean train loss:0.9363245543424074
Epoch 429 mean test loss - interpolation:0.9857243238604322
Epoch 429 mean test loss - extrapolation:4.77736914044626
Start training epoch 430
waveform batch: 1/3
Using ADAM optimizer
Sum of params:84.999954
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7265680955135543
Iteration: 2 || Loss: 2.725036641117738
Iteration: 3 || Loss: 2.723507862809369
Iteration: 4 || Loss: 2.7219760606287746
Iteration: 5 || Loss: 2.7204481440225914
Iteration: 6 || Loss: 2.7204481440225914
saving ADAM checkpoint...
Sum of params:84.99997
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7204481440225914
Iteration: 2 || Loss: 2.109331246876251
Iteration: 3 || Loss: 1.6730764043923192
Iteration: 4 || Loss: 1.670562390950107
Iteration: 5 || Loss: 1.6470589362866763
Iteration: 6 || Loss: 1.6463143443011379
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.071304
Epoch 430 loss:1.6463143443011379
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.071304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.290840650976873
Iteration: 2 || Loss: 6.290428997953168
Iteration: 3 || Loss: 6.290019487558857
Iteration: 4 || Loss: 6.289611020787769
Iteration: 5 || Loss: 6.289205267471197
Iteration: 6 || Loss: 6.289205267471197
saving ADAM checkpoint...
Sum of params:85.07136
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.289205267471197
Iteration: 2 || Loss: 6.253498886360425
Iteration: 3 || Loss: 6.187542954161381
Iteration: 4 || Loss: 6.084668534461286
Iteration: 5 || Loss: 6.077270348323929
Iteration: 6 || Loss: 6.06938684722332
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.11491
Epoch 430 loss:6.06938684722332
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.11491
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.84951427276329
Iteration: 2 || Loss: 19.849039382857327
Iteration: 3 || Loss: 19.848565379165077
Iteration: 4 || Loss: 19.848094448270757
Iteration: 5 || Loss: 19.84762362820903
Iteration: 6 || Loss: 19.84762362820903
saving ADAM checkpoint...
Sum of params:85.114944
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.84762362820903
Iteration: 2 || Loss: 19.79339079753486
Iteration: 3 || Loss: 19.581601177634553
Iteration: 4 || Loss: 19.47219794800419
Iteration: 5 || Loss: 19.443731533533636
Iteration: 6 || Loss: 19.424180191128475
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.00973
Epoch 430 loss:19.424180191128475
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.912026069024315
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.026562280366406
waveform batch: 2/2
Test loss - extrapolation:16.276595994035883
Epoch 430 mean train loss:0.93585797871217
Epoch 430 mean test loss - interpolation:0.9853376781707192
Epoch 430 mean test loss - extrapolation:4.775263189533525
Start training epoch 431
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.00973
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.725847814637751
Iteration: 2 || Loss: 2.724317407552103
Iteration: 3 || Loss: 2.7227820901720055
Iteration: 4 || Loss: 2.72125115543602
Iteration: 5 || Loss: 2.7197230082585424
Iteration: 6 || Loss: 2.7197230082585424
saving ADAM checkpoint...
Sum of params:85.009735
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7197230082585424
Iteration: 2 || Loss: 2.1082584555389365
Iteration: 3 || Loss: 1.6723109596658026
Iteration: 4 || Loss: 1.6697925628534664
Iteration: 5 || Loss: 1.6463382086296536
Iteration: 6 || Loss: 1.645595377838213
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.080986
Epoch 431 loss:1.645595377838213
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.080986
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.288673864717036
Iteration: 2 || Loss: 6.28826240830595
Iteration: 3 || Loss: 6.287852104075014
Iteration: 4 || Loss: 6.287444304413813
Iteration: 5 || Loss: 6.287037534652835
Iteration: 6 || Loss: 6.287037534652835
saving ADAM checkpoint...
Sum of params:85.08104
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.287037534652835
Iteration: 2 || Loss: 6.2512659823440195
Iteration: 3 || Loss: 6.185414400780244
Iteration: 4 || Loss: 6.082579760366585
Iteration: 5 || Loss: 6.075160290089571
Iteration: 6 || Loss: 6.067297734156034
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.124725
Epoch 431 loss:6.067297734156034
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.124725
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.838865634874185
Iteration: 2 || Loss: 19.838390069395395
Iteration: 3 || Loss: 19.837916668965523
Iteration: 4 || Loss: 19.837445212762304
Iteration: 5 || Loss: 19.836975276011678
Iteration: 6 || Loss: 19.836975276011678
saving ADAM checkpoint...
Sum of params:85.124756
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.836975276011678
Iteration: 2 || Loss: 19.782658388348146
Iteration: 3 || Loss: 19.57075962335929
Iteration: 4 || Loss: 19.461471937577492
Iteration: 5 || Loss: 19.433020453405856
Iteration: 6 || Loss: 19.413519426563596
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.019485
Epoch 431 loss:19.413519426563596
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.909729566509688
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:41.01120144580133
waveform batch: 2/2
Test loss - extrapolation:16.266699875666053
Epoch 431 mean train loss:0.9353935358123394
Epoch 431 mean test loss - interpolation:0.9849549277516146
Epoch 431 mean test loss - extrapolation:4.773158443455615
Start training epoch 432
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.019485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.725120834583615
Iteration: 2 || Loss: 2.7235857648794553
Iteration: 3 || Loss: 2.7220520366684515
Iteration: 4 || Loss: 2.7205195149808943
Iteration: 5 || Loss: 2.718989837497787
Iteration: 6 || Loss: 2.718989837497787
saving ADAM checkpoint...
Sum of params:85.01949
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.718989837497787
Iteration: 2 || Loss: 2.107244406870222
Iteration: 3 || Loss: 1.6715451899211589
Iteration: 4 || Loss: 1.6690242359510403
Iteration: 5 || Loss: 1.6456172114316543
Iteration: 6 || Loss: 1.6448756483029614
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.090645
Epoch 432 loss:1.6448756483029614
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.090645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.286522152735474
Iteration: 2 || Loss: 6.286108202210605
Iteration: 3 || Loss: 6.285698721980103
Iteration: 4 || Loss: 6.285289820786438
Iteration: 5 || Loss: 6.28488176802571
Iteration: 6 || Loss: 6.28488176802571
saving ADAM checkpoint...
Sum of params:85.0907
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.28488176802571
Iteration: 2 || Loss: 6.248989543091252
Iteration: 3 || Loss: 6.18326093929019
Iteration: 4 || Loss: 6.080494080374144
Iteration: 5 || Loss: 6.073068784009033
Iteration: 6 || Loss: 6.0652255219334235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.1345
Epoch 432 loss:6.0652255219334235
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.1345
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.828166777969493
Iteration: 2 || Loss: 19.82769184317355
Iteration: 3 || Loss: 19.82721703712784
Iteration: 4 || Loss: 19.826744030969483
Iteration: 5 || Loss: 19.826273145699002
Iteration: 6 || Loss: 19.826273145699002
saving ADAM checkpoint...
Sum of params:85.13452
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.826273145699002
Iteration: 2 || Loss: 19.77184427511968
Iteration: 3 || Loss: 19.5598865759709
Iteration: 4 || Loss: 19.450755906626718
Iteration: 5 || Loss: 19.422329243384574
Iteration: 6 || Loss: 19.402880219133642
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.029205
Epoch 432 loss:19.402880219133642
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.90741106274892
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.99582176334745
waveform batch: 2/2
Test loss - extrapolation:16.25683153531507
Epoch 432 mean train loss:0.9349303927368975
Epoch 432 mean test loss - interpolation:0.9845685104581533
Epoch 432 mean test loss - extrapolation:4.7710544415552105
Start training epoch 433
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.029205
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.724277695912725
Iteration: 2 || Loss: 2.722741537905227
Iteration: 3 || Loss: 2.7212079652691514
Iteration: 4 || Loss: 2.7196766101129017
Iteration: 5 || Loss: 2.7181472350111613
Iteration: 6 || Loss: 2.7181472350111613
saving ADAM checkpoint...
Sum of params:85.02922
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7181472350111613
Iteration: 2 || Loss: 2.106189641630542
Iteration: 3 || Loss: 1.6707841406055057
Iteration: 4 || Loss: 1.668261564988567
Iteration: 5 || Loss: 1.6449025859378672
Iteration: 6 || Loss: 1.6441634952459705
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.10026
Epoch 433 loss:1.6441634952459705
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.10026
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.284258854461712
Iteration: 2 || Loss: 6.283846874884036
Iteration: 3 || Loss: 6.283435448293018
Iteration: 4 || Loss: 6.283027044990599
Iteration: 5 || Loss: 6.282618426920061
Iteration: 6 || Loss: 6.282618426920061
saving ADAM checkpoint...
Sum of params:85.100296
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.282618426920061
Iteration: 2 || Loss: 6.246749632474931
Iteration: 3 || Loss: 6.181167533282996
Iteration: 4 || Loss: 6.078425806695361
Iteration: 5 || Loss: 6.070969912828357
Iteration: 6 || Loss: 6.063148876182094
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.14426
Epoch 433 loss:6.063148876182094
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.14426
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.81761163060048
Iteration: 2 || Loss: 19.817136650841114
Iteration: 3 || Loss: 19.81666106129643
Iteration: 4 || Loss: 19.81618786352049
Iteration: 5 || Loss: 19.815715918315682
Iteration: 6 || Loss: 19.815715918315682
saving ADAM checkpoint...
Sum of params:85.14429
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.815715918315682
Iteration: 2 || Loss: 19.761258834251905
Iteration: 3 || Loss: 19.549158644638492
Iteration: 4 || Loss: 19.4401204345711
Iteration: 5 || Loss: 19.411698197358884
Iteration: 6 || Loss: 19.392286489982993
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.03894
Epoch 433 loss:19.392286489982993
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.905130445340163
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.98060301697749
waveform batch: 2/2
Test loss - extrapolation:16.246993500050287
Epoch 433 mean train loss:0.9344689262555537
Epoch 433 mean test loss - interpolation:0.9841884075566938
Epoch 433 mean test loss - extrapolation:4.768966376418981
Start training epoch 434
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.03894
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.723641353319194
Iteration: 2 || Loss: 2.722104419135704
Iteration: 3 || Loss: 2.7205711058873323
Iteration: 4 || Loss: 2.7190359624170166
Iteration: 5 || Loss: 2.7175073492232045
Iteration: 6 || Loss: 2.7175073492232045
saving ADAM checkpoint...
Sum of params:85.038925
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7175073492232045
Iteration: 2 || Loss: 2.1051119316455864
Iteration: 3 || Loss: 1.6700354623175404
Iteration: 4 || Loss: 1.6675089357980837
Iteration: 5 || Loss: 1.64419309479587
Iteration: 6 || Loss: 1.6434542303337678
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.10989
Epoch 434 loss:1.6434542303337678
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.10989
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.282214336982538
Iteration: 2 || Loss: 6.281801001568559
Iteration: 3 || Loss: 6.281389353796164
Iteration: 4 || Loss: 6.280979047427878
Iteration: 5 || Loss: 6.280571197946797
Iteration: 6 || Loss: 6.280571197946797
saving ADAM checkpoint...
Sum of params:85.109924
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.280571197946797
Iteration: 2 || Loss: 6.244505485106969
Iteration: 3 || Loss: 6.1790251778752
Iteration: 4 || Loss: 6.076363791427928
Iteration: 5 || Loss: 6.06891260205418
Iteration: 6 || Loss: 6.061107291400243
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.153946
Epoch 434 loss:6.061107291400243
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.153946
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.8069296284103
Iteration: 2 || Loss: 19.80645119314062
Iteration: 3 || Loss: 19.805976865715053
Iteration: 4 || Loss: 19.80550240118509
Iteration: 5 || Loss: 19.805030359375294
Iteration: 6 || Loss: 19.805030359375294
saving ADAM checkpoint...
Sum of params:85.15399
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.805030359375294
Iteration: 2 || Loss: 19.750396532238387
Iteration: 3 || Loss: 19.53828852318631
Iteration: 4 || Loss: 19.429438152461834
Iteration: 5 || Loss: 19.401054470827795
Iteration: 6 || Loss: 19.38170951104446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.04863
Epoch 434 loss:19.38170951104446
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.902833746869709
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.96519555834439
waveform batch: 2/2
Test loss - extrapolation:16.237169928145544
Epoch 434 mean train loss:0.9340093459578782
Epoch 434 mean test loss - interpolation:0.9838056244782849
Epoch 434 mean test loss - extrapolation:4.766863790540828
Start training epoch 435
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.04863
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7226743350439606
Iteration: 2 || Loss: 2.7211383292761515
Iteration: 3 || Loss: 2.7196063478842203
Iteration: 4 || Loss: 2.718073729394628
Iteration: 5 || Loss: 2.716539007423618
Iteration: 6 || Loss: 2.716539007423618
saving ADAM checkpoint...
Sum of params:85.04862
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.716539007423618
Iteration: 2 || Loss: 2.104163199848074
Iteration: 3 || Loss: 1.669272195923239
Iteration: 4 || Loss: 1.6667422780284307
Iteration: 5 || Loss: 1.6434831479528305
Iteration: 6 || Loss: 1.6427472664349883
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.11947
Epoch 435 loss:1.6427472664349883
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.11947
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.279973516484598
Iteration: 2 || Loss: 6.2795590805374095
Iteration: 3 || Loss: 6.279147819865939
Iteration: 4 || Loss: 6.278736865564441
Iteration: 5 || Loss: 6.278329144046138
Iteration: 6 || Loss: 6.278329144046138
saving ADAM checkpoint...
Sum of params:85.1195
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.278329144046138
Iteration: 2 || Loss: 6.242260681429494
Iteration: 3 || Loss: 6.176928916865355
Iteration: 4 || Loss: 6.074314768377345
Iteration: 5 || Loss: 6.066838194309814
Iteration: 6 || Loss: 6.059051539322256
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.16366
Epoch 435 loss:6.059051539322256
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.16366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.796432548308733
Iteration: 2 || Loss: 19.79595725802796
Iteration: 3 || Loss: 19.795479288910368
Iteration: 4 || Loss: 19.795006708055286
Iteration: 5 || Loss: 19.794533654447896
Iteration: 6 || Loss: 19.794533654447896
saving ADAM checkpoint...
Sum of params:85.16371
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.794533654447896
Iteration: 2 || Loss: 19.73986083530723
Iteration: 3 || Loss: 19.5276222525864
Iteration: 4 || Loss: 19.4188739332379
Iteration: 5 || Loss: 19.390494294781355
Iteration: 6 || Loss: 19.371185090132432
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.05827
Epoch 435 loss:19.371185090132432
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.900567787294261
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.95004471845475
waveform batch: 2/2
Test loss - extrapolation:16.227393331814117
Epoch 435 mean train loss:0.933551168823782
Epoch 435 mean test loss - interpolation:0.9834279645490435
Epoch 435 mean test loss - extrapolation:4.764786504189072
Start training epoch 436
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.05827
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7220255990918805
Iteration: 2 || Loss: 2.7204894040962917
Iteration: 3 || Loss: 2.718954566079603
Iteration: 4 || Loss: 2.7174185385853233
Iteration: 5 || Loss: 2.7158841024956013
Iteration: 6 || Loss: 2.7158841024956013
saving ADAM checkpoint...
Sum of params:85.05827
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7158841024956013
Iteration: 2 || Loss: 2.103119935140469
Iteration: 3 || Loss: 1.6685315782628025
Iteration: 4 || Loss: 1.6659980560242396
Iteration: 5 || Loss: 1.6427821412630232
Iteration: 6 || Loss: 1.642047591015743
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.12901
Epoch 436 loss:1.642047591015743
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.12901
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2778756618046145
Iteration: 2 || Loss: 6.277460561798821
Iteration: 3 || Loss: 6.2770495353670155
Iteration: 4 || Loss: 6.2766379317485
Iteration: 5 || Loss: 6.2762285574904775
Iteration: 6 || Loss: 6.2762285574904775
saving ADAM checkpoint...
Sum of params:85.12905
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.2762285574904775
Iteration: 2 || Loss: 6.240057025174563
Iteration: 3 || Loss: 6.174834460662041
Iteration: 4 || Loss: 6.072273479066266
Iteration: 5 || Loss: 6.064786878404726
Iteration: 6 || Loss: 6.057016787643132
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.17333
Epoch 436 loss:6.057016787643132
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.17333
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.78588549080399
Iteration: 2 || Loss: 19.78540992576194
Iteration: 3 || Loss: 19.78493237978022
Iteration: 4 || Loss: 19.784457665240904
Iteration: 5 || Loss: 19.783984147443615
Iteration: 6 || Loss: 19.783984147443615
saving ADAM checkpoint...
Sum of params:85.17338
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.783984147443615
Iteration: 2 || Loss: 19.72920148503022
Iteration: 3 || Loss: 19.516918614009644
Iteration: 4 || Loss: 19.40829958709948
Iteration: 5 || Loss: 19.37994266478978
Iteration: 6 || Loss: 19.36068508278067
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.067924
Epoch 436 loss:19.36068508278067
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.89827705366579
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.93479383930615
waveform batch: 2/2
Test loss - extrapolation:16.217640888248823
Epoch 436 mean train loss:0.9330948090151567
Epoch 436 mean test loss - interpolation:0.9830461756109651
Epoch 436 mean test loss - extrapolation:4.762702893962914
Start training epoch 437
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.067924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7211539656695876
Iteration: 2 || Loss: 2.7196154719187664
Iteration: 3 || Loss: 2.718080199836604
Iteration: 4 || Loss: 2.7165439361912913
Iteration: 5 || Loss: 2.715011412277434
Iteration: 6 || Loss: 2.715011412277434
saving ADAM checkpoint...
Sum of params:85.067924
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.715011412277434
Iteration: 2 || Loss: 2.102088922272812
Iteration: 3 || Loss: 1.6677871862643137
Iteration: 4 || Loss: 1.665251503818215
Iteration: 5 || Loss: 1.6420810456397978
Iteration: 6 || Loss: 1.6413479596782476
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.13855
Epoch 437 loss:1.6413479596782476
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.13855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.275699417186697
Iteration: 2 || Loss: 6.275285578677582
Iteration: 3 || Loss: 6.274871490692337
Iteration: 4 || Loss: 6.274461259732616
Iteration: 5 || Loss: 6.274051446879275
Iteration: 6 || Loss: 6.274051446879275
saving ADAM checkpoint...
Sum of params:85.13859
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.274051446879275
Iteration: 2 || Loss: 6.2378237413356405
Iteration: 3 || Loss: 6.172752228366512
Iteration: 4 || Loss: 6.0702377054296335
Iteration: 5 || Loss: 6.062730490639424
Iteration: 6 || Loss: 6.0549847695583425
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.183
Epoch 437 loss:6.0549847695583425
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.183
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.775446728407523
Iteration: 2 || Loss: 19.774968736636108
Iteration: 3 || Loss: 19.77449161967476
Iteration: 4 || Loss: 19.774016923401323
Iteration: 5 || Loss: 19.773541322679733
Iteration: 6 || Loss: 19.773541322679733
saving ADAM checkpoint...
Sum of params:85.18303
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.773541322679733
Iteration: 2 || Loss: 19.718691476945583
Iteration: 3 || Loss: 19.506279307401893
Iteration: 4 || Loss: 19.397781239850925
Iteration: 5 || Loss: 19.369437687782042
Iteration: 6 || Loss: 19.350220100924243
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.077515
Epoch 437 loss:19.350220100924243
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.896028049109295
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.91968291130568
waveform batch: 2/2
Test loss - extrapolation:16.207911605867466
Epoch 437 mean train loss:0.9326397527641667
Epoch 437 mean test loss - interpolation:0.9826713415182158
Epoch 437 mean test loss - extrapolation:4.760632876431095
Start training epoch 438
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.077515
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.720466296801223
Iteration: 2 || Loss: 2.718925865129615
Iteration: 3 || Loss: 2.717388440564466
Iteration: 4 || Loss: 2.715852717848716
Iteration: 5 || Loss: 2.714319999094354
Iteration: 6 || Loss: 2.714319999094354
saving ADAM checkpoint...
Sum of params:85.07752
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.714319999094354
Iteration: 2 || Loss: 2.101081101998191
Iteration: 3 || Loss: 1.6670511040353408
Iteration: 4 || Loss: 1.6645119293260513
Iteration: 5 || Loss: 1.6413874195497453
Iteration: 6 || Loss: 1.6406548224465667
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.14805
Epoch 438 loss:1.6406548224465667
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.14805
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.273631328572735
Iteration: 2 || Loss: 6.273216679122396
Iteration: 3 || Loss: 6.272802183256024
Iteration: 4 || Loss: 6.272391722976367
Iteration: 5 || Loss: 6.271979487952831
Iteration: 6 || Loss: 6.271979487952831
saving ADAM checkpoint...
Sum of params:85.148094
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.271979487952831
Iteration: 2 || Loss: 6.23563374355521
Iteration: 3 || Loss: 6.170673285208778
Iteration: 4 || Loss: 6.068217272705825
Iteration: 5 || Loss: 6.060700391034902
Iteration: 6 || Loss: 6.052969334042643
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.192604
Epoch 438 loss:6.052969334042643
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.192604
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.76498996042188
Iteration: 2 || Loss: 19.764510701883516
Iteration: 3 || Loss: 19.76403423863647
Iteration: 4 || Loss: 19.763558438776453
Iteration: 5 || Loss: 19.763082726260304
Iteration: 6 || Loss: 19.763082726260304
saving ADAM checkpoint...
Sum of params:85.19264
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.763082726260304
Iteration: 2 || Loss: 19.708119690069694
Iteration: 3 || Loss: 19.49563732256956
Iteration: 4 || Loss: 19.38727200606312
Iteration: 5 || Loss: 19.358951992453587
Iteration: 6 || Loss: 19.33978968659681
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.08711
Epoch 438 loss:19.33978968659681
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.893795045586585
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.90452669216869
waveform batch: 2/2
Test loss - extrapolation:16.198190679878547
Epoch 438 mean train loss:0.9321866842443455
Epoch 438 mean test loss - interpolation:0.9822991742644308
Epoch 438 mean test loss - extrapolation:4.758559781003936
Start training epoch 439
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.08711
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7197043046662595
Iteration: 2 || Loss: 2.718160295866039
Iteration: 3 || Loss: 2.716621913996031
Iteration: 4 || Loss: 2.715085256839749
Iteration: 5 || Loss: 2.713549308327755
Iteration: 6 || Loss: 2.713549308327755
saving ADAM checkpoint...
Sum of params:85.08711
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.713549308327755
Iteration: 2 || Loss: 2.1001143066735755
Iteration: 3 || Loss: 1.6663120563547158
Iteration: 4 || Loss: 1.6637698671907142
Iteration: 5 || Loss: 1.6406927088344483
Iteration: 6 || Loss: 1.639962312719235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.15755
Epoch 439 loss:1.639962312719235
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.15755
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2715362509512165
Iteration: 2 || Loss: 6.271119830563474
Iteration: 3 || Loss: 6.270704123461601
Iteration: 4 || Loss: 6.2702915209948875
Iteration: 5 || Loss: 6.269880662455916
Iteration: 6 || Loss: 6.269880662455916
saving ADAM checkpoint...
Sum of params:85.15759
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.269880662455916
Iteration: 2 || Loss: 6.233422506001472
Iteration: 3 || Loss: 6.168583845504214
Iteration: 4 || Loss: 6.066200740560701
Iteration: 5 || Loss: 6.058676188730563
Iteration: 6 || Loss: 6.0509615219731385
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.202194
Epoch 439 loss:6.0509615219731385
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.202194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.75454638923521
Iteration: 2 || Loss: 19.75406541875874
Iteration: 3 || Loss: 19.753589129616426
Iteration: 4 || Loss: 19.753111955264874
Iteration: 5 || Loss: 19.75263757804465
Iteration: 6 || Loss: 19.75263757804465
saving ADAM checkpoint...
Sum of params:85.20223
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.75263757804465
Iteration: 2 || Loss: 19.697562215322325
Iteration: 3 || Loss: 19.485009977271726
Iteration: 4 || Loss: 19.37680043045564
Iteration: 5 || Loss: 19.348501877570328
Iteration: 6 || Loss: 19.32938535738341
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.09665
Epoch 439 loss:19.32938535738341
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.891539856942041
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.88941633961968
waveform batch: 2/2
Test loss - extrapolation:16.188511702385096
Epoch 439 mean train loss:0.9317347997267511
Epoch 439 mean test loss - interpolation:0.9819233094903401
Epoch 439 mean test loss - extrapolation:4.756494003500397
Start training epoch 440
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.09665
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.718896928071062
Iteration: 2 || Loss: 2.717354643735106
Iteration: 3 || Loss: 2.7158151720276003
Iteration: 4 || Loss: 2.714278841500329
Iteration: 5 || Loss: 2.7127401240697973
Iteration: 6 || Loss: 2.7127401240697973
saving ADAM checkpoint...
Sum of params:85.096664
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7127401240697973
Iteration: 2 || Loss: 2.099113983144389
Iteration: 3 || Loss: 1.6655828931722398
Iteration: 4 || Loss: 1.6630380031958292
Iteration: 5 || Loss: 1.6400051781637925
Iteration: 6 || Loss: 1.6392762571209776
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.16697
Epoch 440 loss:1.6392762571209776
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.16697
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.26930978984401
Iteration: 2 || Loss: 6.2688929311577395
Iteration: 3 || Loss: 6.268478458481446
Iteration: 4 || Loss: 6.268066767832249
Iteration: 5 || Loss: 6.267655934803519
Iteration: 6 || Loss: 6.267655934803519
saving ADAM checkpoint...
Sum of params:85.16702
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.267655934803519
Iteration: 2 || Loss: 6.231246036073711
Iteration: 3 || Loss: 6.1665698856105
Iteration: 4 || Loss: 6.06419964643689
Iteration: 5 || Loss: 6.056639738613758
Iteration: 6 || Loss: 6.0489490726441195
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.21179
Epoch 440 loss:6.0489490726441195
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.21179
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.7442162594021
Iteration: 2 || Loss: 19.743736632716452
Iteration: 3 || Loss: 19.743258147223198
Iteration: 4 || Loss: 19.742780330233337
Iteration: 5 || Loss: 19.742306666420962
Iteration: 6 || Loss: 19.742306666420962
saving ADAM checkpoint...
Sum of params:85.21184
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.742306666420962
Iteration: 2 || Loss: 19.687234458171233
Iteration: 3 || Loss: 19.474545918055192
Iteration: 4 || Loss: 19.36640861833436
Iteration: 5 || Loss: 19.33811310821369
Iteration: 6 || Loss: 19.319022534583205
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.10618
Epoch 440 loss:19.319022534583205
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.889322161462033
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.87449935200514
waveform batch: 2/2
Test loss - extrapolation:16.17887160058938
Epoch 440 mean train loss:0.9312844091154586
Epoch 440 mean test loss - interpolation:0.9815536935770055
Epoch 440 mean test loss - extrapolation:4.754447579382877
Start training epoch 441
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.10618
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7183446213964886
Iteration: 2 || Loss: 2.7167999749575853
Iteration: 3 || Loss: 2.7152620493863013
Iteration: 4 || Loss: 2.71372106302882
Iteration: 5 || Loss: 2.7121830589818265
Iteration: 6 || Loss: 2.7121830589818265
saving ADAM checkpoint...
Sum of params:85.10617
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7121830589818265
Iteration: 2 || Loss: 2.0980477682811935
Iteration: 3 || Loss: 1.664865397591312
Iteration: 4 || Loss: 1.6623173392082242
Iteration: 5 || Loss: 1.6393235034041311
Iteration: 6 || Loss: 1.6385956119514058
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.1764
Epoch 441 loss:1.6385956119514058
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.1764
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.267284951593554
Iteration: 2 || Loss: 6.266868916766931
Iteration: 3 || Loss: 6.266453552770116
Iteration: 4 || Loss: 6.266039499379237
Iteration: 5 || Loss: 6.265627315459205
Iteration: 6 || Loss: 6.265627315459205
saving ADAM checkpoint...
Sum of params:85.17645
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.265627315459205
Iteration: 2 || Loss: 6.2291105514933065
Iteration: 3 || Loss: 6.164529694578833
Iteration: 4 || Loss: 6.0622015329130425
Iteration: 5 || Loss: 6.054625478261684
Iteration: 6 || Loss: 6.046956460041799
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.22137
Epoch 441 loss:6.046956460041799
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.22137
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.733871772601873
Iteration: 2 || Loss: 19.733392493864255
Iteration: 3 || Loss: 19.732911043057733
Iteration: 4 || Loss: 19.732435453965547
Iteration: 5 || Loss: 19.73195943784523
Iteration: 6 || Loss: 19.73195943784523
saving ADAM checkpoint...
Sum of params:85.22139
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.73195943784523
Iteration: 2 || Loss: 19.676800295120902
Iteration: 3 || Loss: 19.464012241861735
Iteration: 4 || Loss: 19.35599597389455
Iteration: 5 || Loss: 19.32771909951519
Iteration: 6 || Loss: 19.308676231834973
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.1157
Epoch 441 loss:19.308676231834973
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.887097431368951
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.8594879239366
waveform batch: 2/2
Test loss - extrapolation:16.169235576056717
Epoch 441 mean train loss:0.9308354587526957
Epoch 441 mean test loss - interpolation:0.9811829052281585
Epoch 441 mean test loss - extrapolation:4.752393624999443
Start training epoch 442
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.1157
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7176167681949677
Iteration: 2 || Loss: 2.716071882339794
Iteration: 3 || Loss: 2.714528256159291
Iteration: 4 || Loss: 2.7129895429159583
Iteration: 5 || Loss: 2.7114501166154676
Iteration: 6 || Loss: 2.7114501166154676
saving ADAM checkpoint...
Sum of params:85.11569
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7114501166154676
Iteration: 2 || Loss: 2.0970449732362675
Iteration: 3 || Loss: 1.664146265026735
Iteration: 4 || Loss: 1.6615939294165665
Iteration: 5 || Loss: 1.6386448481383653
Iteration: 6 || Loss: 1.6379181396217093
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.18581
Epoch 442 loss:1.6379181396217093
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.18581
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.265230728370667
Iteration: 2 || Loss: 6.26481531606287
Iteration: 3 || Loss: 6.2643992314264665
Iteration: 4 || Loss: 6.263983317661272
Iteration: 5 || Loss: 6.263570848880723
Iteration: 6 || Loss: 6.263570848880723
saving ADAM checkpoint...
Sum of params:85.18586
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.263570848880723
Iteration: 2 || Loss: 6.226959363667292
Iteration: 3 || Loss: 6.162487723792507
Iteration: 4 || Loss: 6.060219269246793
Iteration: 5 || Loss: 6.052634121217288
Iteration: 6 || Loss: 6.044977567824409
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.23087
Epoch 442 loss:6.044977567824409
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.23087
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.72353699658219
Iteration: 2 || Loss: 19.72305511029703
Iteration: 3 || Loss: 19.7225775095689
Iteration: 4 || Loss: 19.72209815929401
Iteration: 5 || Loss: 19.721622350431744
Iteration: 6 || Loss: 19.721622350431744
saving ADAM checkpoint...
Sum of params:85.230896
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.721622350431744
Iteration: 2 || Loss: 19.666342080281684
Iteration: 3 || Loss: 19.45349615508405
Iteration: 4 || Loss: 19.34561101582414
Iteration: 5 || Loss: 19.317355955782954
Iteration: 6 || Loss: 19.29837072558822
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.125206
Epoch 442 loss:19.29837072558822
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.884880295971734
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.84444392939677
waveform batch: 2/2
Test loss - extrapolation:16.159621513812862
Epoch 442 mean train loss:0.9303884976908393
Epoch 442 mean test loss - interpolation:0.9808133826619557
Epoch 442 mean test loss - extrapolation:4.750338786934136
Start training epoch 443
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.125206
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7167930747626743
Iteration: 2 || Loss: 2.715249013982575
Iteration: 3 || Loss: 2.713706171753732
Iteration: 4 || Loss: 2.712165380228684
Iteration: 5 || Loss: 2.7106250375391356
Iteration: 6 || Loss: 2.7106250375391356
saving ADAM checkpoint...
Sum of params:85.12521
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7106250375391356
Iteration: 2 || Loss: 2.096086082875544
Iteration: 3 || Loss: 1.6634248395013147
Iteration: 4 || Loss: 1.6608681769895792
Iteration: 5 || Loss: 1.6379659993672386
Iteration: 6 || Loss: 1.6372401085607406
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.19521
Epoch 443 loss:1.6372401085607406
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.19521
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.263103212873698
Iteration: 2 || Loss: 6.262684107419926
Iteration: 3 || Loss: 6.262268847470627
Iteration: 4 || Loss: 6.26185264443717
Iteration: 5 || Loss: 6.261441354414876
Iteration: 6 || Loss: 6.261441354414876
saving ADAM checkpoint...
Sum of params:85.19525
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.261441354414876
Iteration: 2 || Loss: 6.2247802401873304
Iteration: 3 || Loss: 6.160453026945614
Iteration: 4 || Loss: 6.058237261393048
Iteration: 5 || Loss: 6.050634033278369
Iteration: 6 || Loss: 6.0429954700697115
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.24039
Epoch 443 loss:6.0429954700697115
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.24039
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.71326004216676
Iteration: 2 || Loss: 19.7127796704645
Iteration: 3 || Loss: 19.712298661963143
Iteration: 4 || Loss: 19.711820419880496
Iteration: 5 || Loss: 19.71134304335714
Iteration: 6 || Loss: 19.71134304335714
saving ADAM checkpoint...
Sum of params:85.240425
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.71134304335714
Iteration: 2 || Loss: 19.65599254042188
Iteration: 3 || Loss: 19.443049184493468
Iteration: 4 || Loss: 19.335284974643155
Iteration: 5 || Loss: 19.307045779231846
Iteration: 6 || Loss: 19.288095376503357
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.13467
Epoch 443 loss:19.288095376503357
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.882666399079536
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.829528343650644
waveform batch: 2/2
Test loss - extrapolation:16.150047749293705
Epoch 443 mean train loss:0.929942446728752
Epoch 443 mean test loss - interpolation:0.9804443998465894
Epoch 443 mean test loss - extrapolation:4.748298007745363
Start training epoch 444
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.13467
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.716091399253421
Iteration: 2 || Loss: 2.7145459065494917
Iteration: 3 || Loss: 2.713002812815193
Iteration: 4 || Loss: 2.711458657306399
Iteration: 5 || Loss: 2.709919487854717
Iteration: 6 || Loss: 2.709919487854717
saving ADAM checkpoint...
Sum of params:85.13467
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.709919487854717
Iteration: 2 || Loss: 2.095074665859317
Iteration: 3 || Loss: 1.6627144767281925
Iteration: 4 || Loss: 1.6601560289399093
Iteration: 5 || Loss: 1.6372936519667
Iteration: 6 || Loss: 1.6365682515389075
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.204575
Epoch 444 loss:1.6365682515389075
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.204575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.261045593536204
Iteration: 2 || Loss: 6.260627064733697
Iteration: 3 || Loss: 6.260209492516354
Iteration: 4 || Loss: 6.259794128366051
Iteration: 5 || Loss: 6.259381458659688
Iteration: 6 || Loss: 6.259381458659688
saving ADAM checkpoint...
Sum of params:85.20461
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.259381458659688
Iteration: 2 || Loss: 6.222649066953044
Iteration: 3 || Loss: 6.158441945529244
Iteration: 4 || Loss: 6.056268468996961
Iteration: 5 || Loss: 6.048648744822573
Iteration: 6 || Loss: 6.0410282823308465
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.24986
Epoch 444 loss:6.0410282823308465
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.24986
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.702998994238154
Iteration: 2 || Loss: 19.70251680231412
Iteration: 3 || Loss: 19.702036128245915
Iteration: 4 || Loss: 19.701557769363436
Iteration: 5 || Loss: 19.701080722984845
Iteration: 6 || Loss: 19.701080722984845
saving ADAM checkpoint...
Sum of params:85.24991
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.701080722984845
Iteration: 2 || Loss: 19.64564872314601
Iteration: 3 || Loss: 19.432628681992462
Iteration: 4 || Loss: 19.324980259222592
Iteration: 5 || Loss: 19.296757439497796
Iteration: 6 || Loss: 19.277851486014697
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.1441
Epoch 444 loss:19.277851486014697
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.880478661825861
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.81463905321195
waveform batch: 2/2
Test loss - extrapolation:16.14049003896739
Epoch 444 mean train loss:0.9294982075822225
Epoch 444 mean test loss - interpolation:0.9800797769709768
Epoch 444 mean test loss - extrapolation:4.746260757681611
Start training epoch 445
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.1441
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.715407438217726
Iteration: 2 || Loss: 2.7138586281519887
Iteration: 3 || Loss: 2.712315445950657
Iteration: 4 || Loss: 2.7107726045160034
Iteration: 5 || Loss: 2.70923230112218
Iteration: 6 || Loss: 2.70923230112218
saving ADAM checkpoint...
Sum of params:85.1441
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.70923230112218
Iteration: 2 || Loss: 2.0941070828942157
Iteration: 3 || Loss: 1.6620053847474656
Iteration: 4 || Loss: 1.6594410827955937
Iteration: 5 || Loss: 1.6366254471459853
Iteration: 6 || Loss: 1.63590064014279
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.21393
Epoch 445 loss:1.63590064014279
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.21393
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.259109070299809
Iteration: 2 || Loss: 6.258689853538892
Iteration: 3 || Loss: 6.258271128495292
Iteration: 4 || Loss: 6.257855653238421
Iteration: 5 || Loss: 6.2574406066232235
Iteration: 6 || Loss: 6.2574406066232235
saving ADAM checkpoint...
Sum of params:85.213974
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.2574406066232235
Iteration: 2 || Loss: 6.220506549956332
Iteration: 3 || Loss: 6.156398070964158
Iteration: 4 || Loss: 6.0543099360322765
Iteration: 5 || Loss: 6.046693316497069
Iteration: 6 || Loss: 6.039083970284658
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.25931
Epoch 445 loss:6.039083970284658
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.25931
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.692749082907152
Iteration: 2 || Loss: 19.692265210695393
Iteration: 3 || Loss: 19.691783842069547
Iteration: 4 || Loss: 19.691304585678413
Iteration: 5 || Loss: 19.690826403063312
Iteration: 6 || Loss: 19.690826403063312
saving ADAM checkpoint...
Sum of params:85.25934
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.690826403063312
Iteration: 2 || Loss: 19.635230427607855
Iteration: 3 || Loss: 19.422156214344263
Iteration: 4 || Loss: 19.3146720830865
Iteration: 5 || Loss: 19.286479386810274
Iteration: 6 || Loss: 19.267637626615887
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.153534
Epoch 445 loss:19.267637626615887
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.878279328636365
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.79964996841248
waveform batch: 2/2
Test loss - extrapolation:16.130947699997538
Epoch 445 mean train loss:0.9290559392083908
Epoch 445 mean test loss - interpolation:0.9797132214393942
Epoch 445 mean test loss - extrapolation:4.744216472367501
Start training epoch 446
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.153534
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7145219804025165
Iteration: 2 || Loss: 2.7129731132221857
Iteration: 3 || Loss: 2.711424828393189
Iteration: 4 || Loss: 2.7098824363345355
Iteration: 5 || Loss: 2.7083434035909053
Iteration: 6 || Loss: 2.7083434035909053
saving ADAM checkpoint...
Sum of params:85.153534
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7083434035909053
Iteration: 2 || Loss: 2.093196328910179
Iteration: 3 || Loss: 1.6612925598530126
Iteration: 4 || Loss: 1.6587258078213694
Iteration: 5 || Loss: 1.6359560455927782
Iteration: 6 || Loss: 1.6352326065582112
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.22322
Epoch 446 loss:1.6352326065582112
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.22322
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.256978217809399
Iteration: 2 || Loss: 6.256557806714671
Iteration: 3 || Loss: 6.2561394503404
Iteration: 4 || Loss: 6.255723782289246
Iteration: 5 || Loss: 6.255309132107482
Iteration: 6 || Loss: 6.255309132107482
saving ADAM checkpoint...
Sum of params:85.223274
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.255309132107482
Iteration: 2 || Loss: 6.21835791837592
Iteration: 3 || Loss: 6.1543972721222
Iteration: 4 || Loss: 6.052359584362177
Iteration: 5 || Loss: 6.04471873158691
Iteration: 6 || Loss: 6.037128224983541
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.26874
Epoch 446 loss:6.037128224983541
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.26874
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.68261098946302
Iteration: 2 || Loss: 19.682128398324195
Iteration: 3 || Loss: 19.681646866261993
Iteration: 4 || Loss: 19.681166238544034
Iteration: 5 || Loss: 19.680687055123183
Iteration: 6 || Loss: 19.680687055123183
saving ADAM checkpoint...
Sum of params:85.26877
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.680687055123183
Iteration: 2 || Loss: 19.62505138650415
Iteration: 3 || Loss: 19.411847653155654
Iteration: 4 || Loss: 19.30446068331732
Iteration: 5 || Loss: 19.276277258373078
Iteration: 6 || Loss: 19.257463459353634
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.1629
Epoch 446 loss:19.257463459353634
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.8761009848959525
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.78488225560524
waveform batch: 2/2
Test loss - extrapolation:16.12145467284916
Epoch 446 mean train loss:0.9286146307205305
Epoch 446 mean test loss - interpolation:0.9793501641493254
Epoch 446 mean test loss - extrapolation:4.7421947440378664
Start training epoch 447
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.1629
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7138779632610666
Iteration: 2 || Loss: 2.7123317577928243
Iteration: 3 || Loss: 2.7107848793939198
Iteration: 4 || Loss: 2.7092405671804713
Iteration: 5 || Loss: 2.7076968658684173
Iteration: 6 || Loss: 2.7076968658684173
saving ADAM checkpoint...
Sum of params:85.16291
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7076968658684173
Iteration: 2 || Loss: 2.092202932883605
Iteration: 3 || Loss: 1.6605966702534052
Iteration: 4 || Loss: 1.6580275308704966
Iteration: 5 || Loss: 1.6352944407648682
Iteration: 6 || Loss: 1.6345722216835723
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.2325
Epoch 447 loss:1.6345722216835723
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.2325
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.254846878501977
Iteration: 2 || Loss: 6.2544274797742005
Iteration: 3 || Loss: 6.2540096729429795
Iteration: 4 || Loss: 6.253594407355515
Iteration: 5 || Loss: 6.253179050027738
Iteration: 6 || Loss: 6.253179050027738
saving ADAM checkpoint...
Sum of params:85.23255
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.253179050027738
Iteration: 2 || Loss: 6.2162707023215305
Iteration: 3 || Loss: 6.152447372904922
Iteration: 4 || Loss: 6.050413256470311
Iteration: 5 || Loss: 6.042737841584966
Iteration: 6 || Loss: 6.0351708107324455
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.27817
Epoch 447 loss:6.0351708107324455
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.27817
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.672489558850437
Iteration: 2 || Loss: 19.6720039721733
Iteration: 3 || Loss: 19.671521418058155
Iteration: 4 || Loss: 19.671042868206634
Iteration: 5 || Loss: 19.670564626108547
Iteration: 6 || Loss: 19.670564626108547
saving ADAM checkpoint...
Sum of params:85.278206
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.670564626108547
Iteration: 2 || Loss: 19.614941812452727
Iteration: 3 || Loss: 19.401604400069736
Iteration: 4 || Loss: 19.294289241168954
Iteration: 5 || Loss: 19.266105040032294
Iteration: 6 || Loss: 19.247312596048822
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.172264
Epoch 447 loss:19.247312596048822
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.87393362537962
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.77023337099804
waveform batch: 2/2
Test loss - extrapolation:16.111986232035505
Epoch 447 mean train loss:0.928174332016029
Epoch 447 mean test loss - interpolation:0.97898893756327
Epoch 447 mean test loss - extrapolation:4.740184966919462
Start training epoch 448
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.172264
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7133916595461445
Iteration: 2 || Loss: 2.7118395458699918
Iteration: 3 || Loss: 2.710294754209193
Iteration: 4 || Loss: 2.7087492668230917
Iteration: 5 || Loss: 2.707202625341634
Iteration: 6 || Loss: 2.707202625341634
saving ADAM checkpoint...
Sum of params:85.172264
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.707202625341634
Iteration: 2 || Loss: 2.0911112632657756
Iteration: 3 || Loss: 1.6599117565462802
Iteration: 4 || Loss: 1.6573381723583962
Iteration: 5 || Loss: 1.6346400447649259
Iteration: 6 || Loss: 1.6339176899332721
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.24177
Epoch 448 loss:1.6339176899332721
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.24177
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.252972118404351
Iteration: 2 || Loss: 6.2525511524700015
Iteration: 3 || Loss: 6.252131725006557
Iteration: 4 || Loss: 6.251715402680859
Iteration: 5 || Loss: 6.2512997674465325
Iteration: 6 || Loss: 6.2512997674465325
saving ADAM checkpoint...
Sum of params:85.24182
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.2512997674465325
Iteration: 2 || Loss: 6.21418315733223
Iteration: 3 || Loss: 6.150447518758512
Iteration: 4 || Loss: 6.048482943335337
Iteration: 5 || Loss: 6.040811636162171
Iteration: 6 || Loss: 6.033255215151991
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.287544
Epoch 448 loss:6.033255215151991
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.287544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.662301585616163
Iteration: 2 || Loss: 19.661818373642156
Iteration: 3 || Loss: 19.661334309803472
Iteration: 4 || Loss: 19.660853760876844
Iteration: 5 || Loss: 19.66037451242709
Iteration: 6 || Loss: 19.66037451242709
saving ADAM checkpoint...
Sum of params:85.28757
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.66037451242709
Iteration: 2 || Loss: 19.604562601587716
Iteration: 3 || Loss: 19.39120208180508
Iteration: 4 || Loss: 19.284049801047825
Iteration: 5 || Loss: 19.255904385988185
Iteration: 6 || Loss: 19.23718218748758
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.18161
Epoch 448 loss:19.23718218748758
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.871757483942091
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.75531474434422
waveform batch: 2/2
Test loss - extrapolation:16.102516252914906
Epoch 448 mean train loss:0.9277363825025118
Epoch 448 mean test loss - interpolation:0.9786262473236819
Epoch 448 mean test loss - extrapolation:4.738152583104927
Start training epoch 449
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.18161
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7124669641849453
Iteration: 2 || Loss: 2.7109184701332087
Iteration: 3 || Loss: 2.7093689892450206
Iteration: 4 || Loss: 2.7078215605071936
Iteration: 5 || Loss: 2.706277036838683
Iteration: 6 || Loss: 2.706277036838683
saving ADAM checkpoint...
Sum of params:85.1816
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.706277036838683
Iteration: 2 || Loss: 2.0902345204169626
Iteration: 3 || Loss: 1.6592051243056434
Iteration: 4 || Loss: 1.656627973941892
Iteration: 5 || Loss: 1.633980666035717
Iteration: 6 || Loss: 1.6332589669345254
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.25102
Epoch 449 loss:1.6332589669345254
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.25102
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.25091264102165
Iteration: 2 || Loss: 6.250491761593201
Iteration: 3 || Loss: 6.250071760087059
Iteration: 4 || Loss: 6.249653477977019
Iteration: 5 || Loss: 6.249236699908414
Iteration: 6 || Loss: 6.249236699908414
saving ADAM checkpoint...
Sum of params:85.25105
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.249236699908414
Iteration: 2 || Loss: 6.21204242822839
Iteration: 3 || Loss: 6.148450882215462
Iteration: 4 || Loss: 6.046558064656507
Iteration: 5 || Loss: 6.038870009853376
Iteration: 6 || Loss: 6.031329073491623
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.29689
Epoch 449 loss:6.031329073491623
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.29689
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.65223064902279
Iteration: 2 || Loss: 19.651746251960713
Iteration: 3 || Loss: 19.651262970164517
Iteration: 4 || Loss: 19.650781354527595
Iteration: 5 || Loss: 19.650301393701135
Iteration: 6 || Loss: 19.650301393701135
saving ADAM checkpoint...
Sum of params:85.29692
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.650301393701135
Iteration: 2 || Loss: 19.594417922594577
Iteration: 3 || Loss: 19.38095711969462
Iteration: 4 || Loss: 19.273916138331295
Iteration: 5 || Loss: 19.245782196695906
Iteration: 6 || Loss: 19.227101269864114
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.19093
Epoch 449 loss:19.227101269864114
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.869608497145375
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.74059502929568
waveform batch: 2/2
Test loss - extrapolation:16.093083425779795
Epoch 449 mean train loss:0.9272996313893194
Epoch 449 mean test loss - interpolation:0.9782680828575625
Epoch 449 mean test loss - extrapolation:4.73613987125629
Start training epoch 450
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.19093
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7117881501821275
Iteration: 2 || Loss: 2.710237158396536
Iteration: 3 || Loss: 2.7086869626218264
Iteration: 4 || Loss: 2.7071387662914
Iteration: 5 || Loss: 2.70559453424091
Iteration: 6 || Loss: 2.70559453424091
saving ADAM checkpoint...
Sum of params:85.190926
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.70559453424091
Iteration: 2 || Loss: 2.0892821988618993
Iteration: 3 || Loss: 1.6585170507837272
Iteration: 4 || Loss: 1.6559357241612958
Iteration: 5 || Loss: 1.6333273776597967
Iteration: 6 || Loss: 1.6326056385851537
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.26021
Epoch 450 loss:1.6326056385851537
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.26021
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.248864353648968
Iteration: 2 || Loss: 6.248440612051462
Iteration: 3 || Loss: 6.248020985193588
Iteration: 4 || Loss: 6.247602602855128
Iteration: 5 || Loss: 6.247186727192877
Iteration: 6 || Loss: 6.247186727192877
saving ADAM checkpoint...
Sum of params:85.26026
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.247186727192877
Iteration: 2 || Loss: 6.209951114483251
Iteration: 3 || Loss: 6.1464970352552
Iteration: 4 || Loss: 6.04463868701908
Iteration: 5 || Loss: 6.0369313949973415
Iteration: 6 || Loss: 6.0294074781754
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.30621
Epoch 450 loss:6.0294074781754
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.30621
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.642180357213434
Iteration: 2 || Loss: 19.64169563147963
Iteration: 3 || Loss: 19.641212085431903
Iteration: 4 || Loss: 19.64072926273224
Iteration: 5 || Loss: 19.64024935079689
Iteration: 6 || Loss: 19.64024935079689
saving ADAM checkpoint...
Sum of params:85.306244
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.64024935079689
Iteration: 2 || Loss: 19.584311021154306
Iteration: 3 || Loss: 19.370748432477193
Iteration: 4 || Loss: 19.263814336467945
Iteration: 5 || Loss: 19.23569273720332
Iteration: 6 || Loss: 19.217042935547056
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.20021
Epoch 450 loss:19.217042935547056
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.867456069623882
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.72595001401086
waveform batch: 2/2
Test loss - extrapolation:16.083686778819896
Epoch 450 mean train loss:0.9268640018037108
Epoch 450 mean test loss - interpolation:0.9779093449373137
Epoch 450 mean test loss - extrapolation:4.734136399402563
Start training epoch 451
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.20021
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7111453401719228
Iteration: 2 || Loss: 2.7095927298836706
Iteration: 3 || Loss: 2.7080441657132313
Iteration: 4 || Loss: 2.7064968389051294
Iteration: 5 || Loss: 2.7049485757101666
Iteration: 6 || Loss: 2.7049485757101666
saving ADAM checkpoint...
Sum of params:85.20021
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7049485757101666
Iteration: 2 || Loss: 2.088283126859726
Iteration: 3 || Loss: 1.6578347633189006
Iteration: 4 || Loss: 1.6552496952384472
Iteration: 5 || Loss: 1.6326792310180958
Iteration: 6 || Loss: 1.6319585524352056
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.26941
Epoch 451 loss:1.6319585524352056
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.26941
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.246881749591294
Iteration: 2 || Loss: 6.246460417758674
Iteration: 3 || Loss: 6.246037243778888
Iteration: 4 || Loss: 6.245618955986369
Iteration: 5 || Loss: 6.245201346725376
Iteration: 6 || Loss: 6.245201346725376
saving ADAM checkpoint...
Sum of params:85.26944
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.245201346725376
Iteration: 2 || Loss: 6.207893330917932
Iteration: 3 || Loss: 6.144548693773162
Iteration: 4 || Loss: 6.042731556532677
Iteration: 5 || Loss: 6.035006516762433
Iteration: 6 || Loss: 6.027499432873753
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.31553
Epoch 451 loss:6.027499432873753
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.31553
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.632165561432036
Iteration: 2 || Loss: 19.63167919875016
Iteration: 3 || Loss: 19.63119428759659
Iteration: 4 || Loss: 19.630712072460753
Iteration: 5 || Loss: 19.630229767964675
Iteration: 6 || Loss: 19.630229767964675
saving ADAM checkpoint...
Sum of params:85.31555
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.630229767964675
Iteration: 2 || Loss: 19.574210456164135
Iteration: 3 || Loss: 19.36054876189986
Iteration: 4 || Loss: 19.253723336789918
Iteration: 5 || Loss: 19.225617257102662
Iteration: 6 || Loss: 19.20700928972499
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.20948
Epoch 451 loss:19.20700928972499
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.865304406444257
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.71128454741274
waveform batch: 2/2
Test loss - extrapolation:16.074307956271518
Epoch 451 mean train loss:0.9264299060356533
Epoch 451 mean test loss - interpolation:0.9775507344073762
Epoch 451 mean test loss - extrapolation:4.732132708640354
Start training epoch 452
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.20948
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.710427603647085
Iteration: 2 || Loss: 2.7088758888516256
Iteration: 3 || Loss: 2.70732208653958
Iteration: 4 || Loss: 2.7057748091430995
Iteration: 5 || Loss: 2.704226207260072
Iteration: 6 || Loss: 2.704226207260072
saving ADAM checkpoint...
Sum of params:85.209465
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.704226207260072
Iteration: 2 || Loss: 2.0873132621142143
Iteration: 3 || Loss: 1.6571516468028256
Iteration: 4 || Loss: 1.6545619928876847
Iteration: 5 || Loss: 1.6320332094131889
Iteration: 6 || Loss: 1.6313126794247843
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.27856
Epoch 452 loss:1.6313126794247843
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.27856
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.244910270324982
Iteration: 2 || Loss: 6.244487478734699
Iteration: 3 || Loss: 6.244066515497435
Iteration: 4 || Loss: 6.243646786682858
Iteration: 5 || Loss: 6.243228728962778
Iteration: 6 || Loss: 6.243228728962778
saving ADAM checkpoint...
Sum of params:85.27862
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.243228728962778
Iteration: 2 || Loss: 6.205818649337601
Iteration: 3 || Loss: 6.142594679459521
Iteration: 4 || Loss: 6.040831092702738
Iteration: 5 || Loss: 6.0330952167411525
Iteration: 6 || Loss: 6.025602832203589
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.3248
Epoch 452 loss:6.025602832203589
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.3248
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.622142840521967
Iteration: 2 || Loss: 19.621656302868498
Iteration: 3 || Loss: 19.621172138854284
Iteration: 4 || Loss: 19.620689079575605
Iteration: 5 || Loss: 19.6202095783841
Iteration: 6 || Loss: 19.6202095783841
saving ADAM checkpoint...
Sum of params:85.32483
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.6202095783841
Iteration: 2 || Loss: 19.56409436799983
Iteration: 3 || Loss: 19.35035299848968
Iteration: 4 || Loss: 19.243653435120795
Iteration: 5 || Loss: 19.215570493193937
Iteration: 6 || Loss: 19.197008382753584
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.218704
Epoch 452 loss:19.197008382753584
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.863203465413995
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.69666479820045
waveform batch: 2/2
Test loss - extrapolation:16.064933132527532
Epoch 452 mean train loss:0.9259973756683434
Epoch 452 mean test loss - interpolation:0.9772005775689991
Epoch 452 mean test loss - extrapolation:4.730133160893999
Start training epoch 453
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.218704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7098096363166264
Iteration: 2 || Loss: 2.7082564579301107
Iteration: 3 || Loss: 2.7067023213658055
Iteration: 4 || Loss: 2.7051515124702137
Iteration: 5 || Loss: 2.7036028184605865
Iteration: 6 || Loss: 2.7036028184605865
saving ADAM checkpoint...
Sum of params:85.218704
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7036028184605865
Iteration: 2 || Loss: 2.08641938403093
Iteration: 3 || Loss: 1.656471484818977
Iteration: 4 || Loss: 1.6538794688230571
Iteration: 5 || Loss: 1.6313898951607402
Iteration: 6 || Loss: 1.6306691816719312
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.28771
Epoch 453 loss:1.6306691816719312
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.28771
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.242897632435643
Iteration: 2 || Loss: 6.242472935267156
Iteration: 3 || Loss: 6.242050441929771
Iteration: 4 || Loss: 6.241630622255487
Iteration: 5 || Loss: 6.241212101303772
Iteration: 6 || Loss: 6.241212101303772
saving ADAM checkpoint...
Sum of params:85.287766
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.241212101303772
Iteration: 2 || Loss: 6.2037532851162895
Iteration: 3 || Loss: 6.140658314452903
Iteration: 4 || Loss: 6.038937327513336
Iteration: 5 || Loss: 6.03117943204186
Iteration: 6 || Loss: 6.023704899726122
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.334045
Epoch 453 loss:6.023704899726122
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.334045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.612191167666097
Iteration: 2 || Loss: 19.611705663387163
Iteration: 3 || Loss: 19.61122136854898
Iteration: 4 || Loss: 19.610736577975704
Iteration: 5 || Loss: 19.610256552938253
Iteration: 6 || Loss: 19.610256552938253
saving ADAM checkpoint...
Sum of params:85.33408
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.610256552938253
Iteration: 2 || Loss: 19.554089021702925
Iteration: 3 || Loss: 19.3402475587327
Iteration: 4 || Loss: 19.233640859511702
Iteration: 5 || Loss: 19.205569205881332
Iteration: 6 || Loss: 19.1870414956709
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.22792
Epoch 453 loss:19.1870414956709
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.861076528525018
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.68211347532686
waveform batch: 2/2
Test loss - extrapolation:16.0556025086179
Epoch 453 mean train loss:0.925566054381688
Epoch 453 mean test loss - interpolation:0.9768460880875031
Epoch 453 mean test loss - extrapolation:4.7281429986620624
Start training epoch 454
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.22792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7091882949779387
Iteration: 2 || Loss: 2.7076292504110056
Iteration: 3 || Loss: 2.7060730560803337
Iteration: 4 || Loss: 2.7045223017685203
Iteration: 5 || Loss: 2.7029750671285306
Iteration: 6 || Loss: 2.7029750671285306
saving ADAM checkpoint...
Sum of params:85.22792
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7029750671285306
Iteration: 2 || Loss: 2.0854260106610307
Iteration: 3 || Loss: 1.6557990374971334
Iteration: 4 || Loss: 1.6532008386315982
Iteration: 5 || Loss: 1.6307519076128423
Iteration: 6 || Loss: 1.6300321482970372
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.29682
Epoch 454 loss:1.6300321482970372
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.29682
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.240956992609942
Iteration: 2 || Loss: 6.24053258881853
Iteration: 3 || Loss: 6.240110296399247
Iteration: 4 || Loss: 6.239689470908511
Iteration: 5 || Loss: 6.239270144631954
Iteration: 6 || Loss: 6.239270144631954
saving ADAM checkpoint...
Sum of params:85.29686
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.239270144631954
Iteration: 2 || Loss: 6.201736632918058
Iteration: 3 || Loss: 6.138735255215585
Iteration: 4 || Loss: 6.037053879789628
Iteration: 5 || Loss: 6.029279139824372
Iteration: 6 || Loss: 6.021821272266661
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.343285
Epoch 454 loss:6.021821272266661
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.343285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.602258174679353
Iteration: 2 || Loss: 19.60177237987138
Iteration: 3 || Loss: 19.60128692283368
Iteration: 4 || Loss: 19.600802662069068
Iteration: 5 || Loss: 19.60032036888173
Iteration: 6 || Loss: 19.60032036888173
saving ADAM checkpoint...
Sum of params:85.34332
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.60032036888173
Iteration: 2 || Loss: 19.544069692205195
Iteration: 3 || Loss: 19.330136008509502
Iteration: 4 || Loss: 19.223636498554615
Iteration: 5 || Loss: 19.19557971815692
Iteration: 6 || Loss: 19.177091878703205
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.23714
Epoch 454 loss:19.177091878703205
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.858924936702593
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.66752003862297
waveform batch: 2/2
Test loss - extrapolation:16.046302243309906
Epoch 454 mean train loss:0.925136044802307
Epoch 454 mean test loss - interpolation:0.9764874894504322
Epoch 454 mean test loss - extrapolation:4.72615185682774
Start training epoch 455
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.23714
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.708409358810865
Iteration: 2 || Loss: 2.7068512086389354
Iteration: 3 || Loss: 2.7052972305475067
Iteration: 4 || Loss: 2.703741663339452
Iteration: 5 || Loss: 2.702192267600301
Iteration: 6 || Loss: 2.702192267600301
saving ADAM checkpoint...
Sum of params:85.237144
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.702192267600301
Iteration: 2 || Loss: 2.0844381659504108
Iteration: 3 || Loss: 1.6551283197184992
Iteration: 4 || Loss: 1.6525269052101008
Iteration: 5 || Loss: 1.6301157936390425
Iteration: 6 || Loss: 1.62939595187918
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.30592
Epoch 455 loss:1.62939595187918
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.30592
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.238986978483387
Iteration: 2 || Loss: 6.238561498367054
Iteration: 3 || Loss: 6.2381396073990585
Iteration: 4 || Loss: 6.2377186884066775
Iteration: 5 || Loss: 6.2372987558566075
Iteration: 6 || Loss: 6.2372987558566075
saving ADAM checkpoint...
Sum of params:85.305984
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.2372987558566075
Iteration: 2 || Loss: 6.199671507325385
Iteration: 3 || Loss: 6.136805569544646
Iteration: 4 || Loss: 6.035178911986935
Iteration: 5 || Loss: 6.027392856656356
Iteration: 6 || Loss: 6.019947930957957
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.352516
Epoch 455 loss:6.019947930957957
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.352516
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.592349382811243
Iteration: 2 || Loss: 19.591861186884334
Iteration: 3 || Loss: 19.59137712207359
Iteration: 4 || Loss: 19.590891931574607
Iteration: 5 || Loss: 19.590409033664034
Iteration: 6 || Loss: 19.590409033664034
saving ADAM checkpoint...
Sum of params:85.35254
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.590409033664034
Iteration: 2 || Loss: 19.534058661463796
Iteration: 3 || Loss: 19.320041863593712
Iteration: 4 || Loss: 19.21366026846046
Iteration: 5 || Loss: 19.185622507748608
Iteration: 6 || Loss: 19.167179717262357
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.246315
Epoch 455 loss:19.167179717262357
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.856824727793894
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.65296708447076
waveform batch: 2/2
Test loss - extrapolation:16.037007449704547
Epoch 455 mean train loss:0.9247077103482584
Epoch 455 mean test loss - interpolation:0.9761374546323157
Epoch 455 mean test loss - extrapolation:4.7241645445146085
Start training epoch 456
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.246315
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.707703905737068
Iteration: 2 || Loss: 2.7061475010024107
Iteration: 3 || Loss: 2.7045911531398863
Iteration: 4 || Loss: 2.7030349584174846
Iteration: 5 || Loss: 2.7014824129880672
Iteration: 6 || Loss: 2.7014824129880672
saving ADAM checkpoint...
Sum of params:85.24632
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7014824129880672
Iteration: 2 || Loss: 2.0835419992948907
Iteration: 3 || Loss: 1.6544567330553155
Iteration: 4 || Loss: 1.6518494773076147
Iteration: 5 || Loss: 1.6294812096016733
Iteration: 6 || Loss: 1.628761532310149
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.31501
Epoch 456 loss:1.628761532310149
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.31501
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.237001777966116
Iteration: 2 || Loss: 6.236576499747571
Iteration: 3 || Loss: 6.236153819104225
Iteration: 4 || Loss: 6.23573183509974
Iteration: 5 || Loss: 6.235311121317965
Iteration: 6 || Loss: 6.235311121317965
saving ADAM checkpoint...
Sum of params:85.31505
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.235311121317965
Iteration: 2 || Loss: 6.197630872440497
Iteration: 3 || Loss: 6.1348888497746055
Iteration: 4 || Loss: 6.033306986447901
Iteration: 5 || Loss: 6.025501039933541
Iteration: 6 || Loss: 6.018073615923235
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.36171
Epoch 456 loss:6.018073615923235
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.36171
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.582489550369953
Iteration: 2 || Loss: 19.582002249806727
Iteration: 3 || Loss: 19.581516595190944
Iteration: 4 || Loss: 19.581031677624097
Iteration: 5 || Loss: 19.580546148216374
Iteration: 6 || Loss: 19.580546148216374
saving ADAM checkpoint...
Sum of params:85.36173
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.580546148216374
Iteration: 2 || Loss: 19.52415063411654
Iteration: 3 || Loss: 19.310017768667784
Iteration: 4 || Loss: 19.20373208016378
Iteration: 5 || Loss: 19.17570639517079
Iteration: 6 || Loss: 19.157291569251772
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.25546
Epoch 456 loss:19.157291569251772
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.85470651662059
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.63851028933513
waveform batch: 2/2
Test loss - extrapolation:16.027751930398527
Epoch 456 mean train loss:0.9242802316374191
Epoch 456 mean test loss - interpolation:0.975784419436765
Epoch 456 mean test loss - extrapolation:4.722188518311138
Start training epoch 457
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.25546
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.707060650214945
Iteration: 2 || Loss: 2.7055025235578456
Iteration: 3 || Loss: 2.7039467007708136
Iteration: 4 || Loss: 2.7023938005371697
Iteration: 5 || Loss: 2.7008386964353703
Iteration: 6 || Loss: 2.7008386964353703
saving ADAM checkpoint...
Sum of params:85.25546
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7008386964353703
Iteration: 2 || Loss: 2.082547127771169
Iteration: 3 || Loss: 1.6537977533250072
Iteration: 4 || Loss: 1.651187831534245
Iteration: 5 || Loss: 1.6288528181620057
Iteration: 6 || Loss: 1.6281322927008928
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.32404
Epoch 457 loss:1.6281322927008928
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.32404
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.235027172612229
Iteration: 2 || Loss: 6.23460309438508
Iteration: 3 || Loss: 6.234178512128362
Iteration: 4 || Loss: 6.233757726183766
Iteration: 5 || Loss: 6.2333359947787015
Iteration: 6 || Loss: 6.2333359947787015
saving ADAM checkpoint...
Sum of params:85.32409
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.2333359947787015
Iteration: 2 || Loss: 6.195608863580442
Iteration: 3 || Loss: 6.132998399151666
Iteration: 4 || Loss: 6.031445896571305
Iteration: 5 || Loss: 6.023615525164285
Iteration: 6 || Loss: 6.016208023148838
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.37089
Epoch 457 loss:6.016208023148838
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.37089
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.57262661569821
Iteration: 2 || Loss: 19.57213908926202
Iteration: 3 || Loss: 19.571651835026973
Iteration: 4 || Loss: 19.57116693576687
Iteration: 5 || Loss: 19.57068444303335
Iteration: 6 || Loss: 19.57068444303335
saving ADAM checkpoint...
Sum of params:85.37092
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.57068444303335
Iteration: 2 || Loss: 19.514239247017475
Iteration: 3 || Loss: 19.300008510432047
Iteration: 4 || Loss: 19.193820926230185
Iteration: 5 || Loss: 19.165808977446513
Iteration: 6 || Loss: 19.147429615581913
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.26458
Epoch 457 loss:19.147429615581913
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.85264374419414
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.62413805888815
waveform batch: 2/2
Test loss - extrapolation:16.018504438021445
Epoch 457 mean train loss:0.9238541355666084
Epoch 457 mean test loss - interpolation:0.9754406240323567
Epoch 457 mean test loss - extrapolation:4.7202202080758
Start training epoch 458
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.26458
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7065722663853817
Iteration: 2 || Loss: 2.7050106225301858
Iteration: 3 || Loss: 2.703452401352846
Iteration: 4 || Loss: 2.7018960651745862
Iteration: 5 || Loss: 2.7003434518938367
Iteration: 6 || Loss: 2.7003434518938367
saving ADAM checkpoint...
Sum of params:85.26459
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7003434518938367
Iteration: 2 || Loss: 2.0816038902197036
Iteration: 3 || Loss: 1.6531396456600502
Iteration: 4 || Loss: 1.650525396008275
Iteration: 5 || Loss: 1.628226615189522
Iteration: 6 || Loss: 1.6275061204232044
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.33306
Epoch 458 loss:1.6275061204232044
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.33306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.233082816329785
Iteration: 2 || Loss: 6.232657452606666
Iteration: 3 || Loss: 6.232233227328849
Iteration: 4 || Loss: 6.231809679225432
Iteration: 5 || Loss: 6.231389070242864
Iteration: 6 || Loss: 6.231389070242864
saving ADAM checkpoint...
Sum of params:85.333115
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.231389070242864
Iteration: 2 || Loss: 6.193608347712254
Iteration: 3 || Loss: 6.131115126373904
Iteration: 4 || Loss: 6.029592956148824
Iteration: 5 || Loss: 6.021739904454145
Iteration: 6 || Loss: 6.014349644438341
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.38003
Epoch 458 loss:6.014349644438341
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.38003
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.562831042020928
Iteration: 2 || Loss: 19.56234117003477
Iteration: 3 || Loss: 19.56185515871888
Iteration: 4 || Loss: 19.56136929164111
Iteration: 5 || Loss: 19.56088468479949
Iteration: 6 || Loss: 19.56088468479949
saving ADAM checkpoint...
Sum of params:85.38007
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.56088468479949
Iteration: 2 || Loss: 19.504386597488608
Iteration: 3 || Loss: 19.29004052563291
Iteration: 4 || Loss: 19.183937991490065
Iteration: 5 || Loss: 19.15593633742661
Iteration: 6 || Loss: 19.13759060372163
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.2737
Epoch 458 loss:19.13759060372163
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.850538129447216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.609730922892304
waveform batch: 2/2
Test loss - extrapolation:16.009293184681155
Epoch 458 mean train loss:0.9234291851235578
Epoch 458 mean test loss - interpolation:0.9750896882412027
Epoch 458 mean test loss - extrapolation:4.718252008964455
Start training epoch 459
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.2737
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.705913786081294
Iteration: 2 || Loss: 2.704353320985552
Iteration: 3 || Loss: 2.702793485150306
Iteration: 4 || Loss: 2.7012374662746184
Iteration: 5 || Loss: 2.6996815080552095
Iteration: 6 || Loss: 2.6996815080552095
saving ADAM checkpoint...
Sum of params:85.273705
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6996815080552095
Iteration: 2 || Loss: 2.0806039339742437
Iteration: 3 || Loss: 1.652485422721041
Iteration: 4 || Loss: 1.6498664124154745
Iteration: 5 || Loss: 1.627605119431728
Iteration: 6 || Loss: 1.6268850365420464
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.34209
Epoch 459 loss:1.6268850365420464
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.34209
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.231288726056486
Iteration: 2 || Loss: 6.230861695649966
Iteration: 3 || Loss: 6.230436256913127
Iteration: 4 || Loss: 6.230013151673528
Iteration: 5 || Loss: 6.229591109196098
Iteration: 6 || Loss: 6.229591109196098
saving ADAM checkpoint...
Sum of params:85.34212
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.229591109196098
Iteration: 2 || Loss: 6.191608080559297
Iteration: 3 || Loss: 6.129196750605308
Iteration: 4 || Loss: 6.027752023510152
Iteration: 5 || Loss: 6.019904452030241
Iteration: 6 || Loss: 6.012520503647669
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.38912
Epoch 459 loss:6.012520503647669
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.38912
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.552967598857364
Iteration: 2 || Loss: 19.552477638425383
Iteration: 3 || Loss: 19.551989673122723
Iteration: 4 || Loss: 19.551502801925558
Iteration: 5 || Loss: 19.551017802457483
Iteration: 6 || Loss: 19.551017802457483
saving ADAM checkpoint...
Sum of params:85.38915
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.551017802457483
Iteration: 2 || Loss: 19.49434545835639
Iteration: 3 || Loss: 19.279978041048572
Iteration: 4 || Loss: 19.174029663135688
Iteration: 5 || Loss: 19.146062075550205
Iteration: 6 || Loss: 19.12777943362189
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.28281
Epoch 459 loss:19.12777943362189
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.8484466584842725
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.595174732132875
waveform batch: 2/2
Test loss - extrapolation:16.000074858515763
Epoch 459 mean train loss:0.9230063784072966
Epoch 459 mean test loss - interpolation:0.9747411097473787
Epoch 459 mean test loss - extrapolation:4.71627079922072
Start training epoch 460
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.28281
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7050347092525655
Iteration: 2 || Loss: 2.7034760191248557
Iteration: 3 || Loss: 2.70191427526398
Iteration: 4 || Loss: 2.700355677526111
Iteration: 5 || Loss: 2.698799363559777
Iteration: 6 || Loss: 2.698799363559777
saving ADAM checkpoint...
Sum of params:85.28282
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.698799363559777
Iteration: 2 || Loss: 2.0797740647627494
Iteration: 3 || Loss: 1.6518211015223334
Iteration: 4 || Loss: 1.649198787855053
Iteration: 5 || Loss: 1.626980926780187
Iteration: 6 || Loss: 1.6262605150795804
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.351105
Epoch 460 loss:1.6262605150795804
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.351105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.229360960187115
Iteration: 2 || Loss: 6.228933438032969
Iteration: 3 || Loss: 6.228507264913805
Iteration: 4 || Loss: 6.228081813312
Iteration: 5 || Loss: 6.227660037513344
Iteration: 6 || Loss: 6.227660037513344
saving ADAM checkpoint...
Sum of params:85.35113
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.227660037513344
Iteration: 2 || Loss: 6.189557990327208
Iteration: 3 || Loss: 6.127280381541713
Iteration: 4 || Loss: 6.025916752710249
Iteration: 5 || Loss: 6.018064539373077
Iteration: 6 || Loss: 6.010688954169188
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.39821
Epoch 460 loss:6.010688954169188
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.39821
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.54317418957923
Iteration: 2 || Loss: 19.542684366929574
Iteration: 3 || Loss: 19.542195129210267
Iteration: 4 || Loss: 19.541708989888967
Iteration: 5 || Loss: 19.541223764640083
Iteration: 6 || Loss: 19.541223764640083
saving ADAM checkpoint...
Sum of params:85.39825
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.541223764640083
Iteration: 2 || Loss: 19.484440256459198
Iteration: 3 || Loss: 19.270011639632827
Iteration: 4 || Loss: 19.16419388868408
Iteration: 5 || Loss: 19.136248257991596
Iteration: 6 || Loss: 19.118010519432826
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.29186
Epoch 460 loss:19.118010519432826
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.846382079458164
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.58076384458739
waveform batch: 2/2
Test loss - extrapolation:15.990889132271862
Epoch 460 mean train loss:0.9225848271959171
Epoch 460 mean test loss - interpolation:0.9743970132430273
Epoch 460 mean test loss - extrapolation:4.714304414738271
Start training epoch 461
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.29186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7043350338783205
Iteration: 2 || Loss: 2.7027754270399047
Iteration: 3 || Loss: 2.701215111061662
Iteration: 4 || Loss: 2.6996553200671882
Iteration: 5 || Loss: 2.6980988196224103
Iteration: 6 || Loss: 2.6980988196224103
saving ADAM checkpoint...
Sum of params:85.29187
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6980988196224103
Iteration: 2 || Loss: 2.0789010687879586
Iteration: 3 || Loss: 1.6511668368955703
Iteration: 4 || Loss: 1.6485374111359279
Iteration: 5 || Loss: 1.6263618636192918
Iteration: 6 || Loss: 1.6256415091032526
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.36005
Epoch 461 loss:1.6256415091032526
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.36005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.227408167809403
Iteration: 2 || Loss: 6.2269811255562715
Iteration: 3 || Loss: 6.226554093723233
Iteration: 4 || Loss: 6.226129878428782
Iteration: 5 || Loss: 6.2257067584602686
Iteration: 6 || Loss: 6.2257067584602686
saving ADAM checkpoint...
Sum of params:85.36008
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.2257067584602686
Iteration: 2 || Loss: 6.187572598891243
Iteration: 3 || Loss: 6.12541318137338
Iteration: 4 || Loss: 6.024088794332145
Iteration: 5 || Loss: 6.016210549918512
Iteration: 6 || Loss: 6.008852434148397
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.4073
Epoch 461 loss:6.008852434148397
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.4073
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.533517064013125
Iteration: 2 || Loss: 19.533026796486087
Iteration: 3 || Loss: 19.5325371413241
Iteration: 4 || Loss: 19.532050807541
Iteration: 5 || Loss: 19.531564399690726
Iteration: 6 || Loss: 19.531564399690726
saving ADAM checkpoint...
Sum of params:85.407326
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.531564399690726
Iteration: 2 || Loss: 19.474745374804712
Iteration: 3 || Loss: 19.260159547022422
Iteration: 4 || Loss: 19.154423577864595
Iteration: 5 || Loss: 19.126484178188335
Iteration: 6 || Loss: 19.10827555039446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.3009
Epoch 461 loss:19.10827555039446
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.844323381571056
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.56648585901265
waveform batch: 2/2
Test loss - extrapolation:15.981743881263935
Epoch 461 mean train loss:0.9221644652981417
Epoch 461 mean test loss - interpolation:0.9740538969285093
Epoch 461 mean test loss - extrapolation:4.712352478356382
Start training epoch 462
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.3009
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7037616618933713
Iteration: 2 || Loss: 2.7022015244622604
Iteration: 3 || Loss: 2.7006394783932266
Iteration: 4 || Loss: 2.699078194719271
Iteration: 5 || Loss: 2.697520105379041
Iteration: 6 || Loss: 2.697520105379041
saving ADAM checkpoint...
Sum of params:85.3009
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.697520105379041
Iteration: 2 || Loss: 2.077949752653468
Iteration: 3 || Loss: 1.6505238072152333
Iteration: 4 || Loss: 1.6478914643709108
Iteration: 5 || Loss: 1.6257463643875207
Iteration: 6 || Loss: 1.6250263233579396
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.36897
Epoch 462 loss:1.6250263233579396
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.36897
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.225419089353183
Iteration: 2 || Loss: 6.224992229187177
Iteration: 3 || Loss: 6.224565896461823
Iteration: 4 || Loss: 6.224141275023327
Iteration: 5 || Loss: 6.223718436622524
Iteration: 6 || Loss: 6.223718436622524
saving ADAM checkpoint...
Sum of params:85.36902
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.223718436622524
Iteration: 2 || Loss: 6.185613707026852
Iteration: 3 || Loss: 6.123583518079637
Iteration: 4 || Loss: 6.0222633154079475
Iteration: 5 || Loss: 6.014353617466984
Iteration: 6 || Loss: 6.007015151076684
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.41637
Epoch 462 loss:6.007015151076684
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.41637
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.523836874628568
Iteration: 2 || Loss: 19.523347630256325
Iteration: 3 || Loss: 19.522859439521884
Iteration: 4 || Loss: 19.522370596710022
Iteration: 5 || Loss: 19.5218849783455
Iteration: 6 || Loss: 19.5218849783455
saving ADAM checkpoint...
Sum of params:85.41641
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.5218849783455
Iteration: 2 || Loss: 19.46505487870551
Iteration: 3 || Loss: 19.25035018721973
Iteration: 4 || Loss: 19.144684419425772
Iteration: 5 || Loss: 19.116749923065722
Iteration: 6 || Loss: 19.098559178785173
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.30989
Epoch 462 loss:19.098559178785173
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.842270855280325
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.552306721302635
waveform batch: 2/2
Test loss - extrapolation:15.97263092805381
Epoch 462 mean train loss:0.9217448501110275
Epoch 462 mean test loss - interpolation:0.9737118092133875
Epoch 462 mean test loss - extrapolation:4.710411470779704
Start training epoch 463
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.30989
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7032870776623645
Iteration: 2 || Loss: 2.7017232750132747
Iteration: 3 || Loss: 2.7001639291509196
Iteration: 4 || Loss: 2.698602302932924
Iteration: 5 || Loss: 2.697040928470651
Iteration: 6 || Loss: 2.697040928470651
saving ADAM checkpoint...
Sum of params:85.3099
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.697040928470651
Iteration: 2 || Loss: 2.0769623321645834
Iteration: 3 || Loss: 1.6498864766008716
Iteration: 4 || Loss: 1.647249408412953
Iteration: 5 || Loss: 1.6251370305866115
Iteration: 6 || Loss: 1.624415684586112
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.37789
Epoch 463 loss:1.624415684586112
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.37789
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.223590497750878
Iteration: 2 || Loss: 6.223161122527943
Iteration: 3 || Loss: 6.2227339986417425
Iteration: 4 || Loss: 6.222309751382176
Iteration: 5 || Loss: 6.221885344564778
Iteration: 6 || Loss: 6.221885344564778
saving ADAM checkpoint...
Sum of params:85.377914
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.221885344564778
Iteration: 2 || Loss: 6.183643779909625
Iteration: 3 || Loss: 6.121726958320766
Iteration: 4 || Loss: 6.020446196545064
Iteration: 5 || Loss: 6.012525356955882
Iteration: 6 || Loss: 6.005202931714974
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.425385
Epoch 463 loss:6.005202931714974
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.425385
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.51414177237633
Iteration: 2 || Loss: 19.513651935813332
Iteration: 3 || Loss: 19.51316361624141
Iteration: 4 || Loss: 19.512674714054576
Iteration: 5 || Loss: 19.512188450435712
Iteration: 6 || Loss: 19.512188450435712
saving ADAM checkpoint...
Sum of params:85.42544
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.512188450435712
Iteration: 2 || Loss: 19.455253051733578
Iteration: 3 || Loss: 19.2404706455758
Iteration: 4 || Loss: 19.134918266231654
Iteration: 5 || Loss: 19.10700670643196
Iteration: 6 || Loss: 19.0888619693216
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.3189
Epoch 463 loss:19.0888619693216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.840206767040713
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.537984759719144
waveform batch: 2/2
Test loss - extrapolation:15.96351887185145
Epoch 463 mean train loss:0.9213269167456098
Epoch 463 mean test loss - interpolation:0.9733677945067855
Epoch 463 mean test loss - extrapolation:4.708458635964216
Start training epoch 464
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.3189
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.70255029194652
Iteration: 2 || Loss: 2.7009883910116503
Iteration: 3 || Loss: 2.699425047202961
Iteration: 4 || Loss: 2.6978630977737863
Iteration: 5 || Loss: 2.6963023974486044
Iteration: 6 || Loss: 2.6963023974486044
saving ADAM checkpoint...
Sum of params:85.31892
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6963023974486044
Iteration: 2 || Loss: 2.0760325313189645
Iteration: 3 || Loss: 1.6492408791060245
Iteration: 4 || Loss: 1.6465996079322878
Iteration: 5 || Loss: 1.6245257209262023
Iteration: 6 || Loss: 1.6238043903471622
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.386795
Epoch 464 loss:1.6238043903471622
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.386795
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.221707306911749
Iteration: 2 || Loss: 6.22127733981011
Iteration: 3 || Loss: 6.220851141115465
Iteration: 4 || Loss: 6.220423867811129
Iteration: 5 || Loss: 6.21999814896037
Iteration: 6 || Loss: 6.21999814896037
saving ADAM checkpoint...
Sum of params:85.38682
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.21999814896037
Iteration: 2 || Loss: 6.181654559027487
Iteration: 3 || Loss: 6.119850409489925
Iteration: 4 || Loss: 6.018639469059222
Iteration: 5 || Loss: 6.01071071578809
Iteration: 6 || Loss: 6.00339758179124
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.4344
Epoch 464 loss:6.00339758179124
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.4344
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.50446348230343
Iteration: 2 || Loss: 19.50397021615062
Iteration: 3 || Loss: 19.503480722266232
Iteration: 4 || Loss: 19.502991597005604
Iteration: 5 || Loss: 19.50250437366979
Iteration: 6 || Loss: 19.50250437366979
saving ADAM checkpoint...
Sum of params:85.43443
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.50250437366979
Iteration: 2 || Loss: 19.445474610496568
Iteration: 3 || Loss: 19.230622122295905
Iteration: 4 || Loss: 19.125186872866312
Iteration: 5 || Loss: 19.097297794189817
Iteration: 6 || Loss: 19.079194970240934
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.327866
Epoch 464 loss:19.079194970240934
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.838167928509225
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.523724803434035
waveform batch: 2/2
Test loss - extrapolation:15.954425265433839
Epoch 464 mean train loss:0.9209102393923909
Epoch 464 mean test loss - interpolation:0.9730279880848709
Epoch 464 mean test loss - extrapolation:4.706512505738989
Start training epoch 465
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.327866
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.701897005920447
Iteration: 2 || Loss: 2.700330140494095
Iteration: 3 || Loss: 2.6987676950692197
Iteration: 4 || Loss: 2.6972068664335023
Iteration: 5 || Loss: 2.695646015639465
Iteration: 6 || Loss: 2.695646015639465
saving ADAM checkpoint...
Sum of params:85.327866
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.695646015639465
Iteration: 2 || Loss: 2.0751477239945832
Iteration: 3 || Loss: 1.648600417988168
Iteration: 4 || Loss: 1.6459545721896116
Iteration: 5 || Loss: 1.623917965804603
Iteration: 6 || Loss: 1.623195014483677
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.395645
Epoch 465 loss:1.623195014483677
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.395645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.219785903125249
Iteration: 2 || Loss: 6.219356419161392
Iteration: 3 || Loss: 6.218928906885653
Iteration: 4 || Loss: 6.218502272522763
Iteration: 5 || Loss: 6.2180774785006365
Iteration: 6 || Loss: 6.2180774785006365
saving ADAM checkpoint...
Sum of params:85.3957
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.2180774785006365
Iteration: 2 || Loss: 6.1796825854308945
Iteration: 3 || Loss: 6.118016701453345
Iteration: 4 || Loss: 6.016838971310436
Iteration: 5 || Loss: 6.008885801639658
Iteration: 6 || Loss: 6.001589404384174
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.44338
Epoch 465 loss:6.001589404384174
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.44338
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.49489145197433
Iteration: 2 || Loss: 19.494399141134558
Iteration: 3 || Loss: 19.49390895088372
Iteration: 4 || Loss: 19.493419537635233
Iteration: 5 || Loss: 19.49293179422841
Iteration: 6 || Loss: 19.49293179422841
saving ADAM checkpoint...
Sum of params:85.44343
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.49293179422841
Iteration: 2 || Loss: 19.435861495908316
Iteration: 3 || Loss: 19.220887160560896
Iteration: 4 || Loss: 19.115521085186128
Iteration: 5 || Loss: 19.087636197727768
Iteration: 6 || Loss: 19.069561857782393
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.336815
Epoch 465 loss:19.069561857782393
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.836125205502964
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.509544213155316
waveform batch: 2/2
Test loss - extrapolation:15.945369030041652
Epoch 465 mean train loss:0.920494699194836
Epoch 465 mean test loss - interpolation:0.972687534250494
Epoch 465 mean test loss - extrapolation:4.704576103599748
Start training epoch 466
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.336815
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7012873221286706
Iteration: 2 || Loss: 2.699722236056702
Iteration: 3 || Loss: 2.698156829186996
Iteration: 4 || Loss: 2.696593894759089
Iteration: 5 || Loss: 2.695030794304614
Iteration: 6 || Loss: 2.695030794304614
saving ADAM checkpoint...
Sum of params:85.33682
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.695030794304614
Iteration: 2 || Loss: 2.074193396582204
Iteration: 3 || Loss: 1.647967623055729
Iteration: 4 || Loss: 1.645317055378649
Iteration: 5 || Loss: 1.6233144104721282
Iteration: 6 || Loss: 1.6225923542197522
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.4045
Epoch 466 loss:1.6225923542197522
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.4045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.217976391533263
Iteration: 2 || Loss: 6.21754594300371
Iteration: 3 || Loss: 6.217116052702864
Iteration: 4 || Loss: 6.216689150634174
Iteration: 5 || Loss: 6.216262861306645
Iteration: 6 || Loss: 6.216262861306645
saving ADAM checkpoint...
Sum of params:85.40453
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.216262861306645
Iteration: 2 || Loss: 6.177761633986587
Iteration: 3 || Loss: 6.116178117896287
Iteration: 4 || Loss: 6.015045924559423
Iteration: 5 || Loss: 6.007083797536215
Iteration: 6 || Loss: 5.999798512973757
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.45235
Epoch 466 loss:5.999798512973757
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.45235
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.485260635117193
Iteration: 2 || Loss: 19.484769842994282
Iteration: 3 || Loss: 19.484278724971198
Iteration: 4 || Loss: 19.483788848164437
Iteration: 5 || Loss: 19.48330055900772
Iteration: 6 || Loss: 19.48330055900772
saving ADAM checkpoint...
Sum of params:85.45238
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.48330055900772
Iteration: 2 || Loss: 19.42612831045499
Iteration: 3 || Loss: 19.211082630430308
Iteration: 4 || Loss: 19.105838263963506
Iteration: 5 || Loss: 19.077976450081042
Iteration: 6 || Loss: 19.05994380544835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.34575
Epoch 466 loss:19.05994380544835
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.8340832580986035
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.495328606806325
waveform batch: 2/2
Test loss - extrapolation:15.936321651738437
Epoch 466 mean train loss:0.9200805059531675
Epoch 466 mean test loss - interpolation:0.9723472096831006
Epoch 466 mean test loss - extrapolation:4.702637521545397
Start training epoch 467
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.34575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7005987918137877
Iteration: 2 || Loss: 2.699031544782468
Iteration: 3 || Loss: 2.6974633836568946
Iteration: 4 || Loss: 2.6958983144336712
Iteration: 5 || Loss: 2.6943373423370125
Iteration: 6 || Loss: 2.6943373423370125
saving ADAM checkpoint...
Sum of params:85.34575
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6943373423370125
Iteration: 2 || Loss: 2.0732841382030593
Iteration: 3 || Loss: 1.6473348353876647
Iteration: 4 || Loss: 1.6446802579183575
Iteration: 5 || Loss: 1.6227121899698493
Iteration: 6 || Loss: 1.621987405112321
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.4133
Epoch 467 loss:1.621987405112321
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.4133
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2159884832500145
Iteration: 2 || Loss: 6.215557880220049
Iteration: 3 || Loss: 6.215128242287828
Iteration: 4 || Loss: 6.21470195868315
Iteration: 5 || Loss: 6.214276156826117
Iteration: 6 || Loss: 6.214276156826117
saving ADAM checkpoint...
Sum of params:85.41335
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.214276156826117
Iteration: 2 || Loss: 6.175781183248526
Iteration: 3 || Loss: 6.114366578511369
Iteration: 4 || Loss: 6.0132570233824
Iteration: 5 || Loss: 6.005258042036454
Iteration: 6 || Loss: 5.997995940134565
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.46131
Epoch 467 loss:5.997995940134565
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.46131
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.475757220102334
Iteration: 2 || Loss: 19.47526518279648
Iteration: 3 || Loss: 19.47477361672582
Iteration: 4 || Loss: 19.47428474532981
Iteration: 5 || Loss: 19.4737964104545
Iteration: 6 || Loss: 19.4737964104545
saving ADAM checkpoint...
Sum of params:85.46134
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.4737964104545
Iteration: 2 || Loss: 19.416641575185547
Iteration: 3 || Loss: 19.201432483414425
Iteration: 4 || Loss: 19.096239300042335
Iteration: 5 || Loss: 19.068378222310074
Iteration: 6 || Loss: 19.050358382372
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.35462
Epoch 467 loss:19.050358382372
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.832082984864299
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.481338489594414
waveform batch: 2/2
Test loss - extrapolation:15.927308047298766
Epoch 467 mean train loss:0.9196669561247892
Epoch 467 mean test loss - interpolation:0.9720138308107166
Epoch 467 mean test loss - extrapolation:4.700720544741098
Start training epoch 468
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.35462
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.7002060056013573
Iteration: 2 || Loss: 2.698637883393727
Iteration: 3 || Loss: 2.6970682060199485
Iteration: 4 || Loss: 2.6955028069859184
Iteration: 5 || Loss: 2.6939407368741644
Iteration: 6 || Loss: 2.6939407368741644
saving ADAM checkpoint...
Sum of params:85.35465
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6939407368741644
Iteration: 2 || Loss: 2.0723114915073704
Iteration: 3 || Loss: 1.6467147665167479
Iteration: 4 || Loss: 1.644055381190356
Iteration: 5 || Loss: 1.6221177281713173
Iteration: 6 || Loss: 1.6213929204012092
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.42213
Epoch 468 loss:1.6213929204012092
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.42213
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.214327936087605
Iteration: 2 || Loss: 6.213894270053405
Iteration: 3 || Loss: 6.213464157835224
Iteration: 4 || Loss: 6.2130354463013475
Iteration: 5 || Loss: 6.212609522998247
Iteration: 6 || Loss: 6.212609522998247
saving ADAM checkpoint...
Sum of params:85.42218
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.212609522998247
Iteration: 2 || Loss: 6.173855385796788
Iteration: 3 || Loss: 6.112506822453618
Iteration: 4 || Loss: 6.011483019718565
Iteration: 5 || Loss: 6.00349769154172
Iteration: 6 || Loss: 5.996236732143836
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.4702
Epoch 468 loss:5.996236732143836
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.4702
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.466145268654078
Iteration: 2 || Loss: 19.465652421791894
Iteration: 3 || Loss: 19.465160390056422
Iteration: 4 || Loss: 19.464669783580455
Iteration: 5 || Loss: 19.464180421367313
Iteration: 6 || Loss: 19.464180421367313
saving ADAM checkpoint...
Sum of params:85.47023
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.464180421367313
Iteration: 2 || Loss: 19.406806699045582
Iteration: 3 || Loss: 19.19159886341609
Iteration: 4 || Loss: 19.08656133819571
Iteration: 5 || Loss: 19.058741024952234
Iteration: 6 || Loss: 19.040794334947403
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.36354
Epoch 468 loss:19.040794334947403
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.830021981257761
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.46695559683917
waveform batch: 2/2
Test loss - extrapolation:15.918293968821882
Epoch 468 mean train loss:0.9192559995687051
Epoch 468 mean test loss - interpolation:0.9716703302096268
Epoch 468 mean test loss - extrapolation:4.698770797138422
Start training epoch 469
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.36354
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.699149407255325
Iteration: 2 || Loss: 2.6975800455574697
Iteration: 3 || Loss: 2.6960129371767874
Iteration: 4 || Loss: 2.694448311881236
Iteration: 5 || Loss: 2.6928846466021223
Iteration: 6 || Loss: 2.6928846466021223
saving ADAM checkpoint...
Sum of params:85.36355
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6928846466021223
Iteration: 2 || Loss: 2.0715006488428465
Iteration: 3 || Loss: 1.6460733046847618
Iteration: 4 || Loss: 1.6434085393096538
Iteration: 5 || Loss: 1.6215146750953435
Iteration: 6 || Loss: 1.6207894022189915
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.43088
Epoch 469 loss:1.6207894022189915
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.43088
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2121906717671544
Iteration: 2 || Loss: 6.211759862055989
Iteration: 3 || Loss: 6.211331131870306
Iteration: 4 || Loss: 6.210902288489907
Iteration: 5 || Loss: 6.210477906146403
Iteration: 6 || Loss: 6.210477906146403
saving ADAM checkpoint...
Sum of params:85.43093
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.210477906146403
Iteration: 2 || Loss: 6.171916978255429
Iteration: 3 || Loss: 6.110740635702248
Iteration: 4 || Loss: 6.00970418015039
Iteration: 5 || Loss: 6.001661093446412
Iteration: 6 || Loss: 5.994428377537652
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.47914
Epoch 469 loss:5.994428377537652
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.47914
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.456730846709405
Iteration: 2 || Loss: 19.456239603398018
Iteration: 3 || Loss: 19.4557466293679
Iteration: 4 || Loss: 19.455255685320363
Iteration: 5 || Loss: 19.45476932755924
Iteration: 6 || Loss: 19.45476932755924
saving ADAM checkpoint...
Sum of params:85.47918
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.45476932755924
Iteration: 2 || Loss: 19.39751963860756
Iteration: 3 || Loss: 19.18209544090585
Iteration: 4 || Loss: 19.077070211692085
Iteration: 5 || Loss: 19.049229067298494
Iteration: 6 || Loss: 19.031265644592224
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.37236
Epoch 469 loss:19.031265644592224
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.82806604140275
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.453231760329636
waveform batch: 2/2
Test loss - extrapolation:15.909331202784864
Epoch 469 mean train loss:0.91884425601203
Epoch 469 mean test loss - interpolation:0.9713443402337917
Epoch 469 mean test loss - extrapolation:4.6968802469262085
Start training epoch 470
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.37236
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.699109611131648
Iteration: 2 || Loss: 2.6975376878687594
Iteration: 3 || Loss: 2.6959709370760803
Iteration: 4 || Loss: 2.6944068990937837
Iteration: 5 || Loss: 2.6928390638738655
Iteration: 6 || Loss: 2.6928390638738655
saving ADAM checkpoint...
Sum of params:85.37238
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6928390638738655
Iteration: 2 || Loss: 2.0704494190803437
Iteration: 3 || Loss: 1.6454741471974597
Iteration: 4 || Loss: 1.642805183701855
Iteration: 5 || Loss: 1.6209312995871692
Iteration: 6 || Loss: 1.6202047700357372
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.439644
Epoch 470 loss:1.6202047700357372
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.439644
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.210670311170147
Iteration: 2 || Loss: 6.21023595055832
Iteration: 3 || Loss: 6.209804556786041
Iteration: 4 || Loss: 6.209375796602315
Iteration: 5 || Loss: 6.208947362065019
Iteration: 6 || Loss: 6.208947362065019
saving ADAM checkpoint...
Sum of params:85.43969
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.208947362065019
Iteration: 2 || Loss: 6.170033419870487
Iteration: 3 || Loss: 6.108903416991639
Iteration: 4 || Loss: 6.007946197979717
Iteration: 5 || Loss: 5.99992689821022
Iteration: 6 || Loss: 5.992693307610742
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.48796
Epoch 470 loss:5.992693307610742
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.48796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.447123735057758
Iteration: 2 || Loss: 19.44662905160499
Iteration: 3 || Loss: 19.44613693375048
Iteration: 4 || Loss: 19.445644708534754
Iteration: 5 || Loss: 19.445155117891648
Iteration: 6 || Loss: 19.445155117891648
saving ADAM checkpoint...
Sum of params:85.488
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.445155117891648
Iteration: 2 || Loss: 19.387638916108127
Iteration: 3 || Loss: 19.17225200603474
Iteration: 4 || Loss: 19.06740661261997
Iteration: 5 || Loss: 19.039619270623223
Iteration: 6 || Loss: 19.021740424723365
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.38124
Epoch 470 loss:19.021740424723365
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.826015616728265
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.43884255620215
waveform batch: 2/2
Test loss - extrapolation:15.900354102471276
Epoch 470 mean train loss:0.9184358104265464
Epoch 470 mean test loss - interpolation:0.9710026027880442
Epoch 470 mean test loss - extrapolation:4.694933054889452
Start training epoch 471
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.38124
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6979701527842828
Iteration: 2 || Loss: 2.6963966244255397
Iteration: 3 || Loss: 2.694828763342817
Iteration: 4 || Loss: 2.693261380928705
Iteration: 5 || Loss: 2.6916946458905864
Iteration: 6 || Loss: 2.6916946458905864
saving ADAM checkpoint...
Sum of params:85.38124
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6916946458905864
Iteration: 2 || Loss: 2.069693144069455
Iteration: 3 || Loss: 1.6448317212760606
Iteration: 4 || Loss: 1.6421561157105642
Iteration: 5 || Loss: 1.6203333697596656
Iteration: 6 || Loss: 1.6196066850143553
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.44838
Epoch 471 loss:1.6196066850143553
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.44838
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.208666297395047
Iteration: 2 || Loss: 6.2082331154855
Iteration: 3 || Loss: 6.207801102875008
Iteration: 4 || Loss: 6.207370676704835
Iteration: 5 || Loss: 6.206943777547629
Iteration: 6 || Loss: 6.206943777547629
saving ADAM checkpoint...
Sum of params:85.44844
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.206943777547629
Iteration: 2 || Loss: 6.168082405830228
Iteration: 3 || Loss: 6.107094296147673
Iteration: 4 || Loss: 6.0061851895627285
Iteration: 5 || Loss: 5.9981308792405335
Iteration: 6 || Loss: 5.990919438622787
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.49685
Epoch 471 loss:5.990919438622787
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.49685
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.43773364068768
Iteration: 2 || Loss: 19.43723840605552
Iteration: 3 || Loss: 19.436746686107053
Iteration: 4 || Loss: 19.436255696718426
Iteration: 5 || Loss: 19.43576681286587
Iteration: 6 || Loss: 19.43576681286587
saving ADAM checkpoint...
Sum of params:85.49688
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.43576681286587
Iteration: 2 || Loss: 19.378284728013746
Iteration: 3 || Loss: 19.162705972193844
Iteration: 4 || Loss: 19.05792328343602
Iteration: 5 || Loss: 19.030133047695116
Iteration: 6 || Loss: 19.012260000569018
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.39004
Epoch 471 loss:19.012260000569018
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.824050494484318
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.425004858083085
waveform batch: 2/2
Test loss - extrapolation:15.89141944619261
Epoch 471 mean train loss:0.918027107731247
Epoch 471 mean test loss - interpolation:0.970675082414053
Epoch 471 mean test loss - extrapolation:4.693035358689642
Start training epoch 472
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.39004
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6976650223750323
Iteration: 2 || Loss: 2.6960941184826726
Iteration: 3 || Loss: 2.694527467671035
Iteration: 4 || Loss: 2.6929578525871802
Iteration: 5 || Loss: 2.691387275487061
Iteration: 6 || Loss: 2.691387275487061
saving ADAM checkpoint...
Sum of params:85.39003
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.691387275487061
Iteration: 2 || Loss: 2.068717830409709
Iteration: 3 || Loss: 1.6442293123952432
Iteration: 4 || Loss: 1.6415492641705516
Iteration: 5 || Loss: 1.6197509377074728
Iteration: 6 || Loss: 1.6190218353745838
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.45707
Epoch 472 loss:1.6190218353745838
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.45707
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.2068670590200545
Iteration: 2 || Loss: 6.206433304963086
Iteration: 3 || Loss: 6.206000318962853
Iteration: 4 || Loss: 6.205570399517583
Iteration: 5 || Loss: 6.205142017825467
Iteration: 6 || Loss: 6.205142017825467
saving ADAM checkpoint...
Sum of params:85.45712
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.205142017825467
Iteration: 2 || Loss: 6.166194108245713
Iteration: 3 || Loss: 6.1053341664102145
Iteration: 4 || Loss: 6.004437978534586
Iteration: 5 || Loss: 5.996363780354853
Iteration: 6 || Loss: 5.9891657476366875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.50567
Epoch 472 loss:5.9891657476366875
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.50567
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.428310421235587
Iteration: 2 || Loss: 19.42781468136928
Iteration: 3 || Loss: 19.427322439249227
Iteration: 4 || Loss: 19.42682977911507
Iteration: 5 || Loss: 19.42633872761787
Iteration: 6 || Loss: 19.42633872761787
saving ADAM checkpoint...
Sum of params:85.50569
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.42633872761787
Iteration: 2 || Loss: 19.368781568392542
Iteration: 3 || Loss: 19.153112993989804
Iteration: 4 || Loss: 19.048411192451773
Iteration: 5 || Loss: 19.02063733061701
Iteration: 6 || Loss: 19.00280258583834
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.3988
Epoch 472 loss:19.00280258583834
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.822067716799687
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.41099358940723
waveform batch: 2/2
Test loss - extrapolation:15.882502765124347
Epoch 472 mean train loss:0.9176203506499866
Epoch 472 mean test loss - interpolation:0.9703446194666144
Epoch 472 mean test loss - extrapolation:4.691124696210964
Start training epoch 473
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.3988
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.697043316701065
Iteration: 2 || Loss: 2.69547002125239
Iteration: 3 || Loss: 2.693897904447398
Iteration: 4 || Loss: 2.692329283797779
Iteration: 5 || Loss: 2.6907618328548963
Iteration: 6 || Loss: 2.6907618328548963
saving ADAM checkpoint...
Sum of params:85.39883
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6907618328548963
Iteration: 2 || Loss: 2.0678454061052225
Iteration: 3 || Loss: 1.643610640431686
Iteration: 4 || Loss: 1.640926415393748
Iteration: 5 || Loss: 1.61916267621514
Iteration: 6 || Loss: 1.6184336024781865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.46579
Epoch 473 loss:1.6184336024781865
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.46579
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.205050433894005
Iteration: 2 || Loss: 6.204616075811793
Iteration: 3 || Loss: 6.204185047620652
Iteration: 4 || Loss: 6.203753556270594
Iteration: 5 || Loss: 6.203325288034167
Iteration: 6 || Loss: 6.203325288034167
saving ADAM checkpoint...
Sum of params:85.46584
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.203325288034167
Iteration: 2 || Loss: 6.164298724471182
Iteration: 3 || Loss: 6.103539993054966
Iteration: 4 || Loss: 6.002692931124276
Iteration: 5 || Loss: 5.9946090598425075
Iteration: 6 || Loss: 5.987420497714352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.51447
Epoch 473 loss:5.987420497714352
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.51447
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.418860471022192
Iteration: 2 || Loss: 19.41836556485943
Iteration: 3 || Loss: 19.417871009657336
Iteration: 4 || Loss: 19.417377537126665
Iteration: 5 || Loss: 19.416888371331115
Iteration: 6 || Loss: 19.416888371331115
saving ADAM checkpoint...
Sum of params:85.514496
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.416888371331115
Iteration: 2 || Loss: 19.359241186178245
Iteration: 3 || Loss: 19.14351177715855
Iteration: 4 || Loss: 19.038918367740283
Iteration: 5 || Loss: 19.011163417122074
Iteration: 6 || Loss: 18.99335987108002
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.407585
Epoch 473 loss:18.99335987108002
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.820046288154881
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.39698431110138
waveform batch: 2/2
Test loss - extrapolation:15.873620199506894
Epoch 473 mean train loss:0.9172142748714676
Epoch 473 mean test loss - interpolation:0.9700077146924801
Epoch 473 mean test loss - extrapolation:4.689217042550689
Start training epoch 474
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.407585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6963131014475605
Iteration: 2 || Loss: 2.694740032380876
Iteration: 3 || Loss: 2.69316704762368
Iteration: 4 || Loss: 2.6915975781286514
Iteration: 5 || Loss: 2.6900304355736115
Iteration: 6 || Loss: 2.6900304355736115
saving ADAM checkpoint...
Sum of params:85.40759
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6900304355736115
Iteration: 2 || Loss: 2.0669090208036924
Iteration: 3 || Loss: 1.6430000951050736
Iteration: 4 || Loss: 1.6403093761733867
Iteration: 5 || Loss: 1.6185809992178037
Iteration: 6 || Loss: 1.6178504402120686
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.47446
Epoch 474 loss:1.6178504402120686
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.47446
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.203243483276381
Iteration: 2 || Loss: 6.202809069801996
Iteration: 3 || Loss: 6.2023771852280385
Iteration: 4 || Loss: 6.201944890845961
Iteration: 5 || Loss: 6.201514699191948
Iteration: 6 || Loss: 6.201514699191948
saving ADAM checkpoint...
Sum of params:85.474495
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.201514699191948
Iteration: 2 || Loss: 6.162395796429702
Iteration: 3 || Loss: 6.101762068640322
Iteration: 4 || Loss: 6.000959478719803
Iteration: 5 || Loss: 5.992859434646422
Iteration: 6 || Loss: 5.985682523385969
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.52325
Epoch 474 loss:5.985682523385969
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.52325
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.409487068779036
Iteration: 2 || Loss: 19.408990720017822
Iteration: 3 || Loss: 19.4084965088338
Iteration: 4 || Loss: 19.40800484200274
Iteration: 5 || Loss: 19.407512719640483
Iteration: 6 || Loss: 19.407512719640483
saving ADAM checkpoint...
Sum of params:85.52329
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.407512719640483
Iteration: 2 || Loss: 19.34979224059464
Iteration: 3 || Loss: 19.133969730974027
Iteration: 4 || Loss: 19.029458865434215
Iteration: 5 || Loss: 19.001719136469266
Iteration: 6 || Loss: 18.98395660401942
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.41634
Epoch 474 loss:18.98395660401942
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.818082621472578
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.38302464730804
waveform batch: 2/2
Test loss - extrapolation:15.86473369274733
Epoch 474 mean train loss:0.9168099850902571
Epoch 474 mean test loss - interpolation:0.9696804369120963
Epoch 474 mean test loss - extrapolation:4.687313195004614
Start training epoch 475
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.41634
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.695701845916199
Iteration: 2 || Loss: 2.694126665168531
Iteration: 3 || Loss: 2.69255264173991
Iteration: 4 || Loss: 2.690982664379186
Iteration: 5 || Loss: 2.6894125378851976
Iteration: 6 || Loss: 2.6894125378851976
saving ADAM checkpoint...
Sum of params:85.41633
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6894125378851976
Iteration: 2 || Loss: 2.066040727151571
Iteration: 3 || Loss: 1.6423880347067994
Iteration: 4 || Loss: 1.6396930663992135
Iteration: 5 || Loss: 1.6179973043572622
Iteration: 6 || Loss: 1.617265484424347
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.48311
Epoch 475 loss:1.617265484424347
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.48311
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.201455861919079
Iteration: 2 || Loss: 6.201019091864661
Iteration: 3 || Loss: 6.200585962846174
Iteration: 4 || Loss: 6.200155021978641
Iteration: 5 || Loss: 6.199724413382093
Iteration: 6 || Loss: 6.199724413382093
saving ADAM checkpoint...
Sum of params:85.48315
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.199724413382093
Iteration: 2 || Loss: 6.160504419105163
Iteration: 3 || Loss: 6.0999791922832545
Iteration: 4 || Loss: 5.999224197603962
Iteration: 5 || Loss: 5.991105536180072
Iteration: 6 || Loss: 5.983946337235973
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.53201
Epoch 475 loss:5.983946337235973
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.53201
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.40013957967954
Iteration: 2 || Loss: 19.399644128380064
Iteration: 3 || Loss: 19.39914927167431
Iteration: 4 || Loss: 19.398655537295618
Iteration: 5 || Loss: 19.398164553612663
Iteration: 6 || Loss: 19.398164553612663
saving ADAM checkpoint...
Sum of params:85.53205
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.398164553612663
Iteration: 2 || Loss: 19.340390200842847
Iteration: 3 || Loss: 19.124453505194097
Iteration: 4 || Loss: 19.020033222616945
Iteration: 5 || Loss: 18.992307913964176
Iteration: 6 || Loss: 18.974569995923666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.42506
Epoch 475 loss:18.974569995923666
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.816111159842195
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.36914526787898
waveform batch: 2/2
Test loss - extrapolation:15.855875576248685
Epoch 475 mean train loss:0.9164062695718617
Epoch 475 mean test loss - interpolation:0.9693518599736991
Epoch 475 mean test loss - extrapolation:4.685418403677305
Start training epoch 476
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.42506
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.695149751972893
Iteration: 2 || Loss: 2.693575633082074
Iteration: 3 || Loss: 2.6920025689238134
Iteration: 4 || Loss: 2.6904307084410166
Iteration: 5 || Loss: 2.6888582148825586
Iteration: 6 || Loss: 2.6888582148825586
saving ADAM checkpoint...
Sum of params:85.42507
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6888582148825586
Iteration: 2 || Loss: 2.0651160153297754
Iteration: 3 || Loss: 1.6417857501884454
Iteration: 4 || Loss: 1.6390849990183984
Iteration: 5 || Loss: 1.61742138175755
Iteration: 6 || Loss: 1.6166879664011782
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.49173
Epoch 476 loss:1.6166879664011782
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.49173
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.199667234311365
Iteration: 2 || Loss: 6.199231204553468
Iteration: 3 || Loss: 6.198797286812176
Iteration: 4 || Loss: 6.198364783524828
Iteration: 5 || Loss: 6.197933622826264
Iteration: 6 || Loss: 6.197933622826264
saving ADAM checkpoint...
Sum of params:85.49177
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.197933622826264
Iteration: 2 || Loss: 6.158628001255144
Iteration: 3 || Loss: 6.098215703132448
Iteration: 4 || Loss: 5.997504936494668
Iteration: 5 || Loss: 5.989372347311138
Iteration: 6 || Loss: 5.982221861510964
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.54074
Epoch 476 loss:5.982221861510964
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.54074
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.390796775389557
Iteration: 2 || Loss: 19.390300516111804
Iteration: 3 || Loss: 19.389804905678897
Iteration: 4 || Loss: 19.38931108533767
Iteration: 5 || Loss: 19.388818831150726
Iteration: 6 || Loss: 19.388818831150726
saving ADAM checkpoint...
Sum of params:85.54078
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.388818831150726
Iteration: 2 || Loss: 19.33095939168248
Iteration: 3 || Loss: 19.11493897493081
Iteration: 4 || Loss: 19.010616205922215
Iteration: 5 || Loss: 18.982908729056845
Iteration: 6 || Loss: 18.965211364951703
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.43379
Epoch 476 loss:18.965211364951703
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.814148866550506
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.35523050687254
waveform batch: 2/2
Test loss - extrapolation:15.847033876758436
Epoch 476 mean train loss:0.9160041790642705
Epoch 476 mean test loss - interpolation:0.9690248110917511
Epoch 476 mean test loss - extrapolation:4.683522031969248
Start training epoch 477
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.43379
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.694512875818258
Iteration: 2 || Loss: 2.6929356890561555
Iteration: 3 || Loss: 2.6913598783077197
Iteration: 4 || Loss: 2.6897860333488404
Iteration: 5 || Loss: 2.68821448713695
Iteration: 6 || Loss: 2.68821448713695
saving ADAM checkpoint...
Sum of params:85.43378
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.68821448713695
Iteration: 2 || Loss: 2.064253662266633
Iteration: 3 || Loss: 1.6411803382190693
Iteration: 4 || Loss: 1.6384751419199926
Iteration: 5 || Loss: 1.6168439860969355
Iteration: 6 || Loss: 1.6161087223623747
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.5003
Epoch 477 loss:1.6161087223623747
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.5003
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.197800807801409
Iteration: 2 || Loss: 6.197365830502743
Iteration: 3 || Loss: 6.1969303045360835
Iteration: 4 || Loss: 6.19649932910309
Iteration: 5 || Loss: 6.196066444868697
Iteration: 6 || Loss: 6.196066444868697
saving ADAM checkpoint...
Sum of params:85.50035
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.196066444868697
Iteration: 2 || Loss: 6.156751546510891
Iteration: 3 || Loss: 6.096476171831795
Iteration: 4 || Loss: 5.995781842178887
Iteration: 5 || Loss: 5.987619763559138
Iteration: 6 || Loss: 5.98049034143905
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.54948
Epoch 477 loss:5.98049034143905
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.54948
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.3815143592353
Iteration: 2 || Loss: 19.381017473767315
Iteration: 3 || Loss: 19.380522014146965
Iteration: 4 || Loss: 19.380028099005862
Iteration: 5 || Loss: 19.37953650642234
Iteration: 6 || Loss: 19.37953650642234
saving ADAM checkpoint...
Sum of params:85.549515
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.37953650642234
Iteration: 2 || Loss: 19.321680994420845
Iteration: 3 || Loss: 19.105519937872586
Iteration: 4 || Loss: 19.001260853914296
Iteration: 5 || Loss: 18.97355499431052
Iteration: 6 || Loss: 18.955867554808627
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.44244
Epoch 477 loss:18.955867554808627
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.812176557679188
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.34148834346186
waveform batch: 2/2
Test loss - extrapolation:15.838239135510403
Epoch 477 mean train loss:0.91560229719345
Epoch 477 mean test loss - interpolation:0.9686960929465314
Epoch 477 mean test loss - extrapolation:4.681643956581023
Start training epoch 478
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.44244
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.694042207197389
Iteration: 2 || Loss: 2.692465437073942
Iteration: 3 || Loss: 2.6908875978044744
Iteration: 4 || Loss: 2.6893122789411414
Iteration: 5 || Loss: 2.687737835252344
Iteration: 6 || Loss: 2.687737835252344
saving ADAM checkpoint...
Sum of params:85.44242
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.687737835252344
Iteration: 2 || Loss: 2.0632534132549294
Iteration: 3 || Loss: 1.640588897918055
Iteration: 4 || Loss: 1.6378781566606195
Iteration: 5 || Loss: 1.6162754152654113
Iteration: 6 || Loss: 1.6155378119226282
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.508896
Epoch 478 loss:1.6155378119226282
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.508896
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.19603680811568
Iteration: 2 || Loss: 6.195600587070184
Iteration: 3 || Loss: 6.19516546876936
Iteration: 4 || Loss: 6.1947327333622795
Iteration: 5 || Loss: 6.194300876314243
Iteration: 6 || Loss: 6.194300876314243
saving ADAM checkpoint...
Sum of params:85.50893
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.194300876314243
Iteration: 2 || Loss: 6.154926119217473
Iteration: 3 || Loss: 6.0947601232979745
Iteration: 4 || Loss: 5.9940788275474635
Iteration: 5 || Loss: 5.985896416386036
Iteration: 6 || Loss: 5.978777488268996
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.55817
Epoch 478 loss:5.978777488268996
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.55817
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.372213750008815
Iteration: 2 || Loss: 19.371716588955792
Iteration: 3 || Loss: 19.37122183949909
Iteration: 4 || Loss: 19.370727399599293
Iteration: 5 || Loss: 19.370233700816897
Iteration: 6 || Loss: 19.370233700816897
saving ADAM checkpoint...
Sum of params:85.55821
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.370233700816897
Iteration: 2 || Loss: 19.312302185677687
Iteration: 3 || Loss: 19.09607311053796
Iteration: 4 || Loss: 18.991887534618165
Iteration: 5 || Loss: 18.96420320766824
Iteration: 6 || Loss: 18.946552424306105
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.45109
Epoch 478 loss:18.946552424306105
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.81024594564266
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.32766145454111
waveform batch: 2/2
Test loss - extrapolation:15.829434287654077
Epoch 478 mean train loss:0.9152023353275079
Epoch 478 mean test loss - interpolation:0.9683743242737767
Epoch 478 mean test loss - extrapolation:4.679757978516265
Start training epoch 479
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.45109
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6934758946076336
Iteration: 2 || Loss: 2.691897023635425
Iteration: 3 || Loss: 2.6903188314637436
Iteration: 4 || Loss: 2.688742797752941
Iteration: 5 || Loss: 2.6871696311054745
Iteration: 6 || Loss: 2.6871696311054745
saving ADAM checkpoint...
Sum of params:85.451096
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6871696311054745
Iteration: 2 || Loss: 2.0623955004782886
Iteration: 3 || Loss: 1.6399869665632094
Iteration: 4 || Loss: 1.6372704177105786
Iteration: 5 || Loss: 1.6157024880348578
Iteration: 6 || Loss: 1.6149631271006821
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.517456
Epoch 479 loss:1.6149631271006821
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.517456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1943061951631195
Iteration: 2 || Loss: 6.193870822240183
Iteration: 3 || Loss: 6.193434912497257
Iteration: 4 || Loss: 6.192999827486529
Iteration: 5 || Loss: 6.192567976492557
Iteration: 6 || Loss: 6.192567976492557
saving ADAM checkpoint...
Sum of params:85.51751
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.192567976492557
Iteration: 2 || Loss: 6.1530500768656555
Iteration: 3 || Loss: 6.092987851348396
Iteration: 4 || Loss: 5.992372573717268
Iteration: 5 || Loss: 5.984183733879577
Iteration: 6 || Loss: 5.977074945292883
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.56683
Epoch 479 loss:5.977074945292883
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.56683
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.36294603403963
Iteration: 2 || Loss: 19.362448331062645
Iteration: 3 || Loss: 19.36195278812496
Iteration: 4 || Loss: 19.361456973078642
Iteration: 5 || Loss: 19.36096457040624
Iteration: 6 || Loss: 19.36096457040624
saving ADAM checkpoint...
Sum of params:85.566864
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.36096457040624
Iteration: 2 || Loss: 19.30291993550448
Iteration: 3 || Loss: 19.08661001229891
Iteration: 4 || Loss: 18.982531220164173
Iteration: 5 || Loss: 18.954867158550112
Iteration: 6 || Loss: 18.93726051065887
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.45973
Epoch 479 loss:18.93726051065887
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.808279456879997
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.31376969639311
waveform batch: 2/2
Test loss - extrapolation:15.820648324089039
Epoch 479 mean train loss:0.9148033994156013
Epoch 479 mean test loss - interpolation:0.9680465761466661
Epoch 479 mean test loss - extrapolation:4.677868168373512
Start training epoch 480
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.45973
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.69273775350916
Iteration: 2 || Loss: 2.6911583804262995
Iteration: 3 || Loss: 2.689578100449168
Iteration: 4 || Loss: 2.6880000315760832
Iteration: 5 || Loss: 2.6864282123767294
Iteration: 6 || Loss: 2.6864282123767294
saving ADAM checkpoint...
Sum of params:85.45974
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6864282123767294
Iteration: 2 || Loss: 2.061523399400382
Iteration: 3 || Loss: 1.6393871629691295
Iteration: 4 || Loss: 1.6366650896876573
Iteration: 5 || Loss: 1.6151313798056353
Iteration: 6 || Loss: 1.6143918640671528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.525986
Epoch 480 loss:1.6143918640671528
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.525986
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.192480008574394
Iteration: 2 || Loss: 6.192043597958794
Iteration: 3 || Loss: 6.191607641750598
Iteration: 4 || Loss: 6.19117333755159
Iteration: 5 || Loss: 6.190741182789389
Iteration: 6 || Loss: 6.190741182789389
saving ADAM checkpoint...
Sum of params:85.52603
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.190741182789389
Iteration: 2 || Loss: 6.151197341899796
Iteration: 3 || Loss: 6.091255099626633
Iteration: 4 || Loss: 5.990673580123667
Iteration: 5 || Loss: 5.98245921656848
Iteration: 6 || Loss: 5.9753647362163065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.57551
Epoch 480 loss:5.9753647362163065
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.57551
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.35375380455748
Iteration: 2 || Loss: 19.35325653142023
Iteration: 3 || Loss: 19.35275826573603
Iteration: 4 || Loss: 19.35226361914843
Iteration: 5 || Loss: 19.35177027364014
Iteration: 6 || Loss: 19.35177027364014
saving ADAM checkpoint...
Sum of params:85.57553
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.35177027364014
Iteration: 2 || Loss: 19.293715700652406
Iteration: 3 || Loss: 19.07727000555113
Iteration: 4 || Loss: 18.973244688111958
Iteration: 5 || Loss: 18.945585033280594
Iteration: 6 || Loss: 18.927996283579084
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.46835
Epoch 480 loss:18.927996283579084
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.806345185825368
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.300068446840314
waveform batch: 2/2
Test loss - extrapolation:15.811893338071453
Epoch 480 mean train loss:0.914405271857329
Epoch 480 mean test loss - interpolation:0.9677241976375613
Epoch 480 mean test loss - extrapolation:4.675996815409314
Start training epoch 481
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.46835
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6922587862065317
Iteration: 2 || Loss: 2.6906769448616736
Iteration: 3 || Loss: 2.689096699747763
Iteration: 4 || Loss: 2.6875197511155737
Iteration: 5 || Loss: 2.6859462303933337
Iteration: 6 || Loss: 2.6859462303933337
saving ADAM checkpoint...
Sum of params:85.468346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6859462303933337
Iteration: 2 || Loss: 2.0605961729603504
Iteration: 3 || Loss: 1.638801473413388
Iteration: 4 || Loss: 1.636074406703524
Iteration: 5 || Loss: 1.6145658081739767
Iteration: 6 || Loss: 1.6138223294921246
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.5345
Epoch 481 loss:1.6138223294921246
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.5345
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1907971787191896
Iteration: 2 || Loss: 6.190358465405916
Iteration: 3 || Loss: 6.189921494762743
Iteration: 4 || Loss: 6.189487560147042
Iteration: 5 || Loss: 6.189053284146252
Iteration: 6 || Loss: 6.189053284146252
saving ADAM checkpoint...
Sum of params:85.53456
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.189053284146252
Iteration: 2 || Loss: 6.149335117860623
Iteration: 3 || Loss: 6.089512929392344
Iteration: 4 || Loss: 5.98898088941072
Iteration: 5 || Loss: 5.980764917223727
Iteration: 6 || Loss: 5.973677851515583
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.58412
Epoch 481 loss:5.973677851515583
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.58412
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.34447331716937
Iteration: 2 || Loss: 19.3439747440554
Iteration: 3 || Loss: 19.343476775277214
Iteration: 4 || Loss: 19.342978965355726
Iteration: 5 || Loss: 19.342486603604602
Iteration: 6 || Loss: 19.342486603604602
saving ADAM checkpoint...
Sum of params:85.58415
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.342486603604602
Iteration: 2 || Loss: 19.28429230901181
Iteration: 3 || Loss: 19.067826679195523
Iteration: 4 || Loss: 18.963923499697195
Iteration: 5 || Loss: 18.936293241749418
Iteration: 6 || Loss: 18.918752904273138
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.47696
Epoch 481 loss:18.918752904273138
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.804404179874576
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.2862047696072
waveform batch: 2/2
Test loss - extrapolation:15.803142357833227
Epoch 481 mean train loss:0.9140087270786499
Epoch 481 mean test loss - interpolation:0.9674006966457626
Epoch 481 mean test loss - extrapolation:4.674112260620036
Start training epoch 482
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.47696
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6915086343292303
Iteration: 2 || Loss: 2.6899284046540237
Iteration: 3 || Loss: 2.6883469659644765
Iteration: 4 || Loss: 2.6867689382008417
Iteration: 5 || Loss: 2.6851919698701203
Iteration: 6 || Loss: 2.6851919698701203
saving ADAM checkpoint...
Sum of params:85.476974
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6851919698701203
Iteration: 2 || Loss: 2.059752783606298
Iteration: 3 || Loss: 1.6381995499204296
Iteration: 4 || Loss: 1.6354668436177224
Iteration: 5 || Loss: 1.613997734907573
Iteration: 6 || Loss: 1.6132525912075202
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.543015
Epoch 482 loss:1.6132525912075202
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.543015
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.188997194280351
Iteration: 2 || Loss: 6.188557858948935
Iteration: 3 || Loss: 6.18812118643337
Iteration: 4 || Loss: 6.18768545883127
Iteration: 5 || Loss: 6.187252462533157
Iteration: 6 || Loss: 6.187252462533157
saving ADAM checkpoint...
Sum of params:85.54306
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.187252462533157
Iteration: 2 || Loss: 6.147480710287263
Iteration: 3 || Loss: 6.087775924360689
Iteration: 4 || Loss: 5.987296635558123
Iteration: 5 || Loss: 5.979057273168084
Iteration: 6 || Loss: 5.971986237899653
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.59273
Epoch 482 loss:5.971986237899653
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.59273
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.33534089874974
Iteration: 2 || Loss: 19.334841096155458
Iteration: 3 || Loss: 19.334344470257143
Iteration: 4 || Loss: 19.333848833687565
Iteration: 5 || Loss: 19.333352700462697
Iteration: 6 || Loss: 19.333352700462697
saving ADAM checkpoint...
Sum of params:85.59275
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.333352700462697
Iteration: 2 || Loss: 19.27512033318305
Iteration: 3 || Loss: 19.058508572355784
Iteration: 4 || Loss: 18.954678076240423
Iteration: 5 || Loss: 18.927058193628067
Iteration: 6 || Loss: 18.909541258552014
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.48553
Epoch 482 loss:18.909541258552014
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.802490578275272
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.272531160517396
waveform batch: 2/2
Test loss - extrapolation:15.794422494876898
Epoch 482 mean train loss:0.9136131064710065
Epoch 482 mean test loss - interpolation:0.9670817630458787
Epoch 482 mean test loss - extrapolation:4.672246137949524
Start training epoch 483
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.48553
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6910001299841806
Iteration: 2 || Loss: 2.6894164400998695
Iteration: 3 || Loss: 2.687836610452457
Iteration: 4 || Loss: 2.686256693564603
Iteration: 5 || Loss: 2.6846794252565793
Iteration: 6 || Loss: 2.6846794252565793
saving ADAM checkpoint...
Sum of params:85.485535
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6846794252565793
Iteration: 2 || Loss: 2.0588811471014634
Iteration: 3 || Loss: 1.6376140745267358
Iteration: 4 || Loss: 1.6348759971514282
Iteration: 5 || Loss: 1.6134354531167245
Iteration: 6 || Loss: 1.6126888844317155
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.55146
Epoch 483 loss:1.6126888844317155
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.55146
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.18716287029809
Iteration: 2 || Loss: 6.1867238406444
Iteration: 3 || Loss: 6.186286156209184
Iteration: 4 || Loss: 6.185851569092963
Iteration: 5 || Loss: 6.185418392075759
Iteration: 6 || Loss: 6.185418392075759
saving ADAM checkpoint...
Sum of params:85.551506
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.185418392075759
Iteration: 2 || Loss: 6.145702553497311
Iteration: 3 || Loss: 6.086108506787063
Iteration: 4 || Loss: 5.985613518850675
Iteration: 5 || Loss: 5.977332003335154
Iteration: 6 || Loss: 5.970282586184797
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.60136
Epoch 483 loss:5.970282586184797
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.60136
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.32623799296573
Iteration: 2 || Loss: 19.325739325974222
Iteration: 3 || Loss: 19.32524096400213
Iteration: 4 || Loss: 19.324745396048876
Iteration: 5 || Loss: 19.324250959516014
Iteration: 6 || Loss: 19.324250959516014
saving ADAM checkpoint...
Sum of params:85.6014
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.324250959516014
Iteration: 2 || Loss: 19.266067207752418
Iteration: 3 || Loss: 19.049284915399195
Iteration: 4 || Loss: 18.94548274761472
Iteration: 5 || Loss: 18.91785865680437
Iteration: 6 || Loss: 18.90033903499371
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.49406
Epoch 483 loss:18.90033903499371
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.8005785926942455
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.25904400684346
waveform batch: 2/2
Test loss - extrapolation:15.7857359920731
Epoch 483 mean train loss:0.9132176036417319
Epoch 483 mean test loss - interpolation:0.9667630987823742
Epoch 483 mean test loss - extrapolation:4.670398333243047
Start training epoch 484
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.49406
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6907187502031027
Iteration: 2 || Loss: 2.6891341788143577
Iteration: 3 || Loss: 2.6875499832478678
Iteration: 4 || Loss: 2.6859695922277163
Iteration: 5 || Loss: 2.6843928605134186
Iteration: 6 || Loss: 2.6843928605134186
saving ADAM checkpoint...
Sum of params:85.49405
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6843928605134186
Iteration: 2 || Loss: 2.057875784714312
Iteration: 3 || Loss: 1.637043811683697
Iteration: 4 || Loss: 1.6343030474859548
Iteration: 5 || Loss: 1.6128808349359038
Iteration: 6 || Loss: 1.6121297659284797
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.55992
Epoch 484 loss:1.6121297659284797
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.55992
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.185522356230752
Iteration: 2 || Loss: 6.185083234199969
Iteration: 3 || Loss: 6.184644685201208
Iteration: 4 || Loss: 6.184207823539195
Iteration: 5 || Loss: 6.183773892233919
Iteration: 6 || Loss: 6.183773892233919
saving ADAM checkpoint...
Sum of params:85.559944
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.183773892233919
Iteration: 2 || Loss: 6.143873002697415
Iteration: 3 || Loss: 6.084397913644988
Iteration: 4 || Loss: 5.983943409278969
Iteration: 5 || Loss: 5.975662533152848
Iteration: 6 || Loss: 5.968616236291129
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.60991
Epoch 484 loss:5.968616236291129
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.60991
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.317045636102122
Iteration: 2 || Loss: 19.31654650664012
Iteration: 3 || Loss: 19.316050052748327
Iteration: 4 || Loss: 19.315551547773786
Iteration: 5 || Loss: 19.31505503300741
Iteration: 6 || Loss: 19.31505503300741
saving ADAM checkpoint...
Sum of params:85.60994
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.31505503300741
Iteration: 2 || Loss: 19.256714597033373
Iteration: 3 || Loss: 19.039912255585726
Iteration: 4 || Loss: 18.936217851141418
Iteration: 5 || Loss: 18.908623888426696
Iteration: 6 || Loss: 18.891162380942035
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.50261
Epoch 484 loss:18.891162380942035
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.798635989690197
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.24518032805788
waveform batch: 2/2
Test loss - extrapolation:15.77704525315429
Epoch 484 mean train loss:0.9128244270055739
Epoch 484 mean test loss - interpolation:0.9664393316150329
Epoch 484 mean test loss - extrapolation:4.668518798434348
Start training epoch 485
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.50261
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.689820384518245
Iteration: 2 || Loss: 2.6882366594386147
Iteration: 3 || Loss: 2.686654069474468
Iteration: 4 || Loss: 2.6850742903296316
Iteration: 5 || Loss: 2.6834928563589076
Iteration: 6 || Loss: 2.6834928563589076
saving ADAM checkpoint...
Sum of params:85.50262
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6834928563589076
Iteration: 2 || Loss: 2.0570634865135196
Iteration: 3 || Loss: 1.6364440007239303
Iteration: 4 || Loss: 1.633694846049545
Iteration: 5 || Loss: 1.6123148572563022
Iteration: 6 || Loss: 1.6115609466213494
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.56834
Epoch 485 loss:1.6115609466213494
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.56834
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.183642641120631
Iteration: 2 || Loss: 6.183203299861847
Iteration: 3 || Loss: 6.182764087483058
Iteration: 4 || Loss: 6.18232796428347
Iteration: 5 || Loss: 6.1818937027644525
Iteration: 6 || Loss: 6.1818937027644525
saving ADAM checkpoint...
Sum of params:85.568375
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.1818937027644525
Iteration: 2 || Loss: 6.142016237549035
Iteration: 3 || Loss: 6.082691373043759
Iteration: 4 || Loss: 5.98226710569788
Iteration: 5 || Loss: 5.97395367024054
Iteration: 6 || Loss: 5.966928685849847
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.61847
Epoch 485 loss:5.966928685849847
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.61847
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.307947768499137
Iteration: 2 || Loss: 19.307448331126423
Iteration: 3 || Loss: 19.306948582413174
Iteration: 4 || Loss: 19.30645244404197
Iteration: 5 || Loss: 19.305957847025695
Iteration: 6 || Loss: 19.305957847025695
saving ADAM checkpoint...
Sum of params:85.618515
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.305957847025695
Iteration: 2 || Loss: 19.247629880953504
Iteration: 3 || Loss: 19.030694768425157
Iteration: 4 || Loss: 18.927058606246717
Iteration: 5 || Loss: 18.89946704868159
Iteration: 6 || Loss: 18.882011773542864
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.511116
Epoch 485 loss:18.882011773542864
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.796758980021191
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.23170582466904
waveform batch: 2/2
Test loss - extrapolation:15.76838126216577
Epoch 485 mean train loss:0.9124310829660021
Epoch 485 mean test loss - interpolation:0.9661264966701985
Epoch 485 mean test loss - extrapolation:4.666673923902901
Start training epoch 486
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.511116
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.689530832412919
Iteration: 2 || Loss: 2.6879448394660064
Iteration: 3 || Loss: 2.686358639371929
Iteration: 4 || Loss: 2.684776471956935
Iteration: 5 || Loss: 2.68319580374511
Iteration: 6 || Loss: 2.68319580374511
saving ADAM checkpoint...
Sum of params:85.511116
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.68319580374511
Iteration: 2 || Loss: 2.056107640200736
Iteration: 3 || Loss: 1.6358721394841371
Iteration: 4 || Loss: 1.633118208117509
Iteration: 5 || Loss: 1.6117620016705707
Iteration: 6 || Loss: 1.6110061738802124
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.57675
Epoch 486 loss:1.6110061738802124
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.57675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.182084226528725
Iteration: 2 || Loss: 6.18164361944571
Iteration: 3 || Loss: 6.181203526294866
Iteration: 4 || Loss: 6.18076602181093
Iteration: 5 || Loss: 6.180330045992271
Iteration: 6 || Loss: 6.180330045992271
saving ADAM checkpoint...
Sum of params:85.5768
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.180330045992271
Iteration: 2 || Loss: 6.140222748075893
Iteration: 3 || Loss: 6.080969529909875
Iteration: 4 || Loss: 5.980614576003329
Iteration: 5 || Loss: 5.972309510893171
Iteration: 6 || Loss: 5.965281461798056
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.626976
Epoch 486 loss:5.965281461798056
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.626976
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.29881176789271
Iteration: 2 || Loss: 19.298310820066977
Iteration: 3 || Loss: 19.297810235454357
Iteration: 4 || Loss: 19.2973117455229
Iteration: 5 || Loss: 19.296815710110984
Iteration: 6 || Loss: 19.296815710110984
saving ADAM checkpoint...
Sum of params:85.627
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.296815710110984
Iteration: 2 || Loss: 19.238290613408797
Iteration: 3 || Loss: 19.02134199641737
Iteration: 4 || Loss: 18.917830499155905
Iteration: 5 || Loss: 18.890274998420285
Iteration: 6 || Loss: 18.872888023626263
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.519646
Epoch 486 loss:18.872888023626263
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.794809106604468
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.21780605989605
waveform batch: 2/2
Test loss - extrapolation:15.759718493328123
Epoch 486 mean train loss:0.9120405399760183
Epoch 486 mean test loss - interpolation:0.9658015177674114
Epoch 486 mean test loss - extrapolation:4.664793712768681
Start training epoch 487
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.519646
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.688500566148226
Iteration: 2 || Loss: 2.6869117786847454
Iteration: 3 || Loss: 2.6853285320835796
Iteration: 4 || Loss: 2.6837466777300323
Iteration: 5 || Loss: 2.6821648280485255
Iteration: 6 || Loss: 2.6821648280485255
saving ADAM checkpoint...
Sum of params:85.51965
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6821648280485255
Iteration: 2 || Loss: 2.055336089723387
Iteration: 3 || Loss: 1.6352740840589566
Iteration: 4 || Loss: 1.6325108822235332
Iteration: 5 || Loss: 1.611198031972647
Iteration: 6 || Loss: 1.610440857859458
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.585175
Epoch 487 loss:1.610440857859458
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.585175
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1802613834020805
Iteration: 2 || Loss: 6.179820259595833
Iteration: 3 || Loss: 6.179380759311292
Iteration: 4 || Loss: 6.178943188904681
Iteration: 5 || Loss: 6.178506833580582
Iteration: 6 || Loss: 6.178506833580582
saving ADAM checkpoint...
Sum of params:85.58522
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.178506833580582
Iteration: 2 || Loss: 6.138371567629243
Iteration: 3 || Loss: 6.079251013120931
Iteration: 4 || Loss: 5.97895008509768
Iteration: 5 || Loss: 5.970622844299818
Iteration: 6 || Loss: 5.963611701042568
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.63548
Epoch 487 loss:5.963611701042568
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.63548
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.289756536424942
Iteration: 2 || Loss: 19.2892551625761
Iteration: 3 || Loss: 19.288754959085406
Iteration: 4 || Loss: 19.288256174241685
Iteration: 5 || Loss: 19.287760789301533
Iteration: 6 || Loss: 19.287760789301533
saving ADAM checkpoint...
Sum of params:85.63551
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.287760789301533
Iteration: 2 || Loss: 19.229240440276634
Iteration: 3 || Loss: 19.012159851720288
Iteration: 4 || Loss: 18.908719456761226
Iteration: 5 || Loss: 18.8811682839566
Iteration: 6 || Loss: 18.863788513818402
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.528076
Epoch 487 loss:18.863788513818402
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.792935834836836
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.204359491409775
waveform batch: 2/2
Test loss - extrapolation:15.751092225886971
Epoch 487 mean train loss:0.9116496921627734
Epoch 487 mean test loss - interpolation:0.9654893058061393
Epoch 487 mean test loss - extrapolation:4.662954309774729
Start training epoch 488
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.528076
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.688164654795966
Iteration: 2 || Loss: 2.6865790481723146
Iteration: 3 || Loss: 2.6849938620974694
Iteration: 4 || Loss: 2.6834071573057185
Iteration: 5 || Loss: 2.6818250789128113
Iteration: 6 || Loss: 2.6818250789128113
saving ADAM checkpoint...
Sum of params:85.52809
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6818250789128113
Iteration: 2 || Loss: 2.05442164520666
Iteration: 3 || Loss: 1.634706798418896
Iteration: 4 || Loss: 1.6319402349503191
Iteration: 5 || Loss: 1.6106502859050866
Iteration: 6 || Loss: 1.609888723618636
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.593506
Epoch 488 loss:1.609888723618636
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.593506
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.178555950319246
Iteration: 2 || Loss: 6.1781150127280275
Iteration: 3 || Loss: 6.177673586346415
Iteration: 4 || Loss: 6.177236429577423
Iteration: 5 || Loss: 6.1768001074597505
Iteration: 6 || Loss: 6.1768001074597505
saving ADAM checkpoint...
Sum of params:85.59356
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.1768001074597505
Iteration: 2 || Loss: 6.136609836150297
Iteration: 3 || Loss: 6.077606979831619
Iteration: 4 || Loss: 5.977303477944845
Iteration: 5 || Loss: 5.968945876617517
Iteration: 6 || Loss: 5.961952966015452
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.64399
Epoch 488 loss:5.961952966015452
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.64399
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.280750390418586
Iteration: 2 || Loss: 19.280248410911625
Iteration: 3 || Loss: 19.279748341663215
Iteration: 4 || Loss: 19.27924930211089
Iteration: 5 || Loss: 19.278752729881706
Iteration: 6 || Loss: 19.278752729881706
saving ADAM checkpoint...
Sum of params:85.644035
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.278752729881706
Iteration: 2 || Loss: 19.220190903742257
Iteration: 3 || Loss: 19.002986291035405
Iteration: 4 || Loss: 18.89960426559897
Iteration: 5 || Loss: 18.872064487304762
Iteration: 6 || Loss: 18.854709237323558
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.53653
Epoch 488 loss:18.854709237323558
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.791066570097637
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.19086421973874
waveform batch: 2/2
Test loss - extrapolation:15.742482816857514
Epoch 488 mean train loss:0.911260376791643
Epoch 488 mean test loss - interpolation:0.9651777616829396
Epoch 488 mean test loss - extrapolation:4.661112253049688
Start training epoch 489
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.53653
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.687725441492192
Iteration: 2 || Loss: 2.6861362351995908
Iteration: 3 || Loss: 2.684547081340082
Iteration: 4 || Loss: 2.682961752537693
Iteration: 5 || Loss: 2.681378742091794
Iteration: 6 || Loss: 2.681378742091794
saving ADAM checkpoint...
Sum of params:85.53656
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.681378742091794
Iteration: 2 || Loss: 2.053530992248484
Iteration: 3 || Loss: 1.6341323892554724
Iteration: 4 || Loss: 1.6313604254116425
Iteration: 5 || Loss: 1.6100980505619777
Iteration: 6 || Loss: 1.6093349973567619
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.60185
Epoch 489 loss:1.6093349973567619
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.60185
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.176844337485621
Iteration: 2 || Loss: 6.176399873926184
Iteration: 3 || Loss: 6.175959566595411
Iteration: 4 || Loss: 6.1755215882725345
Iteration: 5 || Loss: 6.175083163410651
Iteration: 6 || Loss: 6.175083163410651
saving ADAM checkpoint...
Sum of params:85.6019
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.175083163410651
Iteration: 2 || Loss: 6.134829217910174
Iteration: 3 || Loss: 6.0759356638414275
Iteration: 4 || Loss: 5.9756596614690185
Iteration: 5 || Loss: 5.967284840486015
Iteration: 6 || Loss: 5.960301977271154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.65244
Epoch 489 loss:5.960301977271154
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.65244
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.27173479438168
Iteration: 2 || Loss: 19.2712337189254
Iteration: 3 || Loss: 19.270732188108756
Iteration: 4 || Loss: 19.2702332709453
Iteration: 5 || Loss: 19.269736534216115
Iteration: 6 || Loss: 19.269736534216115
saving ADAM checkpoint...
Sum of params:85.652504
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.269736534216115
Iteration: 2 || Loss: 19.21111803450316
Iteration: 3 || Loss: 18.99381479842654
Iteration: 4 || Loss: 18.890501167593417
Iteration: 5 || Loss: 18.862974985692297
Iteration: 6 || Loss: 18.845643618846832
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.54497
Epoch 489 loss:18.845643618846832
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.789156176675263
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.177315192147915
waveform batch: 2/2
Test loss - extrapolation:15.733901360747002
Epoch 489 mean train loss:0.9108717446025775
Epoch 489 mean test loss - interpolation:0.9648593627792105
Epoch 489 mean test loss - extrapolation:4.659268046074576
Start training epoch 490
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.54497
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6870938833707427
Iteration: 2 || Loss: 2.685502914861262
Iteration: 3 || Loss: 2.683913839170936
Iteration: 4 || Loss: 2.6823273878831673
Iteration: 5 || Loss: 2.680742586466343
Iteration: 6 || Loss: 2.680742586466343
saving ADAM checkpoint...
Sum of params:85.54497
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.680742586466343
Iteration: 2 || Loss: 2.052612916989713
Iteration: 3 || Loss: 1.6335593668359694
Iteration: 4 || Loss: 1.63078203671239
Iteration: 5 || Loss: 1.6095490776520216
Iteration: 6 || Loss: 1.6087823311994356
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.61018
Epoch 490 loss:1.6087823311994356
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.61018
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.175122924550639
Iteration: 2 || Loss: 6.174679418033801
Iteration: 3 || Loss: 6.174238338002731
Iteration: 4 || Loss: 6.173799263835919
Iteration: 5 || Loss: 6.173360732166404
Iteration: 6 || Loss: 6.173360732166404
saving ADAM checkpoint...
Sum of params:85.61024
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.173360732166404
Iteration: 2 || Loss: 6.133040843130784
Iteration: 3 || Loss: 6.074272121952096
Iteration: 4 || Loss: 5.9740226565557135
Iteration: 5 || Loss: 5.965628978923872
Iteration: 6 || Loss: 5.958657763545575
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.660904
Epoch 490 loss:5.958657763545575
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.660904
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.262724042821965
Iteration: 2 || Loss: 19.262223063221835
Iteration: 3 || Loss: 19.261722271047297
Iteration: 4 || Loss: 19.261222852610583
Iteration: 5 || Loss: 19.260724429247784
Iteration: 6 || Loss: 19.260724429247784
saving ADAM checkpoint...
Sum of params:85.66094
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.260724429247784
Iteration: 2 || Loss: 19.20204976553332
Iteration: 3 || Loss: 18.98466960896069
Iteration: 4 || Loss: 18.881421079985902
Iteration: 5 || Loss: 18.853910237480598
Iteration: 6 || Loss: 18.83660721051799
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.553375
Epoch 490 loss:18.83660721051799
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.787288800174333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.16380601449215
waveform batch: 2/2
Test loss - extrapolation:15.72531668276188
Epoch 490 mean train loss:0.9104843898366551
Epoch 490 mean test loss - interpolation:0.9645481333623889
Epoch 490 mean test loss - extrapolation:4.657426891437836
Start training epoch 491
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.553375
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.686556248429631
Iteration: 2 || Loss: 2.684962205090092
Iteration: 3 || Loss: 2.6833750586413867
Iteration: 4 || Loss: 2.6817889785175426
Iteration: 5 || Loss: 2.6802034740501863
Iteration: 6 || Loss: 2.6802034740501863
saving ADAM checkpoint...
Sum of params:85.5534
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6802034740501863
Iteration: 2 || Loss: 2.051737265876406
Iteration: 3 || Loss: 1.632986592439133
Iteration: 4 || Loss: 1.6302040488940868
Iteration: 5 || Loss: 1.6090013268008074
Iteration: 6 || Loss: 1.6082330995173624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.61852
Epoch 491 loss:1.6082330995173624
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.61852
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.173509160059351
Iteration: 2 || Loss: 6.173065292220575
Iteration: 3 || Loss: 6.172623076537706
Iteration: 4 || Loss: 6.1721834974631
Iteration: 5 || Loss: 6.171744210265922
Iteration: 6 || Loss: 6.171744210265922
saving ADAM checkpoint...
Sum of params:85.61857
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.171744210265922
Iteration: 2 || Loss: 6.131272397496205
Iteration: 3 || Loss: 6.07258057717743
Iteration: 4 || Loss: 5.972393126836413
Iteration: 5 || Loss: 5.963994204733686
Iteration: 6 || Loss: 5.95702855887321
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.66933
Epoch 491 loss:5.95702855887321
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.66933
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.253752676609807
Iteration: 2 || Loss: 19.253249316719085
Iteration: 3 || Loss: 19.252749171144043
Iteration: 4 || Loss: 19.252247796355135
Iteration: 5 || Loss: 19.25174956817884
Iteration: 6 || Loss: 19.25174956817884
saving ADAM checkpoint...
Sum of params:85.669365
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.25174956817884
Iteration: 2 || Loss: 19.19295273415574
Iteration: 3 || Loss: 18.97549457667248
Iteration: 4 || Loss: 18.87234132369206
Iteration: 5 || Loss: 18.84485330745964
Iteration: 6 || Loss: 18.827596575014038
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.56181
Epoch 491 loss:18.827596575014038
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.785394080053089
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.15018061732133
waveform batch: 2/2
Test loss - extrapolation:15.716747970989903
Epoch 491 mean train loss:0.9100985597725727
Epoch 491 mean test loss - interpolation:0.9642323466755148
Epoch 491 mean test loss - extrapolation:4.65557738235927
Start training epoch 492
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.56181
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6857872614281644
Iteration: 2 || Loss: 2.684195100326224
Iteration: 3 || Loss: 2.682603855511018
Iteration: 4 || Loss: 2.681015539813022
Iteration: 5 || Loss: 2.679429197397485
Iteration: 6 || Loss: 2.679429197397485
saving ADAM checkpoint...
Sum of params:85.56182
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.679429197397485
Iteration: 2 || Loss: 2.0509206869037353
Iteration: 3 || Loss: 1.6324107079652155
Iteration: 4 || Loss: 1.6296190069968102
Iteration: 5 || Loss: 1.6084510541016157
Iteration: 6 || Loss: 1.6076785695442142
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.62684
Epoch 492 loss:1.6076785695442142
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.62684
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1717773450504545
Iteration: 2 || Loss: 6.171333883960526
Iteration: 3 || Loss: 6.170890565643988
Iteration: 4 || Loss: 6.170449488419067
Iteration: 5 || Loss: 6.170009784386272
Iteration: 6 || Loss: 6.170009784386272
saving ADAM checkpoint...
Sum of params:85.62688
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.170009784386272
Iteration: 2 || Loss: 6.1294496231493705
Iteration: 3 || Loss: 6.070895562337053
Iteration: 4 || Loss: 5.970757992692689
Iteration: 5 || Loss: 5.962346708257579
Iteration: 6 || Loss: 5.955392592338939
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.67774
Epoch 492 loss:5.955392592338939
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.67774
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.244785402160407
Iteration: 2 || Loss: 19.24428470318639
Iteration: 3 || Loss: 19.24378163546659
Iteration: 4 || Loss: 19.243280555425436
Iteration: 5 || Loss: 19.242781017553586
Iteration: 6 || Loss: 19.242781017553586
saving ADAM checkpoint...
Sum of params:85.67778
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.242781017553586
Iteration: 2 || Loss: 19.183922194595738
Iteration: 3 || Loss: 18.96637430360628
Iteration: 4 || Loss: 18.8633165650145
Iteration: 5 || Loss: 18.83584432870224
Iteration: 6 || Loss: 18.818606985407396
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.57015
Epoch 492 loss:18.818606985407396
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.783519390720243
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.136730483696766
waveform batch: 2/2
Test loss - extrapolation:15.708214710387297
Epoch 492 mean train loss:0.9097130395617431
Epoch 492 mean test loss - interpolation:0.9639198984533738
Epoch 492 mean test loss - extrapolation:4.6537454328403385
Start training epoch 493
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.57015
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.685232843623294
Iteration: 2 || Loss: 2.683641682220112
Iteration: 3 || Loss: 2.682049755962814
Iteration: 4 || Loss: 2.680457431578288
Iteration: 5 || Loss: 2.678869593183489
Iteration: 6 || Loss: 2.678869593183489
saving ADAM checkpoint...
Sum of params:85.570175
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.678869593183489
Iteration: 2 || Loss: 2.0500397772179157
Iteration: 3 || Loss: 1.6318427924674908
Iteration: 4 || Loss: 1.629047836425141
Iteration: 5 || Loss: 1.6079081917194884
Iteration: 6 || Loss: 1.6071323970387708
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.63508
Epoch 493 loss:1.6071323970387708
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.63508
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.170098663192315
Iteration: 2 || Loss: 6.169654671407296
Iteration: 3 || Loss: 6.169212154337019
Iteration: 4 || Loss: 6.168770004837307
Iteration: 5 || Loss: 6.16833087958045
Iteration: 6 || Loss: 6.16833087958045
saving ADAM checkpoint...
Sum of params:85.635124
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.16833087958045
Iteration: 2 || Loss: 6.127693117185804
Iteration: 3 || Loss: 6.069254427757224
Iteration: 4 || Loss: 5.969139998938753
Iteration: 5 || Loss: 5.9607042851769085
Iteration: 6 || Loss: 5.9537666627653385
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.686104
Epoch 493 loss:5.9537666627653385
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.686104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.23585638977325
Iteration: 2 || Loss: 19.23535275268911
Iteration: 3 || Loss: 19.234851031772962
Iteration: 4 || Loss: 19.23435010739927
Iteration: 5 || Loss: 19.23385233919547
Iteration: 6 || Loss: 19.23385233919547
saving ADAM checkpoint...
Sum of params:85.68616
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.23385233919547
Iteration: 2 || Loss: 19.174960065700827
Iteration: 3 || Loss: 18.95729555892297
Iteration: 4 || Loss: 18.854309202510663
Iteration: 5 || Loss: 18.826848577799947
Iteration: 6 || Loss: 18.80963306594899
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.57849
Epoch 493 loss:18.80963306594899
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.781687960691194
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.123385317586724
waveform batch: 2/2
Test loss - extrapolation:15.699687075133125
Epoch 493 mean train loss:0.9093286939914862
Epoch 493 mean test loss - interpolation:0.963614660115199
Epoch 493 mean test loss - extrapolation:4.651922699393321
Start training epoch 494
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.57849
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.684853275037632
Iteration: 2 || Loss: 2.6832611042845556
Iteration: 3 || Loss: 2.681665638194543
Iteration: 4 || Loss: 2.68007489322679
Iteration: 5 || Loss: 2.678488397204242
Iteration: 6 || Loss: 2.678488397204242
saving ADAM checkpoint...
Sum of params:85.57851
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.678488397204242
Iteration: 2 || Loss: 2.049151698684907
Iteration: 3 || Loss: 1.6312812526395464
Iteration: 4 || Loss: 1.6284801749457254
Iteration: 5 || Loss: 1.607368130655227
Iteration: 6 || Loss: 1.6065882931934037
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.643295
Epoch 494 loss:1.6065882931934037
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.643295
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.168398695058657
Iteration: 2 || Loss: 6.167952776067894
Iteration: 3 || Loss: 6.167508999026189
Iteration: 4 || Loss: 6.167066732208698
Iteration: 5 || Loss: 6.166627505303369
Iteration: 6 || Loss: 6.166627505303369
saving ADAM checkpoint...
Sum of params:85.64336
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.166627505303369
Iteration: 2 || Loss: 6.125961160885953
Iteration: 3 || Loss: 6.067641145577804
Iteration: 4 || Loss: 5.967529526105091
Iteration: 5 || Loss: 5.9590609742201
Iteration: 6 || Loss: 5.952140950217768
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.694496
Epoch 494 loss:5.952140950217768
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.694496
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.227017553544364
Iteration: 2 || Loss: 19.226514273148993
Iteration: 3 || Loss: 19.22601064662599
Iteration: 4 || Loss: 19.22551106936489
Iteration: 5 || Loss: 19.225011180764056
Iteration: 6 || Loss: 19.225011180764056
saving ADAM checkpoint...
Sum of params:85.69453
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.225011180764056
Iteration: 2 || Loss: 19.166106982579127
Iteration: 3 || Loss: 18.948296002804156
Iteration: 4 || Loss: 18.845339148858884
Iteration: 5 || Loss: 18.817884179031953
Iteration: 6 || Loss: 18.8006805693273
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.586815
Epoch 494 loss:18.8006805693273
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.779809780479174
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.110027442907864
waveform batch: 2/2
Test loss - extrapolation:15.691204418184435
Epoch 494 mean train loss:0.9089451659564991
Epoch 494 mean test loss - interpolation:0.9633016300798624
Epoch 494 mean test loss - extrapolation:4.6501026550910245
Start training epoch 495
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.586815
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6843173433478906
Iteration: 2 || Loss: 2.68272086793168
Iteration: 3 || Loss: 2.6811270264020015
Iteration: 4 || Loss: 2.67953567739473
Iteration: 5 || Loss: 2.677946761583933
Iteration: 6 || Loss: 2.677946761583933
saving ADAM checkpoint...
Sum of params:85.58683
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.677946761583933
Iteration: 2 || Loss: 2.048197650119669
Iteration: 3 || Loss: 1.6307221897322388
Iteration: 4 || Loss: 1.6279157171791117
Iteration: 5 || Loss: 1.6068287335972782
Iteration: 6 || Loss: 1.6060439462372398
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.65151
Epoch 495 loss:1.6060439462372398
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.65151
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.16674361416371
Iteration: 2 || Loss: 6.166297033792914
Iteration: 3 || Loss: 6.165853186695677
Iteration: 4 || Loss: 6.165411248797899
Iteration: 5 || Loss: 6.164970158030121
Iteration: 6 || Loss: 6.164970158030121
saving ADAM checkpoint...
Sum of params:85.65158
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.164970158030121
Iteration: 2 || Loss: 6.1242115178523875
Iteration: 3 || Loss: 6.066012869223721
Iteration: 4 || Loss: 5.9659217080663245
Iteration: 5 || Loss: 5.957441971772948
Iteration: 6 || Loss: 5.950529359570894
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.70285
Epoch 495 loss:5.950529359570894
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.70285
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.218074274525502
Iteration: 2 || Loss: 19.21756929146384
Iteration: 3 || Loss: 19.217066112507904
Iteration: 4 || Loss: 19.21656526601537
Iteration: 5 || Loss: 19.216066090497737
Iteration: 6 || Loss: 19.216066090497737
saving ADAM checkpoint...
Sum of params:85.70289
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.216066090497737
Iteration: 2 || Loss: 19.157073992474132
Iteration: 3 || Loss: 18.939228453430403
Iteration: 4 || Loss: 18.836351884500317
Iteration: 5 || Loss: 18.80891816091817
Iteration: 6 || Loss: 18.79175136904881
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.59514
Epoch 495 loss:18.79175136904881
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.777978253126041
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.096597177924444
waveform batch: 2/2
Test loss - extrapolation:15.682704082931972
Epoch 495 mean train loss:0.9085629198226534
Epoch 495 mean test loss - interpolation:0.9629963755210068
Epoch 495 mean test loss - extrapolation:4.6482751050713675
Start training epoch 496
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.59514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.683739448951608
Iteration: 2 || Loss: 2.6821430273548637
Iteration: 3 || Loss: 2.680547197420424
Iteration: 4 || Loss: 2.678954138746597
Iteration: 5 || Loss: 2.6773634451926784
Iteration: 6 || Loss: 2.6773634451926784
saving ADAM checkpoint...
Sum of params:85.59515
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6773634451926784
Iteration: 2 || Loss: 2.0473673424791716
Iteration: 3 || Loss: 1.6301525270580879
Iteration: 4 || Loss: 1.6273385764557542
Iteration: 5 || Loss: 1.606285741456202
Iteration: 6 || Loss: 1.6054994382415608
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.65975
Epoch 496 loss:1.6054994382415608
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.65975
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.165107465473158
Iteration: 2 || Loss: 6.16465920811005
Iteration: 3 || Loss: 6.1642154221878185
Iteration: 4 || Loss: 6.163772281919999
Iteration: 5 || Loss: 6.163331369112324
Iteration: 6 || Loss: 6.163331369112324
saving ADAM checkpoint...
Sum of params:85.6598
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.163331369112324
Iteration: 2 || Loss: 6.122446734963481
Iteration: 3 || Loss: 6.064341266446091
Iteration: 4 || Loss: 5.964316920761269
Iteration: 5 || Loss: 5.955826345176543
Iteration: 6 || Loss: 5.948923106860078
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.71116
Epoch 496 loss:5.948923106860078
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.71116
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.209229045601067
Iteration: 2 || Loss: 19.208722935048353
Iteration: 3 || Loss: 19.208220536901823
Iteration: 4 || Loss: 19.207718532371405
Iteration: 5 || Loss: 19.20721635613258
Iteration: 6 || Loss: 19.20721635613258
saving ADAM checkpoint...
Sum of params:85.7112
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.20721635613258
Iteration: 2 || Loss: 19.14814432361179
Iteration: 3 || Loss: 18.930189455541388
Iteration: 4 || Loss: 18.827395197140913
Iteration: 5 || Loss: 18.799978402095416
Iteration: 6 || Loss: 18.782840247673626
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.60344
Epoch 496 loss:18.782840247673626
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.77610110142109
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.08315835539001
waveform batch: 2/2
Test loss - extrapolation:15.674234392875684
Epoch 496 mean train loss:0.9081814756129402
Epoch 496 mean test loss - interpolation:0.962683516903515
Epoch 496 mean test loss - extrapolation:4.6464493956888075
Start training epoch 497
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.60344
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6830387708410774
Iteration: 2 || Loss: 2.6814437933837874
Iteration: 3 || Loss: 2.6798487510088598
Iteration: 4 || Loss: 2.6782548125817507
Iteration: 5 || Loss: 2.676662621121462
Iteration: 6 || Loss: 2.676662621121462
saving ADAM checkpoint...
Sum of params:85.60346
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.676662621121462
Iteration: 2 || Loss: 2.0464915147991403
Iteration: 3 || Loss: 1.6295932802798705
Iteration: 4 || Loss: 1.6267745978850658
Iteration: 5 || Loss: 1.6057476809487543
Iteration: 6 || Loss: 1.6049523583268122
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.66792
Epoch 497 loss:1.6049523583268122
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.66792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.163249658729994
Iteration: 2 || Loss: 6.162802054677219
Iteration: 3 || Loss: 6.162358136028136
Iteration: 4 || Loss: 6.161916235366619
Iteration: 5 || Loss: 6.161475069573294
Iteration: 6 || Loss: 6.161475069573294
saving ADAM checkpoint...
Sum of params:85.66796
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.161475069573294
Iteration: 2 || Loss: 6.120684837954756
Iteration: 3 || Loss: 6.0627677692291355
Iteration: 4 || Loss: 5.962713731430109
Iteration: 5 || Loss: 5.954177004275303
Iteration: 6 || Loss: 5.947293476831071
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.719505
Epoch 497 loss:5.947293476831071
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.719505
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.200466110040484
Iteration: 2 || Loss: 19.199961222996095
Iteration: 3 || Loss: 19.199458488603337
Iteration: 4 || Loss: 19.198957360343215
Iteration: 5 || Loss: 19.198454507577825
Iteration: 6 || Loss: 19.198454507577825
saving ADAM checkpoint...
Sum of params:85.71955
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.198454507577825
Iteration: 2 || Loss: 19.139438784320646
Iteration: 3 || Loss: 18.92134038512475
Iteration: 4 || Loss: 18.818533651002202
Iteration: 5 || Loss: 18.79110946164779
Iteration: 6 || Loss: 18.77396353414746
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.611694
Epoch 497 loss:18.77396353414746
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.7742944814348665
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.07001111881881
waveform batch: 2/2
Test loss - extrapolation:15.665795114256289
Epoch 497 mean train loss:0.9078003230794945
Epoch 497 mean test loss - interpolation:0.9623824135724778
Epoch 497 mean test loss - extrapolation:4.6446505194229255
Start training epoch 498
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.611694
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.682794403375498
Iteration: 2 || Loss: 2.6811965559834947
Iteration: 3 || Loss: 2.6795988737955017
Iteration: 4 || Loss: 2.6780029626072803
Iteration: 5 || Loss: 2.6764106123664013
Iteration: 6 || Loss: 2.6764106123664013
saving ADAM checkpoint...
Sum of params:85.61171
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6764106123664013
Iteration: 2 || Loss: 2.0455167120440736
Iteration: 3 || Loss: 1.6290408085053056
Iteration: 4 || Loss: 1.6262179145194042
Iteration: 5 || Loss: 1.6052112381242194
Iteration: 6 || Loss: 1.6044139314731007
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.67609
Epoch 498 loss:1.6044139314731007
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.67609
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.161776976221554
Iteration: 2 || Loss: 6.161330900185094
Iteration: 3 || Loss: 6.16088442542761
Iteration: 4 || Loss: 6.160439720568223
Iteration: 5 || Loss: 6.159998069110055
Iteration: 6 || Loss: 6.159998069110055
saving ADAM checkpoint...
Sum of params:85.676125
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.159998069110055
Iteration: 2 || Loss: 6.118967987178843
Iteration: 3 || Loss: 6.061114575677745
Iteration: 4 || Loss: 5.9611202771868
Iteration: 5 || Loss: 5.95259146253139
Iteration: 6 || Loss: 5.945713099847083
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.72776
Epoch 498 loss:5.945713099847083
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.72776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.191560788746056
Iteration: 2 || Loss: 19.191055104292523
Iteration: 3 || Loss: 19.190551634627425
Iteration: 4 || Loss: 19.19004831370914
Iteration: 5 || Loss: 19.189548257364393
Iteration: 6 || Loss: 19.189548257364393
saving ADAM checkpoint...
Sum of params:85.7278
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.189548257364393
Iteration: 2 || Loss: 19.130356716259314
Iteration: 3 || Loss: 18.91224339391116
Iteration: 4 || Loss: 18.8095634109482
Iteration: 5 || Loss: 18.782177259065165
Iteration: 6 || Loss: 18.765092675979293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.619965
Epoch 498 loss:18.765092675979293
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.772426380733078
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.05646979457157
waveform batch: 2/2
Test loss - extrapolation:15.657339890365394
Epoch 498 mean train loss:0.9074213692172234
Epoch 498 mean test loss - interpolation:0.9620710634555131
Epoch 498 mean test loss - extrapolation:4.6428174737447465
Start training epoch 499
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.619965
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6819017569741193
Iteration: 2 || Loss: 2.680303676869977
Iteration: 3 || Loss: 2.6787069173087112
Iteration: 4 || Loss: 2.6771104701188375
Iteration: 5 || Loss: 2.675516762102256
Iteration: 6 || Loss: 2.675516762102256
saving ADAM checkpoint...
Sum of params:85.61998
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.675516762102256
Iteration: 2 || Loss: 2.044706221390583
Iteration: 3 || Loss: 1.6284672658806403
Iteration: 4 || Loss: 1.6256341519504194
Iteration: 5 || Loss: 1.6046691022351018
Iteration: 6 || Loss: 1.6038682851206647
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.684265
Epoch 499 loss:1.6038682851206647
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.684265
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.160079010892594
Iteration: 2 || Loss: 6.159631058449734
Iteration: 3 || Loss: 6.159183093044134
Iteration: 4 || Loss: 6.158738496707341
Iteration: 5 || Loss: 6.158296729770798
Iteration: 6 || Loss: 6.158296729770798
saving ADAM checkpoint...
Sum of params:85.68431
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.158296729770798
Iteration: 2 || Loss: 6.11720618656758
Iteration: 3 || Loss: 6.059475636908482
Iteration: 4 || Loss: 5.959529647681064
Iteration: 5 || Loss: 5.950985468546548
Iteration: 6 || Loss: 5.944115310209333
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.736046
Epoch 499 loss:5.944115310209333
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.736046
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.18276333084303
Iteration: 2 || Loss: 19.18225688823368
Iteration: 3 || Loss: 19.181752914908753
Iteration: 4 || Loss: 19.18124898954087
Iteration: 5 || Loss: 19.180749453637343
Iteration: 6 || Loss: 19.180749453637343
saving ADAM checkpoint...
Sum of params:85.73608
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.180749453637343
Iteration: 2 || Loss: 19.12150830896287
Iteration: 3 || Loss: 18.90330131131648
Iteration: 4 || Loss: 18.80069449906198
Iteration: 5 || Loss: 18.77331836515826
Iteration: 6 || Loss: 18.756252741321937
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.628204
Epoch 499 loss:18.756252741321937
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.770594346615381
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.04317941109866
waveform batch: 2/2
Test loss - extrapolation:15.648920435626582
Epoch 499 mean train loss:0.9070426322983425
Epoch 499 mean test loss - interpolation:0.9617657244358968
Epoch 499 mean test loss - extrapolation:4.641008320560437
Start training epoch 500
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.628204
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6813546161300565
Iteration: 2 || Loss: 2.679754895900332
Iteration: 3 || Loss: 2.6781591535983402
Iteration: 4 || Loss: 2.6765609461141127
Iteration: 5 || Loss: 2.674965377455
Iteration: 6 || Loss: 2.674965377455
saving ADAM checkpoint...
Sum of params:85.62821
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.674965377455
Iteration: 2 || Loss: 2.0438270577613378
Iteration: 3 || Loss: 1.627911821325797
Iteration: 4 || Loss: 1.6250715060373266
Iteration: 5 || Loss: 1.6041364480629254
Iteration: 6 || Loss: 1.6033349186407249
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.6924
Epoch 500 loss:1.6033349186407249
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.6924
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.158560073410592
Iteration: 2 || Loss: 6.158112131674254
Iteration: 3 || Loss: 6.157662554022831
Iteration: 4 || Loss: 6.157217615496339
Iteration: 5 || Loss: 6.1567741450935465
Iteration: 6 || Loss: 6.1567741450935465
saving ADAM checkpoint...
Sum of params:85.69244
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.1567741450935465
Iteration: 2 || Loss: 6.11550088046763
Iteration: 3 || Loss: 6.0578349765014385
Iteration: 4 || Loss: 5.957952839970193
Iteration: 5 || Loss: 5.949402609756671
Iteration: 6 || Loss: 5.94253955703144
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.74427
Epoch 500 loss:5.94253955703144
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.74427
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.17397417703234
Iteration: 2 || Loss: 19.17346591116242
Iteration: 3 || Loss: 19.17296266485494
Iteration: 4 || Loss: 19.17245797088495
Iteration: 5 || Loss: 19.171955726879872
Iteration: 6 || Loss: 19.171955726879872
saving ADAM checkpoint...
Sum of params:85.74429
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.171955726879872
Iteration: 2 || Loss: 19.112597789240557
Iteration: 3 || Loss: 18.89431299067612
Iteration: 4 || Loss: 18.79180556623073
Iteration: 5 || Loss: 18.764452817077583
Iteration: 6 || Loss: 18.74743168868835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.63643
Epoch 500 loss:18.74743168868835
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.7687651701682645
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.02980219844514
waveform batch: 2/2
Test loss - extrapolation:15.640494502564165
Epoch 500 mean train loss:0.9066657298055351
Epoch 500 mean test loss - interpolation:0.9614608616947108
Epoch 500 mean test loss - extrapolation:4.639191391750775
Start training epoch 501
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.63643
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.680684967803367
Iteration: 2 || Loss: 2.679085759855215
Iteration: 3 || Loss: 2.677485948594056
Iteration: 4 || Loss: 2.6758856742296397
Iteration: 5 || Loss: 2.6742907159160914
Iteration: 6 || Loss: 2.6742907159160914
saving ADAM checkpoint...
Sum of params:85.63643
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6742907159160914
Iteration: 2 || Loss: 2.0430129144864657
Iteration: 3 || Loss: 1.6273549138268923
Iteration: 4 || Loss: 1.6245096864215773
Iteration: 5 || Loss: 1.6036029151247857
Iteration: 6 || Loss: 1.6027909262630733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.70048
Epoch 501 loss:1.6027909262630733
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.70048
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.156761151123466
Iteration: 2 || Loss: 6.156311818414493
Iteration: 3 || Loss: 6.155865098043041
Iteration: 4 || Loss: 6.155418515222938
Iteration: 5 || Loss: 6.154974207303753
Iteration: 6 || Loss: 6.154974207303753
saving ADAM checkpoint...
Sum of params:85.70053
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.154974207303753
Iteration: 2 || Loss: 6.1137434105197235
Iteration: 3 || Loss: 6.05626439463551
Iteration: 4 || Loss: 5.95636929820218
Iteration: 5 || Loss: 5.9477830648433
Iteration: 6 || Loss: 5.940937572099405
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.75253
Epoch 501 loss:5.940937572099405
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.75253
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.165261195634738
Iteration: 2 || Loss: 19.16475464440299
Iteration: 3 || Loss: 19.164249412647603
Iteration: 4 || Loss: 19.163745358729468
Iteration: 5 || Loss: 19.163243187554755
Iteration: 6 || Loss: 19.163243187554755
saving ADAM checkpoint...
Sum of params:85.75256
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.163243187554755
Iteration: 2 || Loss: 19.10391638941656
Iteration: 3 || Loss: 18.885495052846917
Iteration: 4 || Loss: 18.783018606797004
Iteration: 5 || Loss: 18.755668143713258
Iteration: 6 || Loss: 18.738635729811453
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.64456
Epoch 501 loss:18.738635729811453
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.76696738967707
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.01672120019912
waveform batch: 2/2
Test loss - extrapolation:15.632128266227516
Epoch 501 mean train loss:0.9062884216611701
Epoch 501 mean test loss - interpolation:0.9611612316128451
Epoch 501 mean test loss - extrapolation:4.63740412220222
Start training epoch 502
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.64456
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6803696723446
Iteration: 2 || Loss: 2.6787681603176074
Iteration: 3 || Loss: 2.677167082189571
Iteration: 4 || Loss: 2.6755685139910574
Iteration: 5 || Loss: 2.6739727307849477
Iteration: 6 || Loss: 2.6739727307849477
saving ADAM checkpoint...
Sum of params:85.64459
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6739727307849477
Iteration: 2 || Loss: 2.0420557933780383
Iteration: 3 || Loss: 1.626806444722238
Iteration: 4 || Loss: 1.6239564639680828
Iteration: 5 || Loss: 1.6030727330185164
Iteration: 6 || Loss: 1.6022552629539861
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.70853
Epoch 502 loss:1.6022552629539861
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.70853
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.155060528859559
Iteration: 2 || Loss: 6.154610634209657
Iteration: 3 || Loss: 6.154164060566236
Iteration: 4 || Loss: 6.153719632907729
Iteration: 5 || Loss: 6.153276096233575
Iteration: 6 || Loss: 6.153276096233575
saving ADAM checkpoint...
Sum of params:85.70856
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.153276096233575
Iteration: 2 || Loss: 6.112094250230437
Iteration: 3 || Loss: 6.054736100240126
Iteration: 4 || Loss: 5.95479904020261
Iteration: 5 || Loss: 5.946168695355195
Iteration: 6 || Loss: 5.939346090596813
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.760765
Epoch 502 loss:5.939346090596813
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.760765
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.1565711866704
Iteration: 2 || Loss: 19.156064554039936
Iteration: 3 || Loss: 19.15555898786814
Iteration: 4 || Loss: 19.15505568240598
Iteration: 5 || Loss: 19.1545528980525
Iteration: 6 || Loss: 19.1545528980525
saving ADAM checkpoint...
Sum of params:85.7608
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.1545528980525
Iteration: 2 || Loss: 19.095271477330847
Iteration: 3 || Loss: 18.87670291407315
Iteration: 4 || Loss: 18.774226377848912
Iteration: 5 || Loss: 18.746878811965047
Iteration: 6 || Loss: 18.72984294050807
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.65274
Epoch 502 loss:18.72984294050807
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.765184428597623
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:40.00371263316504
waveform batch: 2/2
Test loss - extrapolation:15.62377174727643
Epoch 502 mean train loss:0.9059118722089265
Epoch 502 mean test loss - interpolation:0.9608640714329372
Epoch 502 mean test loss - extrapolation:4.6356236983701224
Start training epoch 503
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.65274
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.680152947344461
Iteration: 2 || Loss: 2.678548762301359
Iteration: 3 || Loss: 2.6769471357570476
Iteration: 4 || Loss: 2.6753452552064005
Iteration: 5 || Loss: 2.6737510692521376
Iteration: 6 || Loss: 2.6737510692521376
saving ADAM checkpoint...
Sum of params:85.65274
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6737510692521376
Iteration: 2 || Loss: 2.041073185589383
Iteration: 3 || Loss: 1.6262642519537143
Iteration: 4 || Loss: 1.6234118133557494
Iteration: 5 || Loss: 1.6025465957955487
Iteration: 6 || Loss: 1.6017228461536255
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.7166
Epoch 503 loss:1.6017228461536255
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.7166
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.153624233485442
Iteration: 2 || Loss: 6.153174060585975
Iteration: 3 || Loss: 6.1527255307962765
Iteration: 4 || Loss: 6.152278440821736
Iteration: 5 || Loss: 6.151832916317435
Iteration: 6 || Loss: 6.151832916317435
saving ADAM checkpoint...
Sum of params:85.716644
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.151832916317435
Iteration: 2 || Loss: 6.110369544092279
Iteration: 3 || Loss: 6.053103802519343
Iteration: 4 || Loss: 5.953237496399179
Iteration: 5 || Loss: 5.944617757605174
Iteration: 6 || Loss: 5.937794378093806
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.7689
Epoch 503 loss:5.937794378093806
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.7689
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.147794640030092
Iteration: 2 || Loss: 19.14728780210161
Iteration: 3 || Loss: 19.14678083985197
Iteration: 4 || Loss: 19.146276611439546
Iteration: 5 || Loss: 19.145773326949545
Iteration: 6 || Loss: 19.145773326949545
saving ADAM checkpoint...
Sum of params:85.768936
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.145773326949545
Iteration: 2 || Loss: 19.086282147702654
Iteration: 3 || Loss: 18.8677274817742
Iteration: 4 || Loss: 18.765356671251823
Iteration: 5 || Loss: 18.738044518503443
Iteration: 6 || Loss: 18.721082569605695
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.66093
Epoch 503 loss:18.721082569605695
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.763322302825043
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.99016323954138
waveform batch: 2/2
Test loss - extrapolation:15.61540168374866
Epoch 503 mean train loss:0.9055379239259698
Epoch 503 mean test loss - interpolation:0.9605537171375071
Epoch 503 mean test loss - extrapolation:4.633797076940836
Start training epoch 504
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.66093
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6790496303772344
Iteration: 2 || Loss: 2.6774480211743525
Iteration: 3 || Loss: 2.6758426631310637
Iteration: 4 || Loss: 2.674240757740112
Iteration: 5 || Loss: 2.6726455886265685
Iteration: 6 || Loss: 2.6726455886265685
saving ADAM checkpoint...
Sum of params:85.66095
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6726455886265685
Iteration: 2 || Loss: 2.040295156437213
Iteration: 3 || Loss: 1.6256885106721952
Iteration: 4 || Loss: 1.622825096162756
Iteration: 5 || Loss: 1.6020045926150566
Iteration: 6 || Loss: 1.6011759553049387
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.72467
Epoch 504 loss:1.6011759553049387
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.72467
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.15176392746557
Iteration: 2 || Loss: 6.151314135637783
Iteration: 3 || Loss: 6.150865863425774
Iteration: 4 || Loss: 6.15041930767597
Iteration: 5 || Loss: 6.1499749178826955
Iteration: 6 || Loss: 6.1499749178826955
saving ADAM checkpoint...
Sum of params:85.72473
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.1499749178826955
Iteration: 2 || Loss: 6.108628481179503
Iteration: 3 || Loss: 6.05152364026292
Iteration: 4 || Loss: 5.951662224085154
Iteration: 5 || Loss: 5.9429931410881025
Iteration: 6 || Loss: 5.936196347631803
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.77712
Epoch 504 loss:5.936196347631803
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.77712
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.139184154692455
Iteration: 2 || Loss: 19.138674860593305
Iteration: 3 || Loss: 19.138170185988997
Iteration: 4 || Loss: 19.137665873486053
Iteration: 5 || Loss: 19.13716327829564
Iteration: 6 || Loss: 19.13716327829564
saving ADAM checkpoint...
Sum of params:85.777176
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.13716327829564
Iteration: 2 || Loss: 19.077779576836818
Iteration: 3 || Loss: 18.8590213575763
Iteration: 4 || Loss: 18.75665245927042
Iteration: 5 || Loss: 18.729329024058504
Iteration: 6 || Loss: 18.712333765837553
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.66901
Epoch 504 loss:18.712333765837553
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.761559275366287
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.97730179699246
waveform batch: 2/2
Test loss - extrapolation:15.607081047823831
Epoch 504 mean train loss:0.9051622782335964
Epoch 504 mean test loss - interpolation:0.9602598792277145
Epoch 504 mean test loss - extrapolation:4.632031903734691
Start training epoch 505
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.66901
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.679006355534819
Iteration: 2 || Loss: 2.6774024668601757
Iteration: 3 || Loss: 2.6757991845290534
Iteration: 4 || Loss: 2.674196766844364
Iteration: 5 || Loss: 2.672594844044339
Iteration: 6 || Loss: 2.672594844044339
saving ADAM checkpoint...
Sum of params:85.66902
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.672594844044339
Iteration: 2 || Loss: 2.0392664526096183
Iteration: 3 || Loss: 1.6251601572059273
Iteration: 4 || Loss: 1.6222923685910635
Iteration: 5 || Loss: 1.601485152682721
Iteration: 6 || Loss: 1.6006514107666536
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.73267
Epoch 505 loss:1.6006514107666536
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.73267
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.150401467023959
Iteration: 2 || Loss: 6.149950742693504
Iteration: 3 || Loss: 6.149499692686463
Iteration: 4 || Loss: 6.149051258394713
Iteration: 5 || Loss: 6.14860615849044
Iteration: 6 || Loss: 6.14860615849044
saving ADAM checkpoint...
Sum of params:85.73271
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.14860615849044
Iteration: 2 || Loss: 6.106974220405394
Iteration: 3 || Loss: 6.049931286362262
Iteration: 4 || Loss: 5.9501152052839235
Iteration: 5 || Loss: 5.941451490209107
Iteration: 6 || Loss: 5.9346558411129875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.78525
Epoch 505 loss:5.9346558411129875
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.78525
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.130465388534788
Iteration: 2 || Loss: 19.129956844064143
Iteration: 3 || Loss: 19.129450345785838
Iteration: 4 || Loss: 19.128944053590164
Iteration: 5 || Loss: 19.128441491157936
Iteration: 6 || Loss: 19.128441491157936
saving ADAM checkpoint...
Sum of params:85.78529
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.128441491157936
Iteration: 2 || Loss: 19.068860529008113
Iteration: 3 || Loss: 18.850076561347432
Iteration: 4 || Loss: 18.747824054879437
Iteration: 5 || Loss: 18.720542462294418
Iteration: 6 || Loss: 18.703642075318676
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.67719
Epoch 505 loss:18.703642075318676
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.759715814444251
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.963660178474996
waveform batch: 2/2
Test loss - extrapolation:15.59873239199612
Epoch 505 mean train loss:0.9047913561102867
Epoch 505 mean test loss - interpolation:0.9599526357407084
Epoch 505 mean test loss - extrapolation:4.630199380872593
Start training epoch 506
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.67719
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6777237876978086
Iteration: 2 || Loss: 2.676116846346357
Iteration: 3 || Loss: 2.6745136854462466
Iteration: 4 || Loss: 2.672913405184555
Iteration: 5 || Loss: 2.671313414442302
Iteration: 6 || Loss: 2.671313414442302
saving ADAM checkpoint...
Sum of params:85.67721
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.671313414442302
Iteration: 2 || Loss: 2.038631149518091
Iteration: 3 || Loss: 1.6245740095836179
Iteration: 4 || Loss: 1.6216943360512213
Iteration: 5 || Loss: 1.6009355760847095
Iteration: 6 || Loss: 1.6001006427130768
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.74072
Epoch 506 loss:1.6001006427130768
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.74072
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.148558529291696
Iteration: 2 || Loss: 6.148106017410153
Iteration: 3 || Loss: 6.147656419560502
Iteration: 4 || Loss: 6.147209914178429
Iteration: 5 || Loss: 6.146763767701072
Iteration: 6 || Loss: 6.146763767701072
saving ADAM checkpoint...
Sum of params:85.74077
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.146763767701072
Iteration: 2 || Loss: 6.105234887466228
Iteration: 3 || Loss: 6.048318144569927
Iteration: 4 || Loss: 5.948534004782216
Iteration: 5 || Loss: 5.93983643206772
Iteration: 6 || Loss: 5.933059724263088
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.79342
Epoch 506 loss:5.933059724263088
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.79342
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.121895170179815
Iteration: 2 || Loss: 19.121387003610874
Iteration: 3 || Loss: 19.12087946498028
Iteration: 4 || Loss: 19.12037407085314
Iteration: 5 || Loss: 19.119871153724976
Iteration: 6 || Loss: 19.119871153724976
saving ADAM checkpoint...
Sum of params:85.79348
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.119871153724976
Iteration: 2 || Loss: 19.060355217294724
Iteration: 3 || Loss: 18.841395959493987
Iteration: 4 || Loss: 18.739174986185887
Iteration: 5 || Loss: 18.71188790389219
Iteration: 6 || Loss: 18.69496996791603
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.68528
Epoch 506 loss:18.69496996791603
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.75797550615123
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.950766403257454
waveform batch: 2/2
Test loss - extrapolation:15.590419821757425
Epoch 506 mean train loss:0.9044182874100757
Epoch 506 mean test loss - interpolation:0.9596625843585384
Epoch 506 mean test loss - extrapolation:4.628432185417907
Start training epoch 507
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.68528
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6776206387956965
Iteration: 2 || Loss: 2.6760137016966157
Iteration: 3 || Loss: 2.674409387252309
Iteration: 4 || Loss: 2.6728045479465865
Iteration: 5 || Loss: 2.671203988785818
Iteration: 6 || Loss: 2.671203988785818
saving ADAM checkpoint...
Sum of params:85.68528
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.671203988785818
Iteration: 2 || Loss: 2.037694939157196
Iteration: 3 || Loss: 1.6240412083967686
Iteration: 4 || Loss: 1.621158676144695
Iteration: 5 || Loss: 1.6004125982091608
Iteration: 6 || Loss: 1.5995652164424694
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.748695
Epoch 507 loss:1.5995652164424694
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.748695
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1469138815908675
Iteration: 2 || Loss: 6.146463736246991
Iteration: 3 || Loss: 6.146013235951467
Iteration: 4 || Loss: 6.1455648715075775
Iteration: 5 || Loss: 6.145118927176103
Iteration: 6 || Loss: 6.145118927176103
saving ADAM checkpoint...
Sum of params:85.74875
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.145118927176103
Iteration: 2 || Loss: 6.103546804379665
Iteration: 3 || Loss: 6.046788365509389
Iteration: 4 || Loss: 5.9469737104395985
Iteration: 5 || Loss: 5.9382535469523345
Iteration: 6 || Loss: 5.931490116712326
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.80152
Epoch 507 loss:5.931490116712326
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.80152
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.113245392800827
Iteration: 2 || Loss: 19.112738205382367
Iteration: 3 || Loss: 19.112230728170715
Iteration: 4 || Loss: 19.111724434904016
Iteration: 5 || Loss: 19.111220663567703
Iteration: 6 || Loss: 19.111220663567703
saving ADAM checkpoint...
Sum of params:85.80159
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.111220663567703
Iteration: 2 || Loss: 19.051665008232366
Iteration: 3 || Loss: 18.832651620705516
Iteration: 4 || Loss: 18.730488446447033
Iteration: 5 || Loss: 18.703215726429
Iteration: 6 || Loss: 18.686325844282713
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.69335
Epoch 507 loss:18.686325844282713
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.756174054773844
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.93756666100385
waveform batch: 2/2
Test loss - extrapolation:15.582146717539793
Epoch 507 mean train loss:0.90404762680819
Epoch 507 mean test loss - interpolation:0.9593623424623073
Epoch 507 mean test loss - extrapolation:4.6266427815453035
Start training epoch 508
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.69335
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6768977671423184
Iteration: 2 || Loss: 2.6752912089707395
Iteration: 3 || Loss: 2.673684175671974
Iteration: 4 || Loss: 2.672080032675197
Iteration: 5 || Loss: 2.6704761510781916
Iteration: 6 || Loss: 2.6704761510781916
saving ADAM checkpoint...
Sum of params:85.69337
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6704761510781916
Iteration: 2 || Loss: 2.0368506201570584
Iteration: 3 || Loss: 1.6234749922789646
Iteration: 4 || Loss: 1.620584800233751
Iteration: 5 || Loss: 1.5998728085192486
Iteration: 6 || Loss: 1.5990212106557222
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.75667
Epoch 508 loss:1.5990212106557222
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.75667
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.145273977602292
Iteration: 2 || Loss: 6.144821809848521
Iteration: 3 || Loss: 6.144370607636518
Iteration: 4 || Loss: 6.143921674789222
Iteration: 5 || Loss: 6.1434752482810655
Iteration: 6 || Loss: 6.1434752482810655
saving ADAM checkpoint...
Sum of params:85.75673
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.1434752482810655
Iteration: 2 || Loss: 6.101841669953052
Iteration: 3 || Loss: 6.045189232401658
Iteration: 4 || Loss: 5.945408279660123
Iteration: 5 || Loss: 5.936663461427029
Iteration: 6 || Loss: 5.929915674293378
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.80966
Epoch 508 loss:5.929915674293378
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.80966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.104714350037657
Iteration: 2 || Loss: 19.10420451961128
Iteration: 3 || Loss: 19.103697384409692
Iteration: 4 || Loss: 19.103192002215057
Iteration: 5 || Loss: 19.102686817550037
Iteration: 6 || Loss: 19.102686817550037
saving ADAM checkpoint...
Sum of params:85.809715
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.102686817550037
Iteration: 2 || Loss: 19.043112041458652
Iteration: 3 || Loss: 18.82395958838935
Iteration: 4 || Loss: 18.721840448779716
Iteration: 5 || Loss: 18.694577765303315
Iteration: 6 || Loss: 18.67770625978599
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.70142
Epoch 508 loss:18.67770625978599
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.754395797508112
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.92446372793668
waveform batch: 2/2
Test loss - extrapolation:15.573880202125212
Epoch 508 mean train loss:0.9036773498184514
Epoch 508 mean test loss - interpolation:0.959065966251352
Epoch 508 mean test loss - extrapolation:4.624861994171824
Start training epoch 509
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.70142
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6763419866616553
Iteration: 2 || Loss: 2.674735529568718
Iteration: 3 || Loss: 2.673129223241519
Iteration: 4 || Loss: 2.6715234798018366
Iteration: 5 || Loss: 2.6699196963278156
Iteration: 6 || Loss: 2.6699196963278156
saving ADAM checkpoint...
Sum of params:85.70143
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6699196963278156
Iteration: 2 || Loss: 2.0359967298921484
Iteration: 3 || Loss: 1.6229196705291766
Iteration: 4 || Loss: 1.6200227969125838
Iteration: 5 || Loss: 1.5993369835278886
Iteration: 6 || Loss: 1.5984816682946692
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.76468
Epoch 509 loss:1.5984816682946692
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.76468
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.143796826048847
Iteration: 2 || Loss: 6.143342952748774
Iteration: 3 || Loss: 6.142891941963619
Iteration: 4 || Loss: 6.1424427924634974
Iteration: 5 || Loss: 6.141993864426515
Iteration: 6 || Loss: 6.141993864426515
saving ADAM checkpoint...
Sum of params:85.76475
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.141993864426515
Iteration: 2 || Loss: 6.100108022237394
Iteration: 3 || Loss: 6.043532821031185
Iteration: 4 || Loss: 5.943848325243869
Iteration: 5 || Loss: 5.935119538591585
Iteration: 6 || Loss: 5.9283712733552685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.81772
Epoch 509 loss:5.9283712733552685
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.81772
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.096077951914367
Iteration: 2 || Loss: 19.095567739907608
Iteration: 3 || Loss: 19.095059078204812
Iteration: 4 || Loss: 19.094553008035245
Iteration: 5 || Loss: 19.094046824580698
Iteration: 6 || Loss: 19.094046824580698
saving ADAM checkpoint...
Sum of params:85.81776
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.094046824580698
Iteration: 2 || Loss: 19.03428795109068
Iteration: 3 || Loss: 18.815125098353537
Iteration: 4 || Loss: 18.713156968776772
Iteration: 5 || Loss: 18.68593477808651
Iteration: 6 || Loss: 18.66915047515448
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.70953
Epoch 509 loss:18.66915047515448
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.752552150555829
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.910896469780624
waveform batch: 2/2
Test loss - extrapolation:15.565621169516694
Epoch 509 mean train loss:0.9033104626484283
Epoch 509 mean test loss - interpolation:0.9587586917593048
Epoch 509 mean test loss - extrapolation:4.62304313660811
Start training epoch 510
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.70953
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6749908488841387
Iteration: 2 || Loss: 2.6733811431823433
Iteration: 3 || Loss: 2.67177933728844
Iteration: 4 || Loss: 2.6701706981068707
Iteration: 5 || Loss: 2.668567794361744
Iteration: 6 || Loss: 2.668567794361744
saving ADAM checkpoint...
Sum of params:85.70953
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.668567794361744
Iteration: 2 || Loss: 2.035393335477851
Iteration: 3 || Loss: 1.6223299725848994
Iteration: 4 || Loss: 1.6194205699967945
Iteration: 5 || Loss: 1.5987831745345567
Iteration: 6 || Loss: 1.597926766925595
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.772644
Epoch 510 loss:1.597926766925595
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.772644
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.141795938703669
Iteration: 2 || Loss: 6.14134349635488
Iteration: 3 || Loss: 6.140893434214555
Iteration: 4 || Loss: 6.140445348831934
Iteration: 5 || Loss: 6.139998140007464
Iteration: 6 || Loss: 6.139998140007464
saving ADAM checkpoint...
Sum of params:85.772705
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.139998140007464
Iteration: 2 || Loss: 6.098415943631646
Iteration: 3 || Loss: 6.041990846132145
Iteration: 4 || Loss: 5.94226246826416
Iteration: 5 || Loss: 5.933452183293483
Iteration: 6 || Loss: 5.926746749465771
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.82592
Epoch 510 loss:5.926746749465771
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.82592
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.087755235504467
Iteration: 2 || Loss: 19.08724794680092
Iteration: 3 || Loss: 19.086738970058214
Iteration: 4 || Loss: 19.08623343092396
Iteration: 5 || Loss: 19.08572827597234
Iteration: 6 || Loss: 19.08572827597234
saving ADAM checkpoint...
Sum of params:85.82594
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.08572827597234
Iteration: 2 || Loss: 19.026250195046664
Iteration: 3 || Loss: 18.806736105256466
Iteration: 4 || Loss: 18.704703052031466
Iteration: 5 || Loss: 18.677438483624
Iteration: 6 || Loss: 18.660541958875577
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.71747
Epoch 510 loss:18.660541958875577
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.750912397596416
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.89879459779071
waveform batch: 2/2
Test loss - extrapolation:15.55741025796667
Epoch 510 mean train loss:0.9029384646643773
Epoch 510 mean test loss - interpolation:0.9584853995994026
Epoch 510 mean test loss - extrapolation:4.621350404646448
Start training epoch 511
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.71747
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.675966360532261
Iteration: 2 || Loss: 2.6743558645706553
Iteration: 3 || Loss: 2.672744828204863
Iteration: 4 || Loss: 2.671136891572915
Iteration: 5 || Loss: 2.669530091720367
Iteration: 6 || Loss: 2.669530091720367
saving ADAM checkpoint...
Sum of params:85.717476
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.669530091720367
Iteration: 2 || Loss: 2.0341451768790852
Iteration: 3 || Loss: 1.6218458798376234
Iteration: 4 || Loss: 1.6189423006776014
Iteration: 5 || Loss: 1.5982895458845574
Iteration: 6 || Loss: 1.5974126818063443
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.780525
Epoch 511 loss:1.5974126818063443
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.780525
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1407860556841865
Iteration: 2 || Loss: 6.140329543824452
Iteration: 3 || Loss: 6.139877257694148
Iteration: 4 || Loss: 6.1394254081592265
Iteration: 5 || Loss: 6.138975804312595
Iteration: 6 || Loss: 6.138975804312595
saving ADAM checkpoint...
Sum of params:85.780655
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.138975804312595
Iteration: 2 || Loss: 6.096756272051197
Iteration: 3 || Loss: 6.040411541173002
Iteration: 4 || Loss: 5.940748663536064
Iteration: 5 || Loss: 5.932004886897421
Iteration: 6 || Loss: 5.92527109583606
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.83385
Epoch 511 loss:5.92527109583606
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.83385
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.07898062669445
Iteration: 2 || Loss: 19.0784685447762
Iteration: 3 || Loss: 19.077959647608914
Iteration: 4 || Loss: 19.077451696399535
Iteration: 5 || Loss: 19.07694360625553
Iteration: 6 || Loss: 19.07694360625553
saving ADAM checkpoint...
Sum of params:85.83392
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.07694360625553
Iteration: 2 || Loss: 19.01696579842534
Iteration: 3 || Loss: 18.79771214065951
Iteration: 4 || Loss: 18.695915553480283
Iteration: 5 || Loss: 18.668746306084344
Iteration: 6 || Loss: 18.652076131051775
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.72571
Epoch 511 loss:18.652076131051775
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.748941324150104
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.88429603398089
waveform batch: 2/2
Test loss - extrapolation:15.54918713801707
Epoch 511 mean train loss:0.9025779278860061
Epoch 511 mean test loss - interpolation:0.9581568873583507
Epoch 511 mean test loss - extrapolation:4.61945693099983
Start training epoch 512
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.72571
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.672987617766505
Iteration: 2 || Loss: 2.6713789516442334
Iteration: 3 || Loss: 2.6697705272292023
Iteration: 4 || Loss: 2.6681662668535986
Iteration: 5 || Loss: 2.6665630077369045
Iteration: 6 || Loss: 2.6665630077369045
saving ADAM checkpoint...
Sum of params:85.7257
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6665630077369045
Iteration: 2 || Loss: 2.033907252423435
Iteration: 3 || Loss: 1.6211743527802664
Iteration: 4 || Loss: 1.6182415975086293
Iteration: 5 || Loss: 1.5976905407265067
Iteration: 6 || Loss: 1.5968401425570562
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.78862
Epoch 512 loss:1.5968401425570562
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.78862
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.138497675602529
Iteration: 2 || Loss: 6.138045264719056
Iteration: 3 || Loss: 6.137593317261384
Iteration: 4 || Loss: 6.137144634740722
Iteration: 5 || Loss: 6.136695919242675
Iteration: 6 || Loss: 6.136695919242675
saving ADAM checkpoint...
Sum of params:85.78872
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.136695919242675
Iteration: 2 || Loss: 6.095020588925614
Iteration: 3 || Loss: 6.038726941393844
Iteration: 4 || Loss: 5.939141215362381
Iteration: 5 || Loss: 5.9302975081280245
Iteration: 6 || Loss: 5.923616527697924
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.84206
Epoch 512 loss:5.923616527697924
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.84206
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.070845674481166
Iteration: 2 || Loss: 19.070336322484504
Iteration: 3 || Loss: 19.069828893102496
Iteration: 4 || Loss: 19.069321278898048
Iteration: 5 || Loss: 19.068817099737096
Iteration: 6 || Loss: 19.068817099737096
saving ADAM checkpoint...
Sum of params:85.842125
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.068817099737096
Iteration: 2 || Loss: 19.009272337927673
Iteration: 3 || Loss: 18.789478920925472
Iteration: 4 || Loss: 18.687605711527656
Iteration: 5 || Loss: 18.660365772284937
Iteration: 6 || Loss: 18.643511548624
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.733536
Epoch 512 loss:18.643511548624
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.747385564577924
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.87274967267605
waveform batch: 2/2
Test loss - extrapolation:15.54099769027199
Epoch 512 mean train loss:0.9022058006509993
Epoch 512 mean test loss - interpolation:0.9578975940963207
Epoch 512 mean test loss - extrapolation:4.61781228024567
Start training epoch 513
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.733536
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6748294628736717
Iteration: 2 || Loss: 2.673216392997585
Iteration: 3 || Loss: 2.6716062281189847
Iteration: 4 || Loss: 2.669993816443304
Iteration: 5 || Loss: 2.6683861726996816
Iteration: 6 || Loss: 2.6683861726996816
saving ADAM checkpoint...
Sum of params:85.73353
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6683861726996816
Iteration: 2 || Loss: 2.0325825426319346
Iteration: 3 || Loss: 1.6207457665221336
Iteration: 4 || Loss: 1.617829719233231
Iteration: 5 || Loss: 1.597225573567437
Iteration: 6 || Loss: 1.5963309054817418
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.79637
Epoch 513 loss:1.5963309054817418
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.79637
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.137268113109099
Iteration: 2 || Loss: 6.136813149282443
Iteration: 3 || Loss: 6.136361383593613
Iteration: 4 || Loss: 6.135908920862621
Iteration: 5 || Loss: 6.135459487226469
Iteration: 6 || Loss: 6.135459487226469
saving ADAM checkpoint...
Sum of params:85.79648
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.135459487226469
Iteration: 2 || Loss: 6.093438517087278
Iteration: 3 || Loss: 6.037383178986374
Iteration: 4 || Loss: 5.937625726211906
Iteration: 5 || Loss: 5.928786661531925
Iteration: 6 || Loss: 5.922103901044551
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.850105
Epoch 513 loss:5.922103901044551
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.850105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.062215230982346
Iteration: 2 || Loss: 19.061706330645123
Iteration: 3 || Loss: 19.061197077870183
Iteration: 4 || Loss: 19.06068906573342
Iteration: 5 || Loss: 19.060184162082955
Iteration: 6 || Loss: 19.060184162082955
saving ADAM checkpoint...
Sum of params:85.85014
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.060184162082955
Iteration: 2 || Loss: 19.00040117169144
Iteration: 3 || Loss: 18.780761755580478
Iteration: 4 || Loss: 18.678961138097776
Iteration: 5 || Loss: 18.65177513065531
Iteration: 6 || Loss: 18.635035550009967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.74164
Epoch 513 loss:18.635035550009967
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.745531919974346
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.85907840673564
waveform batch: 2/2
Test loss - extrapolation:15.532855481891062
Epoch 513 mean train loss:0.9018438053978021
Epoch 513 mean test loss - interpolation:0.9575886533290577
Epoch 513 mean test loss - extrapolation:4.615994490718892
Start training epoch 514
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.74164
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6730476773918292
Iteration: 2 || Loss: 2.671433763091065
Iteration: 3 || Loss: 2.669822497598441
Iteration: 4 || Loss: 2.6682161633829042
Iteration: 5 || Loss: 2.6666075658754096
Iteration: 6 || Loss: 2.6666075658754096
saving ADAM checkpoint...
Sum of params:85.74164
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6666075658754096
Iteration: 2 || Loss: 2.0319114980125095
Iteration: 3 || Loss: 1.6201138591422273
Iteration: 4 || Loss: 1.6171790682548037
Iteration: 5 || Loss: 1.5966522975667299
Iteration: 6 || Loss: 1.5957727969529414
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.804375
Epoch 514 loss:1.5957727969529414
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.804375
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.135498912813269
Iteration: 2 || Loss: 6.1350441691100555
Iteration: 3 || Loss: 6.134590776438179
Iteration: 4 || Loss: 6.134140630364347
Iteration: 5 || Loss: 6.133691155078092
Iteration: 6 || Loss: 6.133691155078092
saving ADAM checkpoint...
Sum of params:85.8045
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.133691155078092
Iteration: 2 || Loss: 6.091693298479504
Iteration: 3 || Loss: 6.0356737403224585
Iteration: 4 || Loss: 5.936064347561682
Iteration: 5 || Loss: 5.927200150011973
Iteration: 6 || Loss: 5.920535336162767
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.85815
Epoch 514 loss:5.920535336162767
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.85815
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.053854039290023
Iteration: 2 || Loss: 19.053344363404367
Iteration: 3 || Loss: 19.05283707654262
Iteration: 4 || Loss: 19.052328610628066
Iteration: 5 || Loss: 19.051821732540535
Iteration: 6 || Loss: 19.051821732540535
saving ADAM checkpoint...
Sum of params:85.85819
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.051821732540535
Iteration: 2 || Loss: 18.99206924059071
Iteration: 3 || Loss: 18.772243207214142
Iteration: 4 || Loss: 18.670490093638968
Iteration: 5 || Loss: 18.643304010935047
Iteration: 6 || Loss: 18.626560288640448
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.749626
Epoch 514 loss:18.626560288640448
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.743792058605388
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.84629671744897
waveform batch: 2/2
Test loss - extrapolation:15.524692660630546
Epoch 514 mean train loss:0.9014782214398674
Epoch 514 mean test loss - interpolation:0.9572986764342314
Epoch 514 mean test loss - extrapolation:4.61424911483996
Start training epoch 515
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.749626
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.672751762342645
Iteration: 2 || Loss: 2.671138083341445
Iteration: 3 || Loss: 2.6695261788610964
Iteration: 4 || Loss: 2.667915111636004
Iteration: 5 || Loss: 2.6663069271219806
Iteration: 6 || Loss: 2.6663069271219806
saving ADAM checkpoint...
Sum of params:85.749626
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6663069271219806
Iteration: 2 || Loss: 2.0310351502252364
Iteration: 3 || Loss: 1.6195837261052648
Iteration: 4 || Loss: 1.616643626470481
Iteration: 5 || Loss: 1.5961304096475668
Iteration: 6 || Loss: 1.5952409227538609
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.81231
Epoch 515 loss:1.5952409227538609
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.81231
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.134172948142668
Iteration: 2 || Loss: 6.133715732473029
Iteration: 3 || Loss: 6.133260538028982
Iteration: 4 || Loss: 6.132808863072691
Iteration: 5 || Loss: 6.132356893632675
Iteration: 6 || Loss: 6.132356893632675
saving ADAM checkpoint...
Sum of params:85.81245
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.132356893632675
Iteration: 2 || Loss: 6.089975973453937
Iteration: 3 || Loss: 6.034035538316643
Iteration: 4 || Loss: 5.934525178561151
Iteration: 5 || Loss: 5.925695554294706
Iteration: 6 || Loss: 5.919023504020456
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.86608
Epoch 515 loss:5.919023504020456
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.86608
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.045314577947707
Iteration: 2 || Loss: 19.044802644457377
Iteration: 3 || Loss: 19.044293107186967
Iteration: 4 || Loss: 19.043783564582387
Iteration: 5 || Loss: 19.043275433635557
Iteration: 6 || Loss: 19.043275433635557
saving ADAM checkpoint...
Sum of params:85.86614
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.043275433635557
Iteration: 2 || Loss: 18.983247970229662
Iteration: 3 || Loss: 18.763484299870015
Iteration: 4 || Loss: 18.66192427448312
Iteration: 5 || Loss: 18.63479200437427
Iteration: 6 || Loss: 18.61817535775181
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.75768
Epoch 515 loss:18.61817535775181
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.741967310968836
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.83262003313747
waveform batch: 2/2
Test loss - extrapolation:15.51652061030978
Epoch 515 mean train loss:0.9011186132595216
Epoch 515 mean test loss - interpolation:0.9569945518281394
Epoch 515 mean test loss - extrapolation:4.612428386953937
Start training epoch 516
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.75768
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6710266970060466
Iteration: 2 || Loss: 2.669414327656126
Iteration: 3 || Loss: 2.667802159204364
Iteration: 4 || Loss: 2.6661942134417274
Iteration: 5 || Loss: 2.6645840817710345
Iteration: 6 || Loss: 2.6645840817710345
saving ADAM checkpoint...
Sum of params:85.75769
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6645840817710345
Iteration: 2 || Loss: 2.030573884265154
Iteration: 3 || Loss: 1.6189746849946565
Iteration: 4 || Loss: 1.616016821592459
Iteration: 5 || Loss: 1.5955669140774822
Iteration: 6 || Loss: 1.5946893129959867
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.82027
Epoch 516 loss:1.5946893129959867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.82027
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.132212093170682
Iteration: 2 || Loss: 6.131756457745186
Iteration: 3 || Loss: 6.1313020622960615
Iteration: 4 || Loss: 6.130850065129437
Iteration: 5 || Loss: 6.130400240872402
Iteration: 6 || Loss: 6.130400240872402
saving ADAM checkpoint...
Sum of params:85.82037
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.130400240872402
Iteration: 2 || Loss: 6.088293904106619
Iteration: 3 || Loss: 6.032425953646869
Iteration: 4 || Loss: 5.932954264789104
Iteration: 5 || Loss: 5.92406046858194
Iteration: 6 || Loss: 5.917420117755759
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.87417
Epoch 516 loss:5.917420117755759
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.87417
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.037154446598986
Iteration: 2 || Loss: 19.036642207446484
Iteration: 3 || Loss: 19.036131895709755
Iteration: 4 || Loss: 19.035623688142763
Iteration: 5 || Loss: 19.035115313732867
Iteration: 6 || Loss: 19.035115313732867
saving ADAM checkpoint...
Sum of params:85.87423
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.035115313732867
Iteration: 2 || Loss: 18.975316898910435
Iteration: 3 || Loss: 18.75518750732281
Iteration: 4 || Loss: 18.65361921665949
Iteration: 5 || Loss: 18.626453309154044
Iteration: 6 || Loss: 18.609742853597815
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.76555
Epoch 516 loss:18.609742853597815
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.740344443730715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.82055379922692
waveform batch: 2/2
Test loss - extrapolation:15.5084098452683
Epoch 516 mean train loss:0.9007535270465366
Epoch 516 mean test loss - interpolation:0.9567240739551192
Epoch 516 mean test loss - extrapolation:4.610746970374602
Start training epoch 517
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.76555
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.671828436230279
Iteration: 2 || Loss: 2.6702150266003426
Iteration: 3 || Loss: 2.6686022866320522
Iteration: 4 || Loss: 2.666988170542031
Iteration: 5 || Loss: 2.66537766165743
Iteration: 6 || Loss: 2.66537766165743
saving ADAM checkpoint...
Sum of params:85.76555
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.66537766165743
Iteration: 2 || Loss: 2.029493653716613
Iteration: 3 || Loss: 1.6184937452419708
Iteration: 4 || Loss: 1.615544650460081
Iteration: 5 || Loss: 1.5950751758062962
Iteration: 6 || Loss: 1.594168206547199
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.82804
Epoch 517 loss:1.594168206547199
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.82804
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.130846174653148
Iteration: 2 || Loss: 6.130387902371465
Iteration: 3 || Loss: 6.129933757081125
Iteration: 4 || Loss: 6.129479862553747
Iteration: 5 || Loss: 6.129028070197312
Iteration: 6 || Loss: 6.129028070197312
saving ADAM checkpoint...
Sum of params:85.82815
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.129028070197312
Iteration: 2 || Loss: 6.086673496540897
Iteration: 3 || Loss: 6.030982382403268
Iteration: 4 || Loss: 5.931431851306518
Iteration: 5 || Loss: 5.922535104753556
Iteration: 6 || Loss: 5.915901980737016
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.88213
Epoch 517 loss:5.915901980737016
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.88213
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.028710672608472
Iteration: 2 || Loss: 19.028197692440166
Iteration: 3 || Loss: 19.02768820941014
Iteration: 4 || Loss: 19.027177088851783
Iteration: 5 || Loss: 19.026669916975727
Iteration: 6 || Loss: 19.026669916975727
saving ADAM checkpoint...
Sum of params:85.88219
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.026669916975727
Iteration: 2 || Loss: 18.96671689844819
Iteration: 3 || Loss: 18.746614910050514
Iteration: 4 || Loss: 18.64513206029437
Iteration: 5 || Loss: 18.61800328356619
Iteration: 6 || Loss: 18.601366406560444
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.77352
Epoch 517 loss:18.601366406560444
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.738549434038741
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.80729270097449
waveform batch: 2/2
Test loss - extrapolation:15.500336898173387
Epoch 517 mean train loss:0.9003943653049882
Epoch 517 mean test loss - interpolation:0.9564249056731234
Epoch 517 mean test loss - extrapolation:4.608969133262323
Start training epoch 518
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.77352
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6705533705199134
Iteration: 2 || Loss: 2.6689359613782395
Iteration: 3 || Loss: 2.6673246536516118
Iteration: 4 || Loss: 2.665711484195311
Iteration: 5 || Loss: 2.6641012498047103
Iteration: 6 || Loss: 2.6641012498047103
saving ADAM checkpoint...
Sum of params:85.77354
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6641012498047103
Iteration: 2 || Loss: 2.028767089760902
Iteration: 3 || Loss: 1.6178963719897168
Iteration: 4 || Loss: 1.6149302904210556
Iteration: 5 || Loss: 1.5945211604350185
Iteration: 6 || Loss: 1.593618630577144
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.8359
Epoch 518 loss:1.593618630577144
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.8359
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.129074978424742
Iteration: 2 || Loss: 6.1286170310042625
Iteration: 3 || Loss: 6.128161418305014
Iteration: 4 || Loss: 6.127708906538228
Iteration: 5 || Loss: 6.127256942209319
Iteration: 6 || Loss: 6.127256942209319
saving ADAM checkpoint...
Sum of params:85.83602
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.127256942209319
Iteration: 2 || Loss: 6.084986437840604
Iteration: 3 || Loss: 6.029377393189648
Iteration: 4 || Loss: 5.929885841892557
Iteration: 5 || Loss: 5.920943305911208
Iteration: 6 || Loss: 5.914336541160136
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.89013
Epoch 518 loss:5.914336541160136
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.89013
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.020496960320905
Iteration: 2 || Loss: 19.019984492263916
Iteration: 3 || Loss: 19.019473642051683
Iteration: 4 || Loss: 19.01896521087791
Iteration: 5 || Loss: 19.01845805328352
Iteration: 6 || Loss: 19.01845805328352
saving ADAM checkpoint...
Sum of params:85.8902
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.01845805328352
Iteration: 2 || Loss: 18.958611615228094
Iteration: 3 || Loss: 18.73827506858678
Iteration: 4 || Loss: 18.63678955936743
Iteration: 5 || Loss: 18.609648964314527
Iteration: 6 || Loss: 18.59297165000716
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.7814
Epoch 518 loss:18.59297165000716
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.736859648418287
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.794875306196445
waveform batch: 2/2
Test loss - extrapolation:15.492276799919242
Epoch 518 mean train loss:0.900031959370498
Epoch 518 mean test loss - interpolation:0.9561432747363812
Epoch 518 mean test loss - extrapolation:4.607262675509641
Start training epoch 519
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.7814
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6706273979545334
Iteration: 2 || Loss: 2.6690098071922725
Iteration: 3 || Loss: 2.6673903100182126
Iteration: 4 || Loss: 2.6657787568975664
Iteration: 5 || Loss: 2.664169883429209
Iteration: 6 || Loss: 2.664169883429209
saving ADAM checkpoint...
Sum of params:85.78139
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.664169883429209
Iteration: 2 || Loss: 2.02776109754012
Iteration: 3 || Loss: 1.6173808243775651
Iteration: 4 || Loss: 1.61441424156475
Iteration: 5 || Loss: 1.59401074097977
Iteration: 6 || Loss: 1.5930878076253112
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.84368
Epoch 519 loss:1.5930878076253112
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.84368
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.127702279780844
Iteration: 2 || Loss: 6.127244723120696
Iteration: 3 || Loss: 6.126788020606566
Iteration: 4 || Loss: 6.126332357577241
Iteration: 5 || Loss: 6.125879997051362
Iteration: 6 || Loss: 6.125879997051362
saving ADAM checkpoint...
Sum of params:85.84382
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.125879997051362
Iteration: 2 || Loss: 6.0833526106910325
Iteration: 3 || Loss: 6.027870569447324
Iteration: 4 || Loss: 5.928369182185567
Iteration: 5 || Loss: 5.919439536628003
Iteration: 6 || Loss: 5.912828564536885
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.89803
Epoch 519 loss:5.912828564536885
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.89803
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.012085256532576
Iteration: 2 || Loss: 19.011572698779666
Iteration: 3 || Loss: 19.011060499119356
Iteration: 4 || Loss: 19.010549467950188
Iteration: 5 || Loss: 19.010041553084264
Iteration: 6 || Loss: 19.010041553084264
saving ADAM checkpoint...
Sum of params:85.898094
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.010041553084264
Iteration: 2 || Loss: 18.949988803409383
Iteration: 3 || Loss: 18.729711279930417
Iteration: 4 || Loss: 18.62833795335375
Iteration: 5 || Loss: 18.601238143547697
Iteration: 6 || Loss: 18.58465719624738
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.78936
Epoch 519 loss:18.58465719624738
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.735061589613447
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.78148247609356
waveform batch: 2/2
Test loss - extrapolation:15.484216298647585
Epoch 519 mean train loss:0.8996749506348131
Epoch 519 mean test loss - interpolation:0.9558435982689079
Epoch 519 mean test loss - extrapolation:4.605474897895095
Start training epoch 520
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.78936
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.669100860642875
Iteration: 2 || Loss: 2.6674828293907353
Iteration: 3 || Loss: 2.665865821735299
Iteration: 4 || Loss: 2.664253311861644
Iteration: 5 || Loss: 2.6626424261566783
Iteration: 6 || Loss: 2.6626424261566783
saving ADAM checkpoint...
Sum of params:85.78937
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6626424261566783
Iteration: 2 || Loss: 2.027132562870379
Iteration: 3 || Loss: 1.616774693418998
Iteration: 4 || Loss: 1.6137937032151017
Iteration: 5 || Loss: 1.5934487821502359
Iteration: 6 || Loss: 1.592532755383128
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.85158
Epoch 520 loss:1.592532755383128
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.85158
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1259347684001
Iteration: 2 || Loss: 6.1254769485978775
Iteration: 3 || Loss: 6.125020962819332
Iteration: 4 || Loss: 6.124566144620921
Iteration: 5 || Loss: 6.124113371785249
Iteration: 6 || Loss: 6.124113371785249
saving ADAM checkpoint...
Sum of params:85.8517
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.124113371785249
Iteration: 2 || Loss: 6.081603829955554
Iteration: 3 || Loss: 6.026204722561675
Iteration: 4 || Loss: 5.9268204080204585
Iteration: 5 || Loss: 5.917862591045566
Iteration: 6 || Loss: 5.9112729756219515
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.90596
Epoch 520 loss:5.9112729756219515
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.90596
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.003902941159094
Iteration: 2 || Loss: 19.0033885503415
Iteration: 3 || Loss: 19.002877129288706
Iteration: 4 || Loss: 19.00236714918299
Iteration: 5 || Loss: 19.001859358085166
Iteration: 6 || Loss: 19.001859358085166
saving ADAM checkpoint...
Sum of params:85.906044
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.001859358085166
Iteration: 2 || Loss: 18.941859110158735
Iteration: 3 || Loss: 18.72136207460858
Iteration: 4 || Loss: 18.620039974460493
Iteration: 5 || Loss: 18.59293670897231
Iteration: 6 || Loss: 18.576340857973953
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.7972
Epoch 520 loss:18.576340857973953
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.733395426919212
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.76896700291584
waveform batch: 2/2
Test loss - extrapolation:15.476162672591476
Epoch 520 mean train loss:0.8993153996199666
Epoch 520 mean test loss - interpolation:0.9555659044865353
Epoch 520 mean test loss - extrapolation:4.603760806292277
Start training epoch 521
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.7972
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.669007989188774
Iteration: 2 || Loss: 2.667385817967757
Iteration: 3 || Loss: 2.66576894815569
Iteration: 4 || Loss: 2.6641536048862307
Iteration: 5 || Loss: 2.6625412145148384
Iteration: 6 || Loss: 2.6625412145148384
saving ADAM checkpoint...
Sum of params:85.79722
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6625412145148384
Iteration: 2 || Loss: 2.0262641116302356
Iteration: 3 || Loss: 1.6162479290586413
Iteration: 4 || Loss: 1.613265661074483
Iteration: 5 || Loss: 1.5929295979496687
Iteration: 6 || Loss: 1.5920030668903928
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.859314
Epoch 521 loss:1.5920030668903928
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.859314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.124364515151877
Iteration: 2 || Loss: 6.123904984210741
Iteration: 3 || Loss: 6.123449013425774
Iteration: 4 || Loss: 6.122993850832104
Iteration: 5 || Loss: 6.12254070386674
Iteration: 6 || Loss: 6.12254070386674
saving ADAM checkpoint...
Sum of params:85.85944
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.12254070386674
Iteration: 2 || Loss: 6.080030152899481
Iteration: 3 || Loss: 6.02474052106931
Iteration: 4 || Loss: 5.925297495925956
Iteration: 5 || Loss: 5.916306010617313
Iteration: 6 || Loss: 5.9097345976571685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.913895
Epoch 521 loss:5.9097345976571685
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.913895
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.99566016280836
Iteration: 2 || Loss: 18.99514765997244
Iteration: 3 || Loss: 18.994636801834265
Iteration: 4 || Loss: 18.99412523123575
Iteration: 5 || Loss: 18.993616574653835
Iteration: 6 || Loss: 18.993616574653835
saving ADAM checkpoint...
Sum of params:85.91397
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.993616574653835
Iteration: 2 || Loss: 18.933633766695806
Iteration: 3 || Loss: 18.71301480921545
Iteration: 4 || Loss: 18.611722493266658
Iteration: 5 || Loss: 18.58462795961802
Iteration: 6 || Loss: 18.56802839893955
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.80506
Epoch 521 loss:18.56802839893955
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.731707245216611
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.75636811092516
waveform batch: 2/2
Test loss - extrapolation:15.468147049380855
Epoch 521 mean train loss:0.8989574504650728
Epoch 521 mean test loss - interpolation:0.9552845408694352
Epoch 521 mean test loss - extrapolation:4.602042930025501
Start training epoch 522
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.80506
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.66869161466068
Iteration: 2 || Loss: 2.667070332096151
Iteration: 3 || Loss: 2.6654498761914787
Iteration: 4 || Loss: 2.663831304976757
Iteration: 5 || Loss: 2.6622181146471546
Iteration: 6 || Loss: 2.6622181146471546
saving ADAM checkpoint...
Sum of params:85.80505
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6622181146471546
Iteration: 2 || Loss: 2.02536718936306
Iteration: 3 || Loss: 1.6157038313462218
Iteration: 4 || Loss: 1.612717728790997
Iteration: 5 || Loss: 1.592404351799769
Iteration: 6 || Loss: 1.5914577429912193
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.86705
Epoch 522 loss:1.5914577429912193
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.86705
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.122757755189124
Iteration: 2 || Loss: 6.12229699620024
Iteration: 3 || Loss: 6.121840963165518
Iteration: 4 || Loss: 6.121385952248614
Iteration: 5 || Loss: 6.120933179682475
Iteration: 6 || Loss: 6.120933179682475
saving ADAM checkpoint...
Sum of params:85.86719
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.120933179682475
Iteration: 2 || Loss: 6.078364562174094
Iteration: 3 || Loss: 6.023240806758871
Iteration: 4 || Loss: 5.923774806068568
Iteration: 5 || Loss: 5.914762672671883
Iteration: 6 || Loss: 5.908204776446702
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.9218
Epoch 522 loss:5.908204776446702
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.9218
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.987411897585527
Iteration: 2 || Loss: 18.98689759214177
Iteration: 3 || Loss: 18.9863851891404
Iteration: 4 || Loss: 18.98587484536181
Iteration: 5 || Loss: 18.985367063227606
Iteration: 6 || Loss: 18.985367063227606
saving ADAM checkpoint...
Sum of params:85.92186
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.985367063227606
Iteration: 2 || Loss: 18.92535327344603
Iteration: 3 || Loss: 18.704688525530663
Iteration: 4 || Loss: 18.603416346555093
Iteration: 5 || Loss: 18.576332432281333
Iteration: 6 || Loss: 18.55974978943259
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.81289
Epoch 522 loss:18.55974978943259
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.729951609886941
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.74357130273884
waveform batch: 2/2
Test loss - extrapolation:15.460179671316265
Epoch 522 mean train loss:0.8986004244438108
Epoch 522 mean test loss - interpolation:0.9549919349811568
Epoch 522 mean test loss - extrapolation:4.600312581171259
Start training epoch 523
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.81289
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6679084911378643
Iteration: 2 || Loss: 2.6662842147486883
Iteration: 3 || Loss: 2.664664064920556
Iteration: 4 || Loss: 2.6630460044680575
Iteration: 5 || Loss: 2.661432595295551
Iteration: 6 || Loss: 2.661432595295551
saving ADAM checkpoint...
Sum of params:85.81288
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.661432595295551
Iteration: 2 || Loss: 2.024436190390121
Iteration: 3 || Loss: 1.615136145421249
Iteration: 4 || Loss: 1.6121413981056532
Iteration: 5 || Loss: 1.5918651247226923
Iteration: 6 || Loss: 1.5909132771551733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.87482
Epoch 523 loss:1.5909132771551733
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.87482
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.121304622745669
Iteration: 2 || Loss: 6.120844040993926
Iteration: 3 || Loss: 6.120385591428344
Iteration: 4 || Loss: 6.11992988254281
Iteration: 5 || Loss: 6.119474541932972
Iteration: 6 || Loss: 6.119474541932972
saving ADAM checkpoint...
Sum of params:85.87494
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.119474541932972
Iteration: 2 || Loss: 6.076667590642461
Iteration: 3 || Loss: 6.021623189918449
Iteration: 4 || Loss: 5.922258187082551
Iteration: 5 || Loss: 5.91325146257835
Iteration: 6 || Loss: 5.906699701777405
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.92961
Epoch 523 loss:5.906699701777405
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.92961
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.979155782714642
Iteration: 2 || Loss: 18.978642343840136
Iteration: 3 || Loss: 18.978128559777215
Iteration: 4 || Loss: 18.977618193361824
Iteration: 5 || Loss: 18.977108898885604
Iteration: 6 || Loss: 18.977108898885604
saving ADAM checkpoint...
Sum of params:85.92967
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.977108898885604
Iteration: 2 || Loss: 18.916950993702553
Iteration: 3 || Loss: 18.69623767134975
Iteration: 4 || Loss: 18.595079119341293
Iteration: 5 || Loss: 18.568028758568893
Iteration: 6 || Loss: 18.551514144961065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.820755
Epoch 523 loss:18.551514144961065
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.728210658503714
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.73049625877093
waveform batch: 2/2
Test loss - extrapolation:15.452173689289037
Epoch 523 mean train loss:0.8982457628928842
Epoch 523 mean test loss - interpolation:0.9547017764172856
Epoch 523 mean test loss - extrapolation:4.598555829004997
Start training epoch 524
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.820755
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.666762081804248
Iteration: 2 || Loss: 2.6651415219894483
Iteration: 3 || Loss: 2.663521059432783
Iteration: 4 || Loss: 2.6619045126981096
Iteration: 5 || Loss: 2.660288023515711
Iteration: 6 || Loss: 2.660288023515711
saving ADAM checkpoint...
Sum of params:85.820755
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.660288023515711
Iteration: 2 || Loss: 2.023772954065287
Iteration: 3 || Loss: 1.6145543204868964
Iteration: 4 || Loss: 1.6115430520881269
Iteration: 5 || Loss: 1.5913156286208565
Iteration: 6 || Loss: 1.590366699882803
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.88254
Epoch 524 loss:1.590366699882803
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.88254
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.119428129221908
Iteration: 2 || Loss: 6.118970381340915
Iteration: 3 || Loss: 6.1185134639189
Iteration: 4 || Loss: 6.118056729091244
Iteration: 5 || Loss: 6.117604882452685
Iteration: 6 || Loss: 6.117604882452685
saving ADAM checkpoint...
Sum of params:85.88266
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.117604882452685
Iteration: 2 || Loss: 6.075092577956467
Iteration: 3 || Loss: 6.020143875115261
Iteration: 4 || Loss: 5.920728608793984
Iteration: 5 || Loss: 5.911646740825249
Iteration: 6 || Loss: 5.905129730433242
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.93756
Epoch 524 loss:5.905129730433242
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.93756
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.971158747598512
Iteration: 2 || Loss: 18.970646007580786
Iteration: 3 || Loss: 18.970133863071595
Iteration: 4 || Loss: 18.96962334911402
Iteration: 5 || Loss: 18.969114445911245
Iteration: 6 || Loss: 18.969114445911245
saving ADAM checkpoint...
Sum of params:85.93763
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.969114445911245
Iteration: 2 || Loss: 18.90920137137443
Iteration: 3 || Loss: 18.688148699682124
Iteration: 4 || Loss: 18.58692369180143
Iteration: 5 || Loss: 18.559841821337223
Iteration: 6 || Loss: 18.54322205873929
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.82844
Epoch 524 loss:18.54322205873929
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.726634455658019
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.71870244104819
waveform batch: 2/2
Test loss - extrapolation:15.444215229207035
Epoch 524 mean train loss:0.897886844450184
Epoch 524 mean test loss - interpolation:0.9544390759430031
Epoch 524 mean test loss - extrapolation:4.596909805854602
Start training epoch 525
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.82844
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6676295970962323
Iteration: 2 || Loss: 2.6660034952242264
Iteration: 3 || Loss: 2.6643817313076514
Iteration: 4 || Loss: 2.6627621356336646
Iteration: 5 || Loss: 2.6611432225266847
Iteration: 6 || Loss: 2.6611432225266847
saving ADAM checkpoint...
Sum of params:85.82844
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6611432225266847
Iteration: 2 || Loss: 2.02258357625733
Iteration: 3 || Loss: 1.6140703617420602
Iteration: 4 || Loss: 1.611069650566864
Iteration: 5 || Loss: 1.5908242680667435
Iteration: 6 || Loss: 1.5898344599383243
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.89017
Epoch 525 loss:1.5898344599383243
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.89017
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.118198216382347
Iteration: 2 || Loss: 6.117736048274704
Iteration: 3 || Loss: 6.117277437172797
Iteration: 4 || Loss: 6.1168202451444085
Iteration: 5 || Loss: 6.116365082764743
Iteration: 6 || Loss: 6.116365082764743
saving ADAM checkpoint...
Sum of params:85.89028
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.116365082764743
Iteration: 2 || Loss: 6.073486188041627
Iteration: 3 || Loss: 6.018725310718742
Iteration: 4 || Loss: 5.919238520991963
Iteration: 5 || Loss: 5.910176793074475
Iteration: 6 || Loss: 5.903654911320767
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.94533
Epoch 525 loss:5.903654911320767
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.94533
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.96279465545855
Iteration: 2 || Loss: 18.96228012788389
Iteration: 3 || Loss: 18.961767782889535
Iteration: 4 || Loss: 18.961257582588942
Iteration: 5 || Loss: 18.960745265734378
Iteration: 6 || Loss: 18.960745265734378
saving ADAM checkpoint...
Sum of params:85.94541
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.960745265734378
Iteration: 2 || Loss: 18.900553665229136
Iteration: 3 || Loss: 18.679690448134753
Iteration: 4 || Loss: 18.57856361434398
Iteration: 5 || Loss: 18.551535387659893
Iteration: 6 || Loss: 18.535046406860726
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.83631
Epoch 525 loss:18.535046406860726
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.724825443918566
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.70516641991403
waveform batch: 2/2
Test loss - extrapolation:15.436272493762788
Epoch 525 mean train loss:0.8975357164868902
Epoch 525 mean test loss - interpolation:0.9541375739864276
Epoch 525 mean test loss - extrapolation:4.595119909473068
Start training epoch 526
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.83631
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6655880688869367
Iteration: 2 || Loss: 2.6639645192171972
Iteration: 3 || Loss: 2.6623432401298133
Iteration: 4 || Loss: 2.6607221829376164
Iteration: 5 || Loss: 2.6591040846640683
Iteration: 6 || Loss: 2.6591040846640683
saving ADAM checkpoint...
Sum of params:85.83633
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6591040846640683
Iteration: 2 || Loss: 2.0219625093420825
Iteration: 3 || Loss: 1.6134252445686346
Iteration: 4 || Loss: 1.6103960331784335
Iteration: 5 || Loss: 1.5902388254993778
Iteration: 6 || Loss: 1.5892750737744867
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.89792
Epoch 526 loss:1.5892750737744867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.89792
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.116318168007
Iteration: 2 || Loss: 6.115857873803589
Iteration: 3 || Loss: 6.115400303488203
Iteration: 4 || Loss: 6.11494519119205
Iteration: 5 || Loss: 6.11449015873401
Iteration: 6 || Loss: 6.11449015873401
saving ADAM checkpoint...
Sum of params:85.89804
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.11449015873401
Iteration: 2 || Loss: 6.071833613254967
Iteration: 3 || Loss: 6.017085216469468
Iteration: 4 || Loss: 5.917711348834361
Iteration: 5 || Loss: 5.908592800172912
Iteration: 6 || Loss: 5.9021014682007245
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.953186
Epoch 526 loss:5.9021014682007245
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.953186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.954825271593606
Iteration: 2 || Loss: 18.954311970136953
Iteration: 3 || Loss: 18.953799394065552
Iteration: 4 || Loss: 18.953288279755174
Iteration: 5 || Loss: 18.952777839006735
Iteration: 6 || Loss: 18.952777839006735
saving ADAM checkpoint...
Sum of params:85.953285
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.952777839006735
Iteration: 2 || Loss: 18.892777978522354
Iteration: 3 || Loss: 18.671563789706404
Iteration: 4 || Loss: 18.570420634407714
Iteration: 5 || Loss: 18.543369125205505
Iteration: 6 || Loss: 18.526801380889452
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.844025
Epoch 526 loss:18.526801380889452
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.7232370101948415
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.69325169739533
waveform batch: 2/2
Test loss - extrapolation:15.42831295214301
Epoch 526 mean train loss:0.8971785490642987
Epoch 526 mean test loss - interpolation:0.9538728350324736
Epoch 526 mean test loss - extrapolation:4.593463720794862
Start training epoch 527
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.844025
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6662373044033236
Iteration: 2 || Loss: 2.6646137530802005
Iteration: 3 || Loss: 2.662988007354041
Iteration: 4 || Loss: 2.661364207888113
Iteration: 5 || Loss: 2.659743406424583
Iteration: 6 || Loss: 2.659743406424583
saving ADAM checkpoint...
Sum of params:85.84401
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.659743406424583
Iteration: 2 || Loss: 2.0208880463784844
Iteration: 3 || Loss: 1.6129419637641143
Iteration: 4 || Loss: 1.6099233008841607
Iteration: 5 || Loss: 1.5897443010084775
Iteration: 6 || Loss: 1.5887373261552913
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.905525
Epoch 527 loss:1.5887373261552913
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.905525
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.114983222119746
Iteration: 2 || Loss: 6.11452280296431
Iteration: 3 || Loss: 6.114063438802725
Iteration: 4 || Loss: 6.1136051415368105
Iteration: 5 || Loss: 6.113148419536873
Iteration: 6 || Loss: 6.113148419536873
saving ADAM checkpoint...
Sum of params:85.90565
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.113148419536873
Iteration: 2 || Loss: 6.070222583519293
Iteration: 3 || Loss: 6.015679816600665
Iteration: 4 || Loss: 5.916212040915417
Iteration: 5 || Loss: 5.907092040215112
Iteration: 6 || Loss: 5.900608230274321
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.96099
Epoch 527 loss:5.900608230274321
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.96099
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.94659112680602
Iteration: 2 || Loss: 18.946076415874636
Iteration: 3 || Loss: 18.94556382070469
Iteration: 4 || Loss: 18.9450509710827
Iteration: 5 || Loss: 18.944542118334226
Iteration: 6 || Loss: 18.944542118334226
saving ADAM checkpoint...
Sum of params:85.961075
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.944542118334226
Iteration: 2 || Loss: 18.884385627492648
Iteration: 3 || Loss: 18.663245809969915
Iteration: 4 || Loss: 18.5621592226898
Iteration: 5 || Loss: 18.535142686802423
Iteration: 6 || Loss: 18.518641397856257
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.851814
Epoch 527 loss:18.518641397856257
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.721475795743166
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.680154670388134
waveform batch: 2/2
Test loss - extrapolation:15.420413631642049
Epoch 527 mean train loss:0.8968271363546851
Epoch 527 mean test loss - interpolation:0.9535792992905278
Epoch 527 mean test loss - extrapolation:4.591714025169182
Start training epoch 528
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.851814
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.664801474608437
Iteration: 2 || Loss: 2.6631763393567986
Iteration: 3 || Loss: 2.6615526829550644
Iteration: 4 || Loss: 2.6599301297840268
Iteration: 5 || Loss: 2.658312330299563
Iteration: 6 || Loss: 2.658312330299563
saving ADAM checkpoint...
Sum of params:85.851814
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.658312330299563
Iteration: 2 || Loss: 2.020101690418668
Iteration: 3 || Loss: 1.6123286552326879
Iteration: 4 || Loss: 1.6092961047744807
Iteration: 5 || Loss: 1.589178305465802
Iteration: 6 || Loss: 1.5881852893762576
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.91321
Epoch 528 loss:1.5881852893762576
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.91321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.113303749688862
Iteration: 2 || Loss: 6.112841214485206
Iteration: 3 || Loss: 6.112382883060258
Iteration: 4 || Loss: 6.111924997186408
Iteration: 5 || Loss: 6.111470179499525
Iteration: 6 || Loss: 6.111470179499525
saving ADAM checkpoint...
Sum of params:85.91332
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.111470179499525
Iteration: 2 || Loss: 6.068594583085777
Iteration: 3 || Loss: 6.014081965436555
Iteration: 4 || Loss: 5.914707025663703
Iteration: 5 || Loss: 5.905555324848129
Iteration: 6 || Loss: 5.899089210123294
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.96875
Epoch 528 loss:5.899089210123294
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.96875
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.93856209656062
Iteration: 2 || Loss: 18.938046573556406
Iteration: 3 || Loss: 18.937534433273107
Iteration: 4 || Loss: 18.937022135624396
Iteration: 5 || Loss: 18.936510729167352
Iteration: 6 || Loss: 18.936510729167352
saving ADAM checkpoint...
Sum of params:85.96884
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.936510729167352
Iteration: 2 || Loss: 18.876400069671526
Iteration: 3 || Loss: 18.655057710442644
Iteration: 4 || Loss: 18.553988229123853
Iteration: 5 || Loss: 18.526972414929393
Iteration: 6 || Loss: 18.510462775840534
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.85952
Epoch 528 loss:18.510462775840534
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.719833353714375
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.66777914008282
waveform batch: 2/2
Test loss - extrapolation:15.412476645857106
Epoch 528 mean train loss:0.8964736991496581
Epoch 528 mean test loss - interpolation:0.9533055589523958
Epoch 528 mean test loss - extrapolation:4.590021315494994
Start training epoch 529
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.85952
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6646426359482684
Iteration: 2 || Loss: 2.663014572718805
Iteration: 3 || Loss: 2.6613890173327612
Iteration: 4 || Loss: 2.6597670580243022
Iteration: 5 || Loss: 2.6581451936949865
Iteration: 6 || Loss: 2.6581451936949865
saving ADAM checkpoint...
Sum of params:85.859535
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6581451936949865
Iteration: 2 || Loss: 2.0191906451225337
Iteration: 3 || Loss: 1.6118026517416004
Iteration: 4 || Loss: 1.608766348486721
Iteration: 5 || Loss: 1.5886580523394678
Iteration: 6 || Loss: 1.5876394388675585
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.920845
Epoch 529 loss:1.5876394388675585
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.920845
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.111879004911817
Iteration: 2 || Loss: 6.1114169074442275
Iteration: 3 || Loss: 6.110955997716192
Iteration: 4 || Loss: 6.110498183273127
Iteration: 5 || Loss: 6.110041010898809
Iteration: 6 || Loss: 6.110041010898809
saving ADAM checkpoint...
Sum of params:85.920975
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.110041010898809
Iteration: 2 || Loss: 6.0669497680548545
Iteration: 3 || Loss: 6.012580404684566
Iteration: 4 || Loss: 5.913204150439472
Iteration: 5 || Loss: 5.904060694347334
Iteration: 6 || Loss: 5.897595110299668
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.97648
Epoch 529 loss:5.897595110299668
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.97648
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.930371233608998
Iteration: 2 || Loss: 18.92985460594016
Iteration: 3 || Loss: 18.929341309778383
Iteration: 4 || Loss: 18.92883010506656
Iteration: 5 || Loss: 18.928318381325933
Iteration: 6 || Loss: 18.928318381325933
saving ADAM checkpoint...
Sum of params:85.97658
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.928318381325933
Iteration: 2 || Loss: 18.868052290153226
Iteration: 3 || Loss: 18.646757507722256
Iteration: 4 || Loss: 18.545785569573724
Iteration: 5 || Loss: 18.518800516071863
Iteration: 6 || Loss: 18.502357948705654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.86729
Epoch 529 loss:18.502357948705654
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.718092096617664
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.65473142041388
waveform batch: 2/2
Test loss - extrapolation:15.404578891336692
Epoch 529 mean train loss:0.8961238792369959
Epoch 529 mean test loss - interpolation:0.9530153494362773
Epoch 529 mean test loss - extrapolation:4.588275859312548
Start training epoch 530
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.86729
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.663281394816407
Iteration: 2 || Loss: 2.6616541641046854
Iteration: 3 || Loss: 2.660028792720128
Iteration: 4 || Loss: 2.658408538139981
Iteration: 5 || Loss: 2.6567882920372363
Iteration: 6 || Loss: 2.6567882920372363
saving ADAM checkpoint...
Sum of params:85.86728
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6567882920372363
Iteration: 2 || Loss: 2.01845850284829
Iteration: 3 || Loss: 1.6112019215315672
Iteration: 4 || Loss: 1.608148670620545
Iteration: 5 || Loss: 1.5880970641588208
Iteration: 6 || Loss: 1.5870821742946728
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.928474
Epoch 530 loss:1.5870821742946728
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.928474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.110111394856066
Iteration: 2 || Loss: 6.109648308887885
Iteration: 3 || Loss: 6.109190213892431
Iteration: 4 || Loss: 6.1087310564375015
Iteration: 5 || Loss: 6.108275941102919
Iteration: 6 || Loss: 6.108275941102919
saving ADAM checkpoint...
Sum of params:85.92859
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.108275941102919
Iteration: 2 || Loss: 6.065351978314197
Iteration: 3 || Loss: 6.011061522214999
Iteration: 4 || Loss: 5.911690727634782
Iteration: 5 || Loss: 5.902485565904368
Iteration: 6 || Loss: 5.896054947465265
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.98432
Epoch 530 loss:5.896054947465265
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.98432
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.92247647897233
Iteration: 2 || Loss: 18.921961507422612
Iteration: 3 || Loss: 18.92144775828221
Iteration: 4 || Loss: 18.92093573765604
Iteration: 5 || Loss: 18.920425861771804
Iteration: 6 || Loss: 18.920425861771804
saving ADAM checkpoint...
Sum of params:85.98439
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.920425861771804
Iteration: 2 || Loss: 18.860325102883056
Iteration: 3 || Loss: 18.638733695514016
Iteration: 4 || Loss: 18.537728491382875
Iteration: 5 || Loss: 18.5107253926963
Iteration: 6 || Loss: 18.494205034650903
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.8749
Epoch 530 loss:18.494205034650903
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.716511497107135
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.64283253403757
waveform batch: 2/2
Test loss - extrapolation:15.396704578822112
Epoch 530 mean train loss:0.8957704191865807
Epoch 530 mean test loss - interpolation:0.9527519161845225
Epoch 530 mean test loss - extrapolation:4.586628092738306
Start training epoch 531
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.8749
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6637785024228693
Iteration: 2 || Loss: 2.6621494276617095
Iteration: 3 || Loss: 2.660522439129122
Iteration: 4 || Loss: 2.6588971085438904
Iteration: 5 || Loss: 2.657271810938857
Iteration: 6 || Loss: 2.657271810938857
saving ADAM checkpoint...
Sum of params:85.87488
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.657271810938857
Iteration: 2 || Loss: 2.0173421052957554
Iteration: 3 || Loss: 1.61069563573717
Iteration: 4 || Loss: 1.6076461104510142
Iteration: 5 || Loss: 1.5875907342583628
Iteration: 6 || Loss: 1.586537436202551
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.93599
Epoch 531 loss:1.586537436202551
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.93599
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.10875093169329
Iteration: 2 || Loss: 6.108287951628517
Iteration: 3 || Loss: 6.107826682452304
Iteration: 4 || Loss: 6.1073665741436045
Iteration: 5 || Loss: 6.106909834277498
Iteration: 6 || Loss: 6.106909834277498
saving ADAM checkpoint...
Sum of params:85.93613
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.106909834277498
Iteration: 2 || Loss: 6.063797882720568
Iteration: 3 || Loss: 6.009674819866237
Iteration: 4 || Loss: 5.910205489190773
Iteration: 5 || Loss: 5.900992562609735
Iteration: 6 || Loss: 5.89456843828543
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.99202
Epoch 531 loss:5.89456843828543
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.99202
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.914343514873853
Iteration: 2 || Loss: 18.913828190222926
Iteration: 3 || Loss: 18.913315229288873
Iteration: 4 || Loss: 18.91280164689911
Iteration: 5 || Loss: 18.91229078783652
Iteration: 6 || Loss: 18.91229078783652
saving ADAM checkpoint...
Sum of params:85.99211
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.91229078783652
Iteration: 2 || Loss: 18.852073837415112
Iteration: 3 || Loss: 18.630532712851668
Iteration: 4 || Loss: 18.529557386259434
Iteration: 5 || Loss: 18.502582444480026
Iteration: 6 || Loss: 18.486122182700527
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.88259
Epoch 531 loss:18.486122182700527
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.714818182316267
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.629950203621284
waveform batch: 2/2
Test loss - extrapolation:15.388851350803492
Epoch 531 mean train loss:0.8954216571444312
Epoch 531 mean test loss - interpolation:0.9524696970527112
Epoch 531 mean test loss - extrapolation:4.584900129535398
Start training epoch 532
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.88259
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.662598065158916
Iteration: 2 || Loss: 2.6609685251134705
Iteration: 3 || Loss: 2.6593417858086754
Iteration: 4 || Loss: 2.657716764394533
Iteration: 5 || Loss: 2.656092648150063
Iteration: 6 || Loss: 2.656092648150063
saving ADAM checkpoint...
Sum of params:85.8826
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.656092648150063
Iteration: 2 || Loss: 2.0165360785800197
Iteration: 3 || Loss: 1.6100859051139291
Iteration: 4 || Loss: 1.6070213400698217
Iteration: 5 || Loss: 1.587024344005117
Iteration: 6 || Loss: 1.585980124346328
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.94357
Epoch 532 loss:1.585980124346328
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.94357
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.1070799628430485
Iteration: 2 || Loss: 6.106616488180224
Iteration: 3 || Loss: 6.1061565363338755
Iteration: 4 || Loss: 6.105697768861482
Iteration: 5 || Loss: 6.105239679372074
Iteration: 6 || Loss: 6.105239679372074
saving ADAM checkpoint...
Sum of params:85.9437
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.105239679372074
Iteration: 2 || Loss: 6.062203867013859
Iteration: 3 || Loss: 6.008130740698098
Iteration: 4 || Loss: 5.908710276308468
Iteration: 5 || Loss: 5.899459657217072
Iteration: 6 || Loss: 5.893055477114862
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.99975
Epoch 532 loss:5.893055477114862
waveform batch: 3/3
Using ADAM optimizer
Sum of params:85.99975
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.906388177935636
Iteration: 2 || Loss: 18.90587172863678
Iteration: 3 || Loss: 18.905358277335417
Iteration: 4 || Loss: 18.904845490138698
Iteration: 5 || Loss: 18.904336129593855
Iteration: 6 || Loss: 18.904336129593855
saving ADAM checkpoint...
Sum of params:85.99983
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.904336129593855
Iteration: 2 || Loss: 18.84418201093481
Iteration: 3 || Loss: 18.622456922371914
Iteration: 4 || Loss: 18.52147435249386
Iteration: 5 || Loss: 18.494499177122943
Iteration: 6 || Loss: 18.478010661435015
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.89019
Epoch 532 loss:18.478010661435015
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.713179140349103
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.61772712551186
waveform batch: 2/2
Test loss - extrapolation:15.380999770606511
Epoch 532 mean train loss:0.8950705607895244
Epoch 532 mean test loss - interpolation:0.9521965233915172
Epoch 532 mean test loss - extrapolation:4.583227241343198
Start training epoch 533
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.89019
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.662468461357205
Iteration: 2 || Loss: 2.6608380648914722
Iteration: 3 || Loss: 2.6592084188586527
Iteration: 4 || Loss: 2.65757928805933
Iteration: 5 || Loss: 2.655954398536433
Iteration: 6 || Loss: 2.655954398536433
saving ADAM checkpoint...
Sum of params:85.8902
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.655954398536433
Iteration: 2 || Loss: 2.0155232374033583
Iteration: 3 || Loss: 1.6095552194988934
Iteration: 4 || Loss: 1.6064925348076307
Iteration: 5 || Loss: 1.5865015320424813
Iteration: 6 || Loss: 1.5854193751933268
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.951096
Epoch 533 loss:1.5854193751933268
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.951096
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.105693072331468
Iteration: 2 || Loss: 6.105228462738838
Iteration: 3 || Loss: 6.104766108516825
Iteration: 4 || Loss: 6.104306052685867
Iteration: 5 || Loss: 6.103847913890174
Iteration: 6 || Loss: 6.103847913890174
saving ADAM checkpoint...
Sum of params:85.95121
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.103847913890174
Iteration: 2 || Loss: 6.060562865978549
Iteration: 3 || Loss: 6.006661448349199
Iteration: 4 || Loss: 5.907216269409677
Iteration: 5 || Loss: 5.897971637868631
Iteration: 6 || Loss: 5.8915741929915795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.007355
Epoch 533 loss:5.8915741929915795
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.007355
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.898297331197618
Iteration: 2 || Loss: 18.897781723314356
Iteration: 3 || Loss: 18.897266822370067
Iteration: 4 || Loss: 18.89675245277221
Iteration: 5 || Loss: 18.896241256562742
Iteration: 6 || Loss: 18.896241256562742
saving ADAM checkpoint...
Sum of params:86.00747
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.896241256562742
Iteration: 2 || Loss: 18.83592642759284
Iteration: 3 || Loss: 18.614256757566277
Iteration: 4 || Loss: 18.513348558121038
Iteration: 5 || Loss: 18.48640470654647
Iteration: 6 || Loss: 18.469982653025614
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.89787
Epoch 533 loss:18.469982653025614
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.711432123053268
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.604689932240085
waveform batch: 2/2
Test loss - extrapolation:15.373182268156944
Epoch 533 mean train loss:0.8947233179727765
Epoch 533 mean test loss - interpolation:0.9519053538422114
Epoch 533 mean test loss - extrapolation:4.5814893500330856
Start training epoch 534
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.89787
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.66092716798471
Iteration: 2 || Loss: 2.659299136021277
Iteration: 3 || Loss: 2.6576702183263983
Iteration: 4 || Loss: 2.656040981656883
Iteration: 5 || Loss: 2.654416750652851
Iteration: 6 || Loss: 2.654416750652851
saving ADAM checkpoint...
Sum of params:85.89787
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.654416750652851
Iteration: 2 || Loss: 2.014719405661574
Iteration: 3 || Loss: 1.608935414618037
Iteration: 4 || Loss: 1.6058507480737647
Iteration: 5 || Loss: 1.5859272392044084
Iteration: 6 || Loss: 1.5848607060998767
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.95865
Epoch 534 loss:1.5848607060998767
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.95865
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.104005264876503
Iteration: 2 || Loss: 6.103540485768511
Iteration: 3 || Loss: 6.103079069259928
Iteration: 4 || Loss: 6.102618970116614
Iteration: 5 || Loss: 6.102161938811347
Iteration: 6 || Loss: 6.102161938811347
saving ADAM checkpoint...
Sum of params:85.958786
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.102161938811347
Iteration: 2 || Loss: 6.058993262829187
Iteration: 3 || Loss: 6.005115811422257
Iteration: 4 || Loss: 5.905725165483248
Iteration: 5 || Loss: 5.896433363699132
Iteration: 6 || Loss: 5.89005989067978
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.01505
Epoch 534 loss:5.89005989067978
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.01505
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.89043354940815
Iteration: 2 || Loss: 18.889919014643723
Iteration: 3 || Loss: 18.889403525730472
Iteration: 4 || Loss: 18.888890612341267
Iteration: 5 || Loss: 18.888379282079793
Iteration: 6 || Loss: 18.888379282079793
saving ADAM checkpoint...
Sum of params:86.01515
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.888379282079793
Iteration: 2 || Loss: 18.828170791208862
Iteration: 3 || Loss: 18.606247400190803
Iteration: 4 || Loss: 18.505327604845675
Iteration: 5 || Loss: 18.478377116031346
Iteration: 6 || Loss: 18.461913822200128
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.90543
Epoch 534 loss:18.461913822200128
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.7098621611578615
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.59267112808109
waveform batch: 2/2
Test loss - extrapolation:15.365330700304883
Epoch 534 mean train loss:0.8943736006544754
Epoch 534 mean test loss - interpolation:0.9516436935263103
Epoch 534 mean test loss - extrapolation:4.579833485698831
Start training epoch 535
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.90543
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6611581406877285
Iteration: 2 || Loss: 2.659526224454624
Iteration: 3 || Loss: 2.6578943098465793
Iteration: 4 || Loss: 2.656262440205163
Iteration: 5 || Loss: 2.654638795403388
Iteration: 6 || Loss: 2.654638795403388
saving ADAM checkpoint...
Sum of params:85.90544
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.654638795403388
Iteration: 2 || Loss: 2.0137123590959645
Iteration: 3 || Loss: 1.6084172754460564
Iteration: 4 || Loss: 1.6053344193493329
Iteration: 5 || Loss: 1.585409680091929
Iteration: 6 || Loss: 1.5842977921349315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.96613
Epoch 535 loss:1.5842977921349315
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.96613
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.10267713146437
Iteration: 2 || Loss: 6.102211198124693
Iteration: 3 || Loss: 6.101747459464697
Iteration: 4 || Loss: 6.101286060239637
Iteration: 5 || Loss: 6.100827804396066
Iteration: 6 || Loss: 6.100827804396066
saving ADAM checkpoint...
Sum of params:85.96626
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.100827804396066
Iteration: 2 || Loss: 6.057398910501803
Iteration: 3 || Loss: 6.003693657107203
Iteration: 4 || Loss: 5.904234712652175
Iteration: 5 || Loss: 5.8949499038316615
Iteration: 6 || Loss: 5.888582987450528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.02267
Epoch 535 loss:5.888582987450528
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.02267
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.882363867546488
Iteration: 2 || Loss: 18.881846840514623
Iteration: 3 || Loss: 18.881330712845276
Iteration: 4 || Loss: 18.88081681759691
Iteration: 5 || Loss: 18.88030386286999
Iteration: 6 || Loss: 18.88030386286999
saving ADAM checkpoint...
Sum of params:86.022766
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.88030386286999
Iteration: 2 || Loss: 18.819922645916897
Iteration: 3 || Loss: 18.598085753407407
Iteration: 4 || Loss: 18.4972482769918
Iteration: 5 || Loss: 18.470330911642417
Iteration: 6 || Loss: 18.45394031605081
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.91306
Epoch 535 loss:18.45394031605081
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.708144509043504
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.579654322139405
waveform batch: 2/2
Test loss - extrapolation:15.357539980186731
Epoch 535 mean train loss:0.89402831364263
Epoch 535 mean test loss - interpolation:0.9513574181739174
Epoch 535 mean test loss - extrapolation:4.578099525193845
Start training epoch 536
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.91306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.659605491108545
Iteration: 2 || Loss: 2.6579741801926775
Iteration: 3 || Loss: 2.656345610804715
Iteration: 4 || Loss: 2.654719373453539
Iteration: 5 || Loss: 2.653091504683153
Iteration: 6 || Loss: 2.653091504683153
saving ADAM checkpoint...
Sum of params:85.913055
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.653091504683153
Iteration: 2 || Loss: 2.0129289804691646
Iteration: 3 || Loss: 1.6077846192914274
Iteration: 4 || Loss: 1.604683000906473
Iteration: 5 || Loss: 1.584826685477546
Iteration: 6 || Loss: 1.5837321686130694
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.97368
Epoch 536 loss:1.5837321686130694
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.97368
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.101016135965471
Iteration: 2 || Loss: 6.100551637734279
Iteration: 3 || Loss: 6.1000897941697465
Iteration: 4 || Loss: 6.0996287153705175
Iteration: 5 || Loss: 6.099168558824947
Iteration: 6 || Loss: 6.099168558824947
saving ADAM checkpoint...
Sum of params:85.9738
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.099168558824947
Iteration: 2 || Loss: 6.055752091029663
Iteration: 3 || Loss: 6.002083038841178
Iteration: 4 || Loss: 5.9027465179996454
Iteration: 5 || Loss: 5.8934355625803985
Iteration: 6 || Loss: 5.887084226159453
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.03027
Epoch 536 loss:5.887084226159453
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.03027
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.87448751179669
Iteration: 2 || Loss: 18.873970669225375
Iteration: 3 || Loss: 18.873455740040022
Iteration: 4 || Loss: 18.872941405189092
Iteration: 5 || Loss: 18.87242832227636
Iteration: 6 || Loss: 18.87242832227636
saving ADAM checkpoint...
Sum of params:86.03038
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.87242832227636
Iteration: 2 || Loss: 18.812078755109543
Iteration: 3 || Loss: 18.59005759570443
Iteration: 4 || Loss: 18.489249463834827
Iteration: 5 || Loss: 18.462335779453273
Iteration: 6 || Loss: 18.445933442760328
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.92059
Epoch 536 loss:18.445933442760328
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.706510408794028
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.56739853729312
waveform batch: 2/2
Test loss - extrapolation:15.34972821986067
Epoch 536 mean train loss:0.8936810288804431
Epoch 536 mean test loss - interpolation:0.951085068132338
Epoch 536 mean test loss - extrapolation:4.576427229762817
Start training epoch 537
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.92059
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6593221930013207
Iteration: 2 || Loss: 2.6576872060490357
Iteration: 3 || Loss: 2.6560553954724506
Iteration: 4 || Loss: 2.654424670935394
Iteration: 5 || Loss: 2.6527968323920654
Iteration: 6 || Loss: 2.6527968323920654
saving ADAM checkpoint...
Sum of params:85.9206
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6527968323920654
Iteration: 2 || Loss: 2.0119842598942013
Iteration: 3 || Loss: 1.6072465690492812
Iteration: 4 || Loss: 1.604144026412698
Iteration: 5 || Loss: 1.5842967136755812
Iteration: 6 || Loss: 1.5831720322830138
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.98109
Epoch 537 loss:1.5831720322830138
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.98109
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.099532533198196
Iteration: 2 || Loss: 6.09906649622948
Iteration: 3 || Loss: 6.098603274404279
Iteration: 4 || Loss: 6.09814263189446
Iteration: 5 || Loss: 6.097682069559312
Iteration: 6 || Loss: 6.097682069559312
saving ADAM checkpoint...
Sum of params:85.981224
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.097682069559312
Iteration: 2 || Loss: 6.0542265747159165
Iteration: 3 || Loss: 6.00069525546518
Iteration: 4 || Loss: 5.901258485718166
Iteration: 5 || Loss: 5.891916409840639
Iteration: 6 || Loss: 5.88558593922831
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.03791
Epoch 537 loss:5.88558593922831
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.03791
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.86661143231125
Iteration: 2 || Loss: 18.86609450030549
Iteration: 3 || Loss: 18.865579480294993
Iteration: 4 || Loss: 18.865063884907443
Iteration: 5 || Loss: 18.864553496966362
Iteration: 6 || Loss: 18.864553496966362
saving ADAM checkpoint...
Sum of params:86.03801
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.864553496966362
Iteration: 2 || Loss: 18.80421966663637
Iteration: 3 || Loss: 18.582085534020635
Iteration: 4 || Loss: 18.48128000726937
Iteration: 5 || Loss: 18.45437221507125
Iteration: 6 || Loss: 18.437960737301896
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.928116
Epoch 537 loss:18.437960737301896
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.704924506016796
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.555126802434664
waveform batch: 2/2
Test loss - extrapolation:15.341951237566683
Epoch 537 mean train loss:0.893335127890111
Epoch 537 mean test loss - interpolation:0.9508207510027993
Epoch 537 mean test loss - extrapolation:4.574756503333446
Start training epoch 538
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.928116
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6589655765608975
Iteration: 2 || Loss: 2.6573325177527685
Iteration: 3 || Loss: 2.6556981661793015
Iteration: 4 || Loss: 2.654066328577095
Iteration: 5 || Loss: 2.6524362772878174
Iteration: 6 || Loss: 2.6524362772878174
saving ADAM checkpoint...
Sum of params:85.92809
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6524362772878174
Iteration: 2 || Loss: 2.010992879768925
Iteration: 3 || Loss: 1.6066720125427818
Iteration: 4 || Loss: 1.6035614791908437
Iteration: 5 || Loss: 1.5837477635638137
Iteration: 6 || Loss: 1.582608415467083
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.98847
Epoch 538 loss:1.582608415467083
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.98847
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.097893598957145
Iteration: 2 || Loss: 6.097428437541422
Iteration: 3 || Loss: 6.096965419547605
Iteration: 4 || Loss: 6.096503927453025
Iteration: 5 || Loss: 6.096044461492497
Iteration: 6 || Loss: 6.096044461492497
saving ADAM checkpoint...
Sum of params:85.9886
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.096044461492497
Iteration: 2 || Loss: 6.052785757200636
Iteration: 3 || Loss: 5.999349945022706
Iteration: 4 || Loss: 5.89978592394701
Iteration: 5 || Loss: 5.890380139663927
Iteration: 6 || Loss: 5.884074167634297
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.04556
Epoch 538 loss:5.884074167634297
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.04556
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.85882024715869
Iteration: 2 || Loss: 18.85830513025851
Iteration: 3 || Loss: 18.85778917508565
Iteration: 4 || Loss: 18.85727444280751
Iteration: 5 || Loss: 18.85676305016292
Iteration: 6 || Loss: 18.85676305016292
saving ADAM checkpoint...
Sum of params:86.045685
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.85676305016292
Iteration: 2 || Loss: 18.796570840393997
Iteration: 3 || Loss: 18.574258879343958
Iteration: 4 || Loss: 18.473350861154664
Iteration: 5 || Loss: 18.446432670266038
Iteration: 6 || Loss: 18.429949278629024
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.935585
Epoch 538 loss:18.429949278629024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.703353533203788
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.543291056912615
waveform batch: 2/2
Test loss - extrapolation:15.334223017893747
Epoch 538 mean train loss:0.8929873055769105
Epoch 538 mean test loss - interpolation:0.9505589222006313
Epoch 538 mean test loss - extrapolation:4.573126172900531
Start training epoch 539
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.935585
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.659195178924434
Iteration: 2 || Loss: 2.657559531666364
Iteration: 3 || Loss: 2.655922334030655
Iteration: 4 || Loss: 2.6542864799730537
Iteration: 5 || Loss: 2.6526548295141286
Iteration: 6 || Loss: 2.6526548295141286
saving ADAM checkpoint...
Sum of params:85.93559
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6526548295141286
Iteration: 2 || Loss: 2.009773357998002
Iteration: 3 || Loss: 1.6061323993722678
Iteration: 4 || Loss: 1.6030249161822132
Iteration: 5 || Loss: 1.5832173560978227
Iteration: 6 || Loss: 1.5820062041929022
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.995834
Epoch 539 loss:1.5820062041929022
waveform batch: 2/3
Using ADAM optimizer
Sum of params:85.995834
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.096576328403288
Iteration: 2 || Loss: 6.096110079132812
Iteration: 3 || Loss: 6.095646202798838
Iteration: 4 || Loss: 6.095182201385636
Iteration: 5 || Loss: 6.094722912458885
Iteration: 6 || Loss: 6.094722912458885
saving ADAM checkpoint...
Sum of params:85.99598
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.094722912458885
Iteration: 2 || Loss: 6.051210525890327
Iteration: 3 || Loss: 5.998002489083222
Iteration: 4 || Loss: 5.898294733910781
Iteration: 5 || Loss: 5.888887040172664
Iteration: 6 || Loss: 5.882594206925723
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.05314
Epoch 539 loss:5.882594206925723
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.05314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.850873608308977
Iteration: 2 || Loss: 18.850357073302785
Iteration: 3 || Loss: 18.849839742356956
Iteration: 4 || Loss: 18.849327517422072
Iteration: 5 || Loss: 18.848813596140616
Iteration: 6 || Loss: 18.848813596140616
saving ADAM checkpoint...
Sum of params:86.05323
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.848813596140616
Iteration: 2 || Loss: 18.78847581929467
Iteration: 3 || Loss: 18.5662993353026
Iteration: 4 || Loss: 18.465379242784497
Iteration: 5 || Loss: 18.43849024823237
Iteration: 6 || Loss: 18.422066245963713
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.94312
Epoch 539 loss:18.422066245963713
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.70169753458719
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.53038953528033
waveform batch: 2/2
Test loss - extrapolation:15.326521373346674
Epoch 539 mean train loss:0.8926436778304254
Epoch 539 mean test loss - interpolation:0.9502829224311983
Epoch 539 mean test loss - extrapolation:4.571409242385584
Start training epoch 540
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.94312
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6576477154754956
Iteration: 2 || Loss: 2.656007305257254
Iteration: 3 || Loss: 2.6543700402311385
Iteration: 4 || Loss: 2.6527355225941007
Iteration: 5 || Loss: 2.6511066359045397
Iteration: 6 || Loss: 2.6511066359045397
saving ADAM checkpoint...
Sum of params:85.94312
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6511066359045397
Iteration: 2 || Loss: 2.008848479434822
Iteration: 3 || Loss: 1.6054556578436925
Iteration: 4 || Loss: 1.6023307307072645
Iteration: 5 || Loss: 1.5825998597975093
Iteration: 6 || Loss: 1.5814096243868179
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.00324
Epoch 540 loss:1.5814096243868179
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.00324
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.094876976118305
Iteration: 2 || Loss: 6.094410151678114
Iteration: 3 || Loss: 6.093945656160045
Iteration: 4 || Loss: 6.093483751384305
Iteration: 5 || Loss: 6.093023213753374
Iteration: 6 || Loss: 6.093023213753374
saving ADAM checkpoint...
Sum of params:86.00337
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.093023213753374
Iteration: 2 || Loss: 6.049576345429625
Iteration: 3 || Loss: 5.99640129972032
Iteration: 4 || Loss: 5.89681336062533
Iteration: 5 || Loss: 5.8873748852988905
Iteration: 6 || Loss: 5.881100809746266
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.06064
Epoch 540 loss:5.881100809746266
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.06064
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.84308377608479
Iteration: 2 || Loss: 18.842567288171914
Iteration: 3 || Loss: 18.84205059966126
Iteration: 4 || Loss: 18.841536317973922
Iteration: 5 || Loss: 18.84102349462702
Iteration: 6 || Loss: 18.84102349462702
saving ADAM checkpoint...
Sum of params:86.06076
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.84102349462702
Iteration: 2 || Loss: 18.780713971916192
Iteration: 3 || Loss: 18.558379069007632
Iteration: 4 || Loss: 18.457450703837253
Iteration: 5 || Loss: 18.43056711571138
Iteration: 6 || Loss: 18.414129442774612
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.95056
Epoch 540 loss:18.414129442774612
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.700096188607942
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.51820170900188
waveform batch: 2/2
Test loss - extrapolation:15.318763879414501
Epoch 540 mean train loss:0.8922979267899206
Epoch 540 mean test loss - interpolation:0.950016031434657
Epoch 540 mean test loss - extrapolation:4.569747132368032
Start training epoch 541
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.95056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6573472147270962
Iteration: 2 || Loss: 2.6557079872397282
Iteration: 3 || Loss: 2.6540675883859963
Iteration: 4 || Loss: 2.6524337750064784
Iteration: 5 || Loss: 2.6508024902773215
Iteration: 6 || Loss: 2.6508024902773215
saving ADAM checkpoint...
Sum of params:85.95057
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6508024902773215
Iteration: 2 || Loss: 2.007810889276481
Iteration: 3 || Loss: 1.6048918260525802
Iteration: 4 || Loss: 1.601762036108112
Iteration: 5 || Loss: 1.5820455391742096
Iteration: 6 || Loss: 1.5808094350435011
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.0106
Epoch 541 loss:1.5808094350435011
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.0106
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.09354946206223
Iteration: 2 || Loss: 6.093082973206897
Iteration: 3 || Loss: 6.092618552635525
Iteration: 4 || Loss: 6.092155919691692
Iteration: 5 || Loss: 6.091692298772789
Iteration: 6 || Loss: 6.091692298772789
saving ADAM checkpoint...
Sum of params:86.010735
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.091692298772789
Iteration: 2 || Loss: 6.048033802311285
Iteration: 3 || Loss: 5.994990653659335
Iteration: 4 || Loss: 5.895322621436656
Iteration: 5 || Loss: 5.885890748655876
Iteration: 6 || Loss: 5.879622033596256
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.06814
Epoch 541 loss:5.879622033596256
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.06814
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.835164202925654
Iteration: 2 || Loss: 18.834645320586908
Iteration: 3 || Loss: 18.83412875525804
Iteration: 4 || Loss: 18.83361427177886
Iteration: 5 || Loss: 18.83310019734519
Iteration: 6 || Loss: 18.83310019734519
saving ADAM checkpoint...
Sum of params:86.06822
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.83310019734519
Iteration: 2 || Loss: 18.772634466554706
Iteration: 3 || Loss: 18.55039407907974
Iteration: 4 || Loss: 18.44952731646743
Iteration: 5 || Loss: 18.422671449938846
Iteration: 6 || Loss: 18.40629282719778
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.95803
Epoch 541 loss:18.40629282719778
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.698411167033674
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.505304135421575
waveform batch: 2/2
Test loss - extrapolation:15.311053259521268
Epoch 541 mean train loss:0.8919560102012944
Epoch 541 mean test loss - interpolation:0.9497351945056124
Epoch 541 mean test loss - extrapolation:4.568029782911903
Start training epoch 542
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.95803
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.655799948243304
Iteration: 2 || Loss: 2.654160522258329
Iteration: 3 || Loss: 2.6525229819226577
Iteration: 4 || Loss: 2.650887116312524
Iteration: 5 || Loss: 2.6492559183014643
Iteration: 6 || Loss: 2.6492559183014643
saving ADAM checkpoint...
Sum of params:85.95804
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6492559183014643
Iteration: 2 || Loss: 2.0069191817506558
Iteration: 3 || Loss: 1.6042334340447737
Iteration: 4 || Loss: 1.601083111383755
Iteration: 5 || Loss: 1.5814382602539923
Iteration: 6 || Loss: 1.5802231418582806
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.01797
Epoch 542 loss:1.5802231418582806
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.01797
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.091811586281914
Iteration: 2 || Loss: 6.091344624181762
Iteration: 3 || Loss: 6.090881744762431
Iteration: 4 || Loss: 6.090418971060934
Iteration: 5 || Loss: 6.089957016122723
Iteration: 6 || Loss: 6.089957016122723
saving ADAM checkpoint...
Sum of params:86.01809
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.089957016122723
Iteration: 2 || Loss: 6.0464717435067366
Iteration: 3 || Loss: 5.993459953335986
Iteration: 4 || Loss: 5.893845140391303
Iteration: 5 || Loss: 5.884353638059736
Iteration: 6 || Loss: 5.878114169504062
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.07566
Epoch 542 loss:5.878114169504062
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.07566
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.827516934828484
Iteration: 2 || Loss: 18.826999055734444
Iteration: 3 || Loss: 18.8264837323045
Iteration: 4 || Loss: 18.825968129241232
Iteration: 5 || Loss: 18.825455944875603
Iteration: 6 || Loss: 18.825455944875603
saving ADAM checkpoint...
Sum of params:86.075775
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.825455944875603
Iteration: 2 || Loss: 18.7651483355548
Iteration: 3 || Loss: 18.54258981272108
Iteration: 4 || Loss: 18.441681252514297
Iteration: 5 || Loss: 18.41481587515811
Iteration: 6 || Loss: 18.398364724880736
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.96539
Epoch 542 loss:18.398364724880736
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.6969074059900695
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.493646165127934
waveform batch: 2/2
Test loss - extrapolation:15.303306907369366
Epoch 542 mean train loss:0.8916104150428648
Epoch 542 mean test loss - interpolation:0.9494845676650115
Epoch 542 mean test loss - extrapolation:4.566412756041442
Start training epoch 543
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.96539
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.656343794285466
Iteration: 2 || Loss: 2.654701931838904
Iteration: 3 || Loss: 2.6530618616924904
Iteration: 4 || Loss: 2.6514245670157996
Iteration: 5 || Loss: 2.6497882050131913
Iteration: 6 || Loss: 2.6497882050131913
saving ADAM checkpoint...
Sum of params:85.96539
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6497882050131913
Iteration: 2 || Loss: 2.005771776759947
Iteration: 3 || Loss: 1.6037064480395924
Iteration: 4 || Loss: 1.6005628040888158
Iteration: 5 || Loss: 1.5809075873828966
Iteration: 6 || Loss: 1.5796187236110015
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.02522
Epoch 543 loss:1.5796187236110015
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.02522
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.09047786192187
Iteration: 2 || Loss: 6.090010053429877
Iteration: 3 || Loss: 6.089546941643978
Iteration: 4 || Loss: 6.089081929799678
Iteration: 5 || Loss: 6.0886204679668845
Iteration: 6 || Loss: 6.0886204679668845
saving ADAM checkpoint...
Sum of params:86.02533
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.0886204679668845
Iteration: 2 || Loss: 6.045080918634397
Iteration: 3 || Loss: 5.992249345614009
Iteration: 4 || Loss: 5.892359167273271
Iteration: 5 || Loss: 5.882830031420732
Iteration: 6 || Loss: 5.876613614572031
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.08321
Epoch 543 loss:5.876613614572031
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.08321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.81975916490496
Iteration: 2 || Loss: 18.81924132295272
Iteration: 3 || Loss: 18.818724615787374
Iteration: 4 || Loss: 18.818209396124395
Iteration: 5 || Loss: 18.817697534463644
Iteration: 6 || Loss: 18.817697534463644
saving ADAM checkpoint...
Sum of params:86.08332
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.817697534463644
Iteration: 2 || Loss: 18.757395379589855
Iteration: 3 || Loss: 18.53484214226146
Iteration: 4 || Loss: 18.433850011758004
Iteration: 5 || Loss: 18.40699015506077
Iteration: 6 || Loss: 18.390511263049657
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.97277
Epoch 543 loss:18.390511263049657
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.695313976814951
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.481346343410046
waveform batch: 2/2
Test loss - extrapolation:15.295672590518393
Epoch 543 mean train loss:0.8912670207321617
Epoch 543 mean test loss - interpolation:0.9492189961358252
Epoch 543 mean test loss - extrapolation:4.56475157782737
Start training epoch 544
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.97277
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6556054003343665
Iteration: 2 || Loss: 2.653962712671967
Iteration: 3 || Loss: 2.6523250853878766
Iteration: 4 || Loss: 2.6506846677112508
Iteration: 5 || Loss: 2.6490470266093022
Iteration: 6 || Loss: 2.6490470266093022
saving ADAM checkpoint...
Sum of params:85.972786
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6490470266093022
Iteration: 2 || Loss: 2.0045866405096144
Iteration: 3 || Loss: 1.6030615823997105
Iteration: 4 || Loss: 1.599911375965464
Iteration: 5 || Loss: 1.5803084478051093
Iteration: 6 || Loss: 1.5789831695207495
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.03251
Epoch 544 loss:1.5789831695207495
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.03251
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.089113599672674
Iteration: 2 || Loss: 6.088644032442327
Iteration: 3 || Loss: 6.088178606984945
Iteration: 4 || Loss: 6.087714255839879
Iteration: 5 || Loss: 6.087251056937363
Iteration: 6 || Loss: 6.087251056937363
saving ADAM checkpoint...
Sum of params:86.032646
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.087251056937363
Iteration: 2 || Loss: 6.0434904315087135
Iteration: 3 || Loss: 5.990769330824098
Iteration: 4 || Loss: 5.8908780335013695
Iteration: 5 || Loss: 5.881356297990654
Iteration: 6 || Loss: 5.875148274432348
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.09058
Epoch 544 loss:5.875148274432348
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.09058
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.811902063054514
Iteration: 2 || Loss: 18.81138400155018
Iteration: 3 || Loss: 18.810864946811552
Iteration: 4 || Loss: 18.810350319564325
Iteration: 5 || Loss: 18.80983493773679
Iteration: 6 || Loss: 18.80983493773679
saving ADAM checkpoint...
Sum of params:86.09069
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.80983493773679
Iteration: 2 || Loss: 18.749366932158267
Iteration: 3 || Loss: 18.526948726012957
Iteration: 4 || Loss: 18.425969104196838
Iteration: 5 || Loss: 18.399140834194938
Iteration: 6 || Loss: 18.382734763715384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.98019
Epoch 544 loss:18.382734763715384
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.693629152933174
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.46840842070598
waveform batch: 2/2
Test loss - extrapolation:15.28801001608755
Epoch 544 mean train loss:0.8909264209540856
Epoch 544 mean test loss - interpolation:0.948938192155529
Epoch 544 mean test loss - extrapolation:4.563034869732793
Start training epoch 545
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.98019
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6538631524681158
Iteration: 2 || Loss: 2.6522220986379903
Iteration: 3 || Loss: 2.6505827553334997
Iteration: 4 || Loss: 2.6489422057113434
Iteration: 5 || Loss: 2.647306902466361
Iteration: 6 || Loss: 2.647306902466361
saving ADAM checkpoint...
Sum of params:85.98019
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.647306902466361
Iteration: 2 || Loss: 2.0036136437190226
Iteration: 3 || Loss: 1.602375698097575
Iteration: 4 || Loss: 1.5992017277924748
Iteration: 5 || Loss: 1.5796722771918892
Iteration: 6 || Loss: 1.5783799031705854
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.03978
Epoch 545 loss:1.5783799031705854
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.03978
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.087396033250046
Iteration: 2 || Loss: 6.086928359860342
Iteration: 3 || Loss: 6.086463682899732
Iteration: 4 || Loss: 6.085999866558615
Iteration: 5 || Loss: 6.085536129408781
Iteration: 6 || Loss: 6.085536129408781
saving ADAM checkpoint...
Sum of params:86.0399
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.085536129408781
Iteration: 2 || Loss: 6.041934139339739
Iteration: 3 || Loss: 5.989214307690653
Iteration: 4 || Loss: 5.889406199872565
Iteration: 5 || Loss: 5.879831797143335
Iteration: 6 || Loss: 5.873647071494847
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.098045
Epoch 545 loss:5.873647071494847
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.098045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.804308142741935
Iteration: 2 || Loss: 18.803788826028423
Iteration: 3 || Loss: 18.803272719132224
Iteration: 4 || Loss: 18.80275640487684
Iteration: 5 || Loss: 18.802242009134645
Iteration: 6 || Loss: 18.802242009134645
saving ADAM checkpoint...
Sum of params:86.09817
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.802242009134645
Iteration: 2 || Loss: 18.741879928332224
Iteration: 3 || Loss: 18.51916807479799
Iteration: 4 || Loss: 18.418160576071976
Iteration: 5 || Loss: 18.391329825457383
Iteration: 6 || Loss: 18.374870170779083
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.987495
Epoch 545 loss:18.374870170779083
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.6921145632864185
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.456639761271425
waveform batch: 2/2
Test loss - extrapolation:15.280285840341188
Epoch 545 mean train loss:0.8905826601877419
Epoch 545 mean test loss - interpolation:0.9486857605477365
Epoch 545 mean test loss - extrapolation:4.561410466801051
Start training epoch 546
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.987495
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6541614081337466
Iteration: 2 || Loss: 2.652517692577557
Iteration: 3 || Loss: 2.6508746007041952
Iteration: 4 || Loss: 2.6492352089715485
Iteration: 5 || Loss: 2.647595500363224
Iteration: 6 || Loss: 2.647595500363224
saving ADAM checkpoint...
Sum of params:85.9875
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.647595500363224
Iteration: 2 || Loss: 2.0025018211144454
Iteration: 3 || Loss: 1.6018275398023698
Iteration: 4 || Loss: 1.5986613739893714
Iteration: 5 || Loss: 1.5791240094700185
Iteration: 6 || Loss: 1.5777381974131681
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.046936
Epoch 546 loss:1.5777381974131681
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.046936
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.085807988459138
Iteration: 2 || Loss: 6.085341745060222
Iteration: 3 || Loss: 6.084876627517078
Iteration: 4 || Loss: 6.084415818066272
Iteration: 5 || Loss: 6.08395328488454
Iteration: 6 || Loss: 6.08395328488454
saving ADAM checkpoint...
Sum of params:86.047066
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.08395328488454
Iteration: 2 || Loss: 6.0406567292529445
Iteration: 3 || Loss: 5.988153440799964
Iteration: 4 || Loss: 5.887898346325866
Iteration: 5 || Loss: 5.878237050846416
Iteration: 6 || Loss: 5.8720892204553925
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.105675
Epoch 546 loss:5.8720892204553925
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.105675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.79684335314641
Iteration: 2 || Loss: 18.796325578139687
Iteration: 3 || Loss: 18.795809080171704
Iteration: 4 || Loss: 18.795294040447224
Iteration: 5 || Loss: 18.794782067184872
Iteration: 6 || Loss: 18.794782067184872
saving ADAM checkpoint...
Sum of params:86.10579
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.794782067184872
Iteration: 2 || Loss: 18.734686565089394
Iteration: 3 || Loss: 18.51178431801112
Iteration: 4 || Loss: 18.410524807508374
Iteration: 5 || Loss: 18.3836607287456
Iteration: 6 || Loss: 18.36702905282574
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:85.994675
Epoch 546 loss:18.36702905282574
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.690706687612623
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.44535873656638
waveform batch: 2/2
Test loss - extrapolation:15.272718218659778
Epoch 546 mean train loss:0.8902364300239413
Epoch 546 mean test loss - interpolation:0.9484511146021039
Epoch 546 mean test loss - extrapolation:4.559839746268846
Start training epoch 547
waveform batch: 1/3
Using ADAM optimizer
Sum of params:85.994675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6549703049032822
Iteration: 2 || Loss: 2.653321277453695
Iteration: 3 || Loss: 2.6516762237561635
Iteration: 4 || Loss: 2.6500344015374457
Iteration: 5 || Loss: 2.6483903157706883
Iteration: 6 || Loss: 2.6483903157706883
saving ADAM checkpoint...
Sum of params:85.99469
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6483903157706883
Iteration: 2 || Loss: 2.000868386730075
Iteration: 3 || Loss: 1.601213695278929
Iteration: 4 || Loss: 1.5980606885597002
Iteration: 5 || Loss: 1.5785341925545024
Iteration: 6 || Loss: 1.5770153792688246
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.05417
Epoch 547 loss:1.5770153792688246
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.05417
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.085113548745079
Iteration: 2 || Loss: 6.084643525511882
Iteration: 3 || Loss: 6.084177088158419
Iteration: 4 || Loss: 6.0837107384350135
Iteration: 5 || Loss: 6.083244514531294
Iteration: 6 || Loss: 6.083244514531294
saving ADAM checkpoint...
Sum of params:86.05429
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.083244514531294
Iteration: 2 || Loss: 6.039137931634644
Iteration: 3 || Loss: 5.986784995556775
Iteration: 4 || Loss: 5.886392917212828
Iteration: 5 || Loss: 5.87681261879029
Iteration: 6 || Loss: 5.870661200690998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.11281
Epoch 547 loss:5.870661200690998
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.11281
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.788805039852942
Iteration: 2 || Loss: 18.788284328036323
Iteration: 3 || Loss: 18.787765831765565
Iteration: 4 || Loss: 18.78724833729188
Iteration: 5 || Loss: 18.786732539817457
Iteration: 6 || Loss: 18.786732539817457
saving ADAM checkpoint...
Sum of params:86.11293
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.786732539817457
Iteration: 2 || Loss: 18.72606621423463
Iteration: 3 || Loss: 18.50384814238479
Iteration: 4 || Loss: 18.402657002858746
Iteration: 5 || Loss: 18.375866206154587
Iteration: 6 || Loss: 18.359488940254998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.00214
Epoch 547 loss:18.359488940254998
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.688793491291731
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.43073843221997
waveform batch: 2/2
Test loss - extrapolation:15.265188715112897
Epoch 547 mean train loss:0.8899022593177525
Epoch 547 mean test loss - interpolation:0.9481322485486219
Epoch 547 mean test loss - extrapolation:4.557993928944406
Start training epoch 548
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.00214
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.650190938041766
Iteration: 2 || Loss: 2.648548841958454
Iteration: 3 || Loss: 2.6469072428791685
Iteration: 4 || Loss: 2.645267297817062
Iteration: 5 || Loss: 2.643628388190691
Iteration: 6 || Loss: 2.643628388190691
saving ADAM checkpoint...
Sum of params:86.00213
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.643628388190691
Iteration: 2 || Loss: 2.0000473651556905
Iteration: 3 || Loss: 1.6002878297875927
Iteration: 4 || Loss: 1.5970631831512017
Iteration: 5 || Loss: 1.5777366510099278
Iteration: 6 || Loss: 1.5763914872452092
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.061295
Epoch 548 loss:1.5763914872452092
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.061295
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.082513090001575
Iteration: 2 || Loss: 6.08204575622411
Iteration: 3 || Loss: 6.08158089118821
Iteration: 4 || Loss: 6.081117828445438
Iteration: 5 || Loss: 6.080656803420674
Iteration: 6 || Loss: 6.080656803420674
saving ADAM checkpoint...
Sum of params:86.06145
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.080656803420674
Iteration: 2 || Loss: 6.037315980872392
Iteration: 3 || Loss: 5.984883491091302
Iteration: 4 || Loss: 5.884931927489787
Iteration: 5 || Loss: 5.875226552826935
Iteration: 6 || Loss: 5.869112525636026
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.120255
Epoch 548 loss:5.869112525636026
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.120255
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.781594902768177
Iteration: 2 || Loss: 18.78107659009483
Iteration: 3 || Loss: 18.780558465783116
Iteration: 4 || Loss: 18.780043265483204
Iteration: 5 || Loss: 18.779529841895844
Iteration: 6 || Loss: 18.779529841895844
saving ADAM checkpoint...
Sum of params:86.12037
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.779529841895844
Iteration: 2 || Loss: 18.719329767408155
Iteration: 3 || Loss: 18.4962603034983
Iteration: 4 || Loss: 18.394974017385287
Iteration: 5 || Loss: 18.368149934105144
Iteration: 6 || Loss: 18.351558890348436
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.009224
Epoch 548 loss:18.351558890348436
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.6875177335584155
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.42056027225509
waveform batch: 2/2
Test loss - extrapolation:15.257382153997934
Epoch 548 mean train loss:0.8895538932148163
Epoch 548 mean test loss - interpolation:0.9479196222597359
Epoch 548 mean test loss - extrapolation:4.556495202187752
Start training epoch 549
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.009224
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6532244445786684
Iteration: 2 || Loss: 2.6515756760099323
Iteration: 3 || Loss: 2.6499273127371987
Iteration: 4 || Loss: 2.6482808723760236
Iteration: 5 || Loss: 2.646636090503804
Iteration: 6 || Loss: 2.646636090503804
saving ADAM checkpoint...
Sum of params:86.00925
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.646636090503804
Iteration: 2 || Loss: 1.9986519680017507
Iteration: 3 || Loss: 1.5999033567483276
Iteration: 4 || Loss: 1.5967245528803777
Iteration: 5 || Loss: 1.5772719079688056
Iteration: 6 || Loss: 1.5756522491162652
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.068474
Epoch 549 loss:1.5756522491162652
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.068474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.082061113497424
Iteration: 2 || Loss: 6.0815901244822514
Iteration: 3 || Loss: 6.081122521900271
Iteration: 4 || Loss: 6.080657507727747
Iteration: 5 || Loss: 6.080193363342719
Iteration: 6 || Loss: 6.080193363342719
saving ADAM checkpoint...
Sum of params:86.06861
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.080193363342719
Iteration: 2 || Loss: 6.036342853242218
Iteration: 3 || Loss: 5.984170420249485
Iteration: 4 || Loss: 5.883359797472538
Iteration: 5 || Loss: 5.873674875954733
Iteration: 6 || Loss: 5.8675849815088785
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.127686
Epoch 549 loss:5.8675849815088785
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.127686
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.773835133511408
Iteration: 2 || Loss: 18.773315267720136
Iteration: 3 || Loss: 18.772797772868323
Iteration: 4 || Loss: 18.772281921794995
Iteration: 5 || Loss: 18.77176623333814
Iteration: 6 || Loss: 18.77176623333814
saving ADAM checkpoint...
Sum of params:86.12779
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.77176623333814
Iteration: 2 || Loss: 18.71133381384716
Iteration: 3 || Loss: 18.488840355906753
Iteration: 4 || Loss: 18.387415278510943
Iteration: 5 || Loss: 18.360601633978572
Iteration: 6 || Loss: 18.344051828631752
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.01647
Epoch 549 loss:18.344051828631752
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.6858698704880855
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.40710672755471
waveform batch: 2/2
Test loss - extrapolation:15.249958054641684
Epoch 549 mean train loss:0.8892168641123067
Epoch 549 mean test loss - interpolation:0.9476449784146809
Epoch 549 mean test loss - extrapolation:4.554755398516367
Start training epoch 550
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.01647
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6502404174391527
Iteration: 2 || Loss: 2.6485900853943836
Iteration: 3 || Loss: 2.6469431933447893
Iteration: 4 || Loss: 2.645300281291106
Iteration: 5 || Loss: 2.6436586261000206
Iteration: 6 || Loss: 2.6436586261000206
saving ADAM checkpoint...
Sum of params:86.01647
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6436586261000206
Iteration: 2 || Loss: 1.9972462907075657
Iteration: 3 || Loss: 1.5989663117389379
Iteration: 4 || Loss: 1.5957466365361879
Iteration: 5 || Loss: 1.5764736470280691
Iteration: 6 || Loss: 1.5749388819082377
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.0755
Epoch 550 loss:1.5749388819082377
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.0755
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.0801186503361695
Iteration: 2 || Loss: 6.079650182501414
Iteration: 3 || Loss: 6.0791843849160685
Iteration: 4 || Loss: 6.078718750914323
Iteration: 5 || Loss: 6.078256038791819
Iteration: 6 || Loss: 6.078256038791819
saving ADAM checkpoint...
Sum of params:86.07562
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.078256038791819
Iteration: 2 || Loss: 6.034636631700939
Iteration: 3 || Loss: 5.982441778700263
Iteration: 4 || Loss: 5.881890194063905
Iteration: 5 || Loss: 5.872152731603026
Iteration: 6 || Loss: 5.866080934996186
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.134895
Epoch 550 loss:5.866080934996186
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.134895
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.766426597345
Iteration: 2 || Loss: 18.765906631534506
Iteration: 3 || Loss: 18.7653872398149
Iteration: 4 || Loss: 18.7648719280798
Iteration: 5 || Loss: 18.76435665261845
Iteration: 6 || Loss: 18.76435665261845
saving ADAM checkpoint...
Sum of params:86.134995
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.76435665261845
Iteration: 2 || Loss: 18.70397921441834
Iteration: 3 || Loss: 18.48123807944694
Iteration: 4 || Loss: 18.379719200801603
Iteration: 5 || Loss: 18.352922189418493
Iteration: 6 || Loss: 18.336346833640807
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.02362
Epoch 550 loss:18.336346833640807
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.684341318573994
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.39523767277592
waveform batch: 2/2
Test loss - extrapolation:15.242320080094316
Epoch 550 mean train loss:0.8888747120877667
Epoch 550 mean test loss - interpolation:0.9473902197623323
Epoch 550 mean test loss - extrapolation:4.553129812739186
Start training epoch 551
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.02362
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.650145088407494
Iteration: 2 || Loss: 2.6484962844395135
Iteration: 3 || Loss: 2.6468484180485663
Iteration: 4 || Loss: 2.6452010313536634
Iteration: 5 || Loss: 2.6435576429042267
Iteration: 6 || Loss: 2.6435576429042267
saving ADAM checkpoint...
Sum of params:86.023636
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6435576429042267
Iteration: 2 || Loss: 1.9959545016639728
Iteration: 3 || Loss: 1.5983311031882168
Iteration: 4 || Loss: 1.5951126730698044
Iteration: 5 || Loss: 1.5758386734445065
Iteration: 6 || Loss: 1.574219248932247
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.08261
Epoch 551 loss:1.574219248932247
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.08261
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.078812769943513
Iteration: 2 || Loss: 6.078342865137349
Iteration: 3 || Loss: 6.077875311323867
Iteration: 4 || Loss: 6.077409422489306
Iteration: 5 || Loss: 6.0769459074918455
Iteration: 6 || Loss: 6.0769459074918455
saving ADAM checkpoint...
Sum of params:86.082726
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.0769459074918455
Iteration: 2 || Loss: 6.033252110268094
Iteration: 3 || Loss: 5.9811629500185255
Iteration: 4 || Loss: 5.880359498686469
Iteration: 5 || Loss: 5.870589524602484
Iteration: 6 || Loss: 5.864547584691101
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.14222
Epoch 551 loss:5.864547584691101
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.14222
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.758939645602172
Iteration: 2 || Loss: 18.758419230394296
Iteration: 3 || Loss: 18.75790122003652
Iteration: 4 || Loss: 18.757384058142392
Iteration: 5 || Loss: 18.756869550276043
Iteration: 6 || Loss: 18.756869550276043
saving ADAM checkpoint...
Sum of params:86.14232
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.756869550276043
Iteration: 2 || Loss: 18.696485192164634
Iteration: 3 || Loss: 18.47375017566624
Iteration: 4 || Loss: 18.372137236816485
Iteration: 5 || Loss: 18.345344001148728
Iteration: 6 || Loss: 18.328728156815217
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.03074
Epoch 551 loss:18.328728156815217
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.6828226653803675
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.382962026608006
waveform batch: 2/2
Test loss - extrapolation:15.234753022728166
Epoch 551 mean train loss:0.888534310015123
Epoch 551 mean test loss - interpolation:0.9471371108967279
Epoch 551 mean test loss - extrapolation:4.551476254111347
Start training epoch 552
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.03074
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6492905092602914
Iteration: 2 || Loss: 2.6476408333636328
Iteration: 3 || Loss: 2.6459905695921706
Iteration: 4 || Loss: 2.644343900807331
Iteration: 5 || Loss: 2.642698920083137
Iteration: 6 || Loss: 2.642698920083137
saving ADAM checkpoint...
Sum of params:86.030754
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.642698920083137
Iteration: 2 || Loss: 1.9945719647654911
Iteration: 3 || Loss: 1.5975847742876232
Iteration: 4 || Loss: 1.5943632727610733
Iteration: 5 || Loss: 1.5751445014395278
Iteration: 6 || Loss: 1.5734562076157799
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.08955
Epoch 552 loss:1.5734562076157799
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.08955
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.077065123844278
Iteration: 2 || Loss: 6.0765995875067516
Iteration: 3 || Loss: 6.076133124481005
Iteration: 4 || Loss: 6.075669334792803
Iteration: 5 || Loss: 6.075207219110566
Iteration: 6 || Loss: 6.075207219110566
saving ADAM checkpoint...
Sum of params:86.08967
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.075207219110566
Iteration: 2 || Loss: 6.032014962733373
Iteration: 3 || Loss: 5.9800348193961215
Iteration: 4 || Loss: 5.878828687959204
Iteration: 5 || Loss: 5.868954475574902
Iteration: 6 || Loss: 5.862943506792342
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.149666
Epoch 552 loss:5.862943506792342
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.149666
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.751862041530917
Iteration: 2 || Loss: 18.751343532813813
Iteration: 3 || Loss: 18.750826016629247
Iteration: 4 || Loss: 18.750310939837703
Iteration: 5 || Loss: 18.749797323045673
Iteration: 6 || Loss: 18.749797323045673
saving ADAM checkpoint...
Sum of params:86.14979
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.749797323045673
Iteration: 2 || Loss: 18.68975310903318
Iteration: 3 || Loss: 18.46671769611039
Iteration: 4 || Loss: 18.36473164060291
Iteration: 5 || Loss: 18.33789640587145
Iteration: 6 || Loss: 18.321050381749664
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.037704
Epoch 552 loss:18.321050381749664
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.681542775677185
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.37215249590272
waveform batch: 2/2
Test loss - extrapolation:15.227253038218004
Epoch 552 mean train loss:0.8881879343502684
Epoch 552 mean test loss - interpolation:0.9469237959461975
Epoch 552 mean test loss - extrapolation:4.549950461176727
Start training epoch 553
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.037704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6506895300687523
Iteration: 2 || Loss: 2.649036134842404
Iteration: 3 || Loss: 2.647379337205257
Iteration: 4 || Loss: 2.645725646526178
Iteration: 5 || Loss: 2.6440766389575217
Iteration: 6 || Loss: 2.6440766389575217
saving ADAM checkpoint...
Sum of params:86.03768
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6440766389575217
Iteration: 2 || Loss: 1.9926188992250675
Iteration: 3 || Loss: 1.5969048036804634
Iteration: 4 || Loss: 1.5937087749342087
Iteration: 5 || Loss: 1.5744795675606174
Iteration: 6 || Loss: 1.5724521775927767
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.09658
Epoch 553 loss:1.5724521775927767
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.09658
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.07634454718354
Iteration: 2 || Loss: 6.075875961735755
Iteration: 3 || Loss: 6.0754110194786115
Iteration: 4 || Loss: 6.074946719600052
Iteration: 5 || Loss: 6.074486353756591
Iteration: 6 || Loss: 6.074486353756591
saving ADAM checkpoint...
Sum of params:86.09671
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.074486353756591
Iteration: 2 || Loss: 6.0313746126083485
Iteration: 3 || Loss: 5.979493995890624
Iteration: 4 || Loss: 5.877169017430229
Iteration: 5 || Loss: 5.867220501955844
Iteration: 6 || Loss: 5.861257113135293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.157135
Epoch 553 loss:5.861257113135293
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.157135
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.744993288905686
Iteration: 2 || Loss: 18.744476475027994
Iteration: 3 || Loss: 18.743959244686923
Iteration: 4 || Loss: 18.743444504944527
Iteration: 5 || Loss: 18.74293015699406
Iteration: 6 || Loss: 18.74293015699406
saving ADAM checkpoint...
Sum of params:86.15725
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.74293015699406
Iteration: 2 || Loss: 18.68306127743291
Iteration: 3 || Loss: 18.460348740834853
Iteration: 4 || Loss: 18.357682860582447
Iteration: 5 || Loss: 18.330782557619727
Iteration: 6 || Loss: 18.313706983664154
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.044495
Epoch 553 loss:18.313706983664154
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.680244183652795
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.360336950276235
waveform batch: 2/2
Test loss - extrapolation:15.220100163513719
Epoch 553 mean train loss:0.8878419404962835
Epoch 553 mean test loss - interpolation:0.9467073639421325
Epoch 553 mean test loss - extrapolation:4.5483697594824966
Start training epoch 554
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.044495
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6498574515893827
Iteration: 2 || Loss: 2.648198502920686
Iteration: 3 || Loss: 2.646539914813742
Iteration: 4 || Loss: 2.6448847691981787
Iteration: 5 || Loss: 2.6432317303124404
Iteration: 6 || Loss: 2.6432317303124404
saving ADAM checkpoint...
Sum of params:86.04449
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6432317303124404
Iteration: 2 || Loss: 1.9900292230781282
Iteration: 3 || Loss: 1.5958479004374655
Iteration: 4 || Loss: 1.5926528493052592
Iteration: 5 || Loss: 1.5735476370075117
Iteration: 6 || Loss: 1.5711653125185925
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.10341
Epoch 554 loss:1.5711653125185925
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.10341
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.075125389099521
Iteration: 2 || Loss: 6.074659164660543
Iteration: 3 || Loss: 6.074198323390858
Iteration: 4 || Loss: 6.073739954503171
Iteration: 5 || Loss: 6.07328226053754
Iteration: 6 || Loss: 6.07328226053754
saving ADAM checkpoint...
Sum of params:86.10352
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.07328226053754
Iteration: 2 || Loss: 6.03113261010193
Iteration: 3 || Loss: 5.979161294861936
Iteration: 4 || Loss: 5.875427107973443
Iteration: 5 || Loss: 5.865282184953126
Iteration: 6 || Loss: 5.859369288971794
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.16476
Epoch 554 loss:5.859369288971794
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.16476
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.73931568865999
Iteration: 2 || Loss: 18.738799878128905
Iteration: 3 || Loss: 18.73828551019316
Iteration: 4 || Loss: 18.7377738668167
Iteration: 5 || Loss: 18.737262877855834
Iteration: 6 || Loss: 18.737262877855834
saving ADAM checkpoint...
Sum of params:86.16487
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.737262877855834
Iteration: 2 || Loss: 18.678118805020265
Iteration: 3 || Loss: 18.455171509722327
Iteration: 4 || Loss: 18.351200550408226
Iteration: 5 || Loss: 18.324154861822496
Iteration: 6 || Loss: 18.30643493385284
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.05078
Epoch 554 loss:18.30643493385284
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.679464994841444
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.35128724073643
waveform batch: 2/2
Test loss - extrapolation:15.213202111562925
Epoch 554 mean train loss:0.8874817081152837
Epoch 554 mean test loss - interpolation:0.9465774991402407
Epoch 554 mean test loss - extrapolation:4.54704077935828
Start training epoch 555
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.05078
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6530776754672023
Iteration: 2 || Loss: 2.651411807254452
Iteration: 3 || Loss: 2.649742914322337
Iteration: 4 || Loss: 2.648077996273161
Iteration: 5 || Loss: 2.646414372936298
Iteration: 6 || Loss: 2.646414372936298
saving ADAM checkpoint...
Sum of params:86.050766
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.646414372936298
Iteration: 2 || Loss: 1.9860179440551244
Iteration: 3 || Loss: 1.594751537464271
Iteration: 4 || Loss: 1.5916315012236328
Iteration: 5 || Loss: 1.5725268941966557
Iteration: 6 || Loss: 1.568435925807612
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.11169
Epoch 555 loss:1.568435925807612
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.11169
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.076525075270799
Iteration: 2 || Loss: 6.076079138152453
Iteration: 3 || Loss: 6.075636133077811
Iteration: 4 || Loss: 6.075195492135171
Iteration: 5 || Loss: 6.074755748209754
Iteration: 6 || Loss: 6.074755748209754
saving ADAM checkpoint...
Sum of params:86.111786
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.074755748209754
Iteration: 2 || Loss: 6.036964413941855
Iteration: 3 || Loss: 5.983125532721402
Iteration: 4 || Loss: 5.873156768502316
Iteration: 5 || Loss: 5.862209705409105
Iteration: 6 || Loss: 5.8560558055774
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.17476
Epoch 555 loss:5.8560558055774
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.17476
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.74323377217865
Iteration: 2 || Loss: 18.74273168097182
Iteration: 3 || Loss: 18.742231556306468
Iteration: 4 || Loss: 18.741731560214152
Iteration: 5 || Loss: 18.741236602480978
Iteration: 6 || Loss: 18.741236602480978
saving ADAM checkpoint...
Sum of params:86.17484
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.741236602480978
Iteration: 2 || Loss: 18.68561989830149
Iteration: 3 || Loss: 18.458843761739576
Iteration: 4 || Loss: 18.349040792656062
Iteration: 5 || Loss: 18.321190039025993
Iteration: 6 || Loss: 18.29943137854913
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.054
Epoch 555 loss:18.29943137854913
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.683481775638463
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.3640836596724
waveform batch: 2/2
Test loss - extrapolation:15.2081144862684
Epoch 555 mean train loss:0.8870318313770393
Epoch 555 mean test loss - interpolation:0.9472469626064105
Epoch 555 mean test loss - extrapolation:4.5476831788284
Start training epoch 556
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.054
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.68825127585874
Iteration: 2 || Loss: 2.686524485632389
Iteration: 3 || Loss: 2.6847998126815176
Iteration: 4 || Loss: 2.68307812950351
Iteration: 5 || Loss: 2.681359255601744
Iteration: 6 || Loss: 2.681359255601744
saving ADAM checkpoint...
Sum of params:86.054
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.681359255601744
Iteration: 2 || Loss: 1.9739245931455485
Iteration: 3 || Loss: 1.593675356558508
Iteration: 4 || Loss: 1.5910728230759612
Iteration: 5 || Loss: 1.571193770599774
Iteration: 6 || Loss: 1.5696124299291612
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.12052
Epoch 556 loss:1.5696124299291612
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.12052
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.071080944912667
Iteration: 2 || Loss: 6.070666114104341
Iteration: 3 || Loss: 6.070251759810392
Iteration: 4 || Loss: 6.0698398631893
Iteration: 5 || Loss: 6.069430984546777
Iteration: 6 || Loss: 6.069430984546777
saving ADAM checkpoint...
Sum of params:86.12064
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.069430984546777
Iteration: 2 || Loss: 6.037272363828281
Iteration: 3 || Loss: 5.970544355089444
Iteration: 4 || Loss: 5.873249678881008
Iteration: 5 || Loss: 5.86205580065152
Iteration: 6 || Loss: 5.853466666766523
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.185745
Epoch 556 loss:5.853466666766523
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.185745
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.75862359253509
Iteration: 2 || Loss: 18.75811826263531
Iteration: 3 || Loss: 18.7576131290902
Iteration: 4 || Loss: 18.757110933067565
Iteration: 5 || Loss: 18.75661105625307
Iteration: 6 || Loss: 18.75661105625307
saving ADAM checkpoint...
Sum of params:86.185844
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.75661105625307
Iteration: 2 || Loss: 18.700227442291418
Iteration: 3 || Loss: 18.454119449090822
Iteration: 4 || Loss: 18.340255622829563
Iteration: 5 || Loss: 18.311910749432926
Iteration: 6 || Loss: 18.29074992797502
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.06479
Epoch 556 loss:18.29074992797502
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.682158092445429
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.35385601901916
waveform batch: 2/2
Test loss - extrapolation:15.198707782967285
Epoch 556 mean train loss:0.8866837594714037
Epoch 556 mean test loss - interpolation:0.9470263487409049
Epoch 556 mean test loss - extrapolation:4.54604698349887
Start training epoch 557
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.06479
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.6931936533627603
Iteration: 2 || Loss: 2.6914691967727618
Iteration: 3 || Loss: 2.6897469036489956
Iteration: 4 || Loss: 2.6880277961998074
Iteration: 5 || Loss: 2.6863095431419266
Iteration: 6 || Loss: 2.6863095431419266
saving ADAM checkpoint...
Sum of params:86.06478
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6863095431419266
Iteration: 2 || Loss: 1.9824232332195328
Iteration: 3 || Loss: 1.5951216794362773
Iteration: 4 || Loss: 1.5923192570727451
Iteration: 5 || Loss: 1.5711337007673427
Iteration: 6 || Loss: 1.568870976984641
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.13442
Epoch 557 loss:1.568870976984641
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.13442
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.068810505683557
Iteration: 2 || Loss: 6.068414130834087
Iteration: 3 || Loss: 6.068019362275132
Iteration: 4 || Loss: 6.067626275763139
Iteration: 5 || Loss: 6.0672349646162615
Iteration: 6 || Loss: 6.0672349646162615
saving ADAM checkpoint...
Sum of params:86.13453
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.0672349646162615
Iteration: 2 || Loss: 6.038606231499001
Iteration: 3 || Loss: 5.968319091501196
Iteration: 4 || Loss: 5.868583717071753
Iteration: 5 || Loss: 5.8575141675937274
Iteration: 6 || Loss: 5.847946168560687
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.20024
Epoch 557 loss:5.847946168560687
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.20024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.768055403532813
Iteration: 2 || Loss: 18.767539779055483
Iteration: 3 || Loss: 18.76702539879432
Iteration: 4 || Loss: 18.76651420802556
Iteration: 5 || Loss: 18.766005047161343
Iteration: 6 || Loss: 18.766005047161343
saving ADAM checkpoint...
Sum of params:86.20037
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.766005047161343
Iteration: 2 || Loss: 18.70711835433408
Iteration: 3 || Loss: 18.45254358982633
Iteration: 4 || Loss: 18.337167824013044
Iteration: 5 || Loss: 18.30774788776025
Iteration: 6 || Loss: 18.286875230175703
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.07633
Epoch 557 loss:18.286875230175703
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.678541350956325
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.33158413631032
waveform batch: 2/2
Test loss - extrapolation:15.192166249660307
Epoch 557 mean train loss:0.8863342198524493
Epoch 557 mean test loss - interpolation:0.9464235584927209
Epoch 557 mean test loss - extrapolation:4.5436458654975524
Start training epoch 558
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.07633
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.673621423376785
Iteration: 2 || Loss: 2.671932518787574
Iteration: 3 || Loss: 2.670243551018513
Iteration: 4 || Loss: 2.6685560187899346
Iteration: 5 || Loss: 2.6668709123171124
Iteration: 6 || Loss: 2.6668709123171124
saving ADAM checkpoint...
Sum of params:86.07634
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.6668709123171124
Iteration: 2 || Loss: 1.991280143051797
Iteration: 3 || Loss: 1.5941056843233554
Iteration: 4 || Loss: 1.590782223537654
Iteration: 5 || Loss: 1.5693987083286196
Iteration: 6 || Loss: 1.5611466335533184
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.151825
Epoch 558 loss:1.5611466335533184
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.151825
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.072376647782613
Iteration: 2 || Loss: 6.072029455316606
Iteration: 3 || Loss: 6.071684908603505
Iteration: 4 || Loss: 6.071341182224094
Iteration: 5 || Loss: 6.0710011037244636
Iteration: 6 || Loss: 6.0710011037244636
saving ADAM checkpoint...
Sum of params:86.151855
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.0710011037244636
Iteration: 2 || Loss: 6.051235811085527
Iteration: 3 || Loss: 5.986222750919443
Iteration: 4 || Loss: 5.862597340542975
Iteration: 5 || Loss: 5.849883343184728
Iteration: 6 || Loss: 5.836919403139584
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.20498
Epoch 558 loss:5.836919403139584
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.20498
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.793347938627313
Iteration: 2 || Loss: 18.792763671183604
Iteration: 3 || Loss: 18.792180910395
Iteration: 4 || Loss: 18.79159854159803
Iteration: 5 || Loss: 18.791019644878165
Iteration: 6 || Loss: 18.791019644878165
saving ADAM checkpoint...
Sum of params:86.20508
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.791019644878165
Iteration: 2 || Loss: 18.71320382849663
Iteration: 3 || Loss: 18.466617070671646
Iteration: 4 || Loss: 18.350397516996317
Iteration: 5 || Loss: 18.320561580918394
Iteration: 6 || Loss: 18.30317645741538
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.087036
Epoch 558 loss:18.30317645741538
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.669194545286004
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.28464037182885
waveform batch: 2/2
Test loss - extrapolation:15.199560184703076
Epoch 558 mean train loss:0.8862497411761476
Epoch 558 mean test loss - interpolation:0.9448657575476673
Epoch 558 mean test loss - extrapolation:4.54035004637766
Start training epoch 559
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.087036
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.5801432832203477
Iteration: 2 || Loss: 2.578582758029661
Iteration: 3 || Loss: 2.57702135914911
Iteration: 4 || Loss: 2.5754623031473676
Iteration: 5 || Loss: 2.5739039780430732
Iteration: 6 || Loss: 2.5739039780430732
saving ADAM checkpoint...
Sum of params:86.08705
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.5739039780430732
Iteration: 2 || Loss: 2.0036854205196377
Iteration: 3 || Loss: 1.58680563605548
Iteration: 4 || Loss: 1.581984998186639
Iteration: 5 || Loss: 1.56022529585469
Iteration: 6 || Loss: 1.5596318945635048
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.14899
Epoch 559 loss:1.5596318945635048
waveform batch: 2/3
Using ADAM optimizer
Sum of params:86.14899
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.0388477050330955
Iteration: 2 || Loss: 6.038405956917799
Iteration: 3 || Loss: 6.037967174910397
Iteration: 4 || Loss: 6.037529770731595
Iteration: 5 || Loss: 6.037093434577543
Iteration: 6 || Loss: 6.037093434577543
saving ADAM checkpoint...
Sum of params:86.1491
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.037093434577543
Iteration: 2 || Loss: 5.9996342122507675
Iteration: 3 || Loss: 5.944731250401487
Iteration: 4 || Loss: 5.847884693451456
Iteration: 5 || Loss: 5.8390087595016436
Iteration: 6 || Loss: 5.834378598549846
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.21017
Epoch 559 loss:5.834378598549846
waveform batch: 3/3
Using ADAM optimizer
Sum of params:86.21017
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.72309877574005
Iteration: 2 || Loss: 18.72259832175789
Iteration: 3 || Loss: 18.722099761820875
Iteration: 4 || Loss: 18.72160382263845
Iteration: 5 || Loss: 18.72110875273319
Iteration: 6 || Loss: 18.72110875273319
saving ADAM checkpoint...
Sum of params:86.21028
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.72110875273319
Iteration: 2 || Loss: 18.666444433092714
Iteration: 3 || Loss: 18.438269282397936
Iteration: 4 || Loss: 18.33300639855352
Iteration: 5 || Loss: 18.303798955408386
Iteration: 6 || Loss: 18.279601192750853
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:86.091644
Epoch 559 loss:18.279601192750853
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:5.676896974460615
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:39.33408263919665
waveform batch: 2/2
Test loss - extrapolation:15.174694900320215
Epoch 559 mean train loss:0.8852969546849725
Epoch 559 mean test loss - interpolation:0.9461494957434358
Epoch 559 mean test loss - extrapolation:4.5423981282930725
Start training epoch 560
waveform batch: 1/3
Using ADAM optimizer
Sum of params:86.091644
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2.720952751572692
Iteration: 2 || Loss: 2.719182957961279
Iteration: 3 || Loss: 2.717415643625622
Iteration: 4 || Loss: 2.7156495603650113
Iteration: 5 || Loss: 2.7138862696425554
Iteration: 6 || Loss: 2.7138862696425554
saving ADAM checkpoint...
Sum of params:86.09165
Switching to BFGS optimizer
Max iters:5
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2.7138862696425554
Iteration: 2 || Loss: 1.9733839570027878
Iteration: 3 || Loss: 1.5912897773555776
Iteration: 4 || Loss: 1.5886567705480594
