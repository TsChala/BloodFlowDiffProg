Size of 1D data matrix:(300, 110, 47, 4)
Shape: [timesteps,spatial locations,waveforms,variables]
Number of parameters in neural network: 2330
optimizer 1 is ADAM
optimizer 2 is BFGS optimizer
ODE Time integrator selected:RK4
Fresh training initialized
Batch size:10
Start training epoch 1
waveform batch: 1/3
Using ADAM optimizer
Sum of params:2.5065749
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8494.890814949482
Iteration: 2 || Loss: 8334.63936444706
Iteration: 3 || Loss: 8213.573931855412
Iteration: 4 || Loss: 8099.910086848615
Iteration: 5 || Loss: 7989.341674064098
Iteration: 6 || Loss: 7989.341674064098
saving ADAM checkpoint...
Sum of params:23.83139
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7989.341674064098
Iteration: 2 || Loss: 7516.782713516139
Iteration: 3 || Loss: 7391.779062555702
Iteration: 4 || Loss: 7364.638501251994
Iteration: 5 || Loss: 7321.387643860434
Iteration: 6 || Loss: 6395.649177344886
Iteration: 7 || Loss: 5390.149086554076
Iteration: 8 || Loss: 4626.263618361533
Iteration: 9 || Loss: 4591.89103364355
Iteration: 10 || Loss: 4158.478210242623
Iteration: 11 || Loss: 3402.772118065654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:131.7743
Epoch 1 loss:3402.772118065654
waveform batch: 2/3
Using ADAM optimizer
Sum of params:131.7743
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1520.8640905102966
Iteration: 2 || Loss: NaN
Iteration: 3 || Loss: NaN
Iteration: 4 || Loss: NaN
Iteration: 5 || Loss: NaN
Iteration: 6 || Loss: 1520.8640905102966
saving ADAM checkpoint...
Sum of params:131.7743
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1520.8640905102966
Iteration: 2 || Loss: 1388.7973010274595
Iteration: 3 || Loss: 1375.3744550802899
Iteration: 4 || Loss: 1352.7877972910067
Iteration: 5 || Loss: 1214.3076211698499
Iteration: 6 || Loss: 1192.4088365086138
Iteration: 7 || Loss: 1157.3708948564683
Iteration: 8 || Loss: 1109.7836102264569
Iteration: 9 || Loss: 1087.4403362429978
Iteration: 10 || Loss: 970.2243361822192
Iteration: 11 || Loss: 813.3817522313788
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:121.3857
Epoch 1 loss:813.3817522313788
waveform batch: 3/3
Using ADAM optimizer
Sum of params:121.3857
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 547.3526766676222
Iteration: 2 || Loss: 618.5523817622291
Iteration: 3 || Loss: 457.97549437958645
Iteration: 4 || Loss: 466.53542279382594
Iteration: 5 || Loss: 467.0718739181955
Iteration: 6 || Loss: 457.97549437958645
saving ADAM checkpoint...
Sum of params:130.28287
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 457.97549437958645
Iteration: 2 || Loss: 455.380935061188
Iteration: 3 || Loss: 358.57202405726423
Iteration: 4 || Loss: 255.60987923322674
Iteration: 5 || Loss: 236.64617670856987
Iteration: 6 || Loss: 232.61539452804567
Iteration: 7 || Loss: 203.40098741394524
Iteration: 8 || Loss: 200.70779270424669
Iteration: 9 || Loss: 178.77287137647144
Iteration: 10 || Loss: 149.14373921372226
Iteration: 11 || Loss: 137.8919262169138
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:143.94632
Epoch 1 loss:137.8919262169138
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:172.11969094431868
waveform batch: 2/2
Test loss:311.6579442346623
Epoch 1 mean train loss:145.1348598837982
Epoch 1 mean test loss:28.457507951704763
Start training epoch 2
waveform batch: 1/3
Using ADAM optimizer
Sum of params:143.94632
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 872.5764747110126
Iteration: 2 || Loss: 1041.3939641560194
Iteration: 3 || Loss: 753.3562027770304
Iteration: 4 || Loss: 733.0008072851805
Iteration: 5 || Loss: 784.019967946905
Iteration: 6 || Loss: 733.0008072851805
saving ADAM checkpoint...
Sum of params:137.13402
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 733.0008072851805
Iteration: 2 || Loss: 691.5073098430108
Iteration: 3 || Loss: 682.9925119541588
Iteration: 4 || Loss: 650.0138837938891
Iteration: 5 || Loss: 639.3511830236022
Iteration: 6 || Loss: 625.9786262896321
Iteration: 7 || Loss: 499.7382836820053
Iteration: 8 || Loss: 465.2305350538047
Iteration: 9 || Loss: 451.81730690940407
Iteration: 10 || Loss: 443.6598594311305
Iteration: 11 || Loss: 442.11645692652615
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:154.9544
Epoch 2 loss:442.11645692652615
waveform batch: 2/3
Using ADAM optimizer
Sum of params:154.9544
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 169.5374096981483
Iteration: 2 || Loss: 406.64352662540733
Iteration: 3 || Loss: 164.39143024748967
Iteration: 4 || Loss: 230.68111450190486
Iteration: 5 || Loss: 280.54222735241297
Iteration: 6 || Loss: 164.39143024748967
saving ADAM checkpoint...
Sum of params:152.39182
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 164.39143024748967
Iteration: 2 || Loss: 153.02945486071715
Iteration: 3 || Loss: 150.45088553815057
Iteration: 4 || Loss: 137.14722752586846
Iteration: 5 || Loss: 127.93934450332179
Iteration: 6 || Loss: 125.84295876131254
Iteration: 7 || Loss: 125.30186247166179
Iteration: 8 || Loss: 114.22357728100665
Iteration: 9 || Loss: 104.97309321659128
Iteration: 10 || Loss: 103.50780334794283
Iteration: 11 || Loss: 103.3652410519915
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:159.58861
Epoch 2 loss:103.3652410519915
waveform batch: 3/3
Using ADAM optimizer
Sum of params:159.58861
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 80.3635578788412
Iteration: 2 || Loss: 457.9206798291726
Iteration: 3 || Loss: 98.73945349547395
Iteration: 4 || Loss: 184.02069781061337
Iteration: 5 || Loss: 294.77819053690894
Iteration: 6 || Loss: 80.3635578788412
saving ADAM checkpoint...
Sum of params:159.58861
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 80.3635578788412
Iteration: 2 || Loss: 80.3175825045244
Iteration: 3 || Loss: 62.823565430014035
Iteration: 4 || Loss: 61.638230520143146
Iteration: 5 || Loss: 59.944382587689724
Iteration: 6 || Loss: 59.24921225419805
Iteration: 7 || Loss: 58.596660873122914
Iteration: 8 || Loss: 53.238134427695115
Iteration: 9 || Loss: 50.674943354015916
Iteration: 10 || Loss: 50.36649450097042
Iteration: 11 || Loss: 49.98219059413849
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:156.37854
Epoch 2 loss:49.98219059413849
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:56.73685250370473
waveform batch: 2/2
Test loss:106.21174837570412
Epoch 2 mean train loss:19.848796285755206
Epoch 2 mean test loss:9.585211816435814
Start training epoch 3
waveform batch: 1/3
Using ADAM optimizer
Sum of params:156.37854
Changing learning rate to:0.005
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 339.6379237987953
Iteration: 2 || Loss: 426.5545872705164
Iteration: 3 || Loss: 315.1132217628085
Iteration: 4 || Loss: 347.259224410126
Iteration: 5 || Loss: 363.65364302185617
Iteration: 6 || Loss: 315.1132217628085
saving ADAM checkpoint...
Sum of params:153.26495
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 315.1132217628085
Iteration: 2 || Loss: 313.51318723342683
Iteration: 3 || Loss: 304.7627920119507
Iteration: 4 || Loss: 297.04623286318804
Iteration: 5 || Loss: 291.9860245507621
Iteration: 6 || Loss: 283.70239420623204
Iteration: 7 || Loss: 260.8096848553703
Iteration: 8 || Loss: 255.7984182464388
Iteration: 9 || Loss: 253.14472744577844
Iteration: 10 || Loss: 248.39695556251337
Iteration: 11 || Loss: 245.07953419106752
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:151.6399
Epoch 3 loss:245.07953419106752
waveform batch: 2/3
Using ADAM optimizer
Sum of params:151.6399
Changing learning rate to:0.0025
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 75.73259057119763
Iteration: 2 || Loss: 112.08318209354243
Iteration: 3 || Loss: 73.3928906076753
Iteration: 4 || Loss: 88.1077124692782
Iteration: 5 || Loss: 95.33843243392901
Iteration: 6 || Loss: 73.3928906076753
saving ADAM checkpoint...
Sum of params:151.53473
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.3928906076753
Iteration: 2 || Loss: 72.77850890663517
Iteration: 3 || Loss: 70.82663488140793
Iteration: 4 || Loss: 67.11133286086255
Iteration: 5 || Loss: 66.4130630658967
Iteration: 6 || Loss: 65.9500986763679
Iteration: 7 || Loss: 65.05246374709007
Iteration: 8 || Loss: 64.39384204383269
Iteration: 9 || Loss: 64.15686995725959
Iteration: 10 || Loss: 63.8878469703126
Iteration: 11 || Loss: 63.11896133928855
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:150.67969
Epoch 3 loss:63.11896133928855
waveform batch: 3/3
Using ADAM optimizer
Sum of params:150.67969
Changing learning rate to:0.00125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 54.4051125587677
Iteration: 2 || Loss: 66.13574112964615
Iteration: 3 || Loss: 53.4730400338199
Iteration: 4 || Loss: 54.58213412738694
Iteration: 5 || Loss: 58.22695276661905
Iteration: 6 || Loss: 53.4730400338199
saving ADAM checkpoint...
Sum of params:150.73102
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 53.4730400338199
Iteration: 2 || Loss: 52.5503147319496
Iteration: 3 || Loss: 50.45105070819987
Iteration: 4 || Loss: 49.980947509325354
Iteration: 5 || Loss: 49.78746296581192
Iteration: 6 || Loss: 45.90867484495299
Iteration: 7 || Loss: 45.61383231095779
Iteration: 8 || Loss: 45.209198664743035
Iteration: 9 || Loss: 43.00103373428085
Iteration: 10 || Loss: 42.21450580369373
Iteration: 11 || Loss: 41.982986305672505
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:151.80966
Epoch 3 loss:41.982986305672505
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:46.38971391851675
waveform batch: 2/2
Test loss:63.975417490130475
Epoch 3 mean train loss:11.672716061200953
Epoch 3 mean test loss:6.4920665534498365
Start training epoch 4
waveform batch: 1/3
Using ADAM optimizer
Sum of params:151.80966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 221.0316867563688
Iteration: 2 || Loss: 231.46149583551107
Iteration: 3 || Loss: 218.67800065399308
Iteration: 4 || Loss: 219.94270610964873
Iteration: 5 || Loss: 222.43636743355123
Iteration: 6 || Loss: 218.67800065399308
saving ADAM checkpoint...
Sum of params:151.9314
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 218.67800065399308
Iteration: 2 || Loss: 217.9301048488363
Iteration: 3 || Loss: 216.3551102700661
Iteration: 4 || Loss: 206.6968062037161
Iteration: 5 || Loss: 204.08023495548534
Iteration: 6 || Loss: 201.73473307939335
Iteration: 7 || Loss: 195.76709086706663
Iteration: 8 || Loss: 190.92216571371958
Iteration: 9 || Loss: 188.60867276176054
Iteration: 10 || Loss: 187.58306565189207
Iteration: 11 || Loss: 186.63161792679014
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:153.25766
Epoch 4 loss:186.63161792679014
waveform batch: 2/3
Using ADAM optimizer
Sum of params:153.25766
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 63.59074554714286
Iteration: 2 || Loss: 69.89065501280396
Iteration: 3 || Loss: 62.6313824234644
Iteration: 4 || Loss: 59.16220457808129
Iteration: 5 || Loss: 62.64978335061974
Iteration: 6 || Loss: 59.16220457808129
saving ADAM checkpoint...
Sum of params:152.8435
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 59.16220457808129
Iteration: 2 || Loss: 58.80125930169013
Iteration: 3 || Loss: 58.669795379738446
Iteration: 4 || Loss: 53.21914436739999
Iteration: 5 || Loss: 50.69806097755302
Iteration: 6 || Loss: 49.73546426693941
Iteration: 7 || Loss: 49.27579301957844
Iteration: 8 || Loss: 49.12869049361145
Iteration: 9 || Loss: 48.4714439484551
Iteration: 10 || Loss: 46.14045187847453
Iteration: 11 || Loss: 46.070573713201604
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:154.54832
Epoch 4 loss:46.070573713201604
waveform batch: 3/3
Using ADAM optimizer
Sum of params:154.54832
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.232301525256055
Iteration: 2 || Loss: 58.09423189402545
Iteration: 3 || Loss: 37.06988155912248
Iteration: 4 || Loss: 42.08192947089393
Iteration: 5 || Loss: 48.74060574592956
Iteration: 6 || Loss: 36.232301525256055
saving ADAM checkpoint...
Sum of params:154.54832
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.232301525256055
Iteration: 2 || Loss: 36.222746975548084
Iteration: 3 || Loss: 35.9327096980009
Iteration: 4 || Loss: 35.23970238870009
Iteration: 5 || Loss: 34.48326779306974
Iteration: 6 || Loss: 33.1972178205198
Iteration: 7 || Loss: 32.81178925871467
Iteration: 8 || Loss: 32.40723655555146
Iteration: 9 || Loss: 31.08890151885413
Iteration: 10 || Loss: 30.55853242520543
Iteration: 11 || Loss: 30.128959314031825
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:154.40863
Epoch 4 loss:30.128959314031825
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:34.761011918411505
waveform batch: 2/2
Test loss:50.4042488507731
Epoch 4 mean train loss:8.76103836513412
Epoch 4 mean test loss:5.009721221716742
Start training epoch 5
waveform batch: 1/3
Using ADAM optimizer
Sum of params:154.40863
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 184.09391826294308
Iteration: 2 || Loss: 183.29952092838369
Iteration: 3 || Loss: 181.8470472060838
Iteration: 4 || Loss: 180.4441229896156
Iteration: 5 || Loss: 177.61963974177877
Iteration: 6 || Loss: 177.61963974177877
saving ADAM checkpoint...
Sum of params:153.5586
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 177.61963974177877
Iteration: 2 || Loss: 177.4670137191589
Iteration: 3 || Loss: 176.76606153310354
Iteration: 4 || Loss: 170.64429741017034
Iteration: 5 || Loss: 168.21624261869664
Iteration: 6 || Loss: 163.21673395865915
Iteration: 7 || Loss: 162.34823467801493
Iteration: 8 || Loss: 158.80711763155276
Iteration: 9 || Loss: 156.82007540329366
Iteration: 10 || Loss: 153.393603483831
Iteration: 11 || Loss: 150.74367231175628
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:153.90897
Epoch 5 loss:150.74367231175628
waveform batch: 2/3
Using ADAM optimizer
Sum of params:153.90897
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 44.86516895128767
Iteration: 2 || Loss: 72.05501018737499
Iteration: 3 || Loss: 44.80508000192365
Iteration: 4 || Loss: 48.32626454483835
Iteration: 5 || Loss: 57.503237503633606
Iteration: 6 || Loss: 44.80508000192365
saving ADAM checkpoint...
Sum of params:154.03058
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 44.80508000192365
Iteration: 2 || Loss: 43.64124792645555
Iteration: 3 || Loss: 42.721696626800764
Iteration: 4 || Loss: 42.31856940879385
Iteration: 5 || Loss: 41.551425072680004
Iteration: 6 || Loss: 40.50875403443431
Iteration: 7 || Loss: 40.37138892373571
Iteration: 8 || Loss: 40.18432804443609
Iteration: 9 || Loss: 39.74141849313842
Iteration: 10 || Loss: 38.28264731875708
Iteration: 11 || Loss: 37.30263429243069
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:155.50545
Epoch 5 loss:37.30263429243069
waveform batch: 3/3
Using ADAM optimizer
Sum of params:155.50545
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.126157857678773
Iteration: 2 || Loss: 54.71679546709802
Iteration: 3 || Loss: 31.986039001461794
Iteration: 4 || Loss: 35.7331925997284
Iteration: 5 || Loss: 43.65215796819169
Iteration: 6 || Loss: 31.126157857678773
saving ADAM checkpoint...
Sum of params:155.50545
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.126157857678773
Iteration: 2 || Loss: 30.97103881995979
Iteration: 3 || Loss: 30.889665235909813
Iteration: 4 || Loss: 29.956261445267806
Iteration: 5 || Loss: 29.808133035107478
Iteration: 6 || Loss: 29.485837884895076
Iteration: 7 || Loss: 28.949437382032603
Iteration: 8 || Loss: 28.780207966577965
Iteration: 9 || Loss: 28.643205265683076
Iteration: 10 || Loss: 28.491488718046835
Iteration: 11 || Loss: 28.167188549894718
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:156.46426
Epoch 5 loss:28.167188549894718
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:31.51419137548593
waveform batch: 2/2
Test loss:40.683108994754036
Epoch 5 mean train loss:7.207116505136057
Epoch 5 mean test loss:4.246900021778821
Start training epoch 6
waveform batch: 1/3
Using ADAM optimizer
Sum of params:156.46426
Changing learning rate to:0.000625
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 151.07273286938036
Iteration: 2 || Loss: 149.14374506861455
Iteration: 3 || Loss: 149.28326524074393
Iteration: 4 || Loss: 147.9655640400914
Iteration: 5 || Loss: 148.07704389422787
Iteration: 6 || Loss: 147.9655640400914
saving ADAM checkpoint...
Sum of params:155.83466
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 147.9655640400914
Iteration: 2 || Loss: 147.66684823827254
Iteration: 3 || Loss: 147.64240307623805
Iteration: 4 || Loss: 143.45988934720793
Iteration: 5 || Loss: 140.77015360148414
Iteration: 6 || Loss: 137.56308135044162
Iteration: 7 || Loss: 134.00560856295468
Iteration: 8 || Loss: 132.83543730029655
Iteration: 9 || Loss: 132.0653955412577
Iteration: 10 || Loss: 129.27219529140714
Iteration: 11 || Loss: 122.55641675911913
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:158.3526
Epoch 6 loss:122.55641675911913
waveform batch: 2/3
Using ADAM optimizer
Sum of params:158.3526
Changing learning rate to:0.0003125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 40.139162508431625
Iteration: 2 || Loss: 39.04617436631691
Iteration: 3 || Loss: 39.58168901605185
Iteration: 4 || Loss: 39.3062470552536
Iteration: 5 || Loss: 38.767023369915876
Iteration: 6 || Loss: 38.767023369915876
saving ADAM checkpoint...
Sum of params:158.3541
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.767023369915876
Iteration: 2 || Loss: 38.67375009957937
Iteration: 3 || Loss: 38.47153213754113
Iteration: 4 || Loss: 37.810834580896994
Iteration: 5 || Loss: 36.96166156437488
Iteration: 6 || Loss: 36.30151064665083
Iteration: 7 || Loss: 35.50936337758128
Iteration: 8 || Loss: 34.66158662320395
Iteration: 9 || Loss: 33.662980631241915
Iteration: 10 || Loss: 33.287314170377435
Iteration: 11 || Loss: 32.91851523511046
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:159.9702
Epoch 6 loss:32.91851523511046
waveform batch: 3/3
Using ADAM optimizer
Sum of params:159.9702
Changing learning rate to:0.00015625
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.588363223330674
Iteration: 2 || Loss: 29.50466635076758
Iteration: 3 || Loss: 29.432436088391412
Iteration: 4 || Loss: 29.383372128703325
Iteration: 5 || Loss: 29.309090287194216
Iteration: 6 || Loss: 29.309090287194216
saving ADAM checkpoint...
Sum of params:159.81325
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.309090287194216
Iteration: 2 || Loss: 29.30554399174467
Iteration: 3 || Loss: 29.212149275463474
Iteration: 4 || Loss: 28.927305023889314
Iteration: 5 || Loss: 28.39731446630789
Iteration: 6 || Loss: 27.38096001068771
Iteration: 7 || Loss: 26.77125198905655
Iteration: 8 || Loss: 26.609185433900695
Iteration: 9 || Loss: 26.315999494749335
Iteration: 10 || Loss: 25.834750974376032
Iteration: 11 || Loss: 25.02666702153983
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:161.98235
Epoch 6 loss:25.02666702153983
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:28.442030957453028
waveform batch: 2/2
Test loss:34.57264216603936
Epoch 6 mean train loss:6.016719967192314
Epoch 6 mean test loss:3.7067454778524933
Start training epoch 7
waveform batch: 1/3
Using ADAM optimizer
Sum of params:161.98235
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 129.02259148012024
Iteration: 2 || Loss: 128.24975674209827
Iteration: 3 || Loss: 127.65101013960594
Iteration: 4 || Loss: 127.2001684859443
Iteration: 5 || Loss: 126.85476864774931
Iteration: 6 || Loss: 126.85476864774931
saving ADAM checkpoint...
Sum of params:161.85779
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 126.85476864774931
Iteration: 2 || Loss: 126.77958744299
Iteration: 3 || Loss: 126.56860324020322
Iteration: 4 || Loss: 117.10874943774934
Iteration: 5 || Loss: 115.89497646036733
Iteration: 6 || Loss: 114.68646590024882
Iteration: 7 || Loss: 113.46028880221014
Iteration: 8 || Loss: 113.08721912977077
Iteration: 9 || Loss: 111.04912558170932
Iteration: 10 || Loss: 109.7530493315567
Iteration: 11 || Loss: 108.08644831107866
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:163.01486
Epoch 7 loss:108.08644831107866
waveform batch: 2/3
Using ADAM optimizer
Sum of params:163.01486
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.935704631716355
Iteration: 2 || Loss: 36.05767855004579
Iteration: 3 || Loss: 35.818047132960466
Iteration: 4 || Loss: 35.731635174432824
Iteration: 5 || Loss: 35.74111221990908
Iteration: 6 || Loss: 35.731635174432824
saving ADAM checkpoint...
Sum of params:162.94264
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 35.731635174432824
Iteration: 2 || Loss: 35.710132607330806
Iteration: 3 || Loss: 35.582082561938186
Iteration: 4 || Loss: 34.05001313812092
Iteration: 5 || Loss: 32.758462143043594
Iteration: 6 || Loss: 31.71607895893756
Iteration: 7 || Loss: 31.193392032180164
Iteration: 8 || Loss: 30.506379330472623
Iteration: 9 || Loss: 29.464247499573485
Iteration: 10 || Loss: 29.06789601038473
Iteration: 11 || Loss: 28.779570778896126
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:163.89592
Epoch 7 loss:28.779570778896126
waveform batch: 3/3
Using ADAM optimizer
Sum of params:163.89592
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.083134188257336
Iteration: 2 || Loss: 26.098454917803902
Iteration: 3 || Loss: 26.0698799564386
Iteration: 4 || Loss: 26.046805312064325
Iteration: 5 || Loss: 25.955767288665097
Iteration: 6 || Loss: 25.955767288665097
saving ADAM checkpoint...
Sum of params:163.85617
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.955767288665097
Iteration: 2 || Loss: 25.95300696220161
Iteration: 3 || Loss: 25.92993024440837
Iteration: 4 || Loss: 25.59777002522863
Iteration: 5 || Loss: 25.322562526217595
Iteration: 6 || Loss: 24.805627592716874
Iteration: 7 || Loss: 24.513830026865158
Iteration: 8 || Loss: 24.285834739188367
Iteration: 9 || Loss: 23.942936588163988
Iteration: 10 || Loss: 23.649869573104123
Iteration: 11 || Loss: 23.411972510525057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:164.91779
Epoch 7 loss:23.411972510525057
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:27.01472125841882
waveform batch: 2/2
Test loss:29.570165147116803
Epoch 7 mean train loss:5.342599720016661
Epoch 7 mean test loss:3.32852272973739
Start training epoch 8
waveform batch: 1/3
Using ADAM optimizer
Sum of params:164.91779
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 111.67953437500184
Iteration: 2 || Loss: 111.17138930822234
Iteration: 3 || Loss: 110.81432448330067
Iteration: 4 || Loss: 110.50231098995114
Iteration: 5 || Loss: 110.26051077350998
Iteration: 6 || Loss: 110.26051077350998
saving ADAM checkpoint...
Sum of params:164.72552
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 110.26051077350998
Iteration: 2 || Loss: 110.1679557525128
Iteration: 3 || Loss: 110.0395437753232
Iteration: 4 || Loss: 105.75345308849715
Iteration: 5 || Loss: 102.47496855186454
Iteration: 6 || Loss: 101.00773685554489
Iteration: 7 || Loss: 99.36998405539217
Iteration: 8 || Loss: 98.05104132781196
Iteration: 9 || Loss: 95.61435070622855
Iteration: 10 || Loss: 93.85682359817524
Iteration: 11 || Loss: 92.21987117412289
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:166.43098
Epoch 8 loss:92.21987117412289
waveform batch: 2/3
Using ADAM optimizer
Sum of params:166.43098
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 35.21666375207633
Iteration: 2 || Loss: 33.7316369181991
Iteration: 3 || Loss: 33.10944725870506
Iteration: 4 || Loss: 33.13982928744607
Iteration: 5 || Loss: 33.381344501681994
Iteration: 6 || Loss: 33.10944725870506
saving ADAM checkpoint...
Sum of params:166.41135
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.10944725870506
Iteration: 2 || Loss: 33.079887329535076
Iteration: 3 || Loss: 31.069786225332127
Iteration: 4 || Loss: 30.257487178438083
Iteration: 5 || Loss: 28.919258496233336
Iteration: 6 || Loss: 28.517835710124775
Iteration: 7 || Loss: 28.293923215350038
Iteration: 8 || Loss: 27.466062210270206
Iteration: 9 || Loss: 26.74899062682957
Iteration: 10 || Loss: 26.391466394389226
Iteration: 11 || Loss: 26.095095189502945
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:168.35292
Epoch 8 loss:26.095095189502945
waveform batch: 3/3
Using ADAM optimizer
Sum of params:168.35292
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.450057260738376
Iteration: 2 || Loss: 24.38035214441564
Iteration: 3 || Loss: 24.293014733860716
Iteration: 4 || Loss: 24.291833711313693
Iteration: 5 || Loss: 24.24200572429472
Iteration: 6 || Loss: 24.24200572429472
saving ADAM checkpoint...
Sum of params:168.24911
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.24200572429472
Iteration: 2 || Loss: 24.238476700276784
Iteration: 3 || Loss: 24.210781501644423
Iteration: 4 || Loss: 23.6538432884142
Iteration: 5 || Loss: 23.469337808345838
Iteration: 6 || Loss: 23.335575704448928
Iteration: 7 || Loss: 23.199840220905845
Iteration: 8 || Loss: 22.932699202418192
Iteration: 9 || Loss: 22.57719169332556
Iteration: 10 || Loss: 22.23863194481524
Iteration: 11 || Loss: 22.12275456398666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:169.27774
Epoch 8 loss:22.12275456398666
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:25.801523798809704
waveform batch: 2/2
Test loss:25.949177868663508
Epoch 8 mean train loss:4.681257364253749
Epoch 8 mean test loss:3.044158921616071
Start training epoch 9
waveform batch: 1/3
Using ADAM optimizer
Sum of params:169.27774
Changing learning rate to:7.8125e-5
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 95.41394611810198
Iteration: 2 || Loss: 95.17690468640576
Iteration: 3 || Loss: 95.00611944638194
Iteration: 4 || Loss: 94.8845324604227
Iteration: 5 || Loss: 94.78745461115922
Iteration: 6 || Loss: 94.78745461115922
saving ADAM checkpoint...
Sum of params:169.22903
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 94.78745461115922
Iteration: 2 || Loss: 94.7473400145453
Iteration: 3 || Loss: 94.56055215618355
Iteration: 4 || Loss: 92.05974564420212
Iteration: 5 || Loss: 90.87391176502562
Iteration: 6 || Loss: 88.90459640959465
Iteration: 7 || Loss: 87.43831828689868
Iteration: 8 || Loss: 86.1233568246913
Iteration: 9 || Loss: 84.95247825472795
Iteration: 10 || Loss: 83.28546047404251
Iteration: 11 || Loss: 82.62371564755685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:169.60773
Epoch 9 loss:82.62371564755685
waveform batch: 2/3
Using ADAM optimizer
Sum of params:169.60773
Changing learning rate to:3.90625e-5
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.967257655513503
Iteration: 2 || Loss: 26.87990277517225
Iteration: 3 || Loss: 26.84098751835969
Iteration: 4 || Loss: 26.832674341418024
Iteration: 5 || Loss: 26.825110627702884
Iteration: 6 || Loss: 26.825110627702884
saving ADAM checkpoint...
Sum of params:169.60687
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.825110627702884
Iteration: 2 || Loss: 26.79350909815062
Iteration: 3 || Loss: 26.658894664611054
Iteration: 4 || Loss: 25.559464711155773
Iteration: 5 || Loss: 25.251533056785444
Iteration: 6 || Loss: 25.145627811488882
Iteration: 7 || Loss: 24.800125016605215
Iteration: 8 || Loss: 24.394335488360948
Iteration: 9 || Loss: 24.298367574157542
Iteration: 10 || Loss: 24.230832549062942
Iteration: 11 || Loss: 24.069492100657932
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:170.07996
Epoch 9 loss:24.069492100657932
waveform batch: 3/3
Using ADAM optimizer
Sum of params:170.07996
Changing learning rate to:1.953125e-5
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.19273969276311
Iteration: 2 || Loss: 22.17626140183667
Iteration: 3 || Loss: 22.1732776346393
Iteration: 4 || Loss: 22.174664196151312
Iteration: 5 || Loss: 22.172826015835096
Iteration: 6 || Loss: 22.172826015835096
saving ADAM checkpoint...
Sum of params:170.07672
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.172826015835096
Iteration: 2 || Loss: 22.167049410928445
Iteration: 3 || Loss: 22.15924227473896
Iteration: 4 || Loss: 22.021383627146992
Iteration: 5 || Loss: 21.832024351824455
Iteration: 6 || Loss: 21.643038509674774
Iteration: 7 || Loss: 21.52250902180304
Iteration: 8 || Loss: 21.275313180381694
Iteration: 9 || Loss: 21.165684369318566
Iteration: 10 || Loss: 20.986843000338748
Iteration: 11 || Loss: 20.79384914837043
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:171.16925
Epoch 9 loss:20.79384914837043
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:24.25692861909495
waveform batch: 2/2
Test loss:24.29656719241581
Epoch 9 mean train loss:4.249568563219507
Epoch 9 mean test loss:2.8560879889123973
Start training epoch 10
waveform batch: 1/3
Using ADAM optimizer
Sum of params:171.16925
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 87.516541491791
Iteration: 2 || Loss: 87.43954202534538
Iteration: 3 || Loss: 87.36759340216001
Iteration: 4 || Loss: 87.30074355144339
Iteration: 5 || Loss: 87.23885752118483
Iteration: 6 || Loss: 87.23885752118483
saving ADAM checkpoint...
Sum of params:171.15009
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 87.23885752118483
Iteration: 2 || Loss: 87.09347725080568
Iteration: 3 || Loss: 86.70808290735934
Iteration: 4 || Loss: 85.28175965065246
Iteration: 5 || Loss: 83.93793632087879
Iteration: 6 || Loss: 83.09631646822058
Iteration: 7 || Loss: 82.05777183203267
Iteration: 8 || Loss: 81.76187097833935
Iteration: 9 || Loss: 81.24151589513528
Iteration: 10 || Loss: 79.59358536516964
Iteration: 11 || Loss: 78.95222533087853
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:171.03918
Epoch 10 loss:78.95222533087853
waveform batch: 2/3
Using ADAM optimizer
Sum of params:171.03918
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.416475638627883
Iteration: 2 || Loss: 25.35514473555896
Iteration: 3 || Loss: 25.31114644153632
Iteration: 4 || Loss: 25.28333634941815
Iteration: 5 || Loss: 25.268703198094588
Iteration: 6 || Loss: 25.268703198094588
saving ADAM checkpoint...
Sum of params:171.043
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.268703198094588
Iteration: 2 || Loss: 25.245407557338815
Iteration: 3 || Loss: 25.141981779246287
Iteration: 4 || Loss: 24.050934644851683
Iteration: 5 || Loss: 23.783952058573636
Iteration: 6 || Loss: 23.625514724900654
Iteration: 7 || Loss: 23.21230087566099
Iteration: 8 || Loss: 23.063022611452574
Iteration: 9 || Loss: 22.97187205618324
Iteration: 10 || Loss: 22.913442199743407
Iteration: 11 || Loss: 22.804333530605113
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:171.76486
Epoch 10 loss:22.804333530605113
waveform batch: 3/3
Using ADAM optimizer
Sum of params:171.76486
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.93109077370113
Iteration: 2 || Loss: 20.91860561937457
Iteration: 3 || Loss: 20.910696762358096
Iteration: 4 || Loss: 20.905321675579966
Iteration: 5 || Loss: 20.899955567023262
Iteration: 6 || Loss: 20.899955567023262
saving ADAM checkpoint...
Sum of params:171.75543
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.899955567023262
Iteration: 2 || Loss: 20.897604448465646
Iteration: 3 || Loss: 20.88856294683229
Iteration: 4 || Loss: 20.794360666092302
Iteration: 5 || Loss: 20.692975630190332
Iteration: 6 || Loss: 20.475491119020454
Iteration: 7 || Loss: 20.342857786333976
Iteration: 8 || Loss: 20.233193013025744
Iteration: 9 || Loss: 20.019084549338434
Iteration: 10 || Loss: 19.54432207570198
Iteration: 11 || Loss: 19.37857312826764
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:174.14583
Epoch 10 loss:19.37857312826764
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:23.081189076110427
waveform batch: 2/2
Test loss:24.417669044769532
Epoch 10 mean train loss:4.03783773299171
Epoch 10 mean test loss:2.7940504776988213
Start training epoch 11
waveform batch: 1/3
Using ADAM optimizer
Sum of params:174.14583
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 90.53809333321934
Iteration: 2 || Loss: 90.4500062004348
Iteration: 3 || Loss: 90.36746787427714
Iteration: 4 || Loss: 90.29047125800089
Iteration: 5 || Loss: 90.21892116477207
Iteration: 6 || Loss: 90.21892116477207
saving ADAM checkpoint...
Sum of params:174.11064
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 90.21892116477207
Iteration: 2 || Loss: 90.02448788073117
Iteration: 3 || Loss: 89.6945938783858
Iteration: 4 || Loss: 86.24880716873918
Iteration: 5 || Loss: 83.23398628854396
Iteration: 6 || Loss: 81.97518881988303
Iteration: 7 || Loss: 81.05183912702014
Iteration: 8 || Loss: 80.23250095472802
Iteration: 9 || Loss: 79.36830702019304
Iteration: 10 || Loss: 78.9373917915702
Iteration: 11 || Loss: 77.92667627633271
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:173.71857
Epoch 11 loss:77.92667627633271
waveform batch: 2/3
Using ADAM optimizer
Sum of params:173.71857
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.24539723526206
Iteration: 2 || Loss: 23.23424460106122
Iteration: 3 || Loss: 23.225541496398222
Iteration: 4 || Loss: 23.217631159064165
Iteration: 5 || Loss: 23.210322252130876
Iteration: 6 || Loss: 23.210322252130876
saving ADAM checkpoint...
Sum of params:173.68663
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.210322252130876
Iteration: 2 || Loss: 23.12895778860282
Iteration: 3 || Loss: 23.117374963579024
Iteration: 4 || Loss: 22.689959156031424
Iteration: 5 || Loss: 22.24819146411463
Iteration: 6 || Loss: 22.10603018458006
Iteration: 7 || Loss: 21.791044253177564
Iteration: 8 || Loss: 21.59959042902668
Iteration: 9 || Loss: 21.438895372677713
Iteration: 10 || Loss: 21.17819683739836
Iteration: 11 || Loss: 20.98360474576768
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:173.53983
Epoch 11 loss:20.98360474576768
waveform batch: 3/3
Using ADAM optimizer
Sum of params:173.53983
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.40767010603461
Iteration: 2 || Loss: 19.39686133660638
Iteration: 3 || Loss: 19.396542785902984
Iteration: 4 || Loss: 19.393611908483685
Iteration: 5 || Loss: 19.387987491736737
Iteration: 6 || Loss: 19.387987491736737
saving ADAM checkpoint...
Sum of params:173.54626
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.387987491736737
Iteration: 2 || Loss: 19.38504152510243
Iteration: 3 || Loss: 19.371115400088467
Iteration: 4 || Loss: 19.187881357996247
Iteration: 5 || Loss: 19.07310725207172
Iteration: 6 || Loss: 18.650378648905093
Iteration: 7 || Loss: 18.595661291541894
Iteration: 8 || Loss: 18.56680164987477
Iteration: 9 || Loss: 18.439907307887225
Iteration: 10 || Loss: 18.380913111487743
Iteration: 11 || Loss: 18.29740072694767
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:175.00052
Epoch 11 loss:18.29740072694767
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:21.315460567097393
waveform batch: 2/2
Test loss:21.517448572044202
Epoch 11 mean train loss:3.9069227249682688
Epoch 11 mean test loss:2.5195828905377406
Start training epoch 12
waveform batch: 1/3
Using ADAM optimizer
Sum of params:175.00052
Changing learning rate to:9.765625e-6
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 77.96119725339253
Iteration: 2 || Loss: 77.93143047221255
Iteration: 3 || Loss: 77.90254295334509
Iteration: 4 || Loss: 77.87455281106932
Iteration: 5 || Loss: 77.84745024334427
Iteration: 6 || Loss: 77.84745024334427
saving ADAM checkpoint...
Sum of params:174.98445
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 77.84745024334427
Iteration: 2 || Loss: 77.73250971180308
Iteration: 3 || Loss: 77.46634316799272
Iteration: 4 || Loss: 76.2562936082011
Iteration: 5 || Loss: 74.59122331068625
Iteration: 6 || Loss: 74.09344921635287
Iteration: 7 || Loss: 73.54424971083367
Iteration: 8 || Loss: 72.97869334751914
Iteration: 9 || Loss: 72.25021706039993
Iteration: 10 || Loss: 71.92905193128108
Iteration: 11 || Loss: 71.51887564215549
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:174.71004
Epoch 12 loss:71.51887564215549
waveform batch: 2/3
Using ADAM optimizer
Sum of params:174.71004
Changing learning rate to:4.8828125e-6
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.487733687727104
Iteration: 2 || Loss: 22.464695050779174
Iteration: 3 || Loss: 22.44305163111029
Iteration: 4 || Loss: 22.42282262789542
Iteration: 5 || Loss: 22.404039489267742
Iteration: 6 || Loss: 22.404039489267742
saving ADAM checkpoint...
Sum of params:174.70708
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.404039489267742
Iteration: 2 || Loss: 22.27003985061687
Iteration: 3 || Loss: 22.15303420215314
Iteration: 4 || Loss: 21.565849940089546
Iteration: 5 || Loss: 21.111694167428418
Iteration: 6 || Loss: 20.94433635650253
Iteration: 7 || Loss: 20.872533894254726
Iteration: 8 || Loss: 20.480499794624137
Iteration: 9 || Loss: 20.225928304540435
Iteration: 10 || Loss: 20.200431418265772
Iteration: 11 || Loss: 20.101839841695014
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:175.2954
Epoch 12 loss:20.101839841695014
waveform batch: 3/3
Using ADAM optimizer
Sum of params:175.2954
Changing learning rate to:2.44140625e-6
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.601520021258636
Iteration: 2 || Loss: 18.596272432393572
Iteration: 3 || Loss: 18.591360634499733
Iteration: 4 || Loss: 18.586803545056636
Iteration: 5 || Loss: 18.582586838316168
Iteration: 6 || Loss: 18.582586838316168
saving ADAM checkpoint...
Sum of params:175.29509
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.582586838316168
Iteration: 2 || Loss: 18.55369490512057
Iteration: 3 || Loss: 18.536278542435202
Iteration: 4 || Loss: 18.46148236427978
Iteration: 5 || Loss: 18.325554366923193
Iteration: 6 || Loss: 18.25542294244388
Iteration: 7 || Loss: 18.16302268336428
Iteration: 8 || Loss: 18.023110173297333
Iteration: 9 || Loss: 17.98136579323586
Iteration: 10 || Loss: 17.914981104956926
Iteration: 11 || Loss: 17.879439745879377
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:175.92484
Epoch 12 loss:17.879439745879377
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:20.918020265558685
waveform batch: 2/2
Test loss:20.673632834148673
Epoch 12 mean train loss:3.6500051743243294
Epoch 12 mean test loss:2.4465678293945503
Start training epoch 13
waveform batch: 1/3
Using ADAM optimizer
Sum of params:175.92484
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 73.08077144347783
Iteration: 2 || Loss: 73.07309246470366
Iteration: 3 || Loss: 73.06546548366389
Iteration: 4 || Loss: 73.05790409264044
Iteration: 5 || Loss: 73.05039630920369
Iteration: 6 || Loss: 73.05039630920369
saving ADAM checkpoint...
Sum of params:175.92091
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 73.05039630920369
Iteration: 2 || Loss: 72.90329102653098
Iteration: 3 || Loss: 72.55639782083537
Iteration: 4 || Loss: 71.9677192259213
Iteration: 5 || Loss: 70.31259531227691
Iteration: 6 || Loss: 69.85726293184149
Iteration: 7 || Loss: 69.51728107027311
Iteration: 8 || Loss: 68.8416054148199
Iteration: 9 || Loss: 68.42426862798001
Iteration: 10 || Loss: 68.20781044079031
Iteration: 11 || Loss: 67.78364609764226
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:175.80266
Epoch 13 loss:67.78364609764226
waveform batch: 2/3
Using ADAM optimizer
Sum of params:175.80266
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.363178289631275
Iteration: 2 || Loss: 21.351562876483857
Iteration: 3 || Loss: 21.340332279072832
Iteration: 4 || Loss: 21.329503717860803
Iteration: 5 || Loss: 21.319070911960623
Iteration: 6 || Loss: 21.319070911960623
saving ADAM checkpoint...
Sum of params:175.80087
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.319070911960623
Iteration: 2 || Loss: 21.14879470894313
Iteration: 3 || Loss: 20.898186262024065
Iteration: 4 || Loss: 20.607303437207044
Iteration: 5 || Loss: 20.431780109504757
Iteration: 6 || Loss: 20.127760471755987
Iteration: 7 || Loss: 20.07152861770138
Iteration: 8 || Loss: 20.010392766730664
Iteration: 9 || Loss: 19.674059325186253
Iteration: 10 || Loss: 19.624973029764107
Iteration: 11 || Loss: 19.572606178712775
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:176.23126
Epoch 13 loss:19.572606178712775
waveform batch: 3/3
Using ADAM optimizer
Sum of params:176.23126
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.249119058668064
Iteration: 2 || Loss: 18.244492323148215
Iteration: 3 || Loss: 18.240240056763643
Iteration: 4 || Loss: 18.236363317537467
Iteration: 5 || Loss: 18.23287101587709
Iteration: 6 || Loss: 18.23287101587709
saving ADAM checkpoint...
Sum of params:176.23071
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.23287101587709
Iteration: 2 || Loss: 18.217392724778396
Iteration: 3 || Loss: 18.210693978505446
Iteration: 4 || Loss: 18.1426716158653
Iteration: 5 || Loss: 17.969632336382677
Iteration: 6 || Loss: 17.83382724642822
Iteration: 7 || Loss: 17.682271545713416
Iteration: 8 || Loss: 17.634800397250377
Iteration: 9 || Loss: 17.570589895689903
Iteration: 10 || Loss: 17.5157598207294
Iteration: 11 || Loss: 17.44534574409471
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:177.13187
Epoch 13 loss:17.44534574409471
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:20.510307245370278
waveform batch: 2/2
Test loss:20.124061382545346
Epoch 13 mean train loss:3.4933866006816583
Epoch 13 mean test loss:2.3902569781126837
Start training epoch 14
waveform batch: 1/3
Using ADAM optimizer
Sum of params:177.13187
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 70.16434917287533
Iteration: 2 || Loss: 70.15568526977226
Iteration: 3 || Loss: 70.14707136753599
Iteration: 4 || Loss: 70.13850913625303
Iteration: 5 || Loss: 70.12999331426204
Iteration: 6 || Loss: 70.12999331426204
saving ADAM checkpoint...
Sum of params:177.12814
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 70.12999331426204
Iteration: 2 || Loss: 69.70197306608725
Iteration: 3 || Loss: 69.43026316584354
Iteration: 4 || Loss: 68.8988400653827
Iteration: 5 || Loss: 67.3367745848507
Iteration: 6 || Loss: 66.17498658046848
Iteration: 7 || Loss: 65.97195046616261
Iteration: 8 || Loss: 65.72258155070645
Iteration: 9 || Loss: 65.36440846557672
Iteration: 10 || Loss: 65.18903621918042
Iteration: 11 || Loss: 64.82992703156799
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:177.14626
Epoch 14 loss:64.82992703156799
waveform batch: 2/3
Using ADAM optimizer
Sum of params:177.14626
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.25538039874491
Iteration: 2 || Loss: 20.24320163192453
Iteration: 3 || Loss: 20.23143905221102
Iteration: 4 || Loss: 20.220110871211766
Iteration: 5 || Loss: 20.20921306925386
Iteration: 6 || Loss: 20.20921306925386
saving ADAM checkpoint...
Sum of params:177.14462
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.20921306925386
Iteration: 2 || Loss: 20.038096989884224
Iteration: 3 || Loss: 19.79078805759179
Iteration: 4 || Loss: 19.61031360917465
Iteration: 5 || Loss: 19.54440044636428
Iteration: 6 || Loss: 19.30207231681183
Iteration: 7 || Loss: 19.25456376405585
Iteration: 8 || Loss: 19.22549501400409
Iteration: 9 || Loss: 19.047739595239456
Iteration: 10 || Loss: 18.945035573113888
Iteration: 11 || Loss: 18.927064783291957
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:177.50494
Epoch 14 loss:18.927064783291957
waveform batch: 3/3
Using ADAM optimizer
Sum of params:177.50494
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.561103822081922
Iteration: 2 || Loss: 17.55768022426689
Iteration: 3 || Loss: 17.5546518334813
Iteration: 4 || Loss: 17.55202014551426
Iteration: 5 || Loss: 17.54977757550281
Iteration: 6 || Loss: 17.54977757550281
saving ADAM checkpoint...
Sum of params:177.50418
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.54977757550281
Iteration: 2 || Loss: 17.54269955966116
Iteration: 3 || Loss: 17.532353227760357
Iteration: 4 || Loss: 17.42055523623915
Iteration: 5 || Loss: 17.362676034134015
Iteration: 6 || Loss: 17.200304957269207
Iteration: 7 || Loss: 17.128782564508672
Iteration: 8 || Loss: 17.039409531462187
Iteration: 9 || Loss: 16.97864077735934
Iteration: 10 || Loss: 16.911123020067546
Iteration: 11 || Loss: 16.713452861966825
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:178.62898
Epoch 14 loss:16.713452861966825
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:19.790158482446408
waveform batch: 2/2
Test loss:19.548819938332304
Epoch 14 mean train loss:3.3490148225608922
Epoch 14 mean test loss:2.314057554163454
Start training epoch 15
waveform batch: 1/3
Using ADAM optimizer
Sum of params:178.62898
Changing learning rate to:1.220703125e-6
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.25564010797154
Iteration: 2 || Loss: 68.24693604796961
Iteration: 3 || Loss: 68.23834098030657
Iteration: 4 || Loss: 68.2298646226679
Iteration: 5 || Loss: 68.22150264053673
Iteration: 6 || Loss: 68.22150264053673
saving ADAM checkpoint...
Sum of params:178.62856
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.22150264053673
Iteration: 2 || Loss: 67.78616316050424
Iteration: 3 || Loss: 67.20250024198397
Iteration: 4 || Loss: 66.78994240639884
Iteration: 5 || Loss: 65.68343301655193
Iteration: 6 || Loss: 64.47303107628017
Iteration: 7 || Loss: 64.24656241342626
Iteration: 8 || Loss: 63.77379810608862
Iteration: 9 || Loss: 63.332885830361
Iteration: 10 || Loss: 63.259866843701204
Iteration: 11 || Loss: 62.991353874321426
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:178.48035
Epoch 15 loss:62.991353874321426
waveform batch: 2/3
Using ADAM optimizer
Sum of params:178.48035
Changing learning rate to:6.103515625e-7
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.388012048671122
Iteration: 2 || Loss: 19.38356889929906
Iteration: 3 || Loss: 19.37915755141257
Iteration: 4 || Loss: 19.374770439837746
Iteration: 5 || Loss: 19.370408469911524
Iteration: 6 || Loss: 19.370408469911524
saving ADAM checkpoint...
Sum of params:178.47987
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.370408469911524
Iteration: 2 || Loss: 19.02423059246618
Iteration: 3 || Loss: 18.978915068126074
Iteration: 4 || Loss: 18.93982798947311
Iteration: 5 || Loss: 18.749087335819127
Iteration: 6 || Loss: 18.72630913229217
Iteration: 7 || Loss: 18.38039271624465
Iteration: 8 || Loss: 18.358302384986008
Iteration: 9 || Loss: 18.339283149784592
Iteration: 10 || Loss: 18.31822330672869
Iteration: 11 || Loss: 18.24080222017658
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:178.78334
Epoch 15 loss:18.24080222017658
waveform batch: 3/3
Using ADAM optimizer
Sum of params:178.78334
Changing learning rate to:3.0517578125e-7
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 16.714623805209122
Iteration: 2 || Loss: 16.714448879775837
Iteration: 3 || Loss: 16.714280255021137
Iteration: 4 || Loss: 16.714116290874127
Iteration: 5 || Loss: 16.713963060939793
Iteration: 6 || Loss: 16.713963060939793
saving ADAM checkpoint...
Sum of params:178.7833
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 16.713963060939793
Iteration: 2 || Loss: 16.71302597115022
Iteration: 3 || Loss: 16.708438977902194
Iteration: 4 || Loss: 16.665030680856265
Iteration: 5 || Loss: 16.537985621300088
Iteration: 6 || Loss: 16.379346452222357
Iteration: 7 || Loss: 16.32228437208747
Iteration: 8 || Loss: 16.15738393889377
Iteration: 9 || Loss: 16.069437773458297
Iteration: 10 || Loss: 15.913128072115203
Iteration: 11 || Loss: 15.586143979131185
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:180.59216
Epoch 15 loss:15.586143979131185
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:18.76353963529962
waveform batch: 2/2
Test loss:19.34635772219958
Epoch 15 mean train loss:3.227276669120973
Epoch 15 mean test loss:2.2417586680881882
Start training epoch 16
waveform batch: 1/3
Using ADAM optimizer
Sum of params:180.59216
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 68.98082404246291
Iteration: 2 || Loss: 68.97895119483526
Iteration: 3 || Loss: 68.97708238563514
Iteration: 4 || Loss: 68.97523015704233
Iteration: 5 || Loss: 68.97337310026082
Iteration: 6 || Loss: 68.97337310026082
saving ADAM checkpoint...
Sum of params:180.59209
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 68.97337310026082
Iteration: 2 || Loss: 68.6064766994713
Iteration: 3 || Loss: 67.9285964031001
Iteration: 4 || Loss: 67.11345725752443
Iteration: 5 || Loss: 65.95589463075055
Iteration: 6 || Loss: 63.641815829659016
Iteration: 7 || Loss: 63.44391955970277
Iteration: 8 || Loss: 62.8284050644454
Iteration: 9 || Loss: 62.04057276801171
Iteration: 10 || Loss: 61.830283841659146
Iteration: 11 || Loss: 61.43255119368962
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:180.4017
Epoch 16 loss:61.43255119368962
waveform batch: 2/3
Using ADAM optimizer
Sum of params:180.4017
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.74891270861633
Iteration: 2 || Loss: 18.745902418992518
Iteration: 3 || Loss: 18.742902741523473
Iteration: 4 || Loss: 18.739906866807768
Iteration: 5 || Loss: 18.736919008750977
Iteration: 6 || Loss: 18.736919008750977
saving ADAM checkpoint...
Sum of params:180.40146
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.736919008750977
Iteration: 2 || Loss: 18.181541181139234
Iteration: 3 || Loss: 18.105480548349817
Iteration: 4 || Loss: 17.92803088729629
Iteration: 5 || Loss: 17.808453118699003
Iteration: 6 || Loss: 17.606444188088847
Iteration: 7 || Loss: 17.54673501444435
Iteration: 8 || Loss: 17.53005135493639
Iteration: 9 || Loss: 17.321349204890648
Iteration: 10 || Loss: 17.31037622156274
Iteration: 11 || Loss: 17.22753351199256
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:180.61662
Epoch 16 loss:17.22753351199256
waveform batch: 3/3
Using ADAM optimizer
Sum of params:180.61662
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.710321425364143
Iteration: 2 || Loss: 15.70925879378566
Iteration: 3 || Loss: 15.708205214009626
Iteration: 4 || Loss: 15.707158415333415
Iteration: 5 || Loss: 15.706117993001856
Iteration: 6 || Loss: 15.706117993001856
saving ADAM checkpoint...
Sum of params:180.61664
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.706117993001856
Iteration: 2 || Loss: 15.633291928919643
Iteration: 3 || Loss: 15.614578380349858
Iteration: 4 || Loss: 15.571602307273361
Iteration: 5 || Loss: 15.41148901188725
Iteration: 6 || Loss: 15.370666522860363
Iteration: 7 || Loss: 15.348810915127167
Iteration: 8 || Loss: 15.229041116995665
Iteration: 9 || Loss: 15.157056118405185
Iteration: 10 || Loss: 15.130602407887244
Iteration: 11 || Loss: 15.058578381813058
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:180.82928
Epoch 16 loss:15.058578381813058
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:18.19861623145639
waveform batch: 2/2
Test loss:18.186093777152188
Epoch 16 mean train loss:3.1239554362498416
Epoch 16 mean test loss:2.1402770593299163
Start training epoch 17
waveform batch: 1/3
Using ADAM optimizer
Sum of params:180.82928
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 63.069836327472956
Iteration: 2 || Loss: 63.06878745011008
Iteration: 3 || Loss: 63.06774199502412
Iteration: 4 || Loss: 63.06670666083773
Iteration: 5 || Loss: 63.065676510892125
Iteration: 6 || Loss: 63.065676510892125
saving ADAM checkpoint...
Sum of params:180.82898
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 63.065676510892125
Iteration: 2 || Loss: 62.93799338621508
Iteration: 3 || Loss: 62.665771502133914
Iteration: 4 || Loss: 61.86283015891372
Iteration: 5 || Loss: 60.597636121215274
Iteration: 6 || Loss: 60.43634290848971
Iteration: 7 || Loss: 59.90072662759147
Iteration: 8 || Loss: 59.44732321155759
Iteration: 9 || Loss: 59.323191973949406
Iteration: 10 || Loss: 58.56410580421463
Iteration: 11 || Loss: 58.321510602293685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:181.1176
Epoch 17 loss:58.321510602293685
waveform batch: 2/3
Using ADAM optimizer
Sum of params:181.1176
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.929832094184594
Iteration: 2 || Loss: 18.92608378380855
Iteration: 3 || Loss: 18.92233715999828
Iteration: 4 || Loss: 18.918603426861836
Iteration: 5 || Loss: 18.91487545466215
Iteration: 6 || Loss: 18.91487545466215
saving ADAM checkpoint...
Sum of params:181.11765
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.91487545466215
Iteration: 2 || Loss: 17.9551091296074
Iteration: 3 || Loss: 17.7584971645577
Iteration: 4 || Loss: 17.612760711746397
Iteration: 5 || Loss: 17.5165316384921
Iteration: 6 || Loss: 17.328492922345774
Iteration: 7 || Loss: 17.23192380959524
Iteration: 8 || Loss: 17.119281971921588
Iteration: 9 || Loss: 16.98068670589371
Iteration: 10 || Loss: 16.95638345043496
Iteration: 11 || Loss: 16.936898925480158
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:181.32773
Epoch 17 loss:16.936898925480158
waveform batch: 3/3
Using ADAM optimizer
Sum of params:181.32773
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.797436956954074
Iteration: 2 || Loss: 15.79721466248193
Iteration: 3 || Loss: 15.796999956956801
Iteration: 4 || Loss: 15.796789568517571
Iteration: 5 || Loss: 15.79658366915381
Iteration: 6 || Loss: 15.79658366915381
saving ADAM checkpoint...
Sum of params:181.32777
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.79658366915381
Iteration: 2 || Loss: 15.794361244708345
Iteration: 3 || Loss: 15.5441547136621
Iteration: 4 || Loss: 15.525101368036667
Iteration: 5 || Loss: 15.454317195949269
Iteration: 6 || Loss: 15.246324521244688
Iteration: 7 || Loss: 15.00202176145079
Iteration: 8 || Loss: 14.919141256035475
Iteration: 9 || Loss: 14.860879752233869
Iteration: 10 || Loss: 14.807581169270097
Iteration: 11 || Loss: 14.579439042959192
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:182.1787
Epoch 17 loss:14.579439042959192
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:17.674859449494193
waveform batch: 2/2
Test loss:17.79189807452973
Epoch 17 mean train loss:2.9945949523577675
Epoch 17 mean test loss:2.0862798543543484
Start training epoch 18
waveform batch: 1/3
Using ADAM optimizer
Sum of params:182.1787
Changing learning rate to:1.52587890625e-7
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 62.08062885473107
Iteration: 2 || Loss: 62.07977296607988
Iteration: 3 || Loss: 62.078922780527755
Iteration: 4 || Loss: 62.0780701605355
Iteration: 5 || Loss: 62.077223916572834
Iteration: 6 || Loss: 62.077223916572834
saving ADAM checkpoint...
Sum of params:182.17865
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 62.077223916572834
Iteration: 2 || Loss: 61.80484883427353
Iteration: 3 || Loss: 61.39097908486621
Iteration: 4 || Loss: 60.763045072753236
Iteration: 5 || Loss: 59.36486491561344
Iteration: 6 || Loss: 58.67404372688204
Iteration: 7 || Loss: 58.484415890820735
Iteration: 8 || Loss: 57.82367899923167
Iteration: 9 || Loss: 57.2872307317508
Iteration: 10 || Loss: 57.160750190107166
Iteration: 11 || Loss: 56.91121601228143
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:181.99255
Epoch 18 loss:56.91121601228143
waveform batch: 2/3
Using ADAM optimizer
Sum of params:181.99255
Changing learning rate to:7.62939453125e-8
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 17.02699875349824
Iteration: 2 || Loss: 17.026595648285664
Iteration: 3 || Loss: 17.026194595667846
Iteration: 4 || Loss: 17.0257913169562
Iteration: 5 || Loss: 17.02538862103829
Iteration: 6 || Loss: 17.02538862103829
saving ADAM checkpoint...
Sum of params:181.99255
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 17.02538862103829
Iteration: 2 || Loss: 16.86052222514541
Iteration: 3 || Loss: 16.78122865697169
Iteration: 4 || Loss: 16.54761528146274
Iteration: 5 || Loss: 16.471104619411022
Iteration: 6 || Loss: 16.450151432596634
Iteration: 7 || Loss: 16.401709920436584
Iteration: 8 || Loss: 16.203132047280974
Iteration: 9 || Loss: 16.18044957964551
Iteration: 10 || Loss: 16.14101314160169
Iteration: 11 || Loss: 15.987613834927608
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:182.1897
Epoch 18 loss:15.987613834927608
waveform batch: 3/3
Using ADAM optimizer
Sum of params:182.1897
Changing learning rate to:3.814697265625e-8
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.638728486533779
Iteration: 2 || Loss: 14.638690847858523
Iteration: 3 || Loss: 14.638652863373448
Iteration: 4 || Loss: 14.638616455773992
Iteration: 5 || Loss: 14.638578550527349
Iteration: 6 || Loss: 14.638578550527349
saving ADAM checkpoint...
Sum of params:182.18968
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.638578550527349
Iteration: 2 || Loss: 14.631358079000478
Iteration: 3 || Loss: 14.62006410342946
Iteration: 4 || Loss: 14.599247214680323
Iteration: 5 || Loss: 14.465679313783824
Iteration: 6 || Loss: 14.429156936645622
Iteration: 7 || Loss: 14.30670514404699
Iteration: 8 || Loss: 14.26206236615074
Iteration: 9 || Loss: 14.132294904215673
Iteration: 10 || Loss: 14.099620201305926
Iteration: 11 || Loss: 13.93647120144707
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:183.20206
Epoch 18 loss:13.93647120144707
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:17.05019715603794
waveform batch: 2/2
Test loss:17.40342041944094
Epoch 18 mean train loss:2.8945100349552035
Epoch 18 mean test loss:2.0266833867928753
Start training epoch 19
waveform batch: 1/3
Using ADAM optimizer
Sum of params:183.20206
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 61.724021422934364
Iteration: 2 || Loss: 61.72386011244136
Iteration: 3 || Loss: 61.72369944512941
Iteration: 4 || Loss: 61.723538185855766
Iteration: 5 || Loss: 61.72337740245517
Iteration: 6 || Loss: 61.72337740245517
saving ADAM checkpoint...
Sum of params:183.20203
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 61.72337740245517
Iteration: 2 || Loss: 61.51416472641582
Iteration: 3 || Loss: 61.308852770476335
Iteration: 4 || Loss: 60.60168432862735
Iteration: 5 || Loss: 58.02031268583662
Iteration: 6 || Loss: 57.736046292155564
Iteration: 7 || Loss: 57.27137579969915
Iteration: 8 || Loss: 55.97528498469007
Iteration: 9 || Loss: 55.70347190062701
Iteration: 10 || Loss: 55.52957533960821
Iteration: 11 || Loss: 55.22545534316934
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:182.99443
Epoch 19 loss:55.22545534316934
waveform batch: 2/3
Using ADAM optimizer
Sum of params:182.99443
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 16.553634012961577
Iteration: 2 || Loss: 16.553342552541157
Iteration: 3 || Loss: 16.5530489576581
Iteration: 4 || Loss: 16.55275544143328
Iteration: 5 || Loss: 16.55246276449927
Iteration: 6 || Loss: 16.55246276449927
saving ADAM checkpoint...
Sum of params:182.99442
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 16.55246276449927
Iteration: 2 || Loss: 16.197802946574306
Iteration: 3 || Loss: 16.134966351883257
Iteration: 4 || Loss: 16.095323922000873
Iteration: 5 || Loss: 16.051447044219156
Iteration: 6 || Loss: 15.889047423198198
Iteration: 7 || Loss: 15.852091805799663
Iteration: 8 || Loss: 15.503392266396192
Iteration: 9 || Loss: 15.421393384071127
Iteration: 10 || Loss: 15.405515055369268
Iteration: 11 || Loss: 15.37808126147131
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:183.26279
Epoch 19 loss:15.37808126147131
waveform batch: 3/3
Using ADAM optimizer
Sum of params:183.26279
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.090491342758854
Iteration: 2 || Loss: 14.090480232349039
Iteration: 3 || Loss: 14.090468655415645
Iteration: 4 || Loss: 14.09045784296552
Iteration: 5 || Loss: 14.090446375622747
Iteration: 6 || Loss: 14.090446375622747
saving ADAM checkpoint...
Sum of params:183.26274
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.090446375622747
Iteration: 2 || Loss: 14.089931258664325
Iteration: 3 || Loss: 14.082182043102858
Iteration: 4 || Loss: 14.003468231827776
Iteration: 5 || Loss: 13.910789449831007
Iteration: 6 || Loss: 13.821582510804973
Iteration: 7 || Loss: 13.707391768385751
Iteration: 8 || Loss: 13.57513407080374
Iteration: 9 || Loss: 13.49587423402913
Iteration: 10 || Loss: 13.345337367907462
Iteration: 11 || Loss: 13.106154936667785
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:184.76299
Epoch 19 loss:13.106154936667785
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:16.016253338589145
waveform batch: 2/2
Test loss:16.67686693845746
Epoch 19 mean train loss:2.7903230513769484
Epoch 19 mean test loss:1.923124722179212
Start training epoch 20
waveform batch: 1/3
Using ADAM optimizer
Sum of params:184.76299
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 61.01439841820384
Iteration: 2 || Loss: 61.01425418846878
Iteration: 3 || Loss: 61.01411242464321
Iteration: 4 || Loss: 61.01397204499311
Iteration: 5 || Loss: 61.013829874340296
Iteration: 6 || Loss: 61.013829874340296
saving ADAM checkpoint...
Sum of params:184.76297
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 61.013829874340296
Iteration: 2 || Loss: 60.84110912579637
Iteration: 3 || Loss: 60.47324534180454
Iteration: 4 || Loss: 58.94562280384007
Iteration: 5 || Loss: 56.88292311879947
Iteration: 6 || Loss: 56.51132725911005
Iteration: 7 || Loss: 56.195787884599824
Iteration: 8 || Loss: 55.19951156632803
Iteration: 9 || Loss: 54.38051719065169
Iteration: 10 || Loss: 54.22060852958975
Iteration: 11 || Loss: 53.79366889254783
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:184.44374
Epoch 20 loss:53.79366889254783
waveform batch: 2/3
Using ADAM optimizer
Sum of params:184.44374
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.910758397373394
Iteration: 2 || Loss: 15.910409805857011
Iteration: 3 || Loss: 15.910059224853201
Iteration: 4 || Loss: 15.909706681097092
Iteration: 5 || Loss: 15.909358318792167
Iteration: 6 || Loss: 15.909358318792167
saving ADAM checkpoint...
Sum of params:184.44373
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.909358318792167
Iteration: 2 || Loss: 15.431951153451678
Iteration: 3 || Loss: 15.346211136037546
Iteration: 4 || Loss: 15.199732380634481
Iteration: 5 || Loss: 15.16199643757272
Iteration: 6 || Loss: 14.939662262010653
Iteration: 7 || Loss: 14.922599063922643
Iteration: 8 || Loss: 14.79571670456851
Iteration: 9 || Loss: 14.748830908375075
Iteration: 10 || Loss: 14.72663626908492
Iteration: 11 || Loss: 14.564534159143873
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:184.64694
Epoch 20 loss:14.564534159143873
waveform batch: 3/3
Using ADAM optimizer
Sum of params:184.64694
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.191508922394453
Iteration: 2 || Loss: 13.19142579075833
Iteration: 3 || Loss: 13.191346787885637
Iteration: 4 || Loss: 13.191265924127002
Iteration: 5 || Loss: 13.191183989071003
Iteration: 6 || Loss: 13.191183989071003
saving ADAM checkpoint...
Sum of params:184.64696
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.191183989071003
Iteration: 2 || Loss: 13.164251233925132
Iteration: 3 || Loss: 13.155354952337781
Iteration: 4 || Loss: 13.126049504423808
Iteration: 5 || Loss: 13.013563865217153
Iteration: 6 || Loss: 12.967590089838659
Iteration: 7 || Loss: 12.956684985894505
Iteration: 8 || Loss: 12.902835773983066
Iteration: 9 || Loss: 12.878190559195112
Iteration: 10 || Loss: 12.814158283830565
Iteration: 11 || Loss: 12.783957612011598
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:184.91257
Epoch 20 loss:12.783957612011598
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:15.670207941890572
waveform batch: 2/2
Test loss:15.661124130414438
Epoch 20 mean train loss:2.7047386887901097
Epoch 20 mean test loss:1.8430195336650006
Start training epoch 21
waveform batch: 1/3
Using ADAM optimizer
Sum of params:184.91257
Changing learning rate to:1.9073486328125e-8
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 55.31828386416255
Iteration: 2 || Loss: 55.31824016588357
Iteration: 3 || Loss: 55.31819587104052
Iteration: 4 || Loss: 55.31815230028008
Iteration: 5 || Loss: 55.318106916988945
Iteration: 6 || Loss: 55.318106916988945
saving ADAM checkpoint...
Sum of params:184.91254
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 55.318106916988945
Iteration: 2 || Loss: 55.22963793711226
Iteration: 3 || Loss: 55.03684555434197
Iteration: 4 || Loss: 54.6312293978324
Iteration: 5 || Loss: 53.35249044040845
Iteration: 6 || Loss: 52.861733499567606
Iteration: 7 || Loss: 52.25086440121576
Iteration: 8 || Loss: 52.077547271403276
Iteration: 9 || Loss: 51.823912297197076
Iteration: 10 || Loss: 51.43565082768832
Iteration: 11 || Loss: 51.226232432840845
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:184.9429
Epoch 21 loss:51.226232432840845
waveform batch: 2/3
Using ADAM optimizer
Sum of params:184.9429
Changing learning rate to:9.5367431640625e-9
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 15.555212090620223
Iteration: 2 || Loss: 15.55514622834658
Iteration: 3 || Loss: 15.555079707070249
Iteration: 4 || Loss: 15.55501343160565
Iteration: 5 || Loss: 15.554946106002879
Iteration: 6 || Loss: 15.554946106002879
saving ADAM checkpoint...
Sum of params:184.94289
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 15.554946106002879
Iteration: 2 || Loss: 14.852571610215803
Iteration: 3 || Loss: 14.76912730003208
Iteration: 4 || Loss: 14.71369049473767
Iteration: 5 || Loss: 14.580577043324793
Iteration: 6 || Loss: 14.490817377862756
Iteration: 7 || Loss: 14.472542187253941
Iteration: 8 || Loss: 14.4387900664114
Iteration: 9 || Loss: 14.23488477582076
Iteration: 10 || Loss: 14.228696042425625
Iteration: 11 || Loss: 14.151254000699556
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:185.33533
Epoch 21 loss:14.151254000699556
waveform batch: 3/3
Using ADAM optimizer
Sum of params:185.33533
Changing learning rate to:4.76837158203125e-9
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.946389939379198
Iteration: 2 || Loss: 12.946388958054383
Iteration: 3 || Loss: 12.946387655800224
Iteration: 4 || Loss: 12.946386980785933
Iteration: 5 || Loss: 12.94638577534795
Iteration: 6 || Loss: 12.94638577534795
saving ADAM checkpoint...
Sum of params:185.33533
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.94638577534795
Iteration: 2 || Loss: 12.934027417003975
Iteration: 3 || Loss: 12.92121992348366
Iteration: 4 || Loss: 12.891309041833159
Iteration: 5 || Loss: 12.855962961562625
Iteration: 6 || Loss: 12.77469032031422
Iteration: 7 || Loss: 12.64506276709602
Iteration: 8 || Loss: 12.561765671695191
Iteration: 9 || Loss: 12.451394997588945
Iteration: 10 || Loss: 12.333999259612066
Iteration: 11 || Loss: 12.247729100944218
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:187.2169
Epoch 21 loss:12.247729100944218
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:15.109062313195698
waveform batch: 2/2
Test loss:15.314463981437958
Epoch 21 mean train loss:2.58750718448282
Epoch 21 mean test loss:1.7896191938019799
Start training epoch 22
waveform batch: 1/3
Using ADAM optimizer
Sum of params:187.2169
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 55.76218694362874
Iteration: 2 || Loss: 55.762178615090896
Iteration: 3 || Loss: 55.762170145535684
Iteration: 4 || Loss: 55.76216307073337
Iteration: 5 || Loss: 55.76215555363585
Iteration: 6 || Loss: 55.76215555363585
saving ADAM checkpoint...
Sum of params:187.2169
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 55.76215555363585
Iteration: 2 || Loss: 55.590076540486905
Iteration: 3 || Loss: 55.352167336646765
Iteration: 4 || Loss: 54.602128270988175
Iteration: 5 || Loss: 52.98888476914528
Iteration: 6 || Loss: 52.792996612624904
Iteration: 7 || Loss: 51.8883615399333
Iteration: 8 || Loss: 50.746041823414224
Iteration: 9 || Loss: 50.58813981334004
Iteration: 10 || Loss: 50.26148528562321
Iteration: 11 || Loss: 50.02389476435566
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:187.13742
Epoch 22 loss:50.02389476435566
waveform batch: 2/3
Using ADAM optimizer
Sum of params:187.13742
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.737138383573559
Iteration: 2 || Loss: 14.737126506121077
Iteration: 3 || Loss: 14.737115453401906
Iteration: 4 || Loss: 14.737104769948555
Iteration: 5 || Loss: 14.737093350441
Iteration: 6 || Loss: 14.737093350441
saving ADAM checkpoint...
Sum of params:187.1374
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.737093350441
Iteration: 2 || Loss: 14.529802232288539
Iteration: 3 || Loss: 14.426549373131492
Iteration: 4 || Loss: 14.317697790904045
Iteration: 5 || Loss: 14.273257448066806
Iteration: 6 || Loss: 13.892145195189597
Iteration: 7 || Loss: 13.85674335145555
Iteration: 8 || Loss: 13.796005063960985
Iteration: 9 || Loss: 13.557223828309938
Iteration: 10 || Loss: 13.503593863001237
Iteration: 11 || Loss: 13.494545508155696
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:187.2876
Epoch 22 loss:13.494545508155696
waveform batch: 3/3
Using ADAM optimizer
Sum of params:187.2876
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.279522362759142
Iteration: 2 || Loss: 12.27951978169263
Iteration: 3 || Loss: 12.279517498192543
Iteration: 4 || Loss: 12.279514803469066
Iteration: 5 || Loss: 12.27951275212444
Iteration: 6 || Loss: 12.27951275212444
saving ADAM checkpoint...
Sum of params:187.2876
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.27951275212444
Iteration: 2 || Loss: 12.272052906613121
Iteration: 3 || Loss: 12.268954407726609
Iteration: 4 || Loss: 12.23130708980888
Iteration: 5 || Loss: 12.174005541123323
Iteration: 6 || Loss: 12.155898193964035
Iteration: 7 || Loss: 12.12579486637984
Iteration: 8 || Loss: 12.079939007845613
Iteration: 9 || Loss: 12.003041376905303
Iteration: 10 || Loss: 11.950032243978974
Iteration: 11 || Loss: 11.91415534166337
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:187.97704
Epoch 22 loss:11.91415534166337
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:14.740425994007039
waveform batch: 2/2
Test loss:14.503062194981068
Epoch 22 mean train loss:2.5144198538058244
Epoch 22 mean test loss:1.7202051875875357
Start training epoch 23
waveform batch: 1/3
Using ADAM optimizer
Sum of params:187.97704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 51.794489731980285
Iteration: 2 || Loss: 51.79448494334725
Iteration: 3 || Loss: 51.79448102481425
Iteration: 4 || Loss: 51.79447699136843
Iteration: 5 || Loss: 51.79447160781115
Iteration: 6 || Loss: 51.79447160781115
saving ADAM checkpoint...
Sum of params:187.97704
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 51.79447160781115
Iteration: 2 || Loss: 51.714198893536434
Iteration: 3 || Loss: 51.56958907753326
Iteration: 4 || Loss: 50.6674958737213
Iteration: 5 || Loss: 49.931651225347586
Iteration: 6 || Loss: 49.73158323613383
Iteration: 7 || Loss: 48.92301139828565
Iteration: 8 || Loss: 48.69375916327312
Iteration: 9 || Loss: 48.55836598915938
Iteration: 10 || Loss: 48.11280530921875
Iteration: 11 || Loss: 47.9341480057854
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:187.92535
Epoch 23 loss:47.9341480057854
waveform batch: 2/3
Using ADAM optimizer
Sum of params:187.92535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 14.479530161588722
Iteration: 2 || Loss: 14.479510607708077
Iteration: 3 || Loss: 14.479490268853597
Iteration: 4 || Loss: 14.479470464119453
Iteration: 5 || Loss: 14.479451182590356
Iteration: 6 || Loss: 14.479451182590356
saving ADAM checkpoint...
Sum of params:187.92535
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 14.479451182590356
Iteration: 2 || Loss: 13.908216599528174
Iteration: 3 || Loss: 13.875205922549453
Iteration: 4 || Loss: 13.822820402016537
Iteration: 5 || Loss: 13.735978788752194
Iteration: 6 || Loss: 13.547874043452182
Iteration: 7 || Loss: 13.524945039673534
Iteration: 8 || Loss: 13.419577128604779
Iteration: 9 || Loss: 13.139017358474485
Iteration: 10 || Loss: 13.12435285112462
Iteration: 11 || Loss: 13.120395901903152
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:188.22072
Epoch 23 loss:13.120395901903152
waveform batch: 3/3
Using ADAM optimizer
Sum of params:188.22072
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.979986930975056
Iteration: 2 || Loss: 11.979985483603013
Iteration: 3 || Loss: 11.979983778177445
Iteration: 4 || Loss: 11.979981950920966
Iteration: 5 || Loss: 11.9799800028993
Iteration: 6 || Loss: 11.9799800028993
saving ADAM checkpoint...
Sum of params:188.22072
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.9799800028993
Iteration: 2 || Loss: 11.974977051798433
Iteration: 3 || Loss: 11.970630878530233
Iteration: 4 || Loss: 11.962438252870566
Iteration: 5 || Loss: 11.91870113015003
Iteration: 6 || Loss: 11.894203926128686
Iteration: 7 || Loss: 11.82005666742658
Iteration: 8 || Loss: 11.72520418723984
Iteration: 9 || Loss: 11.672119804375548
Iteration: 10 || Loss: 11.609401121928785
Iteration: 11 || Loss: 11.524035232537532
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:189.5387
Epoch 23 loss:11.524035232537532
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:14.321765374699146
waveform batch: 2/2
Test loss:14.00134009751742
Epoch 23 mean train loss:2.4192859713408694
Epoch 23 mean test loss:1.6660650277774451
Start training epoch 24
waveform batch: 1/3
Using ADAM optimizer
Sum of params:189.5387
Changing learning rate to:2.384185791015625e-9
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 50.58014638093401
Iteration: 2 || Loss: 50.5801456508041
Iteration: 3 || Loss: 50.580144550792234
Iteration: 4 || Loss: 50.58014310691188
Iteration: 5 || Loss: 50.5801416832983
Iteration: 6 || Loss: 50.5801416832983
saving ADAM checkpoint...
Sum of params:189.53868
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 50.5801416832983
Iteration: 2 || Loss: 50.50193214040312
Iteration: 3 || Loss: 50.38050750568847
Iteration: 4 || Loss: 49.86836018135704
Iteration: 5 || Loss: 48.73051026661965
Iteration: 6 || Loss: 48.582784125575444
Iteration: 7 || Loss: 47.6038360548023
Iteration: 8 || Loss: 47.32344824022301
Iteration: 9 || Loss: 47.117025408459455
Iteration: 10 || Loss: 46.70857533974387
Iteration: 11 || Loss: 46.507126546925754
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:189.47249
Epoch 24 loss:46.507126546925754
waveform batch: 2/3
Using ADAM optimizer
Sum of params:189.47249
Changing learning rate to:1.1920928955078125e-9
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 13.766782878699564
Iteration: 2 || Loss: 13.76678213615954
Iteration: 3 || Loss: 13.766780917211602
Iteration: 4 || Loss: 13.766780033866727
Iteration: 5 || Loss: 13.766778996558653
Iteration: 6 || Loss: 13.766778996558653
saving ADAM checkpoint...
Sum of params:189.47249
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 13.766778996558653
Iteration: 2 || Loss: 13.441287472808071
Iteration: 3 || Loss: 13.418915105681963
Iteration: 4 || Loss: 13.344299077151652
Iteration: 5 || Loss: 13.198968493086973
Iteration: 6 || Loss: 13.065295873469676
Iteration: 7 || Loss: 13.049965404926914
Iteration: 8 || Loss: 12.85063457888615
Iteration: 9 || Loss: 12.661111961759586
Iteration: 10 || Loss: 12.656973726438894
Iteration: 11 || Loss: 12.651882704512374
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:189.71388
Epoch 24 loss:12.651882704512374
waveform batch: 3/3
Using ADAM optimizer
Sum of params:189.71388
Changing learning rate to:5.960464477539063e-10
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.51051148904129
Iteration: 2 || Loss: 11.5105115485113
Iteration: 3 || Loss: 11.510511478169246
Iteration: 4 || Loss: 11.510511388955141
Iteration: 5 || Loss: 11.510511386565462
Iteration: 6 || Loss: 11.510511386565462
saving ADAM checkpoint...
Sum of params:189.71388
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.510511386565462
Iteration: 2 || Loss: 11.507297528485006
Iteration: 3 || Loss: 11.49810517267492
Iteration: 4 || Loss: 11.49248487447201
Iteration: 5 || Loss: 11.456216101779379
Iteration: 6 || Loss: 11.431142322045
Iteration: 7 || Loss: 11.371606222236116
Iteration: 8 || Loss: 11.311191686177175
Iteration: 9 || Loss: 11.25242731643844
Iteration: 10 || Loss: 11.169130793095569
Iteration: 11 || Loss: 11.004351969790795
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:191.181
Epoch 24 loss:11.004351969790795
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:13.636762604014585
waveform batch: 2/2
Test loss:13.511308547071176
Epoch 24 mean train loss:2.3387787073742974
Epoch 24 mean test loss:1.596945361828574
Start training epoch 25
waveform batch: 1/3
Using ADAM optimizer
Sum of params:191.181
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 49.23541550199275
Iteration: 2 || Loss: 49.23541561711591
Iteration: 3 || Loss: 49.235415543650404
Iteration: 4 || Loss: 49.23541528718113
Iteration: 5 || Loss: 49.235415544237966
Iteration: 6 || Loss: 49.23541528718113
saving ADAM checkpoint...
Sum of params:191.181
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 49.23541528718113
Iteration: 2 || Loss: 49.173940138492
Iteration: 3 || Loss: 49.06833049444752
Iteration: 4 || Loss: 48.5907546162517
Iteration: 5 || Loss: 47.5347080190576
Iteration: 6 || Loss: 47.33427386763181
Iteration: 7 || Loss: 46.25187083179898
Iteration: 8 || Loss: 46.004336913747224
Iteration: 9 || Loss: 45.765590294244305
Iteration: 10 || Loss: 45.19500262542453
Iteration: 11 || Loss: 44.95024996673399
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:191.12103
Epoch 25 loss:44.95024996673399
waveform batch: 2/3
Using ADAM optimizer
Sum of params:191.12103
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.980181048061354
Iteration: 2 || Loss: 12.980180742017133
Iteration: 3 || Loss: 12.98018059348299
Iteration: 4 || Loss: 12.98018017774157
Iteration: 5 || Loss: 12.980180091543538
Iteration: 6 || Loss: 12.980180091543538
saving ADAM checkpoint...
Sum of params:191.12103
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.980180091543538
Iteration: 2 || Loss: 12.84401140293499
Iteration: 3 || Loss: 12.819046810959437
Iteration: 4 || Loss: 12.716324086845855
Iteration: 5 || Loss: 12.499034525591805
Iteration: 6 || Loss: 12.438596584784639
Iteration: 7 || Loss: 12.392065985673932
Iteration: 8 || Loss: 12.183757022124334
Iteration: 9 || Loss: 12.1599659268981
Iteration: 10 || Loss: 12.132472658247014
Iteration: 11 || Loss: 12.08534104401594
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:191.35757
Epoch 25 loss:12.08534104401594
waveform batch: 3/3
Using ADAM optimizer
Sum of params:191.35757
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.939652867058635
Iteration: 2 || Loss: 10.939652782680065
Iteration: 3 || Loss: 10.939652715845083
Iteration: 4 || Loss: 10.939652750000581
Iteration: 5 || Loss: 10.93965270630898
Iteration: 6 || Loss: 10.93965270630898
saving ADAM checkpoint...
Sum of params:191.35757
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.93965270630898
Iteration: 2 || Loss: 10.935860672003756
Iteration: 3 || Loss: 10.933349601152578
Iteration: 4 || Loss: 10.926071222326538
Iteration: 5 || Loss: 10.885122900237734
Iteration: 6 || Loss: 10.860073499242827
Iteration: 7 || Loss: 10.828403448491486
Iteration: 8 || Loss: 10.791314013405332
Iteration: 9 || Loss: 10.735558890121213
Iteration: 10 || Loss: 10.64706951346431
Iteration: 11 || Loss: 10.540822351400417
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:192.17915
Epoch 25 loss:10.540822351400417
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:13.057874849666197
waveform batch: 2/2
Test loss:12.88110011054525
Epoch 25 mean train loss:2.2525471120716785
Epoch 25 mean test loss:1.5258220564830263
Start training epoch 26
waveform batch: 1/3
Using ADAM optimizer
Sum of params:192.17915
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 46.08657616395612
Iteration: 2 || Loss: 46.08657660016241
Iteration: 3 || Loss: 46.0865764795181
Iteration: 4 || Loss: 46.08657604475434
Iteration: 5 || Loss: 46.086576774364154
Iteration: 6 || Loss: 46.08657604475434
saving ADAM checkpoint...
Sum of params:192.17915
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 46.08657604475434
Iteration: 2 || Loss: 46.05057911372453
Iteration: 3 || Loss: 46.010880783928755
Iteration: 4 || Loss: 45.79238295689051
Iteration: 5 || Loss: 45.15621985991581
Iteration: 6 || Loss: 44.47767713871412
Iteration: 7 || Loss: 44.085498141512886
Iteration: 8 || Loss: 43.90500249379164
Iteration: 9 || Loss: 43.446571997112734
Iteration: 10 || Loss: 42.970639267741504
Iteration: 11 || Loss: 42.61054357809679
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:192.51889
Epoch 26 loss:42.61054357809679
waveform batch: 2/3
Using ADAM optimizer
Sum of params:192.51889
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.353264174586446
Iteration: 2 || Loss: 12.353264207300128
Iteration: 3 || Loss: 12.353263907611767
Iteration: 4 || Loss: 12.353263972378535
Iteration: 5 || Loss: 12.353263752721304
Iteration: 6 || Loss: 12.353263752721304
saving ADAM checkpoint...
Sum of params:192.51889
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.353263752721304
Iteration: 2 || Loss: 12.331158901315598
Iteration: 3 || Loss: 12.30226739086204
Iteration: 4 || Loss: 12.23851509253213
Iteration: 5 || Loss: 11.915725735077372
Iteration: 6 || Loss: 11.863430484363997
Iteration: 7 || Loss: 11.706213901575705
Iteration: 8 || Loss: 11.642063739186701
Iteration: 9 || Loss: 11.634445672842354
Iteration: 10 || Loss: 11.595395795688683
Iteration: 11 || Loss: 11.580200005287107
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:192.98024
Epoch 26 loss:11.580200005287107
waveform batch: 3/3
Using ADAM optimizer
Sum of params:192.98024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.550945062269209
Iteration: 2 || Loss: 10.550945053641207
Iteration: 3 || Loss: 10.550945086246811
Iteration: 4 || Loss: 10.550944958029499
Iteration: 5 || Loss: 10.550944802525514
Iteration: 6 || Loss: 10.550944802525514
saving ADAM checkpoint...
Sum of params:192.98024
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.550944802525514
Iteration: 2 || Loss: 10.546206311863552
Iteration: 3 || Loss: 10.543482462199224
Iteration: 4 || Loss: 10.531689266474288
Iteration: 5 || Loss: 10.492762388712247
Iteration: 6 || Loss: 10.46680322526665
Iteration: 7 || Loss: 10.439910045711569
Iteration: 8 || Loss: 10.385558704843975
Iteration: 9 || Loss: 10.360130567326504
Iteration: 10 || Loss: 10.255717985013387
Iteration: 11 || Loss: 10.207092186805177
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:193.31924
Epoch 26 loss:10.207092186805177
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:12.720287038542754
waveform batch: 2/2
Test loss:12.390297729364137
Epoch 26 mean train loss:2.1465945256729686
Epoch 26 mean test loss:1.477093221641582
Start training epoch 27
waveform batch: 1/3
Using ADAM optimizer
Sum of params:193.31924
Changing learning rate to:2.9802322387695313e-10
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 43.20900750783697
Iteration: 2 || Loss: 43.20900694905365
Iteration: 3 || Loss: 43.20900707646208
Iteration: 4 || Loss: 43.20900690560512
Iteration: 5 || Loss: 43.20900682771378
Iteration: 6 || Loss: 43.20900682771378
saving ADAM checkpoint...
Sum of params:193.31924
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 43.20900682771378
Iteration: 2 || Loss: 43.191564237991315
Iteration: 3 || Loss: 43.1490615721624
Iteration: 4 || Loss: 43.00003683610296
Iteration: 5 || Loss: 42.64126336773691
Iteration: 6 || Loss: 42.47760887984685
Iteration: 7 || Loss: 41.70415617550941
Iteration: 8 || Loss: 41.24222857674942
Iteration: 9 || Loss: 40.60402102263576
Iteration: 10 || Loss: 40.23949081510211
Iteration: 11 || Loss: 39.72484530766836
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:194.98888
Epoch 27 loss:39.72484530766836
waveform batch: 2/3
Using ADAM optimizer
Sum of params:194.98888
Changing learning rate to:1.4901161193847657e-10
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 12.189736017643774
Iteration: 2 || Loss: 12.18973598454224
Iteration: 3 || Loss: 12.189736000360776
Iteration: 4 || Loss: 12.189735929810043
Iteration: 5 || Loss: 12.189736005996254
Iteration: 6 || Loss: 12.189735929810043
saving ADAM checkpoint...
Sum of params:194.98888
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 12.189735929810043
Iteration: 2 || Loss: 12.175572134858585
Iteration: 3 || Loss: 12.160685393303712
Iteration: 4 || Loss: 11.947317955232025
Iteration: 5 || Loss: 11.714469746694254
Iteration: 6 || Loss: 11.521544076201574
Iteration: 7 || Loss: 11.420729912928717
Iteration: 8 || Loss: 11.356725249462054
Iteration: 9 || Loss: 11.297916821525083
Iteration: 10 || Loss: 11.278681415063383
Iteration: 11 || Loss: 11.220418839622166
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:195.31383
Epoch 27 loss:11.220418839622166
waveform batch: 3/3
Using ADAM optimizer
Sum of params:195.31383
Changing learning rate to:7.450580596923828e-11
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.45997060314248
Iteration: 2 || Loss: 10.459970562244296
Iteration: 3 || Loss: 10.459970534876316
Iteration: 4 || Loss: 10.45997058649879
Iteration: 5 || Loss: 10.459970584690367
Iteration: 6 || Loss: 10.459970534876316
saving ADAM checkpoint...
Sum of params:195.31383
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.459970534876316
Iteration: 2 || Loss: 10.458391825473822
Iteration: 3 || Loss: 10.455605846568657
Iteration: 4 || Loss: 10.431665755887458
Iteration: 5 || Loss: 10.345390526832714
Iteration: 6 || Loss: 10.287289820977037
Iteration: 7 || Loss: 10.208442254397902
Iteration: 8 || Loss: 10.085646099628255
Iteration: 9 || Loss: 10.031324891822402
Iteration: 10 || Loss: 9.953597120288167
Iteration: 11 || Loss: 9.893946743798224
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:195.51888
Epoch 27 loss:9.893946743798224
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:12.401805549912682
waveform batch: 2/2
Test loss:12.019098116165903
Epoch 27 mean train loss:2.0279736963696253
Epoch 27 mean test loss:1.4365237450634463
Start training epoch 28
waveform batch: 1/3
Using ADAM optimizer
Sum of params:195.51888
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 39.96655527825344
Iteration: 2 || Loss: 39.966555203115654
Iteration: 3 || Loss: 39.96655540566103
Iteration: 4 || Loss: 39.96655507635091
Iteration: 5 || Loss: 39.96655520271854
Iteration: 6 || Loss: 39.96655507635091
saving ADAM checkpoint...
Sum of params:195.51888
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 39.96655507635091
Iteration: 2 || Loss: 39.92993308299537
Iteration: 3 || Loss: 39.88519640328745
Iteration: 4 || Loss: 39.761482785721
Iteration: 5 || Loss: 39.52669026178485
Iteration: 6 || Loss: 39.28947603725037
Iteration: 7 || Loss: 39.02812814881199
Iteration: 8 || Loss: 38.668121321154025
Iteration: 9 || Loss: 38.433989857527024
Iteration: 10 || Loss: 38.28409575138387
Iteration: 11 || Loss: 37.95840276113929
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:197.1539
Epoch 28 loss:37.95840276113929
waveform batch: 2/3
Using ADAM optimizer
Sum of params:197.1539
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.507667615404442
Iteration: 2 || Loss: 11.507667633973933
Iteration: 3 || Loss: 11.507667716393
Iteration: 4 || Loss: 11.507667563101863
Iteration: 5 || Loss: 11.507667703473304
Iteration: 6 || Loss: 11.507667563101863
saving ADAM checkpoint...
Sum of params:197.1539
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.507667563101863
Iteration: 2 || Loss: 11.501423892231248
Iteration: 3 || Loss: 11.29865053900727
Iteration: 4 || Loss: 11.181731029830972
Iteration: 5 || Loss: 11.092739629477625
Iteration: 6 || Loss: 10.920067811386158
Iteration: 7 || Loss: 10.851697331865184
Iteration: 8 || Loss: 10.786543910362237
Iteration: 9 || Loss: 10.772264230428785
Iteration: 10 || Loss: 10.725049443475566
Iteration: 11 || Loss: 10.700653364160047
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:197.2012
Epoch 28 loss:10.700653364160047
waveform batch: 3/3
Using ADAM optimizer
Sum of params:197.2012
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.88386118458078
Iteration: 2 || Loss: 9.883861224352247
Iteration: 3 || Loss: 9.883861241716511
Iteration: 4 || Loss: 9.883861205395839
Iteration: 5 || Loss: 9.883861086583932
Iteration: 6 || Loss: 9.883861086583932
saving ADAM checkpoint...
Sum of params:197.2012
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.883861086583932
Iteration: 2 || Loss: 9.883210269439308
Iteration: 3 || Loss: 9.880435191449127
Iteration: 4 || Loss: 9.86388207193596
Iteration: 5 || Loss: 9.83883636994567
Iteration: 6 || Loss: 9.81836219529504
Iteration: 7 || Loss: 9.688602592798143
Iteration: 8 || Loss: 9.641008437979888
Iteration: 9 || Loss: 9.583783302403948
Iteration: 10 || Loss: 9.505066288103798
Iteration: 11 || Loss: 9.468054591935188
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:197.40518
Epoch 28 loss:9.468054591935188
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:12.016341860604667
waveform batch: 2/2
Test loss:11.682918765704947
Epoch 28 mean train loss:1.9375703572411507
Epoch 28 mean test loss:1.394074154488801
Start training epoch 29
waveform batch: 1/3
Using ADAM optimizer
Sum of params:197.40518
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 38.593858899734485
Iteration: 2 || Loss: 38.59385868219872
Iteration: 3 || Loss: 38.59385865450006
Iteration: 4 || Loss: 38.59385873871443
Iteration: 5 || Loss: 38.59385890614135
Iteration: 6 || Loss: 38.59385865450006
saving ADAM checkpoint...
Sum of params:197.40518
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 38.59385865450006
Iteration: 2 || Loss: 38.56844642028264
Iteration: 3 || Loss: 38.51640930572569
Iteration: 4 || Loss: 38.160194597137824
Iteration: 5 || Loss: 38.08468351351718
Iteration: 6 || Loss: 37.823152118200895
Iteration: 7 || Loss: 37.31526458598492
Iteration: 8 || Loss: 37.20311816360522
Iteration: 9 || Loss: 36.98454347787984
Iteration: 10 || Loss: 36.800868186917825
Iteration: 11 || Loss: 36.33549625660882
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:199.37718
Epoch 29 loss:36.33549625660882
waveform batch: 2/3
Using ADAM optimizer
Sum of params:199.37718
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 11.264154811921811
Iteration: 2 || Loss: 11.264154838189494
Iteration: 3 || Loss: 11.264154799507269
Iteration: 4 || Loss: 11.264154594387369
Iteration: 5 || Loss: 11.264154587525704
Iteration: 6 || Loss: 11.264154587525704
saving ADAM checkpoint...
Sum of params:199.37718
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 11.264154587525704
Iteration: 2 || Loss: 11.1365450800467
Iteration: 3 || Loss: 11.115609537668028
Iteration: 4 || Loss: 10.835806335975414
Iteration: 5 || Loss: 10.721725336076846
Iteration: 6 || Loss: 10.679221323446997
Iteration: 7 || Loss: 10.499372475324193
Iteration: 8 || Loss: 10.474148301454605
Iteration: 9 || Loss: 10.41758826088337
Iteration: 10 || Loss: 10.374528860803265
Iteration: 11 || Loss: 10.369365698886606
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:199.42496
Epoch 29 loss:10.369365698886606
waveform batch: 3/3
Using ADAM optimizer
Sum of params:199.42496
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.647155661171974
Iteration: 2 || Loss: 9.647155687755186
Iteration: 3 || Loss: 9.647155664433894
Iteration: 4 || Loss: 9.64715563691044
Iteration: 5 || Loss: 9.647155624766043
Iteration: 6 || Loss: 9.647155624766043
saving ADAM checkpoint...
Sum of params:199.42496
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.647155624766043
Iteration: 2 || Loss: 9.63646422825889
Iteration: 3 || Loss: 9.62814021251869
Iteration: 4 || Loss: 9.597331668159963
Iteration: 5 || Loss: 9.547428908342798
Iteration: 6 || Loss: 9.527825329422445
Iteration: 7 || Loss: 9.49199091286453
Iteration: 8 || Loss: 9.456831511548984
Iteration: 9 || Loss: 9.410402381809211
Iteration: 10 || Loss: 9.35519146359872
Iteration: 11 || Loss: 9.306709467709492
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:199.07422
Epoch 29 loss:9.306709467709492
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:11.759287204978637
waveform batch: 2/2
Test loss:11.276028126661867
Epoch 29 mean train loss:1.8670523807734971
Epoch 29 mean test loss:1.3550185489200297
Start training epoch 30
waveform batch: 1/3
Using ADAM optimizer
Sum of params:199.07422
Changing learning rate to:3.725290298461914e-11
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 36.21038917294062
Iteration: 2 || Loss: 36.21038914298095
Iteration: 3 || Loss: 36.21038914194176
Iteration: 4 || Loss: 36.21038902166061
Iteration: 5 || Loss: 36.21038902165995
Iteration: 6 || Loss: 36.21038902165995
saving ADAM checkpoint...
Sum of params:199.07422
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 36.21038902165995
Iteration: 2 || Loss: 36.19669385179418
Iteration: 3 || Loss: 36.185453174528114
Iteration: 4 || Loss: 36.01754085540043
Iteration: 5 || Loss: 35.960953880886514
Iteration: 6 || Loss: 35.843124049596824
Iteration: 7 || Loss: 35.623292284440154
Iteration: 8 || Loss: 35.289787993817924
Iteration: 9 || Loss: 35.124734663737954
Iteration: 10 || Loss: 34.90149810617456
Iteration: 11 || Loss: 34.70697817413623
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:201.16043
Epoch 30 loss:34.70697817413623
waveform batch: 2/3
Using ADAM optimizer
Sum of params:201.16043
Changing learning rate to:1.862645149230957e-11
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.80871002628409
Iteration: 2 || Loss: 10.808710005456904
Iteration: 3 || Loss: 10.808710139600707
Iteration: 4 || Loss: 10.808710125259049
Iteration: 5 || Loss: 10.808710090332712
Iteration: 6 || Loss: 10.808710005456904
saving ADAM checkpoint...
Sum of params:201.16043
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.808710005456904
Iteration: 2 || Loss: 10.737762304931286
Iteration: 3 || Loss: 10.694068146849173
Iteration: 4 || Loss: 10.665805130313128
Iteration: 5 || Loss: 10.462754130044718
Iteration: 6 || Loss: 10.403225049386041
Iteration: 7 || Loss: 10.223656086426306
Iteration: 8 || Loss: 10.208307515051692
Iteration: 9 || Loss: 10.176281909001982
Iteration: 10 || Loss: 10.151095711787457
Iteration: 11 || Loss: 10.130897137667397
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:201.01012
Epoch 30 loss:10.130897137667397
waveform batch: 3/3
Using ADAM optimizer
Sum of params:201.01012
Changing learning rate to:9.313225746154785e-12
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.463694426752069
Iteration: 2 || Loss: 9.463694426752069
Iteration: 3 || Loss: 9.463694426752069
Iteration: 4 || Loss: 9.463694426752069
Iteration: 5 || Loss: 9.463694426752069
Iteration: 6 || Loss: 9.463694426752069
saving ADAM checkpoint...
Sum of params:201.01012
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.463694426752069
Iteration: 2 || Loss: 9.456975101499129
Iteration: 3 || Loss: 9.45229469974601
Iteration: 4 || Loss: 9.44349022226447
Iteration: 5 || Loss: 9.395529862090775
Iteration: 6 || Loss: 9.349741369834671
Iteration: 7 || Loss: 9.314714271210612
Iteration: 8 || Loss: 9.290812840076109
Iteration: 9 || Loss: 9.240770866466919
Iteration: 10 || Loss: 9.18013272224057
Iteration: 11 || Loss: 9.171490965076925
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:200.60611
Epoch 30 loss:9.171490965076925
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:11.568188523691832
waveform batch: 2/2
Test loss:11.021103361396879
Epoch 30 mean train loss:1.8003122092293518
Epoch 30 mean test loss:1.3287818755934537
Start training epoch 31
waveform batch: 1/3
Using ADAM optimizer
Sum of params:200.60611
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 34.560914219700685
Iteration: 2 || Loss: 34.56091419393085
Iteration: 3 || Loss: 34.560914184720545
Iteration: 4 || Loss: 34.560914184720545
Iteration: 5 || Loss: 34.560914184720545
Iteration: 6 || Loss: 34.560914184720545
saving ADAM checkpoint...
Sum of params:200.60611
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 34.560914184720545
Iteration: 2 || Loss: 34.54407402416922
Iteration: 3 || Loss: 34.525980761107306
Iteration: 4 || Loss: 34.4393546632269
Iteration: 5 || Loss: 34.345674567913036
Iteration: 6 || Loss: 34.268838212808156
Iteration: 7 || Loss: 34.1094270546119
Iteration: 8 || Loss: 33.80391748673937
Iteration: 9 || Loss: 33.70892198523158
Iteration: 10 || Loss: 33.59605591062603
Iteration: 11 || Loss: 33.33936715938647
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:203.06915
Epoch 31 loss:33.33936715938647
waveform batch: 2/3
Using ADAM optimizer
Sum of params:203.06915
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.516298833490021
Iteration: 2 || Loss: 10.516298833490021
Iteration: 3 || Loss: 10.516298833490021
Iteration: 4 || Loss: 10.516298833490021
Iteration: 5 || Loss: 10.516298767005368
Iteration: 6 || Loss: 10.516298767005368
saving ADAM checkpoint...
Sum of params:203.06915
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.516298767005368
Iteration: 2 || Loss: 10.5059720203233
Iteration: 3 || Loss: 10.490218479531112
Iteration: 4 || Loss: 10.445247757793782
Iteration: 5 || Loss: 10.224728170754664
Iteration: 6 || Loss: 10.092571702658553
Iteration: 7 || Loss: 10.007977606081147
Iteration: 8 || Loss: 9.978627567644903
Iteration: 9 || Loss: 9.94962405386223
Iteration: 10 || Loss: 9.90853760133256
Iteration: 11 || Loss: 9.821258985341514
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:202.5262
Epoch 31 loss:9.821258985341514
waveform batch: 3/3
Using ADAM optimizer
Sum of params:202.5262
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.28319070424405
Iteration: 2 || Loss: 9.28319070424405
Iteration: 3 || Loss: 9.28319069291435
Iteration: 4 || Loss: 9.283190667652432
Iteration: 5 || Loss: 9.283190667652432
Iteration: 6 || Loss: 9.283190667652432
saving ADAM checkpoint...
Sum of params:202.5262
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.283190667652432
Iteration: 2 || Loss: 9.265536345825412
Iteration: 3 || Loss: 9.262271516988022
Iteration: 4 || Loss: 9.240206456590226
Iteration: 5 || Loss: 9.209221569871714
Iteration: 6 || Loss: 9.186414546857874
Iteration: 7 || Loss: 9.166693558545138
Iteration: 8 || Loss: 9.11916168783113
Iteration: 9 || Loss: 9.106868096506757
Iteration: 10 || Loss: 9.061956058101757
Iteration: 11 || Loss: 9.013061955558733
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:202.1401
Epoch 31 loss:9.013061955558733
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:11.277459479595407
waveform batch: 2/2
Test loss:10.837737404427443
Epoch 31 mean train loss:1.739122936676224
Epoch 31 mean test loss:1.3008939343542854
Start training epoch 32
waveform batch: 1/3
Using ADAM optimizer
Sum of params:202.1401
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 33.03746721644959
Iteration: 2 || Loss: 33.03746721644959
Iteration: 3 || Loss: 33.03746721620189
Iteration: 4 || Loss: 33.03746721620189
Iteration: 5 || Loss: 33.03746721620189
Iteration: 6 || Loss: 33.03746721620189
saving ADAM checkpoint...
Sum of params:202.1401
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 33.03746721620189
Iteration: 2 || Loss: 32.98012802322251
Iteration: 3 || Loss: 32.96744128998375
Iteration: 4 || Loss: 32.93887710544517
Iteration: 5 || Loss: 32.877203979868575
Iteration: 6 || Loss: 32.69748530816717
Iteration: 7 || Loss: 32.65708543388374
Iteration: 8 || Loss: 32.57780940623316
Iteration: 9 || Loss: 32.48272446539907
Iteration: 10 || Loss: 32.28307195366012
Iteration: 11 || Loss: 32.114839583208315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:204.04645
Epoch 32 loss:32.114839583208315
waveform batch: 2/3
Using ADAM optimizer
Sum of params:204.04645
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 10.511374021348162
Iteration: 2 || Loss: 10.511374021348162
Iteration: 3 || Loss: 10.511374021348162
Iteration: 4 || Loss: 10.511374021348162
Iteration: 5 || Loss: 10.511374021348162
Iteration: 6 || Loss: 10.511374021348162
saving ADAM checkpoint...
Sum of params:204.04645
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 10.511374021348162
Iteration: 2 || Loss: 10.388729575811066
Iteration: 3 || Loss: 10.313149119745518
Iteration: 4 || Loss: 10.23500058145872
Iteration: 5 || Loss: 10.08069945958099
Iteration: 6 || Loss: 9.87826805763391
Iteration: 7 || Loss: 9.853170411503626
Iteration: 8 || Loss: 9.779402108775962
Iteration: 9 || Loss: 9.761585173516726
Iteration: 10 || Loss: 9.699921347771244
Iteration: 11 || Loss: 9.689468232323195
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:203.97507
Epoch 32 loss:9.689468232323195
waveform batch: 3/3
Using ADAM optimizer
Sum of params:203.97507
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.16634266827712
Iteration: 2 || Loss: 9.16634266827712
Iteration: 3 || Loss: 9.16634266827712
Iteration: 4 || Loss: 9.16634266827712
Iteration: 5 || Loss: 9.16634266827712
Iteration: 6 || Loss: 9.16634266827712
saving ADAM checkpoint...
Sum of params:203.97507
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.16634266827712
Iteration: 2 || Loss: 9.164740747835618
Iteration: 3 || Loss: 9.162844479298972
Iteration: 4 || Loss: 9.149145094287066
Iteration: 5 || Loss: 9.08614330320527
Iteration: 6 || Loss: 9.049895183910367
Iteration: 7 || Loss: 9.008699330687675
Iteration: 8 || Loss: 8.975909888192245
Iteration: 9 || Loss: 8.90964361777331
Iteration: 10 || Loss: 8.890204016710754
Iteration: 11 || Loss: 8.829172798372765
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:203.4347
Epoch 32 loss:8.829172798372765
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:11.117776329472136
waveform batch: 2/2
Test loss:10.648101537466601
Epoch 32 mean train loss:1.6877826871301425
Epoch 32 mean test loss:1.2803457568787493
Start training epoch 33
waveform batch: 1/3
Using ADAM optimizer
Sum of params:203.4347
Changing learning rate to:4.656612873077393e-12
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 32.0427946058202
Iteration: 2 || Loss: 32.0427946058202
Iteration: 3 || Loss: 32.0427946058202
Iteration: 4 || Loss: 32.0427946058202
Iteration: 5 || Loss: 32.0427946058202
Iteration: 6 || Loss: 32.0427946058202
saving ADAM checkpoint...
Sum of params:203.4347
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 32.0427946058202
Iteration: 2 || Loss: 32.0353148808905
Iteration: 3 || Loss: 32.02017557305928
Iteration: 4 || Loss: 31.950037464490055
Iteration: 5 || Loss: 31.88553024590176
Iteration: 6 || Loss: 31.771190717102712
Iteration: 7 || Loss: 31.602595516653317
Iteration: 8 || Loss: 31.50797015254859
Iteration: 9 || Loss: 31.383136536874474
Iteration: 10 || Loss: 31.254481704914898
Iteration: 11 || Loss: 31.158107531705753
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:205.31516
Epoch 33 loss:31.158107531705753
waveform batch: 2/3
Using ADAM optimizer
Sum of params:205.31516
Changing learning rate to:2.3283064365386963e-12
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.907087178254118
Iteration: 2 || Loss: 9.907087178254118
Iteration: 3 || Loss: 9.907087178254118
Iteration: 4 || Loss: 9.907087178254118
Iteration: 5 || Loss: 9.907087178254118
Iteration: 6 || Loss: 9.907087178254118
saving ADAM checkpoint...
Sum of params:205.31516
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.907087178254118
Iteration: 2 || Loss: 9.901216988256799
Iteration: 3 || Loss: 9.885775616770749
Iteration: 4 || Loss: 9.844276300185374
Iteration: 5 || Loss: 9.659862355190851
Iteration: 6 || Loss: 9.588394161155419
Iteration: 7 || Loss: 9.500683003110284
Iteration: 8 || Loss: 9.470018118529444
Iteration: 9 || Loss: 9.429781233858387
Iteration: 10 || Loss: 9.390962539706013
Iteration: 11 || Loss: 9.317560082986109
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:204.6022
Epoch 33 loss:9.317560082986109
waveform batch: 3/3
Using ADAM optimizer
Sum of params:204.6022
Changing learning rate to:1.1641532182693482e-12
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.853326930212939
Iteration: 2 || Loss: 8.853326930212939
Iteration: 3 || Loss: 8.853326930212939
Iteration: 4 || Loss: 8.853326930212939
Iteration: 5 || Loss: 8.853326930212939
Iteration: 6 || Loss: 8.853326930212939
saving ADAM checkpoint...
Sum of params:204.6022
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.853326930212939
Iteration: 2 || Loss: 8.852115740064857
Iteration: 3 || Loss: 8.847951271893308
Iteration: 4 || Loss: 8.840230310160274
Iteration: 5 || Loss: 8.792343164836769
Iteration: 6 || Loss: 8.75070086016327
Iteration: 7 || Loss: 8.724134640576711
Iteration: 8 || Loss: 8.682554399167213
Iteration: 9 || Loss: 8.644879065863009
Iteration: 10 || Loss: 8.605972942378921
Iteration: 11 || Loss: 8.55581455370151
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:204.52411
Epoch 33 loss:8.55581455370151
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:10.861376145608551
waveform batch: 2/2
Test loss:10.39738822137515
Epoch 33 mean train loss:1.6343827389464456
Epoch 33 mean test loss:1.2505155509990413
Start training epoch 34
waveform batch: 1/3
Using ADAM optimizer
Sum of params:204.52411
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 31.33330818664891
Iteration: 2 || Loss: 31.33330818664891
Iteration: 3 || Loss: 31.33330818664891
Iteration: 4 || Loss: 31.33330818664891
Iteration: 5 || Loss: 31.33330818664891
Iteration: 6 || Loss: 31.33330818664891
saving ADAM checkpoint...
Sum of params:204.52411
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 31.33330818664891
Iteration: 2 || Loss: 31.277218198607255
Iteration: 3 || Loss: 31.231954928078373
Iteration: 4 || Loss: 31.11531014070389
Iteration: 5 || Loss: 31.03868764959801
Iteration: 6 || Loss: 30.95412576785138
Iteration: 7 || Loss: 30.887493777181998
Iteration: 8 || Loss: 30.792328720285425
Iteration: 9 || Loss: 30.702697406064544
Iteration: 10 || Loss: 30.67316151661676
Iteration: 11 || Loss: 30.38164167894794
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:205.58783
Epoch 34 loss:30.38164167894794
waveform batch: 2/3
Using ADAM optimizer
Sum of params:205.58783
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.751553042591071
Iteration: 2 || Loss: 9.751553042591071
Iteration: 3 || Loss: 9.751553042591071
Iteration: 4 || Loss: 9.751553042591071
Iteration: 5 || Loss: 9.751553042591071
Iteration: 6 || Loss: 9.751553042591071
saving ADAM checkpoint...
Sum of params:205.58783
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.751553042591071
Iteration: 2 || Loss: 9.569085451634754
Iteration: 3 || Loss: 9.504357611676047
Iteration: 4 || Loss: 9.37961499382338
Iteration: 5 || Loss: 9.341848700173585
Iteration: 6 || Loss: 9.327310675945146
Iteration: 7 || Loss: 9.236144204371856
Iteration: 8 || Loss: 9.19853900184735
Iteration: 9 || Loss: 9.183271053014199
Iteration: 10 || Loss: 9.15391891587593
Iteration: 11 || Loss: 9.151260960743581
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:205.51477
Epoch 34 loss:9.151260960743581
waveform batch: 3/3
Using ADAM optimizer
Sum of params:205.51477
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.656124205792421
Iteration: 2 || Loss: 8.656124205792421
Iteration: 3 || Loss: 8.656124205792421
Iteration: 4 || Loss: 8.656124205792421
Iteration: 5 || Loss: 8.656124205792421
Iteration: 6 || Loss: 8.656124205792421
saving ADAM checkpoint...
Sum of params:205.51477
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.656124205792421
Iteration: 2 || Loss: 8.655092668474389
Iteration: 3 || Loss: 8.652695039843026
Iteration: 4 || Loss: 8.63075115667099
Iteration: 5 || Loss: 8.581581934131654
Iteration: 6 || Loss: 8.56324385178159
Iteration: 7 || Loss: 8.528768229804912
Iteration: 8 || Loss: 8.500212849955291
Iteration: 9 || Loss: 8.466275148921516
Iteration: 10 || Loss: 8.445785901637539
Iteration: 11 || Loss: 8.413445390590248
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:205.13503
Epoch 34 loss:8.413445390590248
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:10.630387178047656
waveform batch: 2/2
Test loss:10.227005911349975
Epoch 34 mean train loss:1.5982116010093923
Epoch 34 mean test loss:1.2269054758469196
Start training epoch 35
waveform batch: 1/3
Using ADAM optimizer
Sum of params:205.13503
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.626459624823728
Iteration: 2 || Loss: 30.626459624823728
Iteration: 3 || Loss: 30.626459624823728
Iteration: 4 || Loss: 30.626459624823728
Iteration: 5 || Loss: 30.626459624823728
Iteration: 6 || Loss: 30.626459624823728
saving ADAM checkpoint...
Sum of params:205.13503
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.626459624823728
Iteration: 2 || Loss: 30.596095605031298
Iteration: 3 || Loss: 30.584509141542064
Iteration: 4 || Loss: 30.515921907349252
Iteration: 5 || Loss: 30.46591903801921
Iteration: 6 || Loss: 30.26676490411773
Iteration: 7 || Loss: 30.200446107663325
Iteration: 8 || Loss: 30.117707154442623
Iteration: 9 || Loss: 30.087174772815104
Iteration: 10 || Loss: 29.972523877629673
Iteration: 11 || Loss: 29.826825045426762
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:206.16565
Epoch 35 loss:29.826825045426762
waveform batch: 2/3
Using ADAM optimizer
Sum of params:206.16565
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.35183223028861
Iteration: 2 || Loss: 9.35183223028861
Iteration: 3 || Loss: 9.35183223028861
Iteration: 4 || Loss: 9.35183223028861
Iteration: 5 || Loss: 9.35183223028861
Iteration: 6 || Loss: 9.35183223028861
saving ADAM checkpoint...
Sum of params:206.16565
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.35183223028861
Iteration: 2 || Loss: 9.31777889775763
Iteration: 3 || Loss: 9.273102241565796
Iteration: 4 || Loss: 9.242988207195774
Iteration: 5 || Loss: 9.217221695963795
Iteration: 6 || Loss: 9.112087321418365
Iteration: 7 || Loss: 9.06847107042531
Iteration: 8 || Loss: 9.049646561613768
Iteration: 9 || Loss: 9.00056893436593
Iteration: 10 || Loss: 8.988525382577635
Iteration: 11 || Loss: 8.9645685357535
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:205.8785
Epoch 35 loss:8.9645685357535
waveform batch: 3/3
Using ADAM optimizer
Sum of params:205.8785
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.471221962403266
Iteration: 2 || Loss: 8.471221962403266
Iteration: 3 || Loss: 8.471221962403266
Iteration: 4 || Loss: 8.47122196193109
Iteration: 5 || Loss: 8.47122196193109
Iteration: 6 || Loss: 8.47122196193109
saving ADAM checkpoint...
Sum of params:205.8785
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.47122196193109
Iteration: 2 || Loss: 8.470954220813741
Iteration: 3 || Loss: 8.458923891239289
Iteration: 4 || Loss: 8.452167205469216
Iteration: 5 || Loss: 8.436433543018879
Iteration: 6 || Loss: 8.394024977056832
Iteration: 7 || Loss: 8.386442348750123
Iteration: 8 || Loss: 8.346130509903649
Iteration: 9 || Loss: 8.317794259526652
Iteration: 10 || Loss: 8.286574541096751
Iteration: 11 || Loss: 8.232712724074943
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:205.82326
Epoch 35 loss:8.232712724074943
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:10.52313459244432
waveform batch: 2/2
Test loss:10.055738270000454
Epoch 35 mean train loss:1.5674702101751734
Epoch 35 mean test loss:1.2105219330849868
Start training epoch 36
waveform batch: 1/3
Using ADAM optimizer
Sum of params:205.82326
Changing learning rate to:5.820766091346741e-13
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 30.2730874531832
Iteration: 2 || Loss: 30.2730874531832
Iteration: 3 || Loss: 30.2730874531832
Iteration: 4 || Loss: 30.2730874531832
Iteration: 5 || Loss: 30.2730874531832
Iteration: 6 || Loss: 30.2730874531832
saving ADAM checkpoint...
Sum of params:205.82326
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 30.2730874531832
Iteration: 2 || Loss: 30.221083799131197
Iteration: 3 || Loss: 30.18065620730856
Iteration: 4 || Loss: 30.13087704576637
Iteration: 5 || Loss: 30.027059258522726
Iteration: 6 || Loss: 29.867392229816538
Iteration: 7 || Loss: 29.77405238561935
Iteration: 8 || Loss: 29.708747386642965
Iteration: 9 || Loss: 29.633857117401302
Iteration: 10 || Loss: 29.593896361639597
Iteration: 11 || Loss: 29.504262858414144
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:206.28862
Epoch 36 loss:29.504262858414144
waveform batch: 2/3
Using ADAM optimizer
Sum of params:206.28862
Changing learning rate to:2.9103830456733704e-13
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 9.112102660901853
Iteration: 2 || Loss: 9.112102660901853
Iteration: 3 || Loss: 9.112102660901853
Iteration: 4 || Loss: 9.112102660901853
Iteration: 5 || Loss: 9.112102660901853
Iteration: 6 || Loss: 9.112102660901853
saving ADAM checkpoint...
Sum of params:206.28862
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 9.112102660901853
Iteration: 2 || Loss: 9.049935552678848
Iteration: 3 || Loss: 9.027941213938997
Iteration: 4 || Loss: 8.970181259348285
Iteration: 5 || Loss: 8.948982727122232
Iteration: 6 || Loss: 8.911083285030648
Iteration: 7 || Loss: 8.882578171678269
Iteration: 8 || Loss: 8.875765262471262
Iteration: 9 || Loss: 8.832352657733033
Iteration: 10 || Loss: 8.82591430723442
Iteration: 11 || Loss: 8.804664827279561
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:206.10379
Epoch 36 loss:8.804664827279561
waveform batch: 3/3
Using ADAM optimizer
Sum of params:206.10379
Changing learning rate to:1.4551915228366852e-13
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.22373274132822
Iteration: 2 || Loss: 8.22373274132822
Iteration: 3 || Loss: 8.22373274132822
Iteration: 4 || Loss: 8.22373274132822
Iteration: 5 || Loss: 8.22373274132822
Iteration: 6 || Loss: 8.22373274132822
saving ADAM checkpoint...
Sum of params:206.10379
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.22373274132822
Iteration: 2 || Loss: 8.223003193023665
Iteration: 3 || Loss: 8.21980861438434
Iteration: 4 || Loss: 8.214526649708716
Iteration: 5 || Loss: 8.195636418110775
Iteration: 6 || Loss: 8.17867640002462
Iteration: 7 || Loss: 8.154820698162569
Iteration: 8 || Loss: 8.132514234468134
Iteration: 9 || Loss: 8.0976052299179
Iteration: 10 || Loss: 8.051445353243858
Iteration: 11 || Loss: 8.017932017645295
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:206.21854
Epoch 36 loss:8.017932017645295
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:10.270087640002505
waveform batch: 2/2
Test loss:9.851628188079406
Epoch 36 mean train loss:1.5442286567779666
Epoch 36 mean test loss:1.1836303428283477
Start training epoch 37
waveform batch: 1/3
Using ADAM optimizer
Sum of params:206.21854
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.96500381409551
Iteration: 2 || Loss: 29.96500381409551
Iteration: 3 || Loss: 29.96500381409551
Iteration: 4 || Loss: 29.96500381409551
Iteration: 5 || Loss: 29.96500381409551
Iteration: 6 || Loss: 29.96500381409551
saving ADAM checkpoint...
Sum of params:206.21854
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.96500381409551
Iteration: 2 || Loss: 29.918104773939387
Iteration: 3 || Loss: 29.903640053233673
Iteration: 4 || Loss: 29.82934129377431
Iteration: 5 || Loss: 29.763247400563515
Iteration: 6 || Loss: 29.606529516624995
Iteration: 7 || Loss: 29.478721382085812
Iteration: 8 || Loss: 29.32306811321694
Iteration: 9 || Loss: 29.287711288430017
Iteration: 10 || Loss: 29.0890355897667
Iteration: 11 || Loss: 28.917828278805384
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:206.96841
Epoch 37 loss:28.917828278805384
waveform batch: 2/3
Using ADAM optimizer
Sum of params:206.96841
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.995512141546653
Iteration: 2 || Loss: 8.995512141546653
Iteration: 3 || Loss: 8.995512141546653
Iteration: 4 || Loss: 8.995512141546653
Iteration: 5 || Loss: 8.995512141546653
Iteration: 6 || Loss: 8.995512141546653
saving ADAM checkpoint...
Sum of params:206.96841
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.995512141546653
Iteration: 2 || Loss: 8.992176872073307
Iteration: 3 || Loss: 8.976246634478189
Iteration: 4 || Loss: 8.945017305221489
Iteration: 5 || Loss: 8.8905648027813
Iteration: 6 || Loss: 8.771855006461688
Iteration: 7 || Loss: 8.714527453899949
Iteration: 8 || Loss: 8.647937565160309
Iteration: 9 || Loss: 8.628081144954413
Iteration: 10 || Loss: 8.6052236679839
Iteration: 11 || Loss: 8.588867559377185
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:206.71866
Epoch 37 loss:8.588867559377185
waveform batch: 3/3
Using ADAM optimizer
Sum of params:206.71866
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.057544940342337
Iteration: 2 || Loss: 8.057544940342337
Iteration: 3 || Loss: 8.057544940342337
Iteration: 4 || Loss: 8.057544940342337
Iteration: 5 || Loss: 8.057544940342337
Iteration: 6 || Loss: 8.057544940342337
saving ADAM checkpoint...
Sum of params:206.71866
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.057544940342337
Iteration: 2 || Loss: 8.055412191948758
Iteration: 3 || Loss: 8.054147758841117
Iteration: 4 || Loss: 8.050704782665234
Iteration: 5 || Loss: 8.00807368302693
Iteration: 6 || Loss: 7.999205597194022
Iteration: 7 || Loss: 7.956966026929431
Iteration: 8 || Loss: 7.935130028963768
Iteration: 9 || Loss: 7.896103141702945
Iteration: 10 || Loss: 7.873811275210657
Iteration: 11 || Loss: 7.843797867534796
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:206.84607
Epoch 37 loss:7.843797867534796
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:10.053338846025879
waveform batch: 2/2
Test loss:9.585638308741487
Epoch 37 mean train loss:1.511683123523912
Epoch 37 mean test loss:1.1552339502804334
Start training epoch 38
waveform batch: 1/3
Using ADAM optimizer
Sum of params:206.84607
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 29.24013415516734
Iteration: 2 || Loss: 29.24013415516734
Iteration: 3 || Loss: 29.24013415516734
Iteration: 4 || Loss: 29.24013415516734
Iteration: 5 || Loss: 29.24013415516734
Iteration: 6 || Loss: 29.24013415516734
saving ADAM checkpoint...
Sum of params:206.84607
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 29.24013415516734
Iteration: 2 || Loss: 29.220189295070515
Iteration: 3 || Loss: 29.197736654196774
Iteration: 4 || Loss: 29.091373368531876
Iteration: 5 || Loss: 29.02709121616329
Iteration: 6 || Loss: 28.9494549749397
Iteration: 7 || Loss: 28.692017977185667
Iteration: 8 || Loss: 28.586620210430986
Iteration: 9 || Loss: 28.424923711817286
Iteration: 10 || Loss: 28.18056794633569
Iteration: 11 || Loss: 28.139422470601563
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:207.70105
Epoch 38 loss:28.139422470601563
waveform batch: 2/3
Using ADAM optimizer
Sum of params:207.70105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.834493767454042
Iteration: 2 || Loss: 8.834493767454042
Iteration: 3 || Loss: 8.834493767454042
Iteration: 4 || Loss: 8.834493767454042
Iteration: 5 || Loss: 8.834493767454042
Iteration: 6 || Loss: 8.834493767454042
saving ADAM checkpoint...
Sum of params:207.70105
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.834493767454042
Iteration: 2 || Loss: 8.777302291380755
Iteration: 3 || Loss: 8.759138837402464
Iteration: 4 || Loss: 8.731947256551747
Iteration: 5 || Loss: 8.703741174398921
Iteration: 6 || Loss: 8.570702162170704
Iteration: 7 || Loss: 8.555987672103502
Iteration: 8 || Loss: 8.496260255835857
Iteration: 9 || Loss: 8.477420007344652
Iteration: 10 || Loss: 8.46930499127717
Iteration: 11 || Loss: 8.441365039074475
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:207.39394
Epoch 38 loss:8.441365039074475
waveform batch: 3/3
Using ADAM optimizer
Sum of params:207.39394
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.961080740729185
Iteration: 2 || Loss: 7.961080740729185
Iteration: 3 || Loss: 7.961080740729185
Iteration: 4 || Loss: 7.961080740729185
Iteration: 5 || Loss: 7.961080740729185
Iteration: 6 || Loss: 7.961080740729185
saving ADAM checkpoint...
Sum of params:207.39394
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.961080740729185
Iteration: 2 || Loss: 7.960178251131816
Iteration: 3 || Loss: 7.956611390516783
Iteration: 4 || Loss: 7.9502687765642435
Iteration: 5 || Loss: 7.911302911285798
Iteration: 6 || Loss: 7.897706918638928
Iteration: 7 || Loss: 7.853689114001636
Iteration: 8 || Loss: 7.8044668445570515
Iteration: 9 || Loss: 7.76846081585865
Iteration: 10 || Loss: 7.727832371103597
Iteration: 11 || Loss: 7.707004465538192
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:207.51367
Epoch 38 loss:7.707004465538192
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.868935880190723
waveform batch: 2/2
Test loss:9.417866100908164
Epoch 38 mean train loss:1.476259732507141
Epoch 38 mean test loss:1.1345177635940522
Start training epoch 39
waveform batch: 1/3
Using ADAM optimizer
Sum of params:207.51367
Changing learning rate to:7.275957614183426e-14
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 28.49006216838057
Iteration: 2 || Loss: 28.49006216838057
Iteration: 3 || Loss: 28.49006216838057
Iteration: 4 || Loss: 28.49006216838057
Iteration: 5 || Loss: 28.49006216838057
Iteration: 6 || Loss: 28.49006216838057
saving ADAM checkpoint...
Sum of params:207.51367
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 28.49006216838057
Iteration: 2 || Loss: 28.46986771797959
Iteration: 3 || Loss: 28.455595843476242
Iteration: 4 || Loss: 28.34991674719109
Iteration: 5 || Loss: 28.303461684813193
Iteration: 6 || Loss: 28.206894642301656
Iteration: 7 || Loss: 27.989384571181922
Iteration: 8 || Loss: 27.92479211334179
Iteration: 9 || Loss: 27.71577938537498
Iteration: 10 || Loss: 27.545947410343103
Iteration: 11 || Loss: 27.513686262113758
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:208.2739
Epoch 39 loss:27.513686262113758
waveform batch: 2/3
Using ADAM optimizer
Sum of params:208.2739
Changing learning rate to:3.637978807091713e-14
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.663732717157076
Iteration: 2 || Loss: 8.663732717157076
Iteration: 3 || Loss: 8.663732717157076
Iteration: 4 || Loss: 8.663732717157076
Iteration: 5 || Loss: 8.663732717157076
Iteration: 6 || Loss: 8.663732717157076
saving ADAM checkpoint...
Sum of params:208.2739
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.663732717157076
Iteration: 2 || Loss: 8.60274476808721
Iteration: 3 || Loss: 8.567330783017999
Iteration: 4 || Loss: 8.542578633232187
Iteration: 5 || Loss: 8.514700551623392
Iteration: 6 || Loss: 8.414034595327985
Iteration: 7 || Loss: 8.389047826157537
Iteration: 8 || Loss: 8.347541117022013
Iteration: 9 || Loss: 8.313101082133821
Iteration: 10 || Loss: 8.307423166906354
Iteration: 11 || Loss: 8.2917750423898
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:207.99715
Epoch 39 loss:8.2917750423898
waveform batch: 3/3
Using ADAM optimizer
Sum of params:207.99715
Changing learning rate to:1.8189894035458565e-14
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.821794784462392
Iteration: 2 || Loss: 7.821794784462392
Iteration: 3 || Loss: 7.821794784462392
Iteration: 4 || Loss: 7.821794784462392
Iteration: 5 || Loss: 7.821794784462392
Iteration: 6 || Loss: 7.821794784462392
saving ADAM checkpoint...
Sum of params:207.99715
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.821794784462392
Iteration: 2 || Loss: 7.819393390299188
Iteration: 3 || Loss: 7.8121101910262585
Iteration: 4 || Loss: 7.80515931392566
Iteration: 5 || Loss: 7.767202418162164
Iteration: 6 || Loss: 7.755109412004602
Iteration: 7 || Loss: 7.722263055769305
Iteration: 8 || Loss: 7.690648740257613
Iteration: 9 || Loss: 7.659488076485308
Iteration: 10 || Loss: 7.615594274979001
Iteration: 11 || Loss: 7.593740084947386
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:208.13382
Epoch 39 loss:7.593740084947386
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.730708956042946
waveform batch: 2/2
Test loss:9.274991955174562
Epoch 39 mean train loss:1.4466400463150315
Epoch 39 mean test loss:1.1179824065422064
Start training epoch 40
waveform batch: 1/3
Using ADAM optimizer
Sum of params:208.13382
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.90885867455171
Iteration: 2 || Loss: 27.90885867455171
Iteration: 3 || Loss: 27.90885867455171
Iteration: 4 || Loss: 27.90885867455171
Iteration: 5 || Loss: 27.90885867455171
Iteration: 6 || Loss: 27.90885867455171
saving ADAM checkpoint...
Sum of params:208.13382
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.90885867455171
Iteration: 2 || Loss: 27.89485741113525
Iteration: 3 || Loss: 27.878360746783535
Iteration: 4 || Loss: 27.81009677064718
Iteration: 5 || Loss: 27.73965363247287
Iteration: 6 || Loss: 27.610178630853216
Iteration: 7 || Loss: 27.43286386018502
Iteration: 8 || Loss: 27.38899757812468
Iteration: 9 || Loss: 27.149328820981086
Iteration: 10 || Loss: 27.0015480755099
Iteration: 11 || Loss: 26.97286498555667
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:208.82317
Epoch 40 loss:26.97286498555667
waveform batch: 2/3
Using ADAM optimizer
Sum of params:208.82317
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.57172628860979
Iteration: 2 || Loss: 8.57172628860979
Iteration: 3 || Loss: 8.57172628860979
Iteration: 4 || Loss: 8.57172628860979
Iteration: 5 || Loss: 8.57172628860979
Iteration: 6 || Loss: 8.57172628860979
saving ADAM checkpoint...
Sum of params:208.82317
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.57172628860979
Iteration: 2 || Loss: 8.492612976405345
Iteration: 3 || Loss: 8.43169294400999
Iteration: 4 || Loss: 8.416808758648354
Iteration: 5 || Loss: 8.381785356128233
Iteration: 6 || Loss: 8.32873417201957
Iteration: 7 || Loss: 8.251291392586062
Iteration: 8 || Loss: 8.236529546062904
Iteration: 9 || Loss: 8.193967369145454
Iteration: 10 || Loss: 8.187286975003053
Iteration: 11 || Loss: 8.159442054041277
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:208.55966
Epoch 40 loss:8.159442054041277
waveform batch: 3/3
Using ADAM optimizer
Sum of params:208.55966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.707642459921469
Iteration: 2 || Loss: 7.707642459921469
Iteration: 3 || Loss: 7.707642459921469
Iteration: 4 || Loss: 7.707642459921469
Iteration: 5 || Loss: 7.707642459921469
Iteration: 6 || Loss: 7.707642459921469
saving ADAM checkpoint...
Sum of params:208.55966
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.707642459921469
Iteration: 2 || Loss: 7.704457589552344
Iteration: 3 || Loss: 7.697344154786865
Iteration: 4 || Loss: 7.691936167891148
Iteration: 5 || Loss: 7.663631321464136
Iteration: 6 || Loss: 7.650733975754354
Iteration: 7 || Loss: 7.612957976411667
Iteration: 8 || Loss: 7.584249118102105
Iteration: 9 || Loss: 7.5520743029314295
Iteration: 10 || Loss: 7.528096780635919
Iteration: 11 || Loss: 7.486782292766519
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:208.71893
Epoch 40 loss:7.486782292766519
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.6041109283223
waveform batch: 2/2
Test loss:9.138356638374692
Epoch 40 mean train loss:1.4206363110788156
Epoch 40 mean test loss:1.1024980921586465
Start training epoch 41
waveform batch: 1/3
Using ADAM optimizer
Sum of params:208.71893
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 27.378366322892237
Iteration: 2 || Loss: 27.378366322892237
Iteration: 3 || Loss: 27.378366322892237
Iteration: 4 || Loss: 27.378366322892237
Iteration: 5 || Loss: 27.378366322892237
Iteration: 6 || Loss: 27.378366322892237
saving ADAM checkpoint...
Sum of params:208.71893
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 27.378366322892237
Iteration: 2 || Loss: 27.368220510063253
Iteration: 3 || Loss: 27.351033995573705
Iteration: 4 || Loss: 27.31039128795732
Iteration: 5 || Loss: 27.20527384454666
Iteration: 6 || Loss: 27.083425100370572
Iteration: 7 || Loss: 26.914406645988727
Iteration: 8 || Loss: 26.867455268822827
Iteration: 9 || Loss: 26.630523969122045
Iteration: 10 || Loss: 26.501808549236657
Iteration: 11 || Loss: 26.470662591950607
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:209.3829
Epoch 41 loss:26.470662591950607
waveform batch: 2/3
Using ADAM optimizer
Sum of params:209.3829
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.456883975013573
Iteration: 2 || Loss: 8.456883975013573
Iteration: 3 || Loss: 8.456883975013573
Iteration: 4 || Loss: 8.456883975013573
Iteration: 5 || Loss: 8.456883975013573
Iteration: 6 || Loss: 8.456883975013573
saving ADAM checkpoint...
Sum of params:209.3829
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.456883975013573
Iteration: 2 || Loss: 8.366331047342667
Iteration: 3 || Loss: 8.30761790873154
Iteration: 4 || Loss: 8.285161117636182
Iteration: 5 || Loss: 8.247484568763246
Iteration: 6 || Loss: 8.214168310841758
Iteration: 7 || Loss: 8.124239705111217
Iteration: 8 || Loss: 8.115335363372706
Iteration: 9 || Loss: 8.072569849446191
Iteration: 10 || Loss: 8.06467257430571
Iteration: 11 || Loss: 8.046255472609774
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:209.15768
Epoch 41 loss:8.046255472609774
waveform batch: 3/3
Using ADAM optimizer
Sum of params:209.15768
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.625429619355146
Iteration: 2 || Loss: 7.625429619355146
Iteration: 3 || Loss: 7.625429619355146
Iteration: 4 || Loss: 7.625429619355146
Iteration: 5 || Loss: 7.625429619355146
Iteration: 6 || Loss: 7.625429619355146
saving ADAM checkpoint...
Sum of params:209.15768
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.625429619355146
Iteration: 2 || Loss: 7.621288934317467
Iteration: 3 || Loss: 7.610492605097568
Iteration: 4 || Loss: 7.602353094548798
Iteration: 5 || Loss: 7.56948095685208
Iteration: 6 || Loss: 7.545940099399265
Iteration: 7 || Loss: 7.5352278934745325
Iteration: 8 || Loss: 7.493112900861256
Iteration: 9 || Loss: 7.463450111306288
Iteration: 10 || Loss: 7.443673878882063
Iteration: 11 || Loss: 7.402083081526415
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:209.32275
Epoch 41 loss:7.402083081526415
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.528585793795925
waveform batch: 2/2
Test loss:9.03237731742583
Epoch 41 mean train loss:1.397300038202893
Epoch 41 mean test loss:1.0918213594836326
Start training epoch 42
waveform batch: 1/3
Using ADAM optimizer
Sum of params:209.32275
Changing learning rate to:9.094947017729283e-15
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.976810929120656
Iteration: 2 || Loss: 26.976810929120656
Iteration: 3 || Loss: 26.976810929120656
Iteration: 4 || Loss: 26.976810929120656
Iteration: 5 || Loss: 26.976810929120656
Iteration: 6 || Loss: 26.976810929120656
saving ADAM checkpoint...
Sum of params:209.32275
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.976810929120656
Iteration: 2 || Loss: 26.94663285082867
Iteration: 3 || Loss: 26.917576362229894
Iteration: 4 || Loss: 26.882352760876895
Iteration: 5 || Loss: 26.738409364003004
Iteration: 6 || Loss: 26.60770635543756
Iteration: 7 || Loss: 26.48631020636468
Iteration: 8 || Loss: 26.427426324869266
Iteration: 9 || Loss: 26.33381911485769
Iteration: 10 || Loss: 26.12847680397788
Iteration: 11 || Loss: 26.085717095353335
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:209.8933
Epoch 42 loss:26.085717095353335
waveform batch: 2/3
Using ADAM optimizer
Sum of params:209.8933
Changing learning rate to:4.547473508864641e-15
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.253840024707612
Iteration: 2 || Loss: 8.253840024707612
Iteration: 3 || Loss: 8.253840024707612
Iteration: 4 || Loss: 8.253840024707612
Iteration: 5 || Loss: 8.253840024707612
Iteration: 6 || Loss: 8.253840024707612
saving ADAM checkpoint...
Sum of params:209.8933
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.253840024707612
Iteration: 2 || Loss: 8.20493566121266
Iteration: 3 || Loss: 8.178675840405191
Iteration: 4 || Loss: 8.139200590403787
Iteration: 5 || Loss: 8.105677820600777
Iteration: 6 || Loss: 7.985365317220472
Iteration: 7 || Loss: 7.977688160167957
Iteration: 8 || Loss: 7.948595833052824
Iteration: 9 || Loss: 7.940629481433081
Iteration: 10 || Loss: 7.9231877969409075
Iteration: 11 || Loss: 7.9098586438762375
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:209.66905
Epoch 42 loss:7.9098586438762375
waveform batch: 3/3
Using ADAM optimizer
Sum of params:209.66905
Changing learning rate to:2.2737367544323206e-15
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.514129920097372
Iteration: 2 || Loss: 7.514129920097372
Iteration: 3 || Loss: 7.514129920097372
Iteration: 4 || Loss: 7.514129920097372
Iteration: 5 || Loss: 7.514129920097372
Iteration: 6 || Loss: 7.514129920097372
saving ADAM checkpoint...
Sum of params:209.66905
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.514129920097372
Iteration: 2 || Loss: 7.511514993963468
Iteration: 3 || Loss: 7.507650205748121
Iteration: 4 || Loss: 7.500645084930814
Iteration: 5 || Loss: 7.451151899710218
Iteration: 6 || Loss: 7.42906795751681
Iteration: 7 || Loss: 7.4165578204844635
Iteration: 8 || Loss: 7.380790398431378
Iteration: 9 || Loss: 7.355443534797642
Iteration: 10 || Loss: 7.320089593252729
Iteration: 11 || Loss: 7.299417631505692
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:209.77325
Epoch 42 loss:7.299417631505692
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.384902365651875
waveform batch: 2/2
Test loss:8.92790413252129
Epoch 42 mean train loss:1.3764997790245088
Epoch 42 mean test loss:1.077223911657245
Start training epoch 43
waveform batch: 1/3
Using ADAM optimizer
Sum of params:209.77325
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.5082749439948
Iteration: 2 || Loss: 26.5082749439948
Iteration: 3 || Loss: 26.5082749439948
Iteration: 4 || Loss: 26.5082749439948
Iteration: 5 || Loss: 26.5082749439948
Iteration: 6 || Loss: 26.5082749439948
saving ADAM checkpoint...
Sum of params:209.77325
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.5082749439948
Iteration: 2 || Loss: 26.483347573362778
Iteration: 3 || Loss: 26.471164571264445
Iteration: 4 || Loss: 26.443497548047073
Iteration: 5 || Loss: 26.31007334312279
Iteration: 6 || Loss: 26.187941466750722
Iteration: 7 || Loss: 26.055879272871913
Iteration: 8 || Loss: 26.018594101767732
Iteration: 9 || Loss: 25.88698105437772
Iteration: 10 || Loss: 25.71054035396612
Iteration: 11 || Loss: 25.6797428276835
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:210.30731
Epoch 43 loss:25.6797428276835
waveform batch: 2/3
Using ADAM optimizer
Sum of params:210.30731
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 8.165672451436624
Iteration: 2 || Loss: 8.165672451436624
Iteration: 3 || Loss: 8.165672451436624
Iteration: 4 || Loss: 8.165672451436624
Iteration: 5 || Loss: 8.165672451436624
Iteration: 6 || Loss: 8.165672451436624
saving ADAM checkpoint...
Sum of params:210.30731
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 8.165672451436624
Iteration: 2 || Loss: 8.12155852590654
Iteration: 3 || Loss: 8.075584978282741
Iteration: 4 || Loss: 8.047459295522376
Iteration: 5 || Loss: 8.006188456025164
Iteration: 6 || Loss: 7.931504805565505
Iteration: 7 || Loss: 7.896315611380808
Iteration: 8 || Loss: 7.876519003611817
Iteration: 9 || Loss: 7.831644928212852
Iteration: 10 || Loss: 7.8264269452452435
Iteration: 11 || Loss: 7.812295902435315
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:210.12225
Epoch 43 loss:7.812295902435315
waveform batch: 3/3
Using ADAM optimizer
Sum of params:210.12225
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.397519388142563
Iteration: 2 || Loss: 7.397519388142563
Iteration: 3 || Loss: 7.397519388142563
Iteration: 4 || Loss: 7.397519388142563
Iteration: 5 || Loss: 7.397519388142563
Iteration: 6 || Loss: 7.397519388142563
saving ADAM checkpoint...
Sum of params:210.12225
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.397519388142563
Iteration: 2 || Loss: 7.3926298007784945
Iteration: 3 || Loss: 7.384554984788351
Iteration: 4 || Loss: 7.37899881324261
Iteration: 5 || Loss: 7.36072379672863
Iteration: 6 || Loss: 7.3454561178094835
Iteration: 7 || Loss: 7.3173712781511755
Iteration: 8 || Loss: 7.295024015269564
Iteration: 9 || Loss: 7.260307119778103
Iteration: 10 || Loss: 7.2467285419662915
Iteration: 11 || Loss: 7.212615654004316
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:210.26035
Epoch 43 loss:7.212615654004316
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.278605751641862
waveform batch: 2/2
Test loss:8.820858325802268
Epoch 43 mean train loss:1.3568218128041043
Epoch 43 mean test loss:1.0646743574967137
Start training epoch 44
waveform batch: 1/3
Using ADAM optimizer
Sum of params:210.26035
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 26.162840094341366
Iteration: 2 || Loss: 26.162840094341366
Iteration: 3 || Loss: 26.162840094341366
Iteration: 4 || Loss: 26.162840094341366
Iteration: 5 || Loss: 26.162840094341366
Iteration: 6 || Loss: 26.162840094341366
saving ADAM checkpoint...
Sum of params:210.26035
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 26.162840094341366
Iteration: 2 || Loss: 26.157891548556524
Iteration: 3 || Loss: 26.139832778724667
Iteration: 4 || Loss: 26.10764832907736
Iteration: 5 || Loss: 25.97447231747905
Iteration: 6 || Loss: 25.84688320850241
Iteration: 7 || Loss: 25.687399060032334
Iteration: 8 || Loss: 25.437119048977024
Iteration: 9 || Loss: 25.34239813461543
Iteration: 10 || Loss: 25.30330886574647
Iteration: 11 || Loss: 25.206459017295483
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:210.95058
Epoch 44 loss:25.206459017295483
waveform batch: 2/3
Using ADAM optimizer
Sum of params:210.95058
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.976825944352793
Iteration: 2 || Loss: 7.976825944352793
Iteration: 3 || Loss: 7.976825944352793
Iteration: 4 || Loss: 7.976825944352793
Iteration: 5 || Loss: 7.976825944352793
Iteration: 6 || Loss: 7.976825944352793
saving ADAM checkpoint...
Sum of params:210.95058
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.976825944352793
Iteration: 2 || Loss: 7.973902048498619
Iteration: 3 || Loss: 7.960854489242858
Iteration: 4 || Loss: 7.937400310211328
Iteration: 5 || Loss: 7.902265473280841
Iteration: 6 || Loss: 7.823132874079799
Iteration: 7 || Loss: 7.7847926956489735
Iteration: 8 || Loss: 7.74946000251404
Iteration: 9 || Loss: 7.704345930579969
Iteration: 10 || Loss: 7.685937018110827
Iteration: 11 || Loss: 7.6534285305760354
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:210.76826
Epoch 44 loss:7.6534285305760354
waveform batch: 3/3
Using ADAM optimizer
Sum of params:210.76826
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.332147748968487
Iteration: 2 || Loss: 7.332147748968487
Iteration: 3 || Loss: 7.332147748968487
Iteration: 4 || Loss: 7.332147748968487
Iteration: 5 || Loss: 7.332147748968487
Iteration: 6 || Loss: 7.332147748968487
saving ADAM checkpoint...
Sum of params:210.76826
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.332147748968487
Iteration: 2 || Loss: 7.327966705076963
Iteration: 3 || Loss: 7.325555078887913
Iteration: 4 || Loss: 7.31750823326789
Iteration: 5 || Loss: 7.278218446731658
Iteration: 6 || Loss: 7.265721729362922
Iteration: 7 || Loss: 7.238516949727494
Iteration: 8 || Loss: 7.2102365983912176
Iteration: 9 || Loss: 7.171354851494273
Iteration: 10 || Loss: 7.1587882619872625
Iteration: 11 || Loss: 7.128506726620491
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:210.80238
Epoch 44 loss:7.128506726620491
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.21581049296756
waveform batch: 2/2
Test loss:8.702613756796762
Epoch 44 mean train loss:1.3329464758164002
Epoch 44 mean test loss:1.0540249558684895
Start training epoch 45
waveform batch: 1/3
Using ADAM optimizer
Sum of params:210.80238
Changing learning rate to:1.1368683772161603e-15
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.65734480123625
Iteration: 2 || Loss: 25.65734480123625
Iteration: 3 || Loss: 25.65734480123625
Iteration: 4 || Loss: 25.65734480123625
Iteration: 5 || Loss: 25.65734480123625
Iteration: 6 || Loss: 25.65734480123625
saving ADAM checkpoint...
Sum of params:210.80238
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.65734480123625
Iteration: 2 || Loss: 25.618540467669188
Iteration: 3 || Loss: 25.590928227343035
Iteration: 4 || Loss: 25.57167815605531
Iteration: 5 || Loss: 25.406892984387763
Iteration: 6 || Loss: 25.287724552950692
Iteration: 7 || Loss: 25.221229721483937
Iteration: 8 || Loss: 25.16833935480946
Iteration: 9 || Loss: 25.13874161445941
Iteration: 10 || Loss: 24.985527909418288
Iteration: 11 || Loss: 24.9033287166677
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:211.31198
Epoch 45 loss:24.9033287166677
waveform batch: 2/3
Using ADAM optimizer
Sum of params:211.31198
Changing learning rate to:5.684341886080802e-16
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.9393287461943505
Iteration: 2 || Loss: 7.9393287461943505
Iteration: 3 || Loss: 7.9393287461943505
Iteration: 4 || Loss: 7.9393287461943505
Iteration: 5 || Loss: 7.9393287461943505
Iteration: 6 || Loss: 7.9393287461943505
saving ADAM checkpoint...
Sum of params:211.31198
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.9393287461943505
Iteration: 2 || Loss: 7.915878132369484
Iteration: 3 || Loss: 7.872559704309813
Iteration: 4 || Loss: 7.8404455129092
Iteration: 5 || Loss: 7.781680147316276
Iteration: 6 || Loss: 7.666295958591839
Iteration: 7 || Loss: 7.657054788897565
Iteration: 8 || Loss: 7.615728461888052
Iteration: 9 || Loss: 7.603765889011397
Iteration: 10 || Loss: 7.584817955072855
Iteration: 11 || Loss: 7.580125400650127
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:211.20258
Epoch 45 loss:7.580125400650127
waveform batch: 3/3
Using ADAM optimizer
Sum of params:211.20258
Changing learning rate to:2.842170943040401e-16
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.20867950502893
Iteration: 2 || Loss: 7.20867950502893
Iteration: 3 || Loss: 7.20867950502893
Iteration: 4 || Loss: 7.20867950502893
Iteration: 5 || Loss: 7.20867950502893
Iteration: 6 || Loss: 7.20867950502893
saving ADAM checkpoint...
Sum of params:211.20258
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.20867950502893
Iteration: 2 || Loss: 7.2073525195951085
Iteration: 3 || Loss: 7.203991879592693
Iteration: 4 || Loss: 7.200800130087867
Iteration: 5 || Loss: 7.169716830814989
Iteration: 6 || Loss: 7.162399826430134
Iteration: 7 || Loss: 7.140806109732499
Iteration: 8 || Loss: 7.09351468873962
Iteration: 9 || Loss: 7.070459292995969
Iteration: 10 || Loss: 7.040004061277925
Iteration: 11 || Loss: 7.014910198562031
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:211.30829
Epoch 45 loss:7.014910198562031
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:9.069705124345672
waveform batch: 2/2
Test loss:8.631436951184392
Epoch 45 mean train loss:1.316612143862662
Epoch 45 mean test loss:1.0412436515017685
Start training epoch 46
waveform batch: 1/3
Using ADAM optimizer
Sum of params:211.30829
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.503810449569983
Iteration: 2 || Loss: 25.503810449569983
Iteration: 3 || Loss: 25.503810449569983
Iteration: 4 || Loss: 25.503810449569983
Iteration: 5 || Loss: 25.503810449569983
Iteration: 6 || Loss: 25.503810449569983
saving ADAM checkpoint...
Sum of params:211.30829
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.503810449569983
Iteration: 2 || Loss: 25.473990569468942
Iteration: 3 || Loss: 25.457149856557933
Iteration: 4 || Loss: 25.411254760583827
Iteration: 5 || Loss: 25.264900077866365
Iteration: 6 || Loss: 25.144474538706845
Iteration: 7 || Loss: 25.029522179655807
Iteration: 8 || Loss: 24.966903844752885
Iteration: 9 || Loss: 24.89207874041686
Iteration: 10 || Loss: 24.661570353287377
Iteration: 11 || Loss: 24.598371612629446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:211.84845
Epoch 46 loss:24.598371612629446
waveform batch: 2/3
Using ADAM optimizer
Sum of params:211.84845
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.856822119109658
Iteration: 2 || Loss: 7.856822119109658
Iteration: 3 || Loss: 7.856822119109658
Iteration: 4 || Loss: 7.856822119109658
Iteration: 5 || Loss: 7.856822119109658
Iteration: 6 || Loss: 7.856822119109658
saving ADAM checkpoint...
Sum of params:211.84845
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.856822119109658
Iteration: 2 || Loss: 7.841642531292395
Iteration: 3 || Loss: 7.8007357552964205
Iteration: 4 || Loss: 7.778535796194415
Iteration: 5 || Loss: 7.716119494316497
Iteration: 6 || Loss: 7.592408341691698
Iteration: 7 || Loss: 7.575445191853678
Iteration: 8 || Loss: 7.532378403301122
Iteration: 9 || Loss: 7.518381283265382
Iteration: 10 || Loss: 7.498146776163827
Iteration: 11 || Loss: 7.49021336048198
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:211.69177
Epoch 46 loss:7.49021336048198
waveform batch: 3/3
Using ADAM optimizer
Sum of params:211.69177
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.105600093362628
Iteration: 2 || Loss: 7.105600093362628
Iteration: 3 || Loss: 7.105600093362628
Iteration: 4 || Loss: 7.105600093362628
Iteration: 5 || Loss: 7.105600093362628
Iteration: 6 || Loss: 7.105600093362628
saving ADAM checkpoint...
Sum of params:211.69177
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.105600093362628
Iteration: 2 || Loss: 7.103814681051466
Iteration: 3 || Loss: 7.100749260795315
Iteration: 4 || Loss: 7.097963860854861
Iteration: 5 || Loss: 7.068959123407293
Iteration: 6 || Loss: 7.053125868016547
Iteration: 7 || Loss: 7.041431499896176
Iteration: 8 || Loss: 7.001161050634485
Iteration: 9 || Loss: 6.981765049385928
Iteration: 10 || Loss: 6.948317706326394
Iteration: 11 || Loss: 6.931270212632865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:211.7756
Epoch 46 loss:6.931270212632865
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.961224312828
waveform batch: 2/2
Test loss:8.534618301373019
Epoch 46 mean train loss:1.3006618395248097
Epoch 46 mean test loss:1.0291672126000597
Start training epoch 47
waveform batch: 1/3
Using ADAM optimizer
Sum of params:211.7756
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 25.130040486349568
Iteration: 2 || Loss: 25.130040486349568
Iteration: 3 || Loss: 25.130040486349568
Iteration: 4 || Loss: 25.130040486349568
Iteration: 5 || Loss: 25.130040486349568
Iteration: 6 || Loss: 25.130040486349568
saving ADAM checkpoint...
Sum of params:211.7756
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 25.130040486349568
Iteration: 2 || Loss: 25.097678274365563
Iteration: 3 || Loss: 25.084833488054088
Iteration: 4 || Loss: 25.047526294927856
Iteration: 5 || Loss: 24.90680631965028
Iteration: 6 || Loss: 24.781122193368926
Iteration: 7 || Loss: 24.679658441325998
Iteration: 8 || Loss: 24.631662343458178
Iteration: 9 || Loss: 24.543658602282097
Iteration: 10 || Loss: 24.348502138744145
Iteration: 11 || Loss: 24.313707143768728
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:212.28104
Epoch 47 loss:24.313707143768728
waveform batch: 2/3
Using ADAM optimizer
Sum of params:212.28104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.793777536239166
Iteration: 2 || Loss: 7.793777536239166
Iteration: 3 || Loss: 7.793777536239166
Iteration: 4 || Loss: 7.793777536239166
Iteration: 5 || Loss: 7.793777536239166
Iteration: 6 || Loss: 7.793777536239166
saving ADAM checkpoint...
Sum of params:212.28104
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.793777536239166
Iteration: 2 || Loss: 7.7634096977543585
Iteration: 3 || Loss: 7.710734204570585
Iteration: 4 || Loss: 7.678108149860756
Iteration: 5 || Loss: 7.61941171220365
Iteration: 6 || Loss: 7.5107461686710355
Iteration: 7 || Loss: 7.49313519168886
Iteration: 8 || Loss: 7.465125429402123
Iteration: 9 || Loss: 7.445435154659519
Iteration: 10 || Loss: 7.4178253141655945
Iteration: 11 || Loss: 7.409636297118632
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:212.14127
Epoch 47 loss:7.409636297118632
waveform batch: 3/3
Using ADAM optimizer
Sum of params:212.14127
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.02564133006871
Iteration: 2 || Loss: 7.02564133006871
Iteration: 3 || Loss: 7.02564133006871
Iteration: 4 || Loss: 7.02564133006871
Iteration: 5 || Loss: 7.02564133006871
Iteration: 6 || Loss: 7.02564133006871
saving ADAM checkpoint...
Sum of params:212.14127
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.02564133006871
Iteration: 2 || Loss: 7.020272616637584
Iteration: 3 || Loss: 7.017868738416651
Iteration: 4 || Loss: 7.014795930570993
Iteration: 5 || Loss: 6.9939716283836555
Iteration: 6 || Loss: 6.981855055572837
Iteration: 7 || Loss: 6.962045076229813
Iteration: 8 || Loss: 6.938235707287057
Iteration: 9 || Loss: 6.905306174481523
Iteration: 10 || Loss: 6.887804610516251
Iteration: 11 || Loss: 6.85833745683485
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:212.2555
Epoch 47 loss:6.85833745683485
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.900044071587638
waveform batch: 2/2
Test loss:8.44220782960207
Epoch 47 mean train loss:1.2860560299240735
Epoch 47 mean test loss:1.0201324647758652
Start training epoch 48
waveform batch: 1/3
Using ADAM optimizer
Sum of params:212.2555
Changing learning rate to:1.4210854715202004e-16
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.916172641669778
Iteration: 2 || Loss: 24.916172641669778
Iteration: 3 || Loss: 24.916172641669778
Iteration: 4 || Loss: 24.916172641669778
Iteration: 5 || Loss: 24.916172641669778
Iteration: 6 || Loss: 24.916172641669778
saving ADAM checkpoint...
Sum of params:212.2555
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.916172641669778
Iteration: 2 || Loss: 24.88152229167297
Iteration: 3 || Loss: 24.843978439413668
Iteration: 4 || Loss: 24.82855666382275
Iteration: 5 || Loss: 24.658139865017148
Iteration: 6 || Loss: 24.53279697725358
Iteration: 7 || Loss: 24.4364836806676
Iteration: 8 || Loss: 24.379844967597965
Iteration: 9 || Loss: 24.315023701735175
Iteration: 10 || Loss: 24.105478174322442
Iteration: 11 || Loss: 24.048530305965773
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:212.70389
Epoch 48 loss:24.048530305965773
waveform batch: 2/3
Using ADAM optimizer
Sum of params:212.70389
Changing learning rate to:7.105427357601002e-17
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.700979690549623
Iteration: 2 || Loss: 7.700979690549623
Iteration: 3 || Loss: 7.700979690549623
Iteration: 4 || Loss: 7.700979690549623
Iteration: 5 || Loss: 7.700979690549623
Iteration: 6 || Loss: 7.700979690549623
saving ADAM checkpoint...
Sum of params:212.70389
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.700979690549623
Iteration: 2 || Loss: 7.674541709398328
Iteration: 3 || Loss: 7.623806631229022
Iteration: 4 || Loss: 7.587429044580293
Iteration: 5 || Loss: 7.529791765449533
Iteration: 6 || Loss: 7.425722782981211
Iteration: 7 || Loss: 7.408098566716614
Iteration: 8 || Loss: 7.381039621216626
Iteration: 9 || Loss: 7.353733624923173
Iteration: 10 || Loss: 7.334597251016666
Iteration: 11 || Loss: 7.327326068298223
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:212.59254
Epoch 48 loss:7.327326068298223
waveform batch: 3/3
Using ADAM optimizer
Sum of params:212.59254
Changing learning rate to:3.552713678800501e-17
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.942581828710283
Iteration: 2 || Loss: 6.942581828710283
Iteration: 3 || Loss: 6.942581828710283
Iteration: 4 || Loss: 6.942581828710283
Iteration: 5 || Loss: 6.942581828710283
Iteration: 6 || Loss: 6.942581828710283
saving ADAM checkpoint...
Sum of params:212.59254
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.942581828710283
Iteration: 2 || Loss: 6.937343155285813
Iteration: 3 || Loss: 6.932533642517344
Iteration: 4 || Loss: 6.929166001596037
Iteration: 5 || Loss: 6.91112509181975
Iteration: 6 || Loss: 6.8949606865245965
Iteration: 7 || Loss: 6.880267796313823
Iteration: 8 || Loss: 6.861122794068708
Iteration: 9 || Loss: 6.820851179785018
Iteration: 10 || Loss: 6.801375444822703
Iteration: 11 || Loss: 6.776159161788362
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:212.67784
Epoch 48 loss:6.776159161788362
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.801142537015398
waveform batch: 2/2
Test loss:8.363903579226056
Epoch 48 mean train loss:1.2717338512017453
Epoch 48 mean test loss:1.009708595073027
Start training epoch 49
waveform batch: 1/3
Using ADAM optimizer
Sum of params:212.67784
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.656906576724033
Iteration: 2 || Loss: 24.656906576724033
Iteration: 3 || Loss: 24.656906576724033
Iteration: 4 || Loss: 24.656906576724033
Iteration: 5 || Loss: 24.656906576724033
Iteration: 6 || Loss: 24.656906576724033
saving ADAM checkpoint...
Sum of params:212.67784
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.656906576724033
Iteration: 2 || Loss: 24.63238026881582
Iteration: 3 || Loss: 24.611118031296165
Iteration: 4 || Loss: 24.57874860678965
Iteration: 5 || Loss: 24.40173086748343
Iteration: 6 || Loss: 24.27041431085536
Iteration: 7 || Loss: 24.16517376025532
Iteration: 8 || Loss: 24.121426017768115
Iteration: 9 || Loss: 23.972509532615195
Iteration: 10 || Loss: 23.812482385877484
Iteration: 11 || Loss: 23.770076826829484
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:213.16562
Epoch 49 loss:23.770076826829484
waveform batch: 2/3
Using ADAM optimizer
Sum of params:213.16562
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.632356109147633
Iteration: 2 || Loss: 7.632356109147633
Iteration: 3 || Loss: 7.632356109147633
Iteration: 4 || Loss: 7.632356109147633
Iteration: 5 || Loss: 7.632356109147633
Iteration: 6 || Loss: 7.632356109147633
saving ADAM checkpoint...
Sum of params:213.16562
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.632356109147633
Iteration: 2 || Loss: 7.58332904752752
Iteration: 3 || Loss: 7.535228753371029
Iteration: 4 || Loss: 7.500716743853541
Iteration: 5 || Loss: 7.444227348411536
Iteration: 6 || Loss: 7.386460204855508
Iteration: 7 || Loss: 7.3367135602812885
Iteration: 8 || Loss: 7.324609881748661
Iteration: 9 || Loss: 7.301788931179536
Iteration: 10 || Loss: 7.274997452122372
Iteration: 11 || Loss: 7.248990521403851
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:213.06332
Epoch 49 loss:7.248990521403851
waveform batch: 3/3
Using ADAM optimizer
Sum of params:213.06332
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.872560106045693
Iteration: 2 || Loss: 6.872560106045693
Iteration: 3 || Loss: 6.872560106045693
Iteration: 4 || Loss: 6.872560106045693
Iteration: 5 || Loss: 6.872560106045693
Iteration: 6 || Loss: 6.872560106045693
saving ADAM checkpoint...
Sum of params:213.06332
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.872560106045693
Iteration: 2 || Loss: 6.86559072529557
Iteration: 3 || Loss: 6.860859542433178
Iteration: 4 || Loss: 6.8569111971192855
Iteration: 5 || Loss: 6.837050092483944
Iteration: 6 || Loss: 6.821839465017496
Iteration: 7 || Loss: 6.806223135590419
Iteration: 8 || Loss: 6.790333610701325
Iteration: 9 || Loss: 6.743867471895649
Iteration: 10 || Loss: 6.7264988354377895
Iteration: 11 || Loss: 6.702458385119712
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:213.14096
Epoch 49 loss:6.702458385119712
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.721931816461565
waveform batch: 2/2
Test loss:8.276070826201268
Epoch 49 mean train loss:1.2573841911117682
Epoch 49 mean test loss:0.9998825083919314
Start training epoch 50
waveform batch: 1/3
Using ADAM optimizer
Sum of params:213.14096
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.37516356601027
Iteration: 2 || Loss: 24.37516356601027
Iteration: 3 || Loss: 24.37516356601027
Iteration: 4 || Loss: 24.37516356601027
Iteration: 5 || Loss: 24.37516356601027
Iteration: 6 || Loss: 24.37516356601027
saving ADAM checkpoint...
Sum of params:213.14096
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.37516356601027
Iteration: 2 || Loss: 24.34014777150267
Iteration: 3 || Loss: 24.300129646820594
Iteration: 4 || Loss: 24.28272345524733
Iteration: 5 || Loss: 24.103546197272088
Iteration: 6 || Loss: 23.973835659438894
Iteration: 7 || Loss: 23.889587685170437
Iteration: 8 || Loss: 23.848166464399238
Iteration: 9 || Loss: 23.752629428711998
Iteration: 10 || Loss: 23.53993574411082
Iteration: 11 || Loss: 23.50426750686855
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:213.62613
Epoch 50 loss:23.50426750686855
waveform batch: 2/3
Using ADAM optimizer
Sum of params:213.62613
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.506514129819587
Iteration: 2 || Loss: 7.506514129819587
Iteration: 3 || Loss: 7.506514129819587
Iteration: 4 || Loss: 7.506514129819587
Iteration: 5 || Loss: 7.506514129819587
Iteration: 6 || Loss: 7.506514129819587
saving ADAM checkpoint...
Sum of params:213.62613
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.506514129819587
Iteration: 2 || Loss: 7.481319458569588
Iteration: 3 || Loss: 7.449860555746727
Iteration: 4 || Loss: 7.42238022665884
Iteration: 5 || Loss: 7.364462988499911
Iteration: 6 || Loss: 7.274363673966737
Iteration: 7 || Loss: 7.260991424996
Iteration: 8 || Loss: 7.233903624450552
Iteration: 9 || Loss: 7.220710935388175
Iteration: 10 || Loss: 7.173476615750244
Iteration: 11 || Loss: 7.168478296436934
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:213.55382
Epoch 50 loss:7.168478296436934
waveform batch: 3/3
Using ADAM optimizer
Sum of params:213.55382
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.805093723440756
Iteration: 2 || Loss: 6.805093723440756
Iteration: 3 || Loss: 6.805093723440756
Iteration: 4 || Loss: 6.805093723440756
Iteration: 5 || Loss: 6.805093723440756
Iteration: 6 || Loss: 6.805093723440756
saving ADAM checkpoint...
Sum of params:213.55382
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.805093723440756
Iteration: 2 || Loss: 6.798087231828548
Iteration: 3 || Loss: 6.79252089659666
Iteration: 4 || Loss: 6.788297222288672
Iteration: 5 || Loss: 6.767977864918915
Iteration: 6 || Loss: 6.7503039622098635
Iteration: 7 || Loss: 6.736348587474899
Iteration: 8 || Loss: 6.72156726861232
Iteration: 9 || Loss: 6.667305711186032
Iteration: 10 || Loss: 6.648958335812976
Iteration: 11 || Loss: 6.627934969908985
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:213.58351
Epoch 50 loss:6.627934969908985
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.62484574197881
waveform batch: 2/2
Test loss:8.191132406362497
Epoch 50 mean train loss:1.2433560257738157
Epoch 50 mean test loss:0.9891751851965476
Start training epoch 51
waveform batch: 1/3
Using ADAM optimizer
Sum of params:213.58351
Changing learning rate to:1.7763568394002505e-17
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 24.083667994621774
Iteration: 2 || Loss: 24.083667994621774
Iteration: 3 || Loss: 24.083667994621774
Iteration: 4 || Loss: 24.083667994621774
Iteration: 5 || Loss: 24.083667994621774
Iteration: 6 || Loss: 24.083667994621774
saving ADAM checkpoint...
Sum of params:213.58351
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 24.083667994621774
Iteration: 2 || Loss: 24.05690294921129
Iteration: 3 || Loss: 24.019524258383015
Iteration: 4 || Loss: 23.99363156656687
Iteration: 5 || Loss: 23.818262349951834
Iteration: 6 || Loss: 23.688378620126702
Iteration: 7 || Loss: 23.612772840152672
Iteration: 8 || Loss: 23.57534180628937
Iteration: 9 || Loss: 23.424542635840094
Iteration: 10 || Loss: 23.272260139349
Iteration: 11 || Loss: 23.229692895646792
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:214.13074
Epoch 51 loss:23.229692895646792
waveform batch: 2/3
Using ADAM optimizer
Sum of params:214.13074
Changing learning rate to:8.881784197001253e-18
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.435879651512624
Iteration: 2 || Loss: 7.435879651512624
Iteration: 3 || Loss: 7.435879651512624
Iteration: 4 || Loss: 7.435879651512624
Iteration: 5 || Loss: 7.435879651512624
Iteration: 6 || Loss: 7.435879651512624
saving ADAM checkpoint...
Sum of params:214.13074
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.435879651512624
Iteration: 2 || Loss: 7.410777200609036
Iteration: 3 || Loss: 7.372049634065874
Iteration: 4 || Loss: 7.344863751050508
Iteration: 5 || Loss: 7.290565126845509
Iteration: 6 || Loss: 7.2033524977977805
Iteration: 7 || Loss: 7.1858051709773925
Iteration: 8 || Loss: 7.159324550700388
Iteration: 9 || Loss: 7.144243663541394
Iteration: 10 || Loss: 7.097257632576878
Iteration: 11 || Loss: 7.090669151027955
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:214.06467
Epoch 51 loss:7.090669151027955
waveform batch: 3/3
Using ADAM optimizer
Sum of params:214.06467
Changing learning rate to:4.440892098500626e-18
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.739457832344147
Iteration: 2 || Loss: 6.739457832344147
Iteration: 3 || Loss: 6.739457832344147
Iteration: 4 || Loss: 6.739457832344147
Iteration: 5 || Loss: 6.739457832344147
Iteration: 6 || Loss: 6.739457832344147
saving ADAM checkpoint...
Sum of params:214.06467
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.739457832344147
Iteration: 2 || Loss: 6.7318010013310445
Iteration: 3 || Loss: 6.726825831584399
Iteration: 4 || Loss: 6.722397881893707
Iteration: 5 || Loss: 6.701725896980329
Iteration: 6 || Loss: 6.6813599858338995
Iteration: 7 || Loss: 6.668554832364207
Iteration: 8 || Loss: 6.654257805096771
Iteration: 9 || Loss: 6.595258139317771
Iteration: 10 || Loss: 6.581959403815511
Iteration: 11 || Loss: 6.556298358791491
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:214.05612
Epoch 51 loss:6.556298358791491
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.53590798813654
waveform batch: 2/2
Test loss:8.116984109262146
Epoch 51 mean train loss:1.2292220135155414
Epoch 51 mean test loss:0.9795818880822755
Start training epoch 52
waveform batch: 1/3
Using ADAM optimizer
Sum of params:214.05612
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.826819008762723
Iteration: 2 || Loss: 23.826819008762723
Iteration: 3 || Loss: 23.826819008762723
Iteration: 4 || Loss: 23.826819008762723
Iteration: 5 || Loss: 23.826819008762723
Iteration: 6 || Loss: 23.826819008762723
saving ADAM checkpoint...
Sum of params:214.05612
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.826819008762723
Iteration: 2 || Loss: 23.80152362188098
Iteration: 3 || Loss: 23.762675319383387
Iteration: 4 || Loss: 23.72697672989853
Iteration: 5 || Loss: 23.557086666675573
Iteration: 6 || Loss: 23.423687871471707
Iteration: 7 || Loss: 23.352384618761906
Iteration: 8 || Loss: 23.309099644301547
Iteration: 9 || Loss: 23.114731358651657
Iteration: 10 || Loss: 22.99557803200303
Iteration: 11 || Loss: 22.95954360856064
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:214.64629
Epoch 52 loss:22.95954360856064
waveform batch: 2/3
Using ADAM optimizer
Sum of params:214.64629
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.369553656064426
Iteration: 2 || Loss: 7.369553656064426
Iteration: 3 || Loss: 7.369553656064426
Iteration: 4 || Loss: 7.369553656064426
Iteration: 5 || Loss: 7.369553656064426
Iteration: 6 || Loss: 7.369553656064426
saving ADAM checkpoint...
Sum of params:214.64629
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.369553656064426
Iteration: 2 || Loss: 7.337787157741114
Iteration: 3 || Loss: 7.295719008045965
Iteration: 4 || Loss: 7.268414112980133
Iteration: 5 || Loss: 7.215343777860514
Iteration: 6 || Loss: 7.143760529853187
Iteration: 7 || Loss: 7.1119904258633655
Iteration: 8 || Loss: 7.089123017137546
Iteration: 9 || Loss: 7.071740481083995
Iteration: 10 || Loss: 7.024205945363678
Iteration: 11 || Loss: 7.014207339351204
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:214.58966
Epoch 52 loss:7.014207339351204
waveform batch: 3/3
Using ADAM optimizer
Sum of params:214.58966
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.672713211996236
Iteration: 2 || Loss: 6.672713211996236
Iteration: 3 || Loss: 6.672713211996236
Iteration: 4 || Loss: 6.672713211996236
Iteration: 5 || Loss: 6.672713211996236
Iteration: 6 || Loss: 6.672713211996236
saving ADAM checkpoint...
Sum of params:214.58966
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.672713211996236
Iteration: 2 || Loss: 6.66330917191262
Iteration: 3 || Loss: 6.657810647884299
Iteration: 4 || Loss: 6.652367746458289
Iteration: 5 || Loss: 6.6306921700797785
Iteration: 6 || Loss: 6.6087473510324015
Iteration: 7 || Loss: 6.597294256539318
Iteration: 8 || Loss: 6.586273875662894
Iteration: 9 || Loss: 6.5230939872014435
Iteration: 10 || Loss: 6.513627097092774
Iteration: 11 || Loss: 6.485873000912021
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:214.52194
Epoch 52 loss:6.485873000912021
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.465726352292847
waveform batch: 2/2
Test loss:8.037616366071733
Epoch 52 mean train loss:1.215320798294129
Epoch 52 mean test loss:0.9707848657861516
Start training epoch 53
waveform batch: 1/3
Using ADAM optimizer
Sum of params:214.52194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.590696578455766
Iteration: 2 || Loss: 23.590696578455766
Iteration: 3 || Loss: 23.590696578455766
Iteration: 4 || Loss: 23.590696578455766
Iteration: 5 || Loss: 23.590696578455766
Iteration: 6 || Loss: 23.590696578455766
saving ADAM checkpoint...
Sum of params:214.52194
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.590696578455766
Iteration: 2 || Loss: 23.561547724197712
Iteration: 3 || Loss: 23.495871903474264
Iteration: 4 || Loss: 23.401868642328964
Iteration: 5 || Loss: 23.306658064810723
Iteration: 6 || Loss: 23.17187080372813
Iteration: 7 || Loss: 23.11056650185744
Iteration: 8 || Loss: 23.064523191181287
Iteration: 9 || Loss: 22.89553842448177
Iteration: 10 || Loss: 22.74186815519927
Iteration: 11 || Loss: 22.707749391330513
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:215.17761
Epoch 53 loss:22.707749391330513
waveform batch: 2/3
Using ADAM optimizer
Sum of params:215.17761
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.351068377737674
Iteration: 2 || Loss: 7.351068377737674
Iteration: 3 || Loss: 7.351068377737674
Iteration: 4 || Loss: 7.351068377737674
Iteration: 5 || Loss: 7.351068377737674
Iteration: 6 || Loss: 7.351068377737674
saving ADAM checkpoint...
Sum of params:215.17761
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.351068377737674
Iteration: 2 || Loss: 7.315495538468461
Iteration: 3 || Loss: 7.264456303800859
Iteration: 4 || Loss: 7.230452478696345
Iteration: 5 || Loss: 7.163698356898643
Iteration: 6 || Loss: 7.076488258336696
Iteration: 7 || Loss: 7.038219916023896
Iteration: 8 || Loss: 7.020358114962666
Iteration: 9 || Loss: 6.972967294408969
Iteration: 10 || Loss: 6.9526580414673
Iteration: 11 || Loss: 6.940959623487398
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:215.09134
Epoch 53 loss:6.940959623487398
waveform batch: 3/3
Using ADAM optimizer
Sum of params:215.09134
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.593109136386859
Iteration: 2 || Loss: 6.593109136386859
Iteration: 3 || Loss: 6.593109136386859
Iteration: 4 || Loss: 6.593109136386859
Iteration: 5 || Loss: 6.593109136386859
Iteration: 6 || Loss: 6.593109136386859
saving ADAM checkpoint...
Sum of params:215.09134
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.593109136386859
Iteration: 2 || Loss: 6.585445803116542
Iteration: 3 || Loss: 6.582676186600184
Iteration: 4 || Loss: 6.576265300705595
Iteration: 5 || Loss: 6.553112300287664
Iteration: 6 || Loss: 6.531730227826978
Iteration: 7 || Loss: 6.521727713458328
Iteration: 8 || Loss: 6.501765373809239
Iteration: 9 || Loss: 6.446673055727554
Iteration: 10 || Loss: 6.436301124103741
Iteration: 11 || Loss: 6.410267804478252
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:214.98148
Epoch 53 loss:6.410267804478252
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.361050890939454
waveform batch: 2/2
Test loss:7.95169726151733
Epoch 53 mean train loss:1.2019658939765387
Epoch 53 mean test loss:0.959573420732752
Start training epoch 54
waveform batch: 1/3
Using ADAM optimizer
Sum of params:214.98148
Changing learning rate to:2.220446049250313e-18
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.338578860558485
Iteration: 2 || Loss: 23.338578860558485
Iteration: 3 || Loss: 23.338578860558485
Iteration: 4 || Loss: 23.338578860558485
Iteration: 5 || Loss: 23.338578860558485
Iteration: 6 || Loss: 23.338578860558485
saving ADAM checkpoint...
Sum of params:214.98148
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.338578860558485
Iteration: 2 || Loss: 23.301483194459987
Iteration: 3 || Loss: 23.164810384595096
Iteration: 4 || Loss: 23.08916156848231
Iteration: 5 || Loss: 23.029350865363853
Iteration: 6 || Loss: 22.947233726980908
Iteration: 7 || Loss: 22.85304524080378
Iteration: 8 || Loss: 22.818669038327887
Iteration: 9 || Loss: 22.72695065269681
Iteration: 10 || Loss: 22.572511833859764
Iteration: 11 || Loss: 22.460224535430026
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:215.60843
Epoch 54 loss:22.460224535430026
waveform batch: 2/3
Using ADAM optimizer
Sum of params:215.60843
Changing learning rate to:1.1102230246251566e-18
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.258696996465398
Iteration: 2 || Loss: 7.258696996465398
Iteration: 3 || Loss: 7.258696996465398
Iteration: 4 || Loss: 7.258696996465398
Iteration: 5 || Loss: 7.258696996465398
Iteration: 6 || Loss: 7.258696996465398
saving ADAM checkpoint...
Sum of params:215.60843
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.258696996465398
Iteration: 2 || Loss: 7.216142478349857
Iteration: 3 || Loss: 7.177327821427433
Iteration: 4 || Loss: 7.154599268779977
Iteration: 5 || Loss: 7.0853328920801
Iteration: 6 || Loss: 6.999504051187425
Iteration: 7 || Loss: 6.9657187855607
Iteration: 8 || Loss: 6.949576306206742
Iteration: 9 || Loss: 6.921750280717457
Iteration: 10 || Loss: 6.885602875640146
Iteration: 11 || Loss: 6.865774110022254
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:215.58484
Epoch 54 loss:6.865774110022254
waveform batch: 3/3
Using ADAM optimizer
Sum of params:215.58484
Changing learning rate to:5.551115123125783e-19
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.527976971143777
Iteration: 2 || Loss: 6.527976971143777
Iteration: 3 || Loss: 6.527976971143777
Iteration: 4 || Loss: 6.527976971143777
Iteration: 5 || Loss: 6.527976971143777
Iteration: 6 || Loss: 6.527976971143777
saving ADAM checkpoint...
Sum of params:215.58484
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.527976971143777
Iteration: 2 || Loss: 6.5221196406598425
Iteration: 3 || Loss: 6.517854102888508
Iteration: 4 || Loss: 6.491964753107393
Iteration: 5 || Loss: 6.46715967543703
Iteration: 6 || Loss: 6.4612892127514625
Iteration: 7 || Loss: 6.451698630788889
Iteration: 8 || Loss: 6.43428287865491
Iteration: 9 || Loss: 6.378850619638342
Iteration: 10 || Loss: 6.3606102330959615
Iteration: 11 || Loss: 6.335396028554352
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:215.4014
Epoch 54 loss:6.335396028554352
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.254594661144283
waveform batch: 2/2
Test loss:7.881078036845596
Epoch 54 mean train loss:1.188713155800221
Epoch 54 mean test loss:0.9491572175288164
Start training epoch 55
waveform batch: 1/3
Using ADAM optimizer
Sum of params:215.4014
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 23.090290210011837
Iteration: 2 || Loss: 23.090290210011837
Iteration: 3 || Loss: 23.090290210011837
Iteration: 4 || Loss: 23.090290210011837
Iteration: 5 || Loss: 23.090290210011837
Iteration: 6 || Loss: 23.090290210011837
saving ADAM checkpoint...
Sum of params:215.4014
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 23.090290210011837
Iteration: 2 || Loss: 23.05986598833376
Iteration: 3 || Loss: 23.023940306032273
Iteration: 4 || Loss: 22.98693524024132
Iteration: 5 || Loss: 22.769058058042766
Iteration: 6 || Loss: 22.658161011984337
Iteration: 7 || Loss: 22.60128600203716
Iteration: 8 || Loss: 22.55721688602487
Iteration: 9 || Loss: 22.410668422827726
Iteration: 10 || Loss: 22.263552686936276
Iteration: 11 || Loss: 22.20267474564644
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.05736
Epoch 55 loss:22.20267474564644
waveform batch: 2/3
Using ADAM optimizer
Sum of params:216.05736
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.18930227645871
Iteration: 2 || Loss: 7.18930227645871
Iteration: 3 || Loss: 7.18930227645871
Iteration: 4 || Loss: 7.18930227645871
Iteration: 5 || Loss: 7.18930227645871
Iteration: 6 || Loss: 7.18930227645871
saving ADAM checkpoint...
Sum of params:216.05736
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.18930227645871
Iteration: 2 || Loss: 7.153463577427011
Iteration: 3 || Loss: 7.105976883583697
Iteration: 4 || Loss: 7.076612413442647
Iteration: 5 || Loss: 7.008009412728288
Iteration: 6 || Loss: 6.9219503468225065
Iteration: 7 || Loss: 6.889914316778614
Iteration: 8 || Loss: 6.868164266965368
Iteration: 9 || Loss: 6.827582438759314
Iteration: 10 || Loss: 6.800530652531746
Iteration: 11 || Loss: 6.78865979510932
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.0101
Epoch 55 loss:6.78865979510932
waveform batch: 3/3
Using ADAM optimizer
Sum of params:216.0101
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.45687537108759
Iteration: 2 || Loss: 6.45687537108759
Iteration: 3 || Loss: 6.45687537108759
Iteration: 4 || Loss: 6.45687537108759
Iteration: 5 || Loss: 6.45687537108759
Iteration: 6 || Loss: 6.45687537108759
saving ADAM checkpoint...
Sum of params:216.0101
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.45687537108759
Iteration: 2 || Loss: 6.450218258091458
Iteration: 3 || Loss: 6.447168994687724
Iteration: 4 || Loss: 6.415969676556786
Iteration: 5 || Loss: 6.385953330888436
Iteration: 6 || Loss: 6.381415775772607
Iteration: 7 || Loss: 6.373605819597706
Iteration: 8 || Loss: 6.348221499602874
Iteration: 9 || Loss: 6.308972355204415
Iteration: 10 || Loss: 6.294811877978082
Iteration: 11 || Loss: 6.262385966578216
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:215.77502
Epoch 55 loss:6.262385966578216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.16614041354531
waveform batch: 2/2
Test loss:7.796615399300633
Epoch 55 mean train loss:1.1751240169111326
Epoch 55 mean test loss:0.9389856360497614
Start training epoch 56
waveform batch: 1/3
Using ADAM optimizer
Sum of params:215.77502
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.831879672626705
Iteration: 2 || Loss: 22.831879672626705
Iteration: 3 || Loss: 22.831879672626705
Iteration: 4 || Loss: 22.831879672626705
Iteration: 5 || Loss: 22.831879672626705
Iteration: 6 || Loss: 22.831879672626705
saving ADAM checkpoint...
Sum of params:215.77502
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.831879672626705
Iteration: 2 || Loss: 22.79527304544863
Iteration: 3 || Loss: 22.755845442713397
Iteration: 4 || Loss: 22.677601579873073
Iteration: 5 || Loss: 22.49450089381785
Iteration: 6 || Loss: 22.40450018255228
Iteration: 7 || Loss: 22.341770654999262
Iteration: 8 || Loss: 22.303895890155214
Iteration: 9 || Loss: 22.201436894262905
Iteration: 10 || Loss: 22.044484390325003
Iteration: 11 || Loss: 21.955147961971527
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.42653
Epoch 56 loss:21.955147961971527
waveform batch: 2/3
Using ADAM optimizer
Sum of params:216.42653
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.115571745985448
Iteration: 2 || Loss: 7.115571745985448
Iteration: 3 || Loss: 7.115571745985448
Iteration: 4 || Loss: 7.115571745985448
Iteration: 5 || Loss: 7.115571745985448
Iteration: 6 || Loss: 7.115571745985448
saving ADAM checkpoint...
Sum of params:216.42653
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.115571745985448
Iteration: 2 || Loss: 7.074043287871814
Iteration: 3 || Loss: 7.0241596752502
Iteration: 4 || Loss: 6.995712333342921
Iteration: 5 || Loss: 6.924887429767389
Iteration: 6 || Loss: 6.852503385153226
Iteration: 7 || Loss: 6.8125466740769385
Iteration: 8 || Loss: 6.798273788659234
Iteration: 9 || Loss: 6.775109920178732
Iteration: 10 || Loss: 6.725993097653532
Iteration: 11 || Loss: 6.713902230860851
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.42224
Epoch 56 loss:6.713902230860851
waveform batch: 3/3
Using ADAM optimizer
Sum of params:216.42224
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.3920365300593325
Iteration: 2 || Loss: 6.3920365300593325
Iteration: 3 || Loss: 6.3920365300593325
Iteration: 4 || Loss: 6.3920365300593325
Iteration: 5 || Loss: 6.3920365300593325
Iteration: 6 || Loss: 6.3920365300593325
saving ADAM checkpoint...
Sum of params:216.42224
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.3920365300593325
Iteration: 2 || Loss: 6.386022539191328
Iteration: 3 || Loss: 6.382174614374841
Iteration: 4 || Loss: 6.3566601607737745
Iteration: 5 || Loss: 6.343910627003505
Iteration: 6 || Loss: 6.309894483948304
Iteration: 7 || Loss: 6.302131206700334
Iteration: 8 || Loss: 6.287248000587264
Iteration: 9 || Loss: 6.250751350378103
Iteration: 10 || Loss: 6.233636919055316
Iteration: 11 || Loss: 6.195167378312068
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.10684
Epoch 56 loss:6.195167378312068
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.112939629678703
waveform batch: 2/2
Test loss:7.717642156929179
Epoch 56 mean train loss:1.162140585704815
Epoch 56 mean test loss:0.9312106933298754
Start training epoch 57
waveform batch: 1/3
Using ADAM optimizer
Sum of params:216.10684
Changing learning rate to:2.7755575615628914e-19
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.69039114527098
Iteration: 2 || Loss: 22.69039114527098
Iteration: 3 || Loss: 22.69039114527098
Iteration: 4 || Loss: 22.69039114527098
Iteration: 5 || Loss: 22.69039114527098
Iteration: 6 || Loss: 22.69039114527098
saving ADAM checkpoint...
Sum of params:216.10684
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.69039114527098
Iteration: 2 || Loss: 22.63986719551013
Iteration: 3 || Loss: 22.570244741244984
Iteration: 4 || Loss: 22.41136031872308
Iteration: 5 || Loss: 22.277563109539724
Iteration: 6 || Loss: 22.20922006075712
Iteration: 7 || Loss: 22.121641373978527
Iteration: 8 || Loss: 22.095512328632104
Iteration: 9 || Loss: 21.972369468968907
Iteration: 10 || Loss: 21.811346556010243
Iteration: 11 || Loss: 21.74939283647535
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.74588
Epoch 57 loss:21.74939283647535
waveform batch: 2/3
Using ADAM optimizer
Sum of params:216.74588
Changing learning rate to:1.3877787807814457e-19
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 7.088345004343677
Iteration: 2 || Loss: 7.088345004343677
Iteration: 3 || Loss: 7.088345004343677
Iteration: 4 || Loss: 7.088345004343677
Iteration: 5 || Loss: 7.088345004343677
Iteration: 6 || Loss: 7.088345004343677
saving ADAM checkpoint...
Sum of params:216.74588
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 7.088345004343677
Iteration: 2 || Loss: 7.023023895049034
Iteration: 3 || Loss: 6.970407567393866
Iteration: 4 || Loss: 6.928740561064431
Iteration: 5 || Loss: 6.844296499074322
Iteration: 6 || Loss: 6.804567432501899
Iteration: 7 || Loss: 6.733985387321295
Iteration: 8 || Loss: 6.726483456705623
Iteration: 9 || Loss: 6.70787266830779
Iteration: 10 || Loss: 6.697643753816819
Iteration: 11 || Loss: 6.640168521562758
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.7579
Epoch 57 loss:6.640168521562758
waveform batch: 3/3
Using ADAM optimizer
Sum of params:216.7579
Changing learning rate to:6.938893903907229e-20
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.320805235695914
Iteration: 2 || Loss: 6.320805235695914
Iteration: 3 || Loss: 6.320805235695914
Iteration: 4 || Loss: 6.320805235695914
Iteration: 5 || Loss: 6.320805235695914
Iteration: 6 || Loss: 6.320805235695914
saving ADAM checkpoint...
Sum of params:216.7579
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.320805235695914
Iteration: 2 || Loss: 6.314989202783604
Iteration: 3 || Loss: 6.308264471888185
Iteration: 4 || Loss: 6.295119111604383
Iteration: 5 || Loss: 6.2711740491750625
Iteration: 6 || Loss: 6.2355461894332365
Iteration: 7 || Loss: 6.224600655356053
Iteration: 8 || Loss: 6.215165585353958
Iteration: 9 || Loss: 6.1774637390826195
Iteration: 10 || Loss: 6.163131859631004
Iteration: 11 || Loss: 6.124220035012643
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.44595
Epoch 57 loss:6.124220035012643
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:8.024885261087666
waveform batch: 2/2
Test loss:7.652815337837945
Epoch 57 mean train loss:1.1504593797683582
Epoch 57 mean test loss:0.9222176822897419
Start training epoch 58
waveform batch: 1/3
Using ADAM optimizer
Sum of params:216.44595
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.552661719921403
Iteration: 2 || Loss: 22.552661719921403
Iteration: 3 || Loss: 22.552661719921403
Iteration: 4 || Loss: 22.552661719921403
Iteration: 5 || Loss: 22.552661719921403
Iteration: 6 || Loss: 22.552661719921403
saving ADAM checkpoint...
Sum of params:216.44595
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.552661719921403
Iteration: 2 || Loss: 22.502069344322706
Iteration: 3 || Loss: 22.456523518395247
Iteration: 4 || Loss: 22.39963389806075
Iteration: 5 || Loss: 22.098512965370475
Iteration: 6 || Loss: 22.024386322921053
Iteration: 7 || Loss: 21.932937128751337
Iteration: 8 || Loss: 21.902370979441212
Iteration: 9 || Loss: 21.76966306302977
Iteration: 10 || Loss: 21.590974371289338
Iteration: 11 || Loss: 21.556627435317143
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.06416
Epoch 58 loss:21.556627435317143
waveform batch: 2/3
Using ADAM optimizer
Sum of params:217.06416
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.932827160605634
Iteration: 2 || Loss: 6.932827160605634
Iteration: 3 || Loss: 6.932827160605634
Iteration: 4 || Loss: 6.932827160605634
Iteration: 5 || Loss: 6.932827160605634
Iteration: 6 || Loss: 6.932827160605634
saving ADAM checkpoint...
Sum of params:217.06416
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.932827160605634
Iteration: 2 || Loss: 6.911880306961143
Iteration: 3 || Loss: 6.8657264945267515
Iteration: 4 || Loss: 6.837358366958624
Iteration: 5 || Loss: 6.761426885696735
Iteration: 6 || Loss: 6.657713192135358
Iteration: 7 || Loss: 6.648125013960935
Iteration: 8 || Loss: 6.599849240202703
Iteration: 9 || Loss: 6.584356341556524
Iteration: 10 || Loss: 6.5576462210760145
Iteration: 11 || Loss: 6.543270127112333
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.02194
Epoch 58 loss:6.543270127112333
waveform batch: 3/3
Using ADAM optimizer
Sum of params:217.02194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.220336662216282
Iteration: 2 || Loss: 6.220336662216282
Iteration: 3 || Loss: 6.220336662216282
Iteration: 4 || Loss: 6.220336662216282
Iteration: 5 || Loss: 6.220336662216282
Iteration: 6 || Loss: 6.220336662216282
saving ADAM checkpoint...
Sum of params:217.02194
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.220336662216282
Iteration: 2 || Loss: 6.210601293729071
Iteration: 3 || Loss: 6.2081036619241745
Iteration: 4 || Loss: 6.16919189606989
Iteration: 5 || Loss: 6.154455489635
Iteration: 6 || Loss: 6.143226490186418
Iteration: 7 || Loss: 6.136061165239817
Iteration: 8 || Loss: 6.1244539774864535
Iteration: 9 || Loss: 6.089420735065319
Iteration: 10 || Loss: 6.076025581920977
Iteration: 11 || Loss: 6.060039606257234
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:216.77629
Epoch 58 loss:6.060039606257234
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.871000309699138
waveform batch: 2/2
Test loss:7.574896115339456
Epoch 58 mean train loss:1.138664572289557
Epoch 58 mean test loss:0.908582142649329
Start training epoch 59
waveform batch: 1/3
Using ADAM optimizer
Sum of params:216.77629
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 22.129151088242594
Iteration: 2 || Loss: 22.129151088242594
Iteration: 3 || Loss: 22.129151088242594
Iteration: 4 || Loss: 22.129151088242594
Iteration: 5 || Loss: 22.129151088242594
Iteration: 6 || Loss: 22.129151088242594
saving ADAM checkpoint...
Sum of params:216.77629
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 22.129151088242594
Iteration: 2 || Loss: 22.115907399377686
Iteration: 3 || Loss: 22.09111488714655
Iteration: 4 || Loss: 22.009493965712387
Iteration: 5 || Loss: 21.746531149746176
Iteration: 6 || Loss: 21.628149914115024
Iteration: 7 || Loss: 21.590993980972783
Iteration: 8 || Loss: 21.47561503344138
Iteration: 9 || Loss: 21.302870078421012
Iteration: 10 || Loss: 21.26394160232597
Iteration: 11 || Loss: 21.157278450266116
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.70898
Epoch 59 loss:21.157278450266116
waveform batch: 2/3
Using ADAM optimizer
Sum of params:217.70898
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.859035532036998
Iteration: 2 || Loss: 6.859035532036998
Iteration: 3 || Loss: 6.859035532036998
Iteration: 4 || Loss: 6.859035532036998
Iteration: 5 || Loss: 6.859035532036998
Iteration: 6 || Loss: 6.859035532036998
saving ADAM checkpoint...
Sum of params:217.70898
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.859035532036998
Iteration: 2 || Loss: 6.831974275592421
Iteration: 3 || Loss: 6.768187747341819
Iteration: 4 || Loss: 6.731619891006505
Iteration: 5 || Loss: 6.692493136732451
Iteration: 6 || Loss: 6.6340816404468175
Iteration: 7 || Loss: 6.615239866172392
Iteration: 8 || Loss: 6.5354873599830094
Iteration: 9 || Loss: 6.497118894533152
Iteration: 10 || Loss: 6.475699928700316
Iteration: 11 || Loss: 6.451899695710479
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.45956
Epoch 59 loss:6.451899695710479
waveform batch: 3/3
Using ADAM optimizer
Sum of params:217.45956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.203851177537984
Iteration: 2 || Loss: 6.203851177537984
Iteration: 3 || Loss: 6.203851177537984
Iteration: 4 || Loss: 6.203851177537984
Iteration: 5 || Loss: 6.203851177537984
Iteration: 6 || Loss: 6.203851177537984
saving ADAM checkpoint...
Sum of params:217.45956
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.203851177537984
Iteration: 2 || Loss: 6.197162178215534
Iteration: 3 || Loss: 6.194320752613896
Iteration: 4 || Loss: 6.153680338289635
Iteration: 5 || Loss: 6.140756539330326
Iteration: 6 || Loss: 6.114235422924076
Iteration: 7 || Loss: 6.096365356518046
Iteration: 8 || Loss: 6.083078223573341
Iteration: 9 || Loss: 6.039399353240686
Iteration: 10 || Loss: 6.0260260812540505
Iteration: 11 || Loss: 5.985178353567145
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.18575
Epoch 59 loss:5.985178353567145
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.793624143880267
waveform batch: 2/2
Test loss:7.475896508878204
Epoch 59 mean train loss:1.1198118833181245
Epoch 59 mean test loss:0.8982070972210865
Start training epoch 60
waveform batch: 1/3
Using ADAM optimizer
Sum of params:217.18575
Changing learning rate to:3.469446951953614e-20
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.81288992715429
Iteration: 2 || Loss: 21.81288992715429
Iteration: 3 || Loss: 21.81288992715429
Iteration: 4 || Loss: 21.81288992715429
Iteration: 5 || Loss: 21.81288992715429
Iteration: 6 || Loss: 21.81288992715429
saving ADAM checkpoint...
Sum of params:217.18575
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.81288992715429
Iteration: 2 || Loss: 21.78312899879046
Iteration: 3 || Loss: 21.671902011943345
Iteration: 4 || Loss: 21.515421969123402
Iteration: 5 || Loss: 21.475689060252762
Iteration: 6 || Loss: 21.392178980115446
Iteration: 7 || Loss: 21.330521434130947
Iteration: 8 || Loss: 21.287281301755574
Iteration: 9 || Loss: 21.173252250120477
Iteration: 10 || Loss: 21.024304673838397
Iteration: 11 || Loss: 20.959267483977293
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.8545
Epoch 60 loss:20.959267483977293
waveform batch: 2/3
Using ADAM optimizer
Sum of params:217.8545
Changing learning rate to:1.734723475976807e-20
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.7601721031134785
Iteration: 2 || Loss: 6.7601721031134785
Iteration: 3 || Loss: 6.7601721031134785
Iteration: 4 || Loss: 6.7601721031134785
Iteration: 5 || Loss: 6.7601721031134785
Iteration: 6 || Loss: 6.7601721031134785
saving ADAM checkpoint...
Sum of params:217.8545
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.7601721031134785
Iteration: 2 || Loss: 6.720047322655852
Iteration: 3 || Loss: 6.675042159006398
Iteration: 4 || Loss: 6.644058660513607
Iteration: 5 || Loss: 6.579483352875021
Iteration: 6 || Loss: 6.519379421316424
Iteration: 7 || Loss: 6.475320359849596
Iteration: 8 || Loss: 6.455076294305154
Iteration: 9 || Loss: 6.429942129395863
Iteration: 10 || Loss: 6.394166805469953
Iteration: 11 || Loss: 6.379443451480135
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.8689
Epoch 60 loss:6.379443451480135
waveform batch: 3/3
Using ADAM optimizer
Sum of params:217.8689
Changing learning rate to:8.673617379884036e-21
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.107877701802567
Iteration: 2 || Loss: 6.107877701802567
Iteration: 3 || Loss: 6.107877701802567
Iteration: 4 || Loss: 6.107877701802567
Iteration: 5 || Loss: 6.107877701802567
Iteration: 6 || Loss: 6.107877701802567
saving ADAM checkpoint...
Sum of params:217.8689
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.107877701802567
Iteration: 2 || Loss: 6.101149611542655
Iteration: 3 || Loss: 6.096970292057972
Iteration: 4 || Loss: 6.08728003135093
Iteration: 5 || Loss: 6.043960209704683
Iteration: 6 || Loss: 6.006900723752621
Iteration: 7 || Loss: 6.001940475569475
Iteration: 8 || Loss: 5.983869458746282
Iteration: 9 || Loss: 5.963463320190543
Iteration: 10 || Loss: 5.937094968922219
Iteration: 11 || Loss: 5.902305088745459
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.55832
Epoch 60 loss:5.902305088745459
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.653482810639178
waveform batch: 2/2
Test loss:7.3719497556862175
Epoch 60 mean train loss:1.1080338674734296
Epoch 60 mean test loss:0.8838489744897292
Start training epoch 61
waveform batch: 1/3
Using ADAM optimizer
Sum of params:217.55832
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.656909816646028
Iteration: 2 || Loss: 21.656909816646028
Iteration: 3 || Loss: 21.656909816646028
Iteration: 4 || Loss: 21.656909816646028
Iteration: 5 || Loss: 21.656909816646028
Iteration: 6 || Loss: 21.656909816646028
saving ADAM checkpoint...
Sum of params:217.55832
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.656909816646028
Iteration: 2 || Loss: 21.626745775604647
Iteration: 3 || Loss: 21.588742332211503
Iteration: 4 || Loss: 21.34399722461449
Iteration: 5 || Loss: 21.24693103189761
Iteration: 6 || Loss: 21.17091235746656
Iteration: 7 || Loss: 21.110228613113115
Iteration: 8 || Loss: 21.059862717695882
Iteration: 9 || Loss: 20.80052153478907
Iteration: 10 || Loss: 20.774000570089484
Iteration: 11 || Loss: 20.72386878234895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:218.25542
Epoch 61 loss:20.72386878234895
waveform batch: 2/3
Using ADAM optimizer
Sum of params:218.25542
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.767832218631309
Iteration: 2 || Loss: 6.767832218631309
Iteration: 3 || Loss: 6.767832218631309
Iteration: 4 || Loss: 6.767832218631309
Iteration: 5 || Loss: 6.767832218631309
Iteration: 6 || Loss: 6.767832218631309
saving ADAM checkpoint...
Sum of params:218.25542
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.767832218631309
Iteration: 2 || Loss: 6.699319380659101
Iteration: 3 || Loss: 6.640454603791848
Iteration: 4 || Loss: 6.597514805911768
Iteration: 5 || Loss: 6.508112789391368
Iteration: 6 || Loss: 6.4768249057874705
Iteration: 7 || Loss: 6.413301267394404
Iteration: 8 || Loss: 6.406856752571955
Iteration: 9 || Loss: 6.384223078817373
Iteration: 10 || Loss: 6.36843782320927
Iteration: 11 || Loss: 6.312641301963757
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:218.2184
Epoch 61 loss:6.312641301963757
waveform batch: 3/3
Using ADAM optimizer
Sum of params:218.2184
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.016757084320168
Iteration: 2 || Loss: 6.016757084320168
Iteration: 3 || Loss: 6.016757084320168
Iteration: 4 || Loss: 6.016757084320168
Iteration: 5 || Loss: 6.016757084320168
Iteration: 6 || Loss: 6.016757084320168
saving ADAM checkpoint...
Sum of params:218.2184
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.016757084320168
Iteration: 2 || Loss: 6.010360919018419
Iteration: 3 || Loss: 6.0085661655903255
Iteration: 4 || Loss: 5.995641639181045
Iteration: 5 || Loss: 5.953494291397259
Iteration: 6 || Loss: 5.933671035145992
Iteration: 7 || Loss: 5.924213358231987
Iteration: 8 || Loss: 5.900798323677517
Iteration: 9 || Loss: 5.882326568116757
Iteration: 10 || Loss: 5.864588083825597
Iteration: 11 || Loss: 5.813425096747946
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:217.95212
Epoch 61 loss:5.813425096747946
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.5301547784327205
waveform batch: 2/2
Test loss:7.283065213600469
Epoch 61 mean train loss:1.0949978393686886
Epoch 61 mean test loss:0.8713658818843053
Start training epoch 62
waveform batch: 1/3
Using ADAM optimizer
Sum of params:217.95212
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.409221860701138
Iteration: 2 || Loss: 21.409221860701138
Iteration: 3 || Loss: 21.409221860701138
Iteration: 4 || Loss: 21.409221860701138
Iteration: 5 || Loss: 21.409221860701138
Iteration: 6 || Loss: 21.409221860701138
saving ADAM checkpoint...
Sum of params:217.95212
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.409221860701138
Iteration: 2 || Loss: 21.37933614249565
Iteration: 3 || Loss: 21.30594940216017
Iteration: 4 || Loss: 21.157846433641126
Iteration: 5 || Loss: 21.04245613028611
Iteration: 6 || Loss: 20.970314419879955
Iteration: 7 || Loss: 20.89798131557141
Iteration: 8 || Loss: 20.8511441417689
Iteration: 9 || Loss: 20.568799870523318
Iteration: 10 || Loss: 20.536866344239712
Iteration: 11 || Loss: 20.4854239893537
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:218.54889
Epoch 62 loss:20.4854239893537
waveform batch: 2/3
Using ADAM optimizer
Sum of params:218.54889
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.608388799874662
Iteration: 2 || Loss: 6.608388799874662
Iteration: 3 || Loss: 6.608388799874662
Iteration: 4 || Loss: 6.608388799874662
Iteration: 5 || Loss: 6.608388799874662
Iteration: 6 || Loss: 6.608388799874662
saving ADAM checkpoint...
Sum of params:218.54889
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.608388799874662
Iteration: 2 || Loss: 6.56635325702348
Iteration: 3 || Loss: 6.518717973106873
Iteration: 4 || Loss: 6.492362968062233
Iteration: 5 || Loss: 6.426752128181328
Iteration: 6 || Loss: 6.373231750700179
Iteration: 7 || Loss: 6.344261629609054
Iteration: 8 || Loss: 6.324919636419006
Iteration: 9 || Loss: 6.308892792058441
Iteration: 10 || Loss: 6.241850722157205
Iteration: 11 || Loss: 6.230883167434224
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:218.62563
Epoch 62 loss:6.230883167434224
waveform batch: 3/3
Using ADAM optimizer
Sum of params:218.62563
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.940946096492578
Iteration: 2 || Loss: 5.940946096492578
Iteration: 3 || Loss: 5.940946096492578
Iteration: 4 || Loss: 5.940946096492578
Iteration: 5 || Loss: 5.940946096492578
Iteration: 6 || Loss: 5.940946096492578
saving ADAM checkpoint...
Sum of params:218.62563
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.940946096492578
Iteration: 2 || Loss: 5.937607635807329
Iteration: 3 || Loss: 5.93317115221255
Iteration: 4 || Loss: 5.92318741488565
Iteration: 5 || Loss: 5.8889898624009565
Iteration: 6 || Loss: 5.850587868022058
Iteration: 7 || Loss: 5.841249865317209
Iteration: 8 || Loss: 5.824634426941636
Iteration: 9 || Loss: 5.802350919763134
Iteration: 10 || Loss: 5.761550117590819
Iteration: 11 || Loss: 5.728504175342657
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:218.31367
Epoch 62 loss:5.728504175342657
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.40813852014166
waveform batch: 2/2
Test loss:7.19194725941524
Epoch 62 mean train loss:1.0814937110710194
Epoch 62 mean test loss:0.8588285752680529
Start training epoch 63
waveform batch: 1/3
Using ADAM optimizer
Sum of params:218.31367
Changing learning rate to:4.336808689942018e-21
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 21.11544248062153
Iteration: 2 || Loss: 21.11544248062153
Iteration: 3 || Loss: 21.11544248062153
Iteration: 4 || Loss: 21.11544248062153
Iteration: 5 || Loss: 21.11544248062153
Iteration: 6 || Loss: 21.11544248062153
saving ADAM checkpoint...
Sum of params:218.31367
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 21.11544248062153
Iteration: 2 || Loss: 21.09132573420814
Iteration: 3 || Loss: 21.053564221140242
Iteration: 4 || Loss: 20.942633627750002
Iteration: 5 || Loss: 20.77580239310369
Iteration: 6 || Loss: 20.695658774830555
Iteration: 7 || Loss: 20.637556058632722
Iteration: 8 || Loss: 20.556622111534562
Iteration: 9 || Loss: 20.308043769235283
Iteration: 10 || Loss: 20.275719712598296
Iteration: 11 || Loss: 20.18461590788084
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.05879
Epoch 63 loss:20.18461590788084
waveform batch: 2/3
Using ADAM optimizer
Sum of params:219.05879
Changing learning rate to:2.168404344971009e-21
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.58748549256601
Iteration: 2 || Loss: 6.58748549256601
Iteration: 3 || Loss: 6.58748549256601
Iteration: 4 || Loss: 6.58748549256601
Iteration: 5 || Loss: 6.58748549256601
Iteration: 6 || Loss: 6.58748549256601
saving ADAM checkpoint...
Sum of params:219.05879
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.58748549256601
Iteration: 2 || Loss: 6.508989555133349
Iteration: 3 || Loss: 6.452889040380223
Iteration: 4 || Loss: 6.419486211283957
Iteration: 5 || Loss: 6.373541085015224
Iteration: 6 || Loss: 6.343287559411455
Iteration: 7 || Loss: 6.2931598287462185
Iteration: 8 || Loss: 6.285742624607731
Iteration: 9 || Loss: 6.227935458315282
Iteration: 10 || Loss: 6.18942327482257
Iteration: 11 || Loss: 6.151069795570117
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.01407
Epoch 63 loss:6.151069795570117
waveform batch: 3/3
Using ADAM optimizer
Sum of params:219.01407
Changing learning rate to:1.0842021724855045e-21
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.865164015080561
Iteration: 2 || Loss: 5.865164015080561
Iteration: 3 || Loss: 5.865164015080561
Iteration: 4 || Loss: 5.865164015080561
Iteration: 5 || Loss: 5.865164015080561
Iteration: 6 || Loss: 5.865164015080561
saving ADAM checkpoint...
Sum of params:219.01407
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.865164015080561
Iteration: 2 || Loss: 5.860506745402428
Iteration: 3 || Loss: 5.8585243312487325
Iteration: 4 || Loss: 5.843112198957604
Iteration: 5 || Loss: 5.814052098059384
Iteration: 6 || Loss: 5.777217187609817
Iteration: 7 || Loss: 5.766557870242146
Iteration: 8 || Loss: 5.747565935230369
Iteration: 9 || Loss: 5.724572845318828
Iteration: 10 || Loss: 5.67627447764092
Iteration: 11 || Loss: 5.646760598833555
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:218.74222
Epoch 63 loss:5.646760598833555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.2812535053275464
waveform batch: 2/2
Test loss:7.094451437280953
Epoch 63 mean train loss:1.0660815434094837
Epoch 63 mean test loss:0.8456297025063824
Start training epoch 64
waveform batch: 1/3
Using ADAM optimizer
Sum of params:218.74222
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.813139140720992
Iteration: 2 || Loss: 20.813139140720992
Iteration: 3 || Loss: 20.813139140720992
Iteration: 4 || Loss: 20.813139140720992
Iteration: 5 || Loss: 20.813139140720992
Iteration: 6 || Loss: 20.813139140720992
saving ADAM checkpoint...
Sum of params:218.74222
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.813139140720992
Iteration: 2 || Loss: 20.779608775470365
Iteration: 3 || Loss: 20.754938320603635
Iteration: 4 || Loss: 20.545623697270102
Iteration: 5 || Loss: 20.475637724426925
Iteration: 6 || Loss: 20.417607399348665
Iteration: 7 || Loss: 20.345802573556597
Iteration: 8 || Loss: 20.30298846951602
Iteration: 9 || Loss: 20.114178984390517
Iteration: 10 || Loss: 20.033744488445073
Iteration: 11 || Loss: 19.99781224635895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.18272
Epoch 64 loss:19.99781224635895
waveform batch: 2/3
Using ADAM optimizer
Sum of params:219.18272
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.427998254585248
Iteration: 2 || Loss: 6.427998254585248
Iteration: 3 || Loss: 6.427998254585248
Iteration: 4 || Loss: 6.427998254585248
Iteration: 5 || Loss: 6.427998254585248
Iteration: 6 || Loss: 6.427998254585248
saving ADAM checkpoint...
Sum of params:219.18272
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.427998254585248
Iteration: 2 || Loss: 6.4055636375731275
Iteration: 3 || Loss: 6.366164567752055
Iteration: 4 || Loss: 6.341047604198536
Iteration: 5 || Loss: 6.259733000599966
Iteration: 6 || Loss: 6.174611503057277
Iteration: 7 || Loss: 6.156982592080117
Iteration: 8 || Loss: 6.137559055144737
Iteration: 9 || Loss: 6.0987430426184135
Iteration: 10 || Loss: 6.047941120791199
Iteration: 11 || Loss: 6.030432265627568
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.31256
Epoch 64 loss:6.030432265627568
waveform batch: 3/3
Using ADAM optimizer
Sum of params:219.31256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.795976554240603
Iteration: 2 || Loss: 5.795976554240603
Iteration: 3 || Loss: 5.795976554240603
Iteration: 4 || Loss: 5.795976554240603
Iteration: 5 || Loss: 5.795976554240603
Iteration: 6 || Loss: 5.795976554240603
saving ADAM checkpoint...
Sum of params:219.31256
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.795976554240603
Iteration: 2 || Loss: 5.768638505917117
Iteration: 3 || Loss: 5.760671640324837
Iteration: 4 || Loss: 5.75623036467178
Iteration: 5 || Loss: 5.693697942343545
Iteration: 6 || Loss: 5.674620180995242
Iteration: 7 || Loss: 5.664621680985031
Iteration: 8 || Loss: 5.659624839211302
Iteration: 9 || Loss: 5.649218404850006
Iteration: 10 || Loss: 5.624928921627169
Iteration: 11 || Loss: 5.616280420519666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.15964
Epoch 64 loss:5.616280420519666
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.119927498768852
waveform batch: 2/2
Test loss:7.025060418949597
Epoch 64 mean train loss:1.054817497750206
Epoch 64 mean test loss:0.8320581128069676
Start training epoch 65
waveform batch: 1/3
Using ADAM optimizer
Sum of params:219.15964
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.45888134471161
Iteration: 2 || Loss: 20.45888134471161
Iteration: 3 || Loss: 20.45888134471161
Iteration: 4 || Loss: 20.45888134471161
Iteration: 5 || Loss: 20.45888134471161
Iteration: 6 || Loss: 20.45888134471161
saving ADAM checkpoint...
Sum of params:219.15964
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.45888134471161
Iteration: 2 || Loss: 20.4308041759766
Iteration: 3 || Loss: 20.39440406105875
Iteration: 4 || Loss: 20.277043303573627
Iteration: 5 || Loss: 20.12252106044832
Iteration: 6 || Loss: 20.06932797963784
Iteration: 7 || Loss: 20.006144685628485
Iteration: 8 || Loss: 19.979921214700628
Iteration: 9 || Loss: 19.87036895760317
Iteration: 10 || Loss: 19.830028367307534
Iteration: 11 || Loss: 19.785710164243216
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.50229
Epoch 65 loss:19.785710164243216
waveform batch: 2/3
Using ADAM optimizer
Sum of params:219.50229
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.322910924046805
Iteration: 2 || Loss: 6.322910924046805
Iteration: 3 || Loss: 6.322910924046805
Iteration: 4 || Loss: 6.322910924046805
Iteration: 5 || Loss: 6.322910924046805
Iteration: 6 || Loss: 6.322910924046805
saving ADAM checkpoint...
Sum of params:219.50229
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.322910924046805
Iteration: 2 || Loss: 6.265785287338689
Iteration: 3 || Loss: 6.223439394527529
Iteration: 4 || Loss: 6.188440183440228
Iteration: 5 || Loss: 6.118746707873306
Iteration: 6 || Loss: 6.09779984234376
Iteration: 7 || Loss: 6.054503468582216
Iteration: 8 || Loss: 6.050337394170185
Iteration: 9 || Loss: 5.9937679663503145
Iteration: 10 || Loss: 5.983243384228319
Iteration: 11 || Loss: 5.9435663015101685
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.59776
Epoch 65 loss:5.9435663015101685
waveform batch: 3/3
Using ADAM optimizer
Sum of params:219.59776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.730331643579305
Iteration: 2 || Loss: 5.730331643579305
Iteration: 3 || Loss: 5.730331643579305
Iteration: 4 || Loss: 5.730331643579305
Iteration: 5 || Loss: 5.730331643579305
Iteration: 6 || Loss: 5.730331643579305
saving ADAM checkpoint...
Sum of params:219.59776
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.730331643579305
Iteration: 2 || Loss: 5.720985145902776
Iteration: 3 || Loss: 5.717435802446255
Iteration: 4 || Loss: 5.710554740901648
Iteration: 5 || Loss: 5.656815898494545
Iteration: 6 || Loss: 5.628150255717997
Iteration: 7 || Loss: 5.614964223499338
Iteration: 8 || Loss: 5.587350457720389
Iteration: 9 || Loss: 5.575346021020421
Iteration: 10 || Loss: 5.564604530553895
Iteration: 11 || Loss: 5.5241411435194285
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.36171
Epoch 65 loss:5.5241411435194285
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:7.0280189186495345
waveform batch: 2/2
Test loss:6.920818630836732
Epoch 65 mean train loss:1.0417805869757606
Epoch 65 mean test loss:0.8205198558521334
Start training epoch 66
waveform batch: 1/3
Using ADAM optimizer
Sum of params:219.36171
Changing learning rate to:5.421010862427522e-22
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 20.291709006015765
Iteration: 2 || Loss: 20.291709006015765
Iteration: 3 || Loss: 20.291709006015765
Iteration: 4 || Loss: 20.291709006015765
Iteration: 5 || Loss: 20.291709006015765
Iteration: 6 || Loss: 20.291709006015765
saving ADAM checkpoint...
Sum of params:219.36171
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 20.291709006015765
Iteration: 2 || Loss: 20.274435636152027
Iteration: 3 || Loss: 20.25793441990668
Iteration: 4 || Loss: 20.03958199325802
Iteration: 5 || Loss: 19.96932193611547
Iteration: 6 || Loss: 19.89982017494492
Iteration: 7 || Loss: 19.85934959928138
Iteration: 8 || Loss: 19.72244586576549
Iteration: 9 || Loss: 19.541591098620124
Iteration: 10 || Loss: 19.517351603816437
Iteration: 11 || Loss: 19.4756833119867
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:220.00458
Epoch 66 loss:19.4756833119867
waveform batch: 2/3
Using ADAM optimizer
Sum of params:220.00458
Changing learning rate to:2.710505431213761e-22
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.215042772006152
Iteration: 2 || Loss: 6.215042772006152
Iteration: 3 || Loss: 6.215042772006152
Iteration: 4 || Loss: 6.215042772006152
Iteration: 5 || Loss: 6.215042772006152
Iteration: 6 || Loss: 6.215042772006152
saving ADAM checkpoint...
Sum of params:220.00458
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.215042772006152
Iteration: 2 || Loss: 6.207755837983537
Iteration: 3 || Loss: 6.183917384125539
Iteration: 4 || Loss: 6.166864332126692
Iteration: 5 || Loss: 6.117263095570605
Iteration: 6 || Loss: 6.065190245356176
Iteration: 7 || Loss: 6.016968722129679
Iteration: 8 || Loss: 5.947447072618097
Iteration: 9 || Loss: 5.885213298698823
Iteration: 10 || Loss: 5.865458309516665
Iteration: 11 || Loss: 5.849846797656176
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.99385
Epoch 66 loss:5.849846797656176
waveform batch: 3/3
Using ADAM optimizer
Sum of params:219.99385
Changing learning rate to:1.3552527156068806e-22
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.665238514223508
Iteration: 2 || Loss: 5.665238514223508
Iteration: 3 || Loss: 5.665238514223508
Iteration: 4 || Loss: 5.665238514223508
Iteration: 5 || Loss: 5.665238514223508
Iteration: 6 || Loss: 5.665238514223508
saving ADAM checkpoint...
Sum of params:219.99385
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.665238514223508
Iteration: 2 || Loss: 5.657505520028635
Iteration: 3 || Loss: 5.649658653122702
Iteration: 4 || Loss: 5.645920113178657
Iteration: 5 || Loss: 5.589301310565872
Iteration: 6 || Loss: 5.554144757378631
Iteration: 7 || Loss: 5.5391836317218495
Iteration: 8 || Loss: 5.507666398154782
Iteration: 9 || Loss: 5.4861613532243485
Iteration: 10 || Loss: 5.476051310738118
Iteration: 11 || Loss: 5.4342160869394895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:219.857
Epoch 66 loss:5.4342160869394895
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.8813285657750605
waveform batch: 2/2
Test loss:6.810241025067315
Epoch 66 mean train loss:1.0253248732194122
Epoch 66 mean test loss:0.8053864465201397
Start training epoch 67
waveform batch: 1/3
Using ADAM optimizer
Sum of params:219.857
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.96647971924843
Iteration: 2 || Loss: 19.96647971924843
Iteration: 3 || Loss: 19.96647971924843
Iteration: 4 || Loss: 19.96647971924843
Iteration: 5 || Loss: 19.96647971924843
Iteration: 6 || Loss: 19.96647971924843
saving ADAM checkpoint...
Sum of params:219.857
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.96647971924843
Iteration: 2 || Loss: 19.954600414207984
Iteration: 3 || Loss: 19.9401137505922
Iteration: 4 || Loss: 19.735758183060504
Iteration: 5 || Loss: 19.678824379119124
Iteration: 6 || Loss: 19.601203012389547
Iteration: 7 || Loss: 19.553052763692694
Iteration: 8 || Loss: 19.352661062968203
Iteration: 9 || Loss: 19.297159587126707
Iteration: 10 || Loss: 19.193270291158402
Iteration: 11 || Loss: 19.164338359345575
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:220.48776
Epoch 67 loss:19.164338359345575
waveform batch: 2/3
Using ADAM optimizer
Sum of params:220.48776
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.127056747101636
Iteration: 2 || Loss: 6.127056747101636
Iteration: 3 || Loss: 6.127056747101636
Iteration: 4 || Loss: 6.127056747101636
Iteration: 5 || Loss: 6.127056747101636
Iteration: 6 || Loss: 6.127056747101636
saving ADAM checkpoint...
Sum of params:220.48776
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.127056747101636
Iteration: 2 || Loss: 6.1020656965005005
Iteration: 3 || Loss: 6.073461792154846
Iteration: 4 || Loss: 6.056057171081377
Iteration: 5 || Loss: 6.020099184598886
Iteration: 6 || Loss: 5.974510078708543
Iteration: 7 || Loss: 5.947984171394639
Iteration: 8 || Loss: 5.891512591548122
Iteration: 9 || Loss: 5.831355095127645
Iteration: 10 || Loss: 5.777092415228684
Iteration: 11 || Loss: 5.762416036622409
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:220.49911
Epoch 67 loss:5.762416036622409
waveform batch: 3/3
Using ADAM optimizer
Sum of params:220.49911
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.580119534995842
Iteration: 2 || Loss: 5.580119534995842
Iteration: 3 || Loss: 5.580119534995842
Iteration: 4 || Loss: 5.580119534995842
Iteration: 5 || Loss: 5.580119534995842
Iteration: 6 || Loss: 5.580119534995842
saving ADAM checkpoint...
Sum of params:220.49911
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.580119534995842
Iteration: 2 || Loss: 5.570866613668253
Iteration: 3 || Loss: 5.567720835927936
Iteration: 4 || Loss: 5.529784228715285
Iteration: 5 || Loss: 5.4887855777522345
Iteration: 6 || Loss: 5.474120513691135
Iteration: 7 || Loss: 5.452347847981819
Iteration: 8 || Loss: 5.4454049389485535
Iteration: 9 || Loss: 5.417247311459699
Iteration: 10 || Loss: 5.401550744726557
Iteration: 11 || Loss: 5.364525634581085
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:220.29597
Epoch 67 loss:5.364525634581085
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.760325215974424
waveform batch: 2/2
Test loss:6.698037910856038
Epoch 67 mean train loss:1.0097093343516357
Epoch 67 mean test loss:0.7916684192253214
Start training epoch 68
waveform batch: 1/3
Using ADAM optimizer
Sum of params:220.29597
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.675192951251027
Iteration: 2 || Loss: 19.675192951251027
Iteration: 3 || Loss: 19.675192951251027
Iteration: 4 || Loss: 19.675192951251027
Iteration: 5 || Loss: 19.675192951251027
Iteration: 6 || Loss: 19.675192951251027
saving ADAM checkpoint...
Sum of params:220.29597
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.675192951251027
Iteration: 2 || Loss: 19.6621421682862
Iteration: 3 || Loss: 19.634432616697243
Iteration: 4 || Loss: 19.435584345116823
Iteration: 5 || Loss: 19.34441695784496
Iteration: 6 || Loss: 19.274406165724898
Iteration: 7 || Loss: 19.231226013268643
Iteration: 8 || Loss: 19.080138908576487
Iteration: 9 || Loss: 19.032111757460058
Iteration: 10 || Loss: 18.921596812332382
Iteration: 11 || Loss: 18.872453720695127
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:220.96396
Epoch 68 loss:18.872453720695127
waveform batch: 2/3
Using ADAM optimizer
Sum of params:220.96396
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 6.06837264550737
Iteration: 2 || Loss: 6.06837264550737
Iteration: 3 || Loss: 6.06837264550737
Iteration: 4 || Loss: 6.06837264550737
Iteration: 5 || Loss: 6.06837264550737
Iteration: 6 || Loss: 6.06837264550737
saving ADAM checkpoint...
Sum of params:220.96396
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 6.06837264550737
Iteration: 2 || Loss: 6.047826556148127
Iteration: 3 || Loss: 6.016603288438042
Iteration: 4 || Loss: 5.996753408463956
Iteration: 5 || Loss: 5.9472608302209
Iteration: 6 || Loss: 5.892145184789374
Iteration: 7 || Loss: 5.860997569664628
Iteration: 8 || Loss: 5.813675116228531
Iteration: 9 || Loss: 5.729503336722622
Iteration: 10 || Loss: 5.690880223390494
Iteration: 11 || Loss: 5.667250545880637
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:220.94833
Epoch 68 loss:5.667250545880637
waveform batch: 3/3
Using ADAM optimizer
Sum of params:220.94833
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.4946342981377265
Iteration: 2 || Loss: 5.4946342981377265
Iteration: 3 || Loss: 5.4946342981377265
Iteration: 4 || Loss: 5.4946342981377265
Iteration: 5 || Loss: 5.4946342981377265
Iteration: 6 || Loss: 5.4946342981377265
saving ADAM checkpoint...
Sum of params:220.94833
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.4946342981377265
Iteration: 2 || Loss: 5.491781465241766
Iteration: 3 || Loss: 5.4872807325584985
Iteration: 4 || Loss: 5.467301700082816
Iteration: 5 || Loss: 5.437877393327464
Iteration: 6 || Loss: 5.391982705533141
Iteration: 7 || Loss: 5.36697681099844
Iteration: 8 || Loss: 5.348649953326961
Iteration: 9 || Loss: 5.321145169094762
Iteration: 10 || Loss: 5.291926349577886
Iteration: 11 || Loss: 5.256205784940121
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:220.74124
Epoch 68 loss:5.256205784940121
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.639845053583502
waveform batch: 2/2
Test loss:6.590941007314567
Epoch 68 mean train loss:0.9931970017171962
Epoch 68 mean test loss:0.778281532994004
Start training epoch 69
waveform batch: 1/3
Using ADAM optimizer
Sum of params:220.74124
Changing learning rate to:6.776263578034403e-23
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.413970076936124
Iteration: 2 || Loss: 19.413970076936124
Iteration: 3 || Loss: 19.413970076936124
Iteration: 4 || Loss: 19.413970076936124
Iteration: 5 || Loss: 19.413970076936124
Iteration: 6 || Loss: 19.413970076936124
saving ADAM checkpoint...
Sum of params:220.74124
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.413970076936124
Iteration: 2 || Loss: 19.38317696216583
Iteration: 3 || Loss: 19.36562239889078
Iteration: 4 || Loss: 19.26646739042428
Iteration: 5 || Loss: 19.09318112992428
Iteration: 6 || Loss: 19.043496231400912
Iteration: 7 || Loss: 18.988162874077847
Iteration: 8 || Loss: 18.946222238683895
Iteration: 9 || Loss: 18.853178400831613
Iteration: 10 || Loss: 18.69776846994053
Iteration: 11 || Loss: 18.67965067801303
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.1505
Epoch 69 loss:18.67965067801303
waveform batch: 2/3
Using ADAM optimizer
Sum of params:221.1505
Changing learning rate to:3.3881317890172014e-23
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.942550690190459
Iteration: 2 || Loss: 5.942550690190459
Iteration: 3 || Loss: 5.942550690190459
Iteration: 4 || Loss: 5.942550690190459
Iteration: 5 || Loss: 5.942550690190459
Iteration: 6 || Loss: 5.942550690190459
saving ADAM checkpoint...
Sum of params:221.1505
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.942550690190459
Iteration: 2 || Loss: 5.919898049609821
Iteration: 3 || Loss: 5.880918182739224
Iteration: 4 || Loss: 5.856908433884234
Iteration: 5 || Loss: 5.793152114124066
Iteration: 6 || Loss: 5.732116652235705
Iteration: 7 || Loss: 5.709913832168186
Iteration: 8 || Loss: 5.68438364072231
Iteration: 9 || Loss: 5.621484466343234
Iteration: 10 || Loss: 5.573126985067763
Iteration: 11 || Loss: 5.562532770541582
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.26204
Epoch 69 loss:5.562532770541582
waveform batch: 3/3
Using ADAM optimizer
Sum of params:221.26204
Changing learning rate to:1.6940658945086007e-23
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.3863289796872955
Iteration: 2 || Loss: 5.3863289796872955
Iteration: 3 || Loss: 5.3863289796872955
Iteration: 4 || Loss: 5.3863289796872955
Iteration: 5 || Loss: 5.3863289796872955
Iteration: 6 || Loss: 5.3863289796872955
saving ADAM checkpoint...
Sum of params:221.26204
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.3863289796872955
Iteration: 2 || Loss: 5.370162396773202
Iteration: 3 || Loss: 5.367729338565372
Iteration: 4 || Loss: 5.3121642965773
Iteration: 5 || Loss: 5.28539801148374
Iteration: 6 || Loss: 5.271941706162615
Iteration: 7 || Loss: 5.2550067622678736
Iteration: 8 || Loss: 5.249890461853332
Iteration: 9 || Loss: 5.214036531067096
Iteration: 10 || Loss: 5.203452790492559
Iteration: 11 || Loss: 5.181158319859113
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.15079
Epoch 69 loss:5.181158319859113
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.518110926166534
waveform batch: 2/2
Test loss:6.47648539410903
Epoch 69 mean train loss:0.9807780589471242
Epoch 69 mean test loss:0.7643880188397391
Start training epoch 70
waveform batch: 1/3
Using ADAM optimizer
Sum of params:221.15079
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 19.119368485799793
Iteration: 2 || Loss: 19.119368485799793
Iteration: 3 || Loss: 19.119368485799793
Iteration: 4 || Loss: 19.119368485799793
Iteration: 5 || Loss: 19.119368485799793
Iteration: 6 || Loss: 19.119368485799793
saving ADAM checkpoint...
Sum of params:221.15079
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 19.119368485799793
Iteration: 2 || Loss: 19.080473485329737
Iteration: 3 || Loss: 19.063738968108034
Iteration: 4 || Loss: 18.95844059615663
Iteration: 5 || Loss: 18.83725219364199
Iteration: 6 || Loss: 18.799373150297654
Iteration: 7 || Loss: 18.730222303039486
Iteration: 8 || Loss: 18.70670037535478
Iteration: 9 || Loss: 18.536145644627464
Iteration: 10 || Loss: 18.512196199964535
Iteration: 11 || Loss: 18.402351236521056
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.62537
Epoch 70 loss:18.402351236521056
waveform batch: 2/3
Using ADAM optimizer
Sum of params:221.62537
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.807877428971444
Iteration: 2 || Loss: 5.807877428971444
Iteration: 3 || Loss: 5.807877428971444
Iteration: 4 || Loss: 5.807877428971444
Iteration: 5 || Loss: 5.807877428971444
Iteration: 6 || Loss: 5.807877428971444
saving ADAM checkpoint...
Sum of params:221.62537
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.807877428971444
Iteration: 2 || Loss: 5.798806729763335
Iteration: 3 || Loss: 5.776038708407111
Iteration: 4 || Loss: 5.763260186300261
Iteration: 5 || Loss: 5.7014592682933385
Iteration: 6 || Loss: 5.668508124940137
Iteration: 7 || Loss: 5.629696014386305
Iteration: 8 || Loss: 5.541435281716547
Iteration: 9 || Loss: 5.508103082912896
Iteration: 10 || Loss: 5.487222323870201
Iteration: 11 || Loss: 5.465428925370848
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.65018
Epoch 70 loss:5.465428925370848
waveform batch: 3/3
Using ADAM optimizer
Sum of params:221.65018
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.30932062473917
Iteration: 2 || Loss: 5.30932062473917
Iteration: 3 || Loss: 5.30932062473917
Iteration: 4 || Loss: 5.30932062473917
Iteration: 5 || Loss: 5.30932062473917
Iteration: 6 || Loss: 5.30932062473917
saving ADAM checkpoint...
Sum of params:221.65018
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.30932062473917
Iteration: 2 || Loss: 5.303623607335956
Iteration: 3 || Loss: 5.3000772559703035
Iteration: 4 || Loss: 5.291724882442708
Iteration: 5 || Loss: 5.233768186904333
Iteration: 6 || Loss: 5.184792608654522
Iteration: 7 || Loss: 5.17715287035058
Iteration: 8 || Loss: 5.14378682855922
Iteration: 9 || Loss: 5.123196791766686
Iteration: 10 || Loss: 5.1129028480603225
Iteration: 11 || Loss: 5.074870629423523
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.55489
Epoch 70 loss:5.074870629423523
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.372349563992641
waveform batch: 2/2
Test loss:6.367067510590943
Epoch 70 mean train loss:0.9647550263771809
Epoch 70 mean test loss:0.749377474975505
Start training epoch 71
waveform batch: 1/3
Using ADAM optimizer
Sum of params:221.55489
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.882577746369797
Iteration: 2 || Loss: 18.882577746369797
Iteration: 3 || Loss: 18.882577746369797
Iteration: 4 || Loss: 18.882577746369797
Iteration: 5 || Loss: 18.882577746369797
Iteration: 6 || Loss: 18.882577746369797
saving ADAM checkpoint...
Sum of params:221.55489
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.882577746369797
Iteration: 2 || Loss: 18.845249483250697
Iteration: 3 || Loss: 18.830049226874763
Iteration: 4 || Loss: 18.780364065691195
Iteration: 5 || Loss: 18.622109395325356
Iteration: 6 || Loss: 18.579300482017793
Iteration: 7 || Loss: 18.505170414232637
Iteration: 8 || Loss: 18.47208598698857
Iteration: 9 || Loss: 18.355714514547863
Iteration: 10 || Loss: 18.252419630626253
Iteration: 11 || Loss: 18.215598213744702
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.88312
Epoch 71 loss:18.215598213744702
waveform batch: 2/3
Using ADAM optimizer
Sum of params:221.88312
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.739700471719553
Iteration: 2 || Loss: 5.739700471719553
Iteration: 3 || Loss: 5.739700471719553
Iteration: 4 || Loss: 5.739700471719553
Iteration: 5 || Loss: 5.739700471719553
Iteration: 6 || Loss: 5.739700471719553
saving ADAM checkpoint...
Sum of params:221.88312
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.739700471719553
Iteration: 2 || Loss: 5.709672582793016
Iteration: 3 || Loss: 5.666145299577512
Iteration: 4 || Loss: 5.660804520309068
Iteration: 5 || Loss: 5.603505802891725
Iteration: 6 || Loss: 5.558624805340379
Iteration: 7 || Loss: 5.539457852221191
Iteration: 8 || Loss: 5.508305206232769
Iteration: 9 || Loss: 5.479753754869504
Iteration: 10 || Loss: 5.398321110423774
Iteration: 11 || Loss: 5.389053652486651
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.99957
Epoch 71 loss:5.389053652486651
waveform batch: 3/3
Using ADAM optimizer
Sum of params:221.99957
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.216066744820362
Iteration: 2 || Loss: 5.216066744820362
Iteration: 3 || Loss: 5.216066744820362
Iteration: 4 || Loss: 5.216066744820362
Iteration: 5 || Loss: 5.216066744820362
Iteration: 6 || Loss: 5.216066744820362
saving ADAM checkpoint...
Sum of params:221.99957
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.216066744820362
Iteration: 2 || Loss: 5.208432762701582
Iteration: 3 || Loss: 5.205729121644648
Iteration: 4 || Loss: 5.195431902787583
Iteration: 5 || Loss: 5.126194678847611
Iteration: 6 || Loss: 5.076040046701559
Iteration: 7 || Loss: 5.070095941042728
Iteration: 8 || Loss: 5.038743709773183
Iteration: 9 || Loss: 5.026304011604011
Iteration: 10 || Loss: 5.00558193058942
Iteration: 11 || Loss: 4.977358261548479
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:221.88419
Epoch 71 loss:4.977358261548479
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.281924761875008
waveform batch: 2/2
Test loss:6.252081749728236
Epoch 71 mean train loss:0.9527336709259944
Epoch 71 mean test loss:0.7372945006825438
Start training epoch 72
waveform batch: 1/3
Using ADAM optimizer
Sum of params:221.88419
Changing learning rate to:8.470329472543004e-24
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.72425333695616
Iteration: 2 || Loss: 18.72425333695616
Iteration: 3 || Loss: 18.72425333695616
Iteration: 4 || Loss: 18.72425333695616
Iteration: 5 || Loss: 18.72425333695616
Iteration: 6 || Loss: 18.72425333695616
saving ADAM checkpoint...
Sum of params:221.88419
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.72425333695616
Iteration: 2 || Loss: 18.703858684736087
Iteration: 3 || Loss: 18.688577922414165
Iteration: 4 || Loss: 18.61007520107396
Iteration: 5 || Loss: 18.430040747870944
Iteration: 6 || Loss: 18.36963499812941
Iteration: 7 || Loss: 18.28611251492995
Iteration: 8 || Loss: 18.20921360091657
Iteration: 9 || Loss: 18.093022204655178
Iteration: 10 || Loss: 17.990654058651117
Iteration: 11 || Loss: 17.924678280818977
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:222.43353
Epoch 72 loss:17.924678280818977
waveform batch: 2/3
Using ADAM optimizer
Sum of params:222.43353
Changing learning rate to:4.235164736271502e-24
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.6706388813816915
Iteration: 2 || Loss: 5.6706388813816915
Iteration: 3 || Loss: 5.6706388813816915
Iteration: 4 || Loss: 5.6706388813816915
Iteration: 5 || Loss: 5.6706388813816915
Iteration: 6 || Loss: 5.6706388813816915
saving ADAM checkpoint...
Sum of params:222.43353
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.6706388813816915
Iteration: 2 || Loss: 5.662606978581112
Iteration: 3 || Loss: 5.626282780889112
Iteration: 4 || Loss: 5.621941337567993
Iteration: 5 || Loss: 5.5835937561573115
Iteration: 6 || Loss: 5.531164537047896
Iteration: 7 || Loss: 5.458642001553266
Iteration: 8 || Loss: 5.395937584986175
Iteration: 9 || Loss: 5.323722338128201
Iteration: 10 || Loss: 5.29854326432456
Iteration: 11 || Loss: 5.2802180195889585
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:222.37183
Epoch 72 loss:5.2802180195889585
waveform batch: 3/3
Using ADAM optimizer
Sum of params:222.37183
Changing learning rate to:2.117582368135751e-24
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.09613382701444
Iteration: 2 || Loss: 5.09613382701444
Iteration: 3 || Loss: 5.09613382701444
Iteration: 4 || Loss: 5.09613382701444
Iteration: 5 || Loss: 5.09613382701444
Iteration: 6 || Loss: 5.09613382701444
saving ADAM checkpoint...
Sum of params:222.37183
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.09613382701444
Iteration: 2 || Loss: 5.092510955761512
Iteration: 3 || Loss: 5.089531879345171
Iteration: 4 || Loss: 5.076830485857334
Iteration: 5 || Loss: 5.034246140310431
Iteration: 6 || Loss: 4.997589543912933
Iteration: 7 || Loss: 4.971570640745007
Iteration: 8 || Loss: 4.952791029957732
Iteration: 9 || Loss: 4.924926692882214
Iteration: 10 || Loss: 4.907749539423452
Iteration: 11 || Loss: 4.881434817228539
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:222.3287
Epoch 72 loss:4.881434817228539
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.1123725531609105
waveform batch: 2/2
Test loss:6.112627873975093
Epoch 72 mean train loss:0.9362110372545491
Epoch 72 mean test loss:0.7191176721844708
Start training epoch 73
waveform batch: 1/3
Using ADAM optimizer
Sum of params:222.3287
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.364853566510007
Iteration: 2 || Loss: 18.364853566510007
Iteration: 3 || Loss: 18.364853566510007
Iteration: 4 || Loss: 18.364853566510007
Iteration: 5 || Loss: 18.364853566510007
Iteration: 6 || Loss: 18.364853566510007
saving ADAM checkpoint...
Sum of params:222.3287
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.364853566510007
Iteration: 2 || Loss: 18.350539973627527
Iteration: 3 || Loss: 18.332290560183345
Iteration: 4 || Loss: 18.300414524423584
Iteration: 5 || Loss: 18.113372718199958
Iteration: 6 || Loss: 18.040150228455737
Iteration: 7 || Loss: 17.971389642664253
Iteration: 8 || Loss: 17.900160422368785
Iteration: 9 || Loss: 17.789871625332697
Iteration: 10 || Loss: 17.706835600276474
Iteration: 11 || Loss: 17.647846113021146
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:222.84937
Epoch 73 loss:17.647846113021146
waveform batch: 2/3
Using ADAM optimizer
Sum of params:222.84937
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.57685180511446
Iteration: 2 || Loss: 5.57685180511446
Iteration: 3 || Loss: 5.57685180511446
Iteration: 4 || Loss: 5.57685180511446
Iteration: 5 || Loss: 5.57685180511446
Iteration: 6 || Loss: 5.57685180511446
saving ADAM checkpoint...
Sum of params:222.84937
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.57685180511446
Iteration: 2 || Loss: 5.554147160724246
Iteration: 3 || Loss: 5.532915238829177
Iteration: 4 || Loss: 5.5189356936081095
Iteration: 5 || Loss: 5.480032813181946
Iteration: 6 || Loss: 5.438391229101161
Iteration: 7 || Loss: 5.391685615551722
Iteration: 8 || Loss: 5.337179524889884
Iteration: 9 || Loss: 5.2516861615210155
Iteration: 10 || Loss: 5.224890712571937
Iteration: 11 || Loss: 5.205783517901675
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:222.81104
Epoch 73 loss:5.205783517901675
waveform batch: 3/3
Using ADAM optimizer
Sum of params:222.81104
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.990083639118775
Iteration: 2 || Loss: 4.990083639118775
Iteration: 3 || Loss: 4.990083639118775
Iteration: 4 || Loss: 4.990083639118775
Iteration: 5 || Loss: 4.990083639118775
Iteration: 6 || Loss: 4.990083639118775
saving ADAM checkpoint...
Sum of params:222.81104
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4.990083639118775
Iteration: 2 || Loss: 4.989558461889717
Iteration: 3 || Loss: 4.9851825881229015
Iteration: 4 || Loss: 4.977503993389356
Iteration: 5 || Loss: 4.948334771803875
Iteration: 6 || Loss: 4.880453927731207
Iteration: 7 || Loss: 4.866486337365199
Iteration: 8 || Loss: 4.836686136303657
Iteration: 9 || Loss: 4.7979241745504515
Iteration: 10 || Loss: 4.76255747882549
Iteration: 11 || Loss: 4.743739500495715
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:222.71146
Epoch 73 loss:4.743739500495715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing:
waveform batch: 1/2
Test loss:6.041226753498857
waveform batch: 2/2
Test loss:6.0299793590091175
Epoch 73 mean train loss:0.9199123043806179
Epoch 73 mean test loss:0.7100709477945868
Start training epoch 74
waveform batch: 1/3
Using ADAM optimizer
Sum of params:222.71146
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 18.33532469421959
Iteration: 2 || Loss: 18.33532469421959
Iteration: 3 || Loss: 18.33532469421959
Iteration: 4 || Loss: 18.33532469421959
Iteration: 5 || Loss: 18.33532469421959
Iteration: 6 || Loss: 18.33532469421959
saving ADAM checkpoint...
Sum of params:222.71146
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 18.33532469421959
Iteration: 2 || Loss: 18.2932447597379
Iteration: 3 || Loss: 18.25709255234326
Iteration: 4 || Loss: 18.22809952206063
Iteration: 5 || Loss: 17.96581597148956
Iteration: 6 || Loss: 17.907370167997044
Iteration: 7 || Loss: 17.80672783836657
Iteration: 8 || Loss: 17.76298727792653
Iteration: 9 || Loss: 17.702633123278154
Iteration: 10 || Loss: 17.533386408337645
Iteration: 11 || Loss: 17.50260527406656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:222.98352
Epoch 74 loss:17.50260527406656
waveform batch: 2/3
Using ADAM optimizer
Sum of params:222.98352
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 5.432573053258556
Iteration: 2 || Loss: 5.432573053258556
Iteration: 3 || Loss: 5.432573053258556
Iteration: 4 || Loss: 5.432573053258556
Iteration: 5 || Loss: 5.432573053258556
Iteration: 6 || Loss: 5.432573053258556
saving ADAM checkpoint...
Sum of params:222.98352
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 5.432573053258556
Iteration: 2 || Loss: 5.424968237136701
Iteration: 3 || Loss: 5.379742833018405
Iteration: 4 || Loss: 5.374900570615906
Iteration: 5 || Loss: 5.319194589288848
Iteration: 6 || Loss: 5.270070608833303
Iteration: 7 || Loss: 5.233964527848194
Iteration: 8 || Loss: 5.210063727296384
Iteration: 9 || Loss: 5.128348557593197
Iteration: 10 || Loss: 5.1062747208980666
Iteration: 11 || Loss: 5.087940560104392
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:223.16672
Epoch 74 loss:5.087940560104392
waveform batch: 3/3
Using ADAM optimizer
Sum of params:223.16672
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4.845406229441367
Iteration: 2 || Loss: 4.845406229441367
