Size of 1D data matrix:(300, 110, 47, 4)
Shape: [timesteps,spatial locations,waveforms,variables]
Number of parameters in neural network: 2330
optimizer 1 is ADAM
optimizer 2 is BFGS optimizer
ODE Time integrator selected:RK4
Transfer learning - loaded weights from file
Batch size:10
Start training epoch 1
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-92.83048
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 878.1423087071637
Iteration: 2 || Loss: 1.0310854415974057e6
Iteration: 3 || Loss: 63670.57884427744
Iteration: 4 || Loss: 231044.18383489203
Iteration: 5 || Loss: 428971.15550415486
Iteration: 6 || Loss: 878.1423087071637
saving ADAM checkpoint...
Sum of params:-92.83048
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 878.1423087071637
Iteration: 2 || Loss: 778.5222479140381
Iteration: 3 || Loss: 722.57379816877
Iteration: 4 || Loss: 630.3137897138782
Iteration: 5 || Loss: 606.9328191913849
Iteration: 6 || Loss: 583.1318815177857
Iteration: 7 || Loss: 550.0341875225033
Iteration: 8 || Loss: 504.6402471876033
Iteration: 9 || Loss: 449.9694297872293
Iteration: 10 || Loss: 411.706084062629
Iteration: 11 || Loss: 372.2996551331553
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-93.39341
Epoch 1 loss:372.2996551331553
MSE loss S20.077107335269762
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-93.39341
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1407.4445454533159
Iteration: 2 || Loss: 494292.45687635837
Iteration: 3 || Loss: 190995.72617535526
Iteration: 4 || Loss: 266481.3268833176
Iteration: 5 || Loss: 191658.59437726432
Iteration: 6 || Loss: 1407.4445454533159
saving ADAM checkpoint...
Sum of params:-93.39341
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1407.4445454533159
Iteration: 2 || Loss: 1299.4793800582563
Iteration: 3 || Loss: 1275.8454723584864
Iteration: 4 || Loss: 1239.9243879113806
Iteration: 5 || Loss: 1178.7993455049032
Iteration: 6 || Loss: 1090.4219962667232
Iteration: 7 || Loss: 1015.8652532526559
Iteration: 8 || Loss: 936.1997795129969
Iteration: 9 || Loss: 869.3904499465867
Iteration: 10 || Loss: 821.4363093830475
Iteration: 11 || Loss: 764.5237385934928
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-94.0621
Epoch 1 loss:764.5237385934928
MSE loss S45.11474988595213
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-94.0621
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 4862.383791849177
Iteration: 2 || Loss: 233490.19265929665
Iteration: 3 || Loss: 74687.89386508193
Iteration: 4 || Loss: 87143.60589399883
Iteration: 5 || Loss: 31917.176484048614
Iteration: 6 || Loss: 4862.383791849177
saving ADAM checkpoint...
Sum of params:-94.0621
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 4862.383791849177
Iteration: 2 || Loss: 4800.069645505944
Iteration: 3 || Loss: 4641.79982145092
Iteration: 4 || Loss: 4415.702045332825
Iteration: 5 || Loss: 4321.951899163594
Iteration: 6 || Loss: 3972.0503403195767
Iteration: 7 || Loss: 3754.2681659349078
Iteration: 8 || Loss: 3518.282051142035
Iteration: 9 || Loss: 3201.684929072451
Iteration: 10 || Loss: 2985.853441577015
Iteration: 11 || Loss: 2852.3353438243894
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.308014
Epoch 1 loss:2852.3353438243894
MSE loss S256.2971095929248
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:1030.0324527035693
MSE loss S - interpolation152.7526698937964
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6589.751963615039
MSE loss S - extrapolation439.667499216574
waveform batch: 2/2
Test loss - extrapolation:3446.452435978287
MSE loss S - extrapolation217.74400580091603
Epoch 1 mean train loss:137.5571978465875
Epoch 1 mean test loss - interpolation:171.6720754505949
Epoch 1 mean test loss - extrapolation:836.3503666327773
Start training epoch 2
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.308014
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1852.7310949308003
Iteration: 2 || Loss: 509593.7327883015
Iteration: 3 || Loss: 183477.1118924457
Iteration: 4 || Loss: 250838.27943549928
Iteration: 5 || Loss: 197184.78214611497
Iteration: 6 || Loss: 1852.7310949308003
saving ADAM checkpoint...
Sum of params:-95.308014
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1852.7310949308003
Iteration: 2 || Loss: 1292.3901918605907
Iteration: 3 || Loss: 1122.0637581964138
Iteration: 4 || Loss: 782.5257394090539
Iteration: 5 || Loss: 632.087613974724
Iteration: 6 || Loss: 548.1530066319957
Iteration: 7 || Loss: 435.7577994742544
Iteration: 8 || Loss: 391.8364001982661
Iteration: 9 || Loss: 358.24051875036554
Iteration: 10 || Loss: 332.2537130592145
Iteration: 11 || Loss: 315.0889448586302
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.312996
Epoch 2 loss:315.0889448586302
MSE loss S53.45950238799279
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.312996
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1038.6356491910126
Iteration: 2 || Loss: 789702.3611399732
Iteration: 3 || Loss: 74453.19016660325
Iteration: 4 || Loss: 288696.66563347314
Iteration: 5 || Loss: 303468.6878941381
Iteration: 6 || Loss: 1038.6356491910126
saving ADAM checkpoint...
Sum of params:-95.312996
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1038.6356491910126
Iteration: 2 || Loss: 985.0899909709531
Iteration: 3 || Loss: 871.0394818913215
Iteration: 4 || Loss: 826.1573167185798
Iteration: 5 || Loss: 795.0579124741375
Iteration: 6 || Loss: 759.4720748985698
Iteration: 7 || Loss: 722.2517540553367
Iteration: 8 || Loss: 695.5171738136238
Iteration: 9 || Loss: 639.6020536424439
Iteration: 10 || Loss: 574.7820930103264
Iteration: 11 || Loss: 525.497547107176
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.16947
Epoch 2 loss:525.497547107176
MSE loss S56.175790318417306
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.16947
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2854.047961122251
Iteration: 2 || Loss: 841193.6008557572
Iteration: 3 || Loss: 86645.4347875252
Iteration: 4 || Loss: 147357.0617157
Iteration: 5 || Loss: 295149.69100413925
Iteration: 6 || Loss: 2854.047961122251
saving ADAM checkpoint...
Sum of params:-95.16947
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2854.047961122251
Iteration: 2 || Loss: 2781.131297688144
Iteration: 3 || Loss: 2710.4467509546753
Iteration: 4 || Loss: 2637.939958736126
Iteration: 5 || Loss: 2601.279027240999
Iteration: 6 || Loss: 2561.9465107691813
Iteration: 7 || Loss: 2497.8617225941634
Iteration: 8 || Loss: 2405.4309102861366
Iteration: 9 || Loss: 2281.221926515673
Iteration: 10 || Loss: 2222.785104216175
Iteration: 11 || Loss: 2179.8661095903394
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.510475
Epoch 2 loss:2179.8661095903394
MSE loss S140.10353405840402
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:699.6303234462686
MSE loss S - interpolation77.35193009870912
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5998.914011941325
MSE loss S - extrapolation369.5909010316925
waveform batch: 2/2
Test loss - extrapolation:3090.922970314663
MSE loss S - extrapolation156.38204891941325
Epoch 2 mean train loss:104.1535379846947
Epoch 2 mean test loss - interpolation:116.60505390771142
Epoch 2 mean test loss - extrapolation:757.4864151879989
Start training epoch 3
Changing learning rate to:0.001
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.510475
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1458.4639422846867
Iteration: 2 || Loss: 7923.300481581319
Iteration: 3 || Loss: 1751.5127486920815
Iteration: 4 || Loss: 3784.80649918141
Iteration: 5 || Loss: 4198.879017535617
Iteration: 6 || Loss: 1458.4639422846867
saving ADAM checkpoint...
Sum of params:-95.510475
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1458.4639422846867
Iteration: 2 || Loss: 1181.6988279308284
Iteration: 3 || Loss: 867.4951372437594
Iteration: 4 || Loss: 547.4189115895548
Iteration: 5 || Loss: 432.79770208669714
Iteration: 6 || Loss: 366.78706811740045
Iteration: 7 || Loss: 308.8442478725138
Iteration: 8 || Loss: 281.38062008771334
Iteration: 9 || Loss: 266.3857802662758
Iteration: 10 || Loss: 254.855530993461
Iteration: 11 || Loss: 239.47269647488653
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.49129
Epoch 3 loss:239.47269647488653
MSE loss S39.56988278004652
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.49129
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 743.5762564348408
Iteration: 2 || Loss: 1685.6899488057916
Iteration: 3 || Loss: 3371.937251460838
Iteration: 4 || Loss: 999.8895449327412
Iteration: 5 || Loss: 1508.7744410823373
Iteration: 6 || Loss: 743.5762564348408
saving ADAM checkpoint...
Sum of params:-95.49129
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 743.5762564348408
Iteration: 2 || Loss: 642.7776570383783
Iteration: 3 || Loss: 613.1654547527189
Iteration: 4 || Loss: 596.3913976976336
Iteration: 5 || Loss: 580.3302411543515
Iteration: 6 || Loss: 542.4929585436838
Iteration: 7 || Loss: 522.0378214344128
Iteration: 8 || Loss: 507.9873416196905
Iteration: 9 || Loss: 478.9412078693267
Iteration: 10 || Loss: 445.7783217817425
Iteration: 11 || Loss: 419.1880679084357
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.21503
Epoch 3 loss:419.1880679084357
MSE loss S47.392363776144094
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.21503
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2346.5401199966736
Iteration: 2 || Loss: 4620.407138959494
Iteration: 3 || Loss: 2894.0722201039152
Iteration: 4 || Loss: 3281.8463745396025
Iteration: 5 || Loss: 2704.1227286414514
Iteration: 6 || Loss: 2346.5401199966736
saving ADAM checkpoint...
Sum of params:-95.21503
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2346.5401199966736
Iteration: 2 || Loss: 2322.0270506289885
Iteration: 3 || Loss: 2248.65557260707
Iteration: 4 || Loss: 2197.0444717804226
Iteration: 5 || Loss: 2180.680799303883
Iteration: 6 || Loss: 2141.9985795961547
Iteration: 7 || Loss: 2106.32149458852
Iteration: 8 || Loss: 2069.872033321984
Iteration: 9 || Loss: 2020.4332225017872
Iteration: 10 || Loss: 1977.5974126222604
Iteration: 11 || Loss: 1953.2847192089248
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.98146
Epoch 3 loss:1953.2847192089248
MSE loss S118.57037069422688
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:573.5696671660035
MSE loss S - interpolation60.17260413013035
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6046.857751983717
MSE loss S - extrapolation468.54223709007147
waveform batch: 2/2
Test loss - extrapolation:2923.521724515819
MSE loss S - extrapolation139.39357717300283
Epoch 3 mean train loss:90.06708564111196
Epoch 3 mean test loss - interpolation:95.59494452766724
Epoch 3 mean test loss - extrapolation:747.531623041628
Start training epoch 4
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.98146
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1320.5120711534305
Iteration: 2 || Loss: 7021.287359402879
Iteration: 3 || Loss: 1629.7889259415056
Iteration: 4 || Loss: 3605.5802138379217
Iteration: 5 || Loss: 3549.071731188086
Iteration: 6 || Loss: 1320.5120711534305
saving ADAM checkpoint...
Sum of params:-95.98146
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1320.5120711534305
Iteration: 2 || Loss: 1001.8572207659722
Iteration: 3 || Loss: 764.2731047109397
Iteration: 4 || Loss: 457.477345727778
Iteration: 5 || Loss: 361.6213224615623
Iteration: 6 || Loss: 311.57480128643425
Iteration: 7 || Loss: 257.12653936891394
Iteration: 8 || Loss: 225.2218803243069
Iteration: 9 || Loss: 209.4392621816935
Iteration: 10 || Loss: 199.25020841826526
Iteration: 11 || Loss: 186.67118396076108
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.908295
Epoch 4 loss:186.67118396076108
MSE loss S30.80307258837276
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.908295
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 602.6748368051753
Iteration: 2 || Loss: 7151.661199264136
Iteration: 3 || Loss: 1407.717180499467
Iteration: 4 || Loss: 3171.3981559049626
Iteration: 5 || Loss: 3372.71221083491
Iteration: 6 || Loss: 602.6748368051753
saving ADAM checkpoint...
Sum of params:-95.908295
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 602.6748368051753
Iteration: 2 || Loss: 576.5805142745393
Iteration: 3 || Loss: 527.0643439297615
Iteration: 4 || Loss: 495.3316704160476
Iteration: 5 || Loss: 476.54399887911296
Iteration: 6 || Loss: 453.71018303771973
Iteration: 7 || Loss: 438.5710957318354
Iteration: 8 || Loss: 429.139741326785
Iteration: 9 || Loss: 415.76210511618655
Iteration: 10 || Loss: 392.31381290392835
Iteration: 11 || Loss: 373.00377905171035
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.84314
Epoch 4 loss:373.00377905171035
MSE loss S42.696989241983154
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.84314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2128.8094688937945
Iteration: 2 || Loss: 3990.95291148812
Iteration: 3 || Loss: 3279.730492450918
Iteration: 4 || Loss: 2613.0366257810842
Iteration: 5 || Loss: 2394.6662774435445
Iteration: 6 || Loss: 2128.8094688937945
saving ADAM checkpoint...
Sum of params:-95.84314
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2128.8094688937945
Iteration: 2 || Loss: 2070.811469912417
Iteration: 3 || Loss: 2034.643179016985
Iteration: 4 || Loss: 1999.0278154337295
Iteration: 5 || Loss: 1976.770849257259
Iteration: 6 || Loss: 1959.4416467061453
Iteration: 7 || Loss: 1936.1431777017228
Iteration: 8 || Loss: 1911.2800194209083
Iteration: 9 || Loss: 1876.4970499824399
Iteration: 10 || Loss: 1840.0624642257933
Iteration: 11 || Loss: 1817.6166743545275
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.9659
Epoch 4 loss:1817.6166743545275
MSE loss S116.04395796730583
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:497.19009133663076
MSE loss S - interpolation48.82318697278029
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5963.477568663238
MSE loss S - extrapolation559.701797446709
waveform batch: 2/2
Test loss - extrapolation:2768.8604297696647
MSE loss S - extrapolation134.1155599699539
Epoch 4 mean train loss:81.9755737023103
Epoch 4 mean test loss - interpolation:82.86501522277179
Epoch 4 mean test loss - extrapolation:727.694833202742
Start training epoch 5
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-95.9659
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1154.338254921341
Iteration: 2 || Loss: 6508.924406980146
Iteration: 3 || Loss: 1370.2921252030396
Iteration: 4 || Loss: 2790.794453188947
Iteration: 5 || Loss: 3040.4698242526265
Iteration: 6 || Loss: 1154.338254921341
saving ADAM checkpoint...
Sum of params:-95.9659
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1154.338254921341
Iteration: 2 || Loss: 788.514547723924
Iteration: 3 || Loss: 609.1312261677473
Iteration: 4 || Loss: 365.17135485896955
Iteration: 5 || Loss: 297.08888679944846
Iteration: 6 || Loss: 262.34918428167623
Iteration: 7 || Loss: 224.54526900698562
Iteration: 8 || Loss: 197.45385159780156
Iteration: 9 || Loss: 183.00411049160732
Iteration: 10 || Loss: 175.08340456137142
Iteration: 11 || Loss: 164.71304414060526
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.92649
Epoch 5 loss:164.71304414060526
MSE loss S28.668615452050382
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-95.92649
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 531.1684188887085
Iteration: 2 || Loss: 7337.870225657304
Iteration: 3 || Loss: 1290.7365807518056
Iteration: 4 || Loss: 2821.830385022997
Iteration: 5 || Loss: 3517.1060563321744
Iteration: 6 || Loss: 531.1684188887085
saving ADAM checkpoint...
Sum of params:-95.92649
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 531.1684188887085
Iteration: 2 || Loss: 502.30702408659255
Iteration: 3 || Loss: 464.7185213910688
Iteration: 4 || Loss: 441.2084487306991
Iteration: 5 || Loss: 426.362669314004
Iteration: 6 || Loss: 407.78279706039393
Iteration: 7 || Loss: 398.2961508685071
Iteration: 8 || Loss: 391.778847904291
Iteration: 9 || Loss: 383.40881850572845
Iteration: 10 || Loss: 363.1888266913742
Iteration: 11 || Loss: 344.2134729642057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-95.87777
Epoch 5 loss:344.2134729642057
MSE loss S40.436514800580724
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-95.87777
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 2002.4953376145447
Iteration: 2 || Loss: 3927.5512633244043
Iteration: 3 || Loss: 2996.961044275528
Iteration: 4 || Loss: 2389.93524961539
Iteration: 5 || Loss: 2396.7946256241394
Iteration: 6 || Loss: 2002.4953376145447
saving ADAM checkpoint...
Sum of params:-95.87777
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 2002.4953376145447
Iteration: 2 || Loss: 1973.4939471417756
Iteration: 3 || Loss: 1920.4137839988764
Iteration: 4 || Loss: 1873.3754728529454
Iteration: 5 || Loss: 1854.0694702320986
Iteration: 6 || Loss: 1833.615733689764
Iteration: 7 || Loss: 1819.8251759672733
Iteration: 8 || Loss: 1806.057759469885
Iteration: 9 || Loss: 1783.7329977345328
Iteration: 10 || Loss: 1753.5374048511433
Iteration: 11 || Loss: 1730.7070750445046
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.209274
Epoch 5 loss:1730.7070750445046
MSE loss S116.7135323213206
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:465.81709131000343
MSE loss S - interpolation47.81218601839266
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6213.745437085061
MSE loss S - extrapolation687.5241769049064
waveform batch: 2/2
Test loss - extrapolation:2676.8386889200697
MSE loss S - extrapolation126.72813120657267
Epoch 5 mean train loss:77.22874455687295
Epoch 5 mean test loss - interpolation:77.63618188500057
Epoch 5 mean test loss - extrapolation:740.8820105004276
Start training epoch 6
Changing learning rate to:0.0001
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.209274
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1210.0576444581543
Iteration: 2 || Loss: 891.8086870725996
Iteration: 3 || Loss: 731.2568824645583
Iteration: 4 || Loss: 667.6115210119096
Iteration: 5 || Loss: 638.9135778126125
Iteration: 6 || Loss: 638.9135778126125
saving ADAM checkpoint...
Sum of params:-96.127785
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 638.9135778126125
Iteration: 2 || Loss: 576.0567914711924
Iteration: 3 || Loss: 392.1179232635486
Iteration: 4 || Loss: 342.6126557972393
Iteration: 5 || Loss: 297.4991512206887
Iteration: 6 || Loss: 251.5003872073747
Iteration: 7 || Loss: 212.2894060504744
Iteration: 8 || Loss: 183.97388503554538
Iteration: 9 || Loss: 167.74037035726923
Iteration: 10 || Loss: 154.96540404326618
Iteration: 11 || Loss: 145.50847476857808
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.16032
Epoch 6 loss:145.50847476857808
MSE loss S26.169338148464266
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.16032
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 488.6296443259751
Iteration: 2 || Loss: 453.8539570927364
Iteration: 3 || Loss: 444.5361287274307
Iteration: 4 || Loss: 429.635927166209
Iteration: 5 || Loss: 421.7681022389077
Iteration: 6 || Loss: 421.7681022389077
saving ADAM checkpoint...
Sum of params:-96.33006
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 421.7681022389077
Iteration: 2 || Loss: 409.92258635607817
Iteration: 3 || Loss: 403.14906615749913
Iteration: 4 || Loss: 386.7344011717121
Iteration: 5 || Loss: 374.04761329218104
Iteration: 6 || Loss: 366.90554505440656
Iteration: 7 || Loss: 358.82387658374716
Iteration: 8 || Loss: 353.16615818051787
Iteration: 9 || Loss: 346.64240095035115
Iteration: 10 || Loss: 330.1637070505944
Iteration: 11 || Loss: 312.6204197331783
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.28461
Epoch 6 loss:312.6204197331783
MSE loss S38.68915174053577
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.28461
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1903.5972193699722
Iteration: 2 || Loss: 1883.8962263956903
Iteration: 3 || Loss: 1845.6743251791954
Iteration: 4 || Loss: 1828.1791957517148
Iteration: 5 || Loss: 1824.3012318622511
Iteration: 6 || Loss: 1824.3012318622511
saving ADAM checkpoint...
Sum of params:-96.414665
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1824.3012318622511
Iteration: 2 || Loss: 1808.6719511291506
Iteration: 3 || Loss: 1790.8044155680377
Iteration: 4 || Loss: 1765.9727829929907
Iteration: 5 || Loss: 1749.3250608081128
Iteration: 6 || Loss: 1740.1075231767993
Iteration: 7 || Loss: 1729.0410635477472
Iteration: 8 || Loss: 1717.7170981598574
Iteration: 9 || Loss: 1695.5061813162936
Iteration: 10 || Loss: 1670.3492822677308
Iteration: 11 || Loss: 1652.2846871696247
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.58514
Epoch 6 loss:1652.2846871696247
MSE loss S115.38295307647147
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:432.9342061982239
MSE loss S - interpolation46.687026588796066
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6256.9230779456375
MSE loss S - extrapolation760.7847883485646
waveform batch: 2/2
Test loss - extrapolation:2602.3533876900915
MSE loss S - extrapolation123.80961248681217
Epoch 6 mean train loss:72.77288212659934
Epoch 6 mean test loss - interpolation:72.15570103303732
Epoch 6 mean test loss - extrapolation:738.2730388029773
Start training epoch 7
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.58514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1180.4433448765842
Iteration: 2 || Loss: 846.4501783717614
Iteration: 3 || Loss: 670.4310633511353
Iteration: 4 || Loss: 601.0560900848335
Iteration: 5 || Loss: 576.5845115970565
Iteration: 6 || Loss: 576.5845115970565
saving ADAM checkpoint...
Sum of params:-96.50621
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 576.5845115970565
Iteration: 2 || Loss: 517.816279831778
Iteration: 3 || Loss: 372.3008200169764
Iteration: 4 || Loss: 311.1789807003729
Iteration: 5 || Loss: 267.0840533935807
Iteration: 6 || Loss: 224.89006437753136
Iteration: 7 || Loss: 194.57399603910426
Iteration: 8 || Loss: 169.3805508173847
Iteration: 9 || Loss: 153.02788644969533
Iteration: 10 || Loss: 141.99836221049515
Iteration: 11 || Loss: 133.48533389487042
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.544014
Epoch 7 loss:133.48533389487042
MSE loss S24.798286955980426
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.544014
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 459.523674409949
Iteration: 2 || Loss: 413.46640266231856
Iteration: 3 || Loss: 413.7782550812056
Iteration: 4 || Loss: 400.8113054950089
Iteration: 5 || Loss: 388.84702875185235
Iteration: 6 || Loss: 388.84702875185235
saving ADAM checkpoint...
Sum of params:-96.68489
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 388.84702875185235
Iteration: 2 || Loss: 375.44167347639046
Iteration: 3 || Loss: 372.97745815449224
Iteration: 4 || Loss: 357.9376583878845
Iteration: 5 || Loss: 347.49461773845167
Iteration: 6 || Loss: 341.71396927704194
Iteration: 7 || Loss: 335.0525920864601
Iteration: 8 || Loss: 329.9789862632566
Iteration: 9 || Loss: 324.3604310321285
Iteration: 10 || Loss: 309.4251079173501
Iteration: 11 || Loss: 293.3721858098562
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.64864
Epoch 7 loss:293.3721858098562
MSE loss S38.00969466857741
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-96.64864
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1836.8771388174303
Iteration: 2 || Loss: 1800.4410106075075
Iteration: 3 || Loss: 1769.9326693332755
Iteration: 4 || Loss: 1750.3253266693255
Iteration: 5 || Loss: 1746.5302281494135
Iteration: 6 || Loss: 1746.5302281494135
saving ADAM checkpoint...
Sum of params:-96.79029
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1746.5302281494135
Iteration: 2 || Loss: 1732.3254800203315
Iteration: 3 || Loss: 1716.6296404285117
Iteration: 4 || Loss: 1689.756084203128
Iteration: 5 || Loss: 1673.8302200875917
Iteration: 6 || Loss: 1664.6339534977308
Iteration: 7 || Loss: 1656.6737163374721
Iteration: 8 || Loss: 1647.5336490076015
Iteration: 9 || Loss: 1629.9730069148677
Iteration: 10 || Loss: 1608.7351090069674
Iteration: 11 || Loss: 1591.8478301523105
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.94701
Epoch 7 loss:1591.8478301523105
MSE loss S113.7723233313603
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:408.6962429502351
MSE loss S - interpolation46.5159497404248
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6331.810630702343
MSE loss S - extrapolation830.2890004657698
waveform batch: 2/2
Test loss - extrapolation:2550.5194903739152
MSE loss S - extrapolation118.7887495010872
Epoch 7 mean train loss:69.61052930541507
Epoch 7 mean test loss - interpolation:68.11604049170585
Epoch 7 mean test loss - extrapolation:740.1941767563549
Start training epoch 8
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-96.94701
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1169.9466835988273
Iteration: 2 || Loss: 832.565504495232
Iteration: 3 || Loss: 648.5352426308136
Iteration: 4 || Loss: 569.8260767405891
Iteration: 5 || Loss: 539.0132300270063
Iteration: 6 || Loss: 539.0132300270063
saving ADAM checkpoint...
Sum of params:-96.869995
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 539.0132300270063
Iteration: 2 || Loss: 484.2398402252374
Iteration: 3 || Loss: 367.803144366654
Iteration: 4 || Loss: 300.4062046579928
Iteration: 5 || Loss: 249.6012142238508
Iteration: 6 || Loss: 208.23678204250862
Iteration: 7 || Loss: 179.93788246601855
Iteration: 8 || Loss: 159.51290883688475
Iteration: 9 || Loss: 143.1673825935002
Iteration: 10 || Loss: 132.053966601744
Iteration: 11 || Loss: 124.12023499980201
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-96.901955
Epoch 8 loss:124.12023499980201
MSE loss S23.827261638265863
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-96.901955
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 434.80231444404336
Iteration: 2 || Loss: 381.2769876437904
Iteration: 3 || Loss: 384.04227389096906
Iteration: 4 || Loss: 374.133040197882
Iteration: 5 || Loss: 361.4246298305551
Iteration: 6 || Loss: 361.4246298305551
saving ADAM checkpoint...
Sum of params:-97.044716
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 361.4246298305551
Iteration: 2 || Loss: 346.6548045039491
Iteration: 3 || Loss: 344.5642176091554
Iteration: 4 || Loss: 330.4509984831762
Iteration: 5 || Loss: 322.2003164561721
Iteration: 6 || Loss: 317.0249552422952
Iteration: 7 || Loss: 312.34570648282244
Iteration: 8 || Loss: 307.62821618547576
Iteration: 9 || Loss: 302.6384284635942
Iteration: 10 || Loss: 289.0944164418286
Iteration: 11 || Loss: 274.500650918631
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.01101
Epoch 8 loss:274.500650918631
MSE loss S36.14776811304122
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.01101
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1778.1381355530425
Iteration: 2 || Loss: 1738.1214178030043
Iteration: 3 || Loss: 1711.6886749271598
Iteration: 4 || Loss: 1693.4876706627585
Iteration: 5 || Loss: 1688.6797389793044
Iteration: 6 || Loss: 1688.6797389793044
saving ADAM checkpoint...
Sum of params:-97.148865
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1688.6797389793044
Iteration: 2 || Loss: 1673.8021628410675
Iteration: 3 || Loss: 1659.7528503805431
Iteration: 4 || Loss: 1631.3044936674887
Iteration: 5 || Loss: 1616.3961071954884
Iteration: 6 || Loss: 1605.9144748011374
Iteration: 7 || Loss: 1595.2510145310973
Iteration: 8 || Loss: 1591.623274499116
Iteration: 9 || Loss: 1577.6401118316599
Iteration: 10 || Loss: 1559.1748035963337
Iteration: 11 || Loss: 1544.0582466310145
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.25261
Epoch 8 loss:1544.0582466310145
MSE loss S111.75022066467336
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:384.38530238620893
MSE loss S - interpolation45.2394405712442
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6342.800980647313
MSE loss S - extrapolation875.375063334836
waveform batch: 2/2
Test loss - extrapolation:2513.6606427133083
MSE loss S - extrapolation115.6998178733825
Epoch 8 mean train loss:66.98893560515336
Epoch 8 mean test loss - interpolation:64.06421706436815
Epoch 8 mean test loss - extrapolation:738.0384686133851
Start training epoch 9
Changing learning rate to:1.0e-5
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.25261
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1128.6674103452265
Iteration: 2 || Loss: 1087.7905342738507
Iteration: 3 || Loss: 1048.5878488681217
Iteration: 4 || Loss: 1011.0653145104162
Iteration: 5 || Loss: 975.2534270203923
Iteration: 6 || Loss: 975.2534270203923
saving ADAM checkpoint...
Sum of params:-97.24378
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 975.2534270203923
Iteration: 2 || Loss: 582.2041505492551
Iteration: 3 || Loss: 432.862036610612
Iteration: 4 || Loss: 275.35818358618695
Iteration: 5 || Loss: 232.39884006755108
Iteration: 6 || Loss: 198.97499732743458
Iteration: 7 || Loss: 174.45498010278334
Iteration: 8 || Loss: 150.9302075893026
Iteration: 9 || Loss: 139.15671238175548
Iteration: 10 || Loss: 130.0831150475614
Iteration: 11 || Loss: 122.71975150140496
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.194565
Epoch 9 loss:122.71975150140496
MSE loss S24.65571599298632
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.194565
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 406.5501095523233
Iteration: 2 || Loss: 393.52118197744124
Iteration: 3 || Loss: 382.4276915701014
Iteration: 4 || Loss: 373.0264037258182
Iteration: 5 || Loss: 365.1691230114805
Iteration: 6 || Loss: 365.1691230114805
saving ADAM checkpoint...
Sum of params:-97.20733
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 365.1691230114805
Iteration: 2 || Loss: 345.1253402235725
Iteration: 3 || Loss: 334.077942937219
Iteration: 4 || Loss: 317.3836500106971
Iteration: 5 || Loss: 311.2024481905717
Iteration: 6 || Loss: 302.1850749102172
Iteration: 7 || Loss: 295.2594705684851
Iteration: 8 || Loss: 293.02483205266725
Iteration: 9 || Loss: 288.7142116260128
Iteration: 10 || Loss: 271.31661372981824
Iteration: 11 || Loss: 255.68860272484076
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.226105
Epoch 9 loss:255.68860272484076
MSE loss S32.985784227272035
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.226105
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1747.2003797697323
Iteration: 2 || Loss: 1739.4851157139733
Iteration: 3 || Loss: 1732.7702988720282
Iteration: 4 || Loss: 1726.8519152686486
Iteration: 5 || Loss: 1721.4649316411817
Iteration: 6 || Loss: 1721.4649316411817
saving ADAM checkpoint...
Sum of params:-97.24486
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1721.4649316411817
Iteration: 2 || Loss: 1695.7629410256648
Iteration: 3 || Loss: 1665.0608579417415
Iteration: 4 || Loss: 1612.954396435573
Iteration: 5 || Loss: 1594.1313663244314
Iteration: 6 || Loss: 1581.6960066894017
Iteration: 7 || Loss: 1567.255599550205
Iteration: 8 || Loss: 1558.913378333593
Iteration: 9 || Loss: 1543.9273092767528
Iteration: 10 || Loss: 1524.8964409712285
Iteration: 11 || Loss: 1512.217048076136
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.35214
Epoch 9 loss:1512.217048076136
MSE loss S109.16959207735246
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:350.5529876723668
MSE loss S - interpolation42.21056030289947
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6246.941800261456
MSE loss S - extrapolation890.19311417822
waveform batch: 2/2
Test loss - extrapolation:2519.3415058348014
MSE loss S - extrapolation116.27645937485191
Epoch 9 mean train loss:65.1939793897373
Epoch 9 mean test loss - interpolation:58.42549794539446
Epoch 9 mean test loss - extrapolation:730.5236088413548
Start training epoch 10
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.35214
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1013.4218038116129
Iteration: 2 || Loss: 974.9542823096752
Iteration: 3 || Loss: 938.1283926553115
Iteration: 4 || Loss: 902.95193591167
Iteration: 5 || Loss: 869.4376129093031
Iteration: 6 || Loss: 869.4376129093031
saving ADAM checkpoint...
Sum of params:-97.3435
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 869.4376129093031
Iteration: 2 || Loss: 517.2036753444268
Iteration: 3 || Loss: 379.4658536600867
Iteration: 4 || Loss: 249.01419881494965
Iteration: 5 || Loss: 208.57602204851347
Iteration: 6 || Loss: 179.34818513384374
Iteration: 7 || Loss: 159.25213193463043
Iteration: 8 || Loss: 139.1423180316936
Iteration: 9 || Loss: 128.12611238845008
Iteration: 10 || Loss: 119.7645568374491
Iteration: 11 || Loss: 112.48782795031822
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.29625
Epoch 10 loss:112.48782795031822
MSE loss S23.114086465035765
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.29625
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 373.5889620728628
Iteration: 2 || Loss: 361.11208706966806
Iteration: 3 || Loss: 350.3134698048341
Iteration: 4 || Loss: 341.0752449776397
Iteration: 5 || Loss: 333.3471945080792
Iteration: 6 || Loss: 333.3471945080792
saving ADAM checkpoint...
Sum of params:-97.31089
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 333.3471945080792
Iteration: 2 || Loss: 313.5457187017842
Iteration: 3 || Loss: 303.5480713924133
Iteration: 4 || Loss: 289.82041486163905
Iteration: 5 || Loss: 283.64177730413576
Iteration: 6 || Loss: 275.9079089584172
Iteration: 7 || Loss: 271.1543957547781
Iteration: 8 || Loss: 268.538170660166
Iteration: 9 || Loss: 264.53692047084684
Iteration: 10 || Loss: 250.51688396109935
Iteration: 11 || Loss: 238.04137292675503
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.331696
Epoch 10 loss:238.04137292675503
MSE loss S30.9242941996703
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.331696
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1707.417363665864
Iteration: 2 || Loss: 1700.0796744621857
Iteration: 3 || Loss: 1693.6977213749344
Iteration: 4 || Loss: 1688.0123892651716
Iteration: 5 || Loss: 1682.7801223374283
Iteration: 6 || Loss: 1682.7801223374283
saving ADAM checkpoint...
Sum of params:-97.350426
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1682.7801223374283
Iteration: 2 || Loss: 1659.1369635277706
Iteration: 3 || Loss: 1631.8140100028902
Iteration: 4 || Loss: 1581.6526710088297
Iteration: 5 || Loss: 1563.286218634507
Iteration: 6 || Loss: 1553.1931371976063
Iteration: 7 || Loss: 1541.5764297561266
Iteration: 8 || Loss: 1531.086433797824
Iteration: 9 || Loss: 1515.794949876088
Iteration: 10 || Loss: 1497.2176266253468
Iteration: 11 || Loss: 1484.33996717106
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.43679
Epoch 10 loss:1484.33996717106
MSE loss S107.79913606598274
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:334.6444128764675
MSE loss S - interpolation40.78496029280549
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6199.690532889711
MSE loss S - extrapolation901.9883786249902
waveform batch: 2/2
Test loss - extrapolation:2497.514164449122
MSE loss S - extrapolation115.21404978098309
Epoch 10 mean train loss:63.271350622349416
Epoch 10 mean test loss - interpolation:55.77406881274459
Epoch 10 mean test loss - extrapolation:724.7670581115694
Start training epoch 11
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.43679
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 965.9984414892939
Iteration: 2 || Loss: 928.1815148440559
Iteration: 3 || Loss: 891.9848216506173
Iteration: 4 || Loss: 857.4146319560016
Iteration: 5 || Loss: 824.4934433488291
Iteration: 6 || Loss: 824.4934433488291
saving ADAM checkpoint...
Sum of params:-97.42812
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 824.4934433488291
Iteration: 2 || Loss: 484.2696221890739
Iteration: 3 || Loss: 353.4324869523664
Iteration: 4 || Loss: 236.761480475515
Iteration: 5 || Loss: 198.10650615475146
Iteration: 6 || Loss: 171.16231763118148
Iteration: 7 || Loss: 152.8746677450171
Iteration: 8 || Loss: 133.79055749177218
Iteration: 9 || Loss: 122.98146113318053
Iteration: 10 || Loss: 114.76005700005759
Iteration: 11 || Loss: 107.90893600106877
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.383675
Epoch 11 loss:107.90893600106877
MSE loss S22.501856676656413
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.383675
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 355.2737932727496
Iteration: 2 || Loss: 343.02554535168434
Iteration: 3 || Loss: 332.51045006344964
Iteration: 4 || Loss: 323.5335089621274
Iteration: 5 || Loss: 316.0391420164414
Iteration: 6 || Loss: 316.0391420164414
saving ADAM checkpoint...
Sum of params:-97.39768
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 316.0391420164414
Iteration: 2 || Loss: 296.9191259631257
Iteration: 3 || Loss: 287.38128194537427
Iteration: 4 || Loss: 275.1143709438811
Iteration: 5 || Loss: 268.95164596849304
Iteration: 6 || Loss: 262.19079648751296
Iteration: 7 || Loss: 258.3964336301384
Iteration: 8 || Loss: 255.20389703382736
Iteration: 9 || Loss: 251.29031648498548
Iteration: 10 || Loss: 238.5988007723672
Iteration: 11 || Loss: 226.96084528008294
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.42008
Epoch 11 loss:226.96084528008294
MSE loss S29.657795742367448
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.42008
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1671.8803538181235
Iteration: 2 || Loss: 1664.6771638173218
Iteration: 3 || Loss: 1658.4250737993714
Iteration: 4 || Loss: 1652.8833274584374
Iteration: 5 || Loss: 1647.7821829995412
Iteration: 6 || Loss: 1647.7821829995412
saving ADAM checkpoint...
Sum of params:-97.43837
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1647.7821829995412
Iteration: 2 || Loss: 1625.0974214122996
Iteration: 3 || Loss: 1599.911422329145
Iteration: 4 || Loss: 1550.1790569038017
Iteration: 5 || Loss: 1532.563034518554
Iteration: 6 || Loss: 1523.0723640517917
Iteration: 7 || Loss: 1513.012650198572
Iteration: 8 || Loss: 1502.1610984864076
Iteration: 9 || Loss: 1487.4272114493294
Iteration: 10 || Loss: 1469.5737301774868
Iteration: 11 || Loss: 1456.9736077548669
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.51407
Epoch 11 loss:1456.9736077548669
MSE loss S105.96288364895048
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:321.6083100573626
MSE loss S - interpolation39.50446442976034
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6152.729296133157
MSE loss S - extrapolation908.592037407143
waveform batch: 2/2
Test loss - extrapolation:2474.216624807865
MSE loss S - extrapolation113.4590676361023
Epoch 11 mean train loss:61.78770307020754
Epoch 11 mean test loss - interpolation:53.60138500956043
Epoch 11 mean test loss - extrapolation:718.9121600784184
Start training epoch 12
Changing learning rate to:1.0000000000000002e-6
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.51407
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 927.5813172355574
Iteration: 2 || Loss: 923.7997576612343
Iteration: 3 || Loss: 920.034246894025
Iteration: 4 || Loss: 916.284578054053
Iteration: 5 || Loss: 912.5495965690592
Iteration: 6 || Loss: 912.5495965690592
saving ADAM checkpoint...
Sum of params:-97.5132
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 912.5495965690592
Iteration: 2 || Loss: 479.7133767555395
Iteration: 3 || Loss: 336.99749139743824
Iteration: 4 || Loss: 225.40365366290393
Iteration: 5 || Loss: 189.4983366956032
Iteration: 6 || Loss: 166.25625294907434
Iteration: 7 || Loss: 147.84705964885455
Iteration: 8 || Loss: 129.410107159609
Iteration: 9 || Loss: 120.16190189515098
Iteration: 10 || Loss: 110.86651469012617
Iteration: 11 || Loss: 105.63851866925249
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.46124
Epoch 12 loss:105.63851866925249
MSE loss S22.430241827788656
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.46124
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 341.91662939261903
Iteration: 2 || Loss: 340.57585623831164
Iteration: 3 || Loss: 339.25318084673216
Iteration: 4 || Loss: 337.950274470504
Iteration: 5 || Loss: 336.66924394849417
Iteration: 6 || Loss: 336.66924394849417
saving ADAM checkpoint...
Sum of params:-97.46213
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 336.66924394849417
Iteration: 2 || Loss: 287.38212434679474
Iteration: 3 || Loss: 276.3257693679568
Iteration: 4 || Loss: 262.776198833805
Iteration: 5 || Loss: 257.8867156205056
Iteration: 6 || Loss: 251.92555053071547
Iteration: 7 || Loss: 247.37610919811502
Iteration: 8 || Loss: 245.14825480105867
Iteration: 9 || Loss: 241.24441954698642
Iteration: 10 || Loss: 229.8431428121657
Iteration: 11 || Loss: 219.69830804601276
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.48664
Epoch 12 loss:219.69830804601276
MSE loss S28.327024989311035
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.48664
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1623.3938476071153
Iteration: 2 || Loss: 1622.7615011258645
Iteration: 3 || Loss: 1622.1335868284712
Iteration: 4 || Loss: 1621.5111932526333
Iteration: 5 || Loss: 1620.8931595419458
Iteration: 6 || Loss: 1620.8931595419458
saving ADAM checkpoint...
Sum of params:-97.48911
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1620.8931595419458
Iteration: 2 || Loss: 1597.045410788517
Iteration: 3 || Loss: 1568.3757965598195
Iteration: 4 || Loss: 1520.586877763761
Iteration: 5 || Loss: 1504.6749543830845
Iteration: 6 || Loss: 1493.2877731204685
Iteration: 7 || Loss: 1484.1846234846134
Iteration: 8 || Loss: 1473.4626244940716
Iteration: 9 || Loss: 1458.6018092526278
Iteration: 10 || Loss: 1442.3771420197756
Iteration: 11 || Loss: 1430.4636109142364
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.569115
Epoch 12 loss:1430.4636109142364
MSE loss S103.92920996371414
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:309.6871377939789
MSE loss S - interpolation38.2241616927062
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6102.070344560659
MSE loss S - extrapolation911.1093997786147
waveform batch: 2/2
Test loss - extrapolation:2449.6268488189585
MSE loss S - extrapolation110.62021411622334
Epoch 12 mean train loss:60.54484267687937
Epoch 12 mean test loss - interpolation:51.61452296566315
Epoch 12 mean test loss - extrapolation:712.6414327816348
Start training epoch 13
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.569115
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 889.2035202606883
Iteration: 2 || Loss: 885.5185811171373
Iteration: 3 || Loss: 881.8498166817519
Iteration: 4 || Loss: 878.19648870466
Iteration: 5 || Loss: 874.5584340282036
Iteration: 6 || Loss: 874.5584340282036
saving ADAM checkpoint...
Sum of params:-97.568275
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 874.5584340282036
Iteration: 2 || Loss: 459.94391121473546
Iteration: 3 || Loss: 321.820430192214
Iteration: 4 || Loss: 219.13377665868822
Iteration: 5 || Loss: 184.63039396804834
Iteration: 6 || Loss: 161.70953160868143
Iteration: 7 || Loss: 144.22954239815235
Iteration: 8 || Loss: 126.73430648614338
Iteration: 9 || Loss: 117.23263457916224
Iteration: 10 || Loss: 107.72825649194976
Iteration: 11 || Loss: 102.65160186451064
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.51873
Epoch 13 loss:102.65160186451064
MSE loss S22.01982514571193
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.51873
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 331.426730572315
Iteration: 2 || Loss: 330.08530247648633
Iteration: 3 || Loss: 328.76244463751453
Iteration: 4 || Loss: 327.4594366335161
Iteration: 5 || Loss: 326.1779077544778
Iteration: 6 || Loss: 326.1779077544778
saving ADAM checkpoint...
Sum of params:-97.51964
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 326.1779077544778
Iteration: 2 || Loss: 276.89538326280996
Iteration: 3 || Loss: 266.04313130637627
Iteration: 4 || Loss: 253.4272890626342
Iteration: 5 || Loss: 248.59257146448223
Iteration: 6 || Loss: 243.186492663814
Iteration: 7 || Loss: 238.92742059686267
Iteration: 8 || Loss: 236.72855491494371
Iteration: 9 || Loss: 233.0196828706639
Iteration: 10 || Loss: 222.40238964898182
Iteration: 11 || Loss: 212.83398952634136
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.54564
Epoch 13 loss:212.83398952634136
MSE loss S27.58821300967739
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.54564
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1593.3137258670035
Iteration: 2 || Loss: 1592.695833671744
Iteration: 3 || Loss: 1592.0825202099809
Iteration: 4 || Loss: 1591.4743783454212
Iteration: 5 || Loss: 1590.870992211062
Iteration: 6 || Loss: 1590.870992211062
saving ADAM checkpoint...
Sum of params:-97.548065
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1590.870992211062
Iteration: 2 || Loss: 1567.7706178421447
Iteration: 3 || Loss: 1540.0286417361528
Iteration: 4 || Loss: 1492.7123948996996
Iteration: 5 || Loss: 1477.1316925558003
Iteration: 6 || Loss: 1466.3127770544922
Iteration: 7 || Loss: 1457.8151548995588
Iteration: 8 || Loss: 1446.966985403804
Iteration: 9 || Loss: 1432.8238260565545
Iteration: 10 || Loss: 1417.4375976324513
Iteration: 11 || Loss: 1406.095654781
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.619576
Epoch 13 loss:1406.095654781
MSE loss S102.13550937858503
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:299.38279402223947
MSE loss S - interpolation37.19810704037775
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6058.765145773875
MSE loss S - extrapolation914.358700869726
waveform batch: 2/2
Test loss - extrapolation:2427.042478683909
MSE loss S - extrapolation108.61189736271473
Epoch 13 mean train loss:59.364870557650065
Epoch 13 mean test loss - interpolation:49.89713233703991
Epoch 13 mean test loss - extrapolation:707.150635371482
Start training epoch 14
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.619576
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 857.2758538983014
Iteration: 2 || Loss: 853.6524743834677
Iteration: 3 || Loss: 850.0443015312325
Iteration: 4 || Loss: 846.4512292201209
Iteration: 5 || Loss: 842.8726583176353
Iteration: 6 || Loss: 842.8726583176353
saving ADAM checkpoint...
Sum of params:-97.61874
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 842.8726583176353
Iteration: 2 || Loss: 440.2389165914344
Iteration: 3 || Loss: 306.57626821680356
Iteration: 4 || Loss: 211.4294285885741
Iteration: 5 || Loss: 178.52837873951242
Iteration: 6 || Loss: 156.5012309802095
Iteration: 7 || Loss: 139.9782551624906
Iteration: 8 || Loss: 123.42879102786571
Iteration: 9 || Loss: 114.05135785304752
Iteration: 10 || Loss: 104.74288526776205
Iteration: 11 || Loss: 99.87553609112663
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.56988
Epoch 14 loss:99.87553609112663
MSE loss S21.62569644545483
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.56988
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 321.259866456096
Iteration: 2 || Loss: 319.9231001556662
Iteration: 3 || Loss: 318.6059292163704
Iteration: 4 || Loss: 317.3078106870071
Iteration: 5 || Loss: 316.03051011645357
Iteration: 6 || Loss: 316.03051011645357
saving ADAM checkpoint...
Sum of params:-97.57077
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 316.03051011645357
Iteration: 2 || Loss: 267.17355149011087
Iteration: 3 || Loss: 256.496727946571
Iteration: 4 || Loss: 244.81382143301607
Iteration: 5 || Loss: 240.01684277302698
Iteration: 6 || Loss: 235.1353923598971
Iteration: 7 || Loss: 231.20267402498882
Iteration: 8 || Loss: 228.9763875998073
Iteration: 9 || Loss: 225.4428280711009
Iteration: 10 || Loss: 215.4573713516026
Iteration: 11 || Loss: 206.27049355657982
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.5988
Epoch 14 loss:206.27049355657982
MSE loss S26.877125061366286
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.5988
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1566.1449058916162
Iteration: 2 || Loss: 1565.5432564492255
Iteration: 3 || Loss: 1564.9465024981118
Iteration: 4 || Loss: 1564.3540777962335
Iteration: 5 || Loss: 1563.7664650744755
Iteration: 6 || Loss: 1563.7664650744755
saving ADAM checkpoint...
Sum of params:-97.601166
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1563.7664650744755
Iteration: 2 || Loss: 1541.4285242864871
Iteration: 3 || Loss: 1514.6139699620546
Iteration: 4 || Loss: 1467.8959907090332
Iteration: 5 || Loss: 1452.4285363647748
Iteration: 6 || Loss: 1442.1461433633249
Iteration: 7 || Loss: 1433.963903424905
Iteration: 8 || Loss: 1422.9536163816558
Iteration: 9 || Loss: 1409.2433636964568
Iteration: 10 || Loss: 1394.504437998021
Iteration: 11 || Loss: 1383.6661324463248
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.66589
Epoch 14 loss:1383.6661324463248
MSE loss S100.44621920037338
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:289.83156274170807
MSE loss S - interpolation36.24609435128869
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:6009.690888377583
MSE loss S - extrapolation914.6301786156091
waveform batch: 2/2
Test loss - extrapolation:2406.9129742465425
MSE loss S - extrapolation106.84242104827217
Epoch 14 mean train loss:58.26938489979418
Epoch 14 mean test loss - interpolation:48.305260456951345
Epoch 14 mean test loss - extrapolation:701.3836552186772
Start training epoch 15
Changing learning rate to:1.0000000000000002e-7
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.66589
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 825.4642095694717
Iteration: 2 || Loss: 825.0955535133496
Iteration: 3 || Loss: 824.7275817238404
Iteration: 4 || Loss: 824.3593390573712
Iteration: 5 || Loss: 823.9912686686514
Iteration: 6 || Loss: 823.9912686686514
saving ADAM checkpoint...
Sum of params:-97.66581
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 823.9912686686514
Iteration: 2 || Loss: 423.59891973437635
Iteration: 3 || Loss: 292.85565395971787
Iteration: 4 || Loss: 204.032119411383
Iteration: 5 || Loss: 172.70052239789965
Iteration: 6 || Loss: 151.7054801029642
Iteration: 7 || Loss: 136.02009272128322
Iteration: 8 || Loss: 120.27872190780764
Iteration: 9 || Loss: 111.13901993526525
Iteration: 10 || Loss: 102.01924609520144
Iteration: 11 || Loss: 97.44462526499491
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.61659
Epoch 15 loss:97.44462526499491
MSE loss S21.283008674066032
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.61659
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 311.7001982729031
Iteration: 2 || Loss: 311.5622340222512
Iteration: 3 || Loss: 311.42539432272287
Iteration: 4 || Loss: 311.2875734113746
Iteration: 5 || Loss: 311.15060459167364
Iteration: 6 || Loss: 311.15060459167364
saving ADAM checkpoint...
Sum of params:-97.61668
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 311.15060459167364
Iteration: 2 || Loss: 258.67235975899104
Iteration: 3 || Loss: 247.99926517614168
Iteration: 4 || Loss: 236.85508656922738
Iteration: 5 || Loss: 232.12282933327307
Iteration: 6 || Loss: 227.6586524133571
Iteration: 7 || Loss: 223.67992099518833
Iteration: 8 || Loss: 221.76236456408483
Iteration: 9 || Loss: 218.2679203968064
Iteration: 10 || Loss: 209.0977246003163
Iteration: 11 || Loss: 200.4242670433977
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.64651
Epoch 15 loss:200.4242670433977
MSE loss S26.28846951883991
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.64651
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1540.0691441222575
Iteration: 2 || Loss: 1540.0066376237535
Iteration: 3 || Loss: 1539.944020164058
Iteration: 4 || Loss: 1539.8815615306805
Iteration: 5 || Loss: 1539.819880034789
Iteration: 6 || Loss: 1539.819880034789
saving ADAM checkpoint...
Sum of params:-97.64663
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1539.819880034789
Iteration: 2 || Loss: 1517.3050702900225
Iteration: 3 || Loss: 1490.821992269777
Iteration: 4 || Loss: 1444.4423838882137
Iteration: 5 || Loss: 1428.9110926481026
Iteration: 6 || Loss: 1419.1124017848315
Iteration: 7 || Loss: 1411.2502838494138
Iteration: 8 || Loss: 1400.3358320313032
Iteration: 9 || Loss: 1387.0641036662353
Iteration: 10 || Loss: 1372.9944083683254
Iteration: 11 || Loss: 1362.5655219319065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.70802
Epoch 15 loss:1362.5655219319065
MSE loss S98.80890335664972
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:281.53943784156064
MSE loss S - interpolation35.40897716311186
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5960.140855459106
MSE loss S - extrapolation913.1499023843961
waveform batch: 2/2
Test loss - extrapolation:2387.263532104622
MSE loss S - extrapolation105.0140236156868
Epoch 15 mean train loss:57.25635911173445
Epoch 15 mean test loss - interpolation:46.92323964026011
Epoch 15 mean test loss - extrapolation:695.6170322969774
Start training epoch 16
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.70802
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 796.8924922028629
Iteration: 2 || Loss: 796.5324945347479
Iteration: 3 || Loss: 796.1702329532292
Iteration: 4 || Loss: 795.8105294280105
Iteration: 5 || Loss: 795.4483741438725
Iteration: 6 || Loss: 795.4483741438725
saving ADAM checkpoint...
Sum of params:-97.70793
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 795.4483741438725
Iteration: 2 || Loss: 407.84188050584885
Iteration: 3 || Loss: 281.2926422148494
Iteration: 4 || Loss: 198.12459415585607
Iteration: 5 || Loss: 168.03551417792121
Iteration: 6 || Loss: 147.63352919502685
Iteration: 7 || Loss: 132.76073106152617
Iteration: 8 || Loss: 117.67500342447782
Iteration: 9 || Loss: 108.59798750762535
Iteration: 10 || Loss: 99.69791206369511
Iteration: 11 || Loss: 95.21555091540354
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.65921
Epoch 16 loss:95.21555091540354
MSE loss S20.94177261548936
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.65921
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 302.84279385345656
Iteration: 2 || Loss: 302.70592636141555
Iteration: 3 || Loss: 302.56930023843137
Iteration: 4 || Loss: 302.43279603599836
Iteration: 5 || Loss: 302.297740097307
Iteration: 6 || Loss: 302.297740097307
saving ADAM checkpoint...
Sum of params:-97.65929
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 302.297740097307
Iteration: 2 || Loss: 250.77634810210301
Iteration: 3 || Loss: 240.26343941094098
Iteration: 4 || Loss: 229.8034246770763
Iteration: 5 || Loss: 225.0821420538267
Iteration: 6 || Loss: 220.99162912118692
Iteration: 7 || Loss: 217.30613625100577
Iteration: 8 || Loss: 215.3245364791995
Iteration: 9 || Loss: 211.98898405013227
Iteration: 10 || Loss: 203.34397427737738
Iteration: 11 || Loss: 194.96014905848446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.69061
Epoch 16 loss:194.96014905848446
MSE loss S25.690668024409472
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.69061
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1516.3104676228177
Iteration: 2 || Loss: 1516.2491129940693
Iteration: 3 || Loss: 1516.18709146466
Iteration: 4 || Loss: 1516.124670955714
Iteration: 5 || Loss: 1516.0627908166841
Iteration: 6 || Loss: 1516.0627908166841
saving ADAM checkpoint...
Sum of params:-97.690704
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1516.0627908166841
Iteration: 2 || Loss: 1494.263446849853
Iteration: 3 || Loss: 1468.5228898690848
Iteration: 4 || Loss: 1422.8341174600826
Iteration: 5 || Loss: 1407.2549507421745
Iteration: 6 || Loss: 1397.9482415884
Iteration: 7 || Loss: 1390.185834058059
Iteration: 8 || Loss: 1379.3344213567084
Iteration: 9 || Loss: 1366.4150295903773
Iteration: 10 || Loss: 1352.8570129284453
Iteration: 11 || Loss: 1342.8001904993694
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.74753
Epoch 16 loss:1342.8001904993694
MSE loss S97.30380173834207
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:273.91774522787745
MSE loss S - interpolation34.65981856463191
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5908.881768890811
MSE loss S - extrapolation910.285099338864
waveform batch: 2/2
Test loss - extrapolation:2369.088014937082
MSE loss S - extrapolation103.45221303620943
Epoch 16 mean train loss:56.30951346459508
Epoch 16 mean test loss - interpolation:45.652957537979574
Epoch 16 mean test loss - extrapolation:689.8308153189911
Start training epoch 17
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.74753
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 769.893222973831
Iteration: 2 || Loss: 769.537347264723
Iteration: 3 || Loss: 769.1818916451913
Iteration: 4 || Loss: 768.8262726545818
Iteration: 5 || Loss: 768.471345359104
Iteration: 6 || Loss: 768.471345359104
saving ADAM checkpoint...
Sum of params:-97.74753
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 768.471345359104
Iteration: 2 || Loss: 393.0034504869074
Iteration: 3 || Loss: 270.37904237790434
Iteration: 4 || Loss: 192.26038734636353
Iteration: 5 || Loss: 163.4046550269907
Iteration: 6 || Loss: 143.6080465866704
Iteration: 7 || Loss: 129.5899786355376
Iteration: 8 || Loss: 115.10851594620823
Iteration: 9 || Loss: 106.1661356173509
Iteration: 10 || Loss: 97.53527298263651
Iteration: 11 || Loss: 93.1378095324055
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.69935
Epoch 17 loss:93.1378095324055
MSE loss S20.608000974028126
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.69935
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 294.414446802502
Iteration: 2 || Loss: 294.27955185059784
Iteration: 3 || Loss: 294.1455691669597
Iteration: 4 || Loss: 294.01088851609666
Iteration: 5 || Loss: 293.87621051928915
Iteration: 6 || Loss: 293.87621051928915
saving ADAM checkpoint...
Sum of params:-97.699425
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 293.87621051928915
Iteration: 2 || Loss: 243.50400049708776
Iteration: 3 || Loss: 233.14084006087975
Iteration: 4 || Loss: 223.27563492788406
Iteration: 5 || Loss: 218.56623578945457
Iteration: 6 || Loss: 214.79406229169166
Iteration: 7 || Loss: 211.42956490334905
Iteration: 8 || Loss: 209.3328418541485
Iteration: 9 || Loss: 206.14525636089502
Iteration: 10 || Loss: 197.9729132474554
Iteration: 11 || Loss: 189.86011314277218
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.73211
Epoch 17 loss:189.86011314277218
MSE loss S25.130628944671592
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.73211
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1493.9624001144846
Iteration: 2 || Loss: 1493.9009843231206
Iteration: 3 || Loss: 1493.8402180633946
Iteration: 4 || Loss: 1493.778965608608
Iteration: 5 || Loss: 1493.717024889311
Iteration: 6 || Loss: 1493.717024889311
saving ADAM checkpoint...
Sum of params:-97.73218
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1493.717024889311
Iteration: 2 || Loss: 1472.635993553065
Iteration: 3 || Loss: 1447.567513367735
Iteration: 4 || Loss: 1402.6056732112472
Iteration: 5 || Loss: 1386.911254280469
Iteration: 6 || Loss: 1378.0799315725603
Iteration: 7 || Loss: 1370.3730883277362
Iteration: 8 || Loss: 1359.6367402955243
Iteration: 9 || Loss: 1347.0162042696977
Iteration: 10 || Loss: 1333.9036312630496
Iteration: 11 || Loss: 1324.1662059155651
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.785095
Epoch 17 loss:1324.1662059155651
MSE loss S95.88968842027984
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:267.05906727114706
MSE loss S - interpolation33.990022943505764
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5856.730725624534
MSE loss S - extrapolation906.1920268588015
waveform batch: 2/2
Test loss - extrapolation:2351.8155477852397
MSE loss S - extrapolation102.00803020932149
Epoch 17 mean train loss:55.41945271002562
Epoch 17 mean test loss - interpolation:44.50984454519118
Epoch 17 mean test loss - extrapolation:684.0455227841479
Start training epoch 18
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.785095
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 744.9776394528285
Iteration: 2 || Loss: 744.6273808805507
Iteration: 3 || Loss: 744.276892725919
Iteration: 4 || Loss: 743.9267068432924
Iteration: 5 || Loss: 743.5765386491413
Iteration: 6 || Loss: 743.5765386491413
saving ADAM checkpoint...
Sum of params:-97.785
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 743.5765386491413
Iteration: 2 || Loss: 379.52364503716365
Iteration: 3 || Loss: 260.65363672442817
Iteration: 4 || Loss: 186.9821677067945
Iteration: 5 || Loss: 159.21712748313925
Iteration: 6 || Loss: 139.9488009195342
Iteration: 7 || Loss: 126.71638490820389
Iteration: 8 || Loss: 112.72082316596287
Iteration: 9 || Loss: 103.94681441642864
Iteration: 10 || Loss: 95.57775529551441
Iteration: 11 || Loss: 91.24001210931812
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.73739
Epoch 18 loss:91.24001210931812
MSE loss S20.28997969447828
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.73739
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 286.48070421677977
Iteration: 2 || Loss: 286.3475509539877
Iteration: 3 || Loss: 286.2139644109319
Iteration: 4 || Loss: 286.082259334537
Iteration: 5 || Loss: 285.9495094904745
Iteration: 6 || Loss: 285.9495094904745
saving ADAM checkpoint...
Sum of params:-97.73747
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 285.9495094904745
Iteration: 2 || Loss: 236.83332757455753
Iteration: 3 || Loss: 226.6296956091966
Iteration: 4 || Loss: 217.28400133070312
Iteration: 5 || Loss: 212.60069614585396
Iteration: 6 || Loss: 209.08838331294803
Iteration: 7 || Loss: 205.99873689647347
Iteration: 8 || Loss: 203.80735232419428
Iteration: 9 || Loss: 200.75658651371344
Iteration: 10 || Loss: 193.00572386866435
Iteration: 11 || Loss: 185.17187612531606
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.7713
Epoch 18 loss:185.17187612531606
MSE loss S24.61579695295082
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.7713
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1472.7182467409411
Iteration: 2 || Loss: 1472.6573467103217
Iteration: 3 || Loss: 1472.5963411427463
Iteration: 4 || Loss: 1472.5358972665301
Iteration: 5 || Loss: 1472.4749391505493
Iteration: 6 || Loss: 1472.4749391505493
saving ADAM checkpoint...
Sum of params:-97.77137
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1472.4749391505493
Iteration: 2 || Loss: 1452.0919605476145
Iteration: 3 || Loss: 1427.592286026434
Iteration: 4 || Loss: 1383.3865115220908
Iteration: 5 || Loss: 1367.5579290918336
Iteration: 6 || Loss: 1359.176195039921
Iteration: 7 || Loss: 1351.5097646585464
Iteration: 8 || Loss: 1340.9496587310898
Iteration: 9 || Loss: 1328.6258881310846
Iteration: 10 || Loss: 1315.914631879143
Iteration: 11 || Loss: 1306.4620582906514
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.82101
Epoch 18 loss:1306.4620582906514
MSE loss S94.5499852468652
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:260.869399395302
MSE loss S - interpolation33.385007919405936
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5804.272655261074
MSE loss S - extrapolation901.1324231079741
waveform batch: 2/2
Test loss - extrapolation:2335.101031911854
MSE loss S - extrapolation100.63307419031035
Epoch 18 mean train loss:54.58186022500985
Epoch 18 mean test loss - interpolation:43.47823323255034
Epoch 18 mean test loss - extrapolation:678.281140597744
Start training epoch 19
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.82101
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 722.0632472841235
Iteration: 2 || Loss: 721.7182543762772
Iteration: 3 || Loss: 721.3726417099664
Iteration: 4 || Loss: 721.0273966161121
Iteration: 5 || Loss: 720.6819464410852
Iteration: 6 || Loss: 720.6819464410852
saving ADAM checkpoint...
Sum of params:-97.82093
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 720.6819464410852
Iteration: 2 || Loss: 367.3756531319042
Iteration: 3 || Loss: 252.01978458237343
Iteration: 4 || Loss: 182.2260158799397
Iteration: 5 || Loss: 155.44935758386654
Iteration: 6 || Loss: 136.64217867378684
Iteration: 7 || Loss: 124.12651583920764
Iteration: 8 || Loss: 110.51491022032234
Iteration: 9 || Loss: 101.93207085203916
Iteration: 10 || Loss: 93.81241181912405
Iteration: 11 || Loss: 89.5153794989243
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.77385
Epoch 19 loss:89.5153794989243
MSE loss S19.989000743103986
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.77385
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 279.186406151845
Iteration: 2 || Loss: 279.0544238761506
Iteration: 3 || Loss: 278.9235627193003
Iteration: 4 || Loss: 278.7922046186061
Iteration: 5 || Loss: 278.6620864247056
Iteration: 6 || Loss: 278.6620864247056
saving ADAM checkpoint...
Sum of params:-97.77394
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 278.6620864247056
Iteration: 2 || Loss: 230.7654535110137
Iteration: 3 || Loss: 220.7123923914222
Iteration: 4 || Loss: 211.81224279117984
Iteration: 5 || Loss: 207.17182035839141
Iteration: 6 || Loss: 203.8707744645687
Iteration: 7 || Loss: 201.02267528974318
Iteration: 8 || Loss: 198.7463837456722
Iteration: 9 || Loss: 195.81453547547596
Iteration: 10 || Loss: 188.44085166674358
Iteration: 11 || Loss: 180.88810066373645
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.80874
Epoch 19 loss:180.88810066373645
MSE loss S24.147247411193593
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.80874
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1452.387610072755
Iteration: 2 || Loss: 1452.327819117072
Iteration: 3 || Loss: 1452.2669527985784
Iteration: 4 || Loss: 1452.206443570606
Iteration: 5 || Loss: 1452.1464412140115
Iteration: 6 || Loss: 1452.1464412140115
saving ADAM checkpoint...
Sum of params:-97.808815
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1452.1464412140115
Iteration: 2 || Loss: 1432.4367230113096
Iteration: 3 || Loss: 1408.4297389394596
Iteration: 4 || Loss: 1365.0076536786864
Iteration: 5 || Loss: 1349.0359275986104
Iteration: 6 || Loss: 1341.0780220936745
Iteration: 7 || Loss: 1333.4472403710888
Iteration: 8 || Loss: 1323.106341872553
Iteration: 9 || Loss: 1311.0788444939064
Iteration: 10 || Loss: 1298.742761078402
Iteration: 11 || Loss: 1289.5601278340419
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.855606
Epoch 19 loss:1289.5601278340419
MSE loss S93.27700755662443
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:255.2255966621733
MSE loss S - interpolation32.829377075079556
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5752.050433371035
MSE loss S - extrapolation895.3791797751454
waveform batch: 2/2
Test loss - extrapolation:2318.8087906194655
MSE loss S - extrapolation99.31346397417848
Epoch 19 mean train loss:53.791848551610435
Epoch 19 mean test loss - interpolation:42.53759944369555
Epoch 19 mean test loss - extrapolation:672.5716019992084
Start training epoch 20
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.855606
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 700.9405498942988
Iteration: 2 || Loss: 700.6000623972471
Iteration: 3 || Loss: 700.2594658459227
Iteration: 4 || Loss: 699.9186943273152
Iteration: 5 || Loss: 699.5786007412644
Iteration: 6 || Loss: 699.5786007412644
saving ADAM checkpoint...
Sum of params:-97.85553
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 699.5786007412644
Iteration: 2 || Loss: 356.27763981672155
Iteration: 3 || Loss: 244.2672741654707
Iteration: 4 || Loss: 177.89731221003564
Iteration: 5 || Loss: 152.0248329860249
Iteration: 6 || Loss: 133.62722836875298
Iteration: 7 || Loss: 121.7638439961921
Iteration: 8 || Loss: 108.46411594243163
Iteration: 9 || Loss: 100.09190840927889
Iteration: 10 || Loss: 92.2103022806633
Iteration: 11 || Loss: 87.94877881172486
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.809006
Epoch 20 loss:87.94877881172486
MSE loss S19.705356386167793
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.809006
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 272.4636731558011
Iteration: 2 || Loss: 272.3334769080664
Iteration: 3 || Loss: 272.2038193915812
Iteration: 4 || Loss: 272.0753320800923
Iteration: 5 || Loss: 271.94608453163704
Iteration: 6 || Loss: 271.94608453163704
saving ADAM checkpoint...
Sum of params:-97.80907
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 271.94608453163704
Iteration: 2 || Loss: 225.22369616146133
Iteration: 3 || Loss: 215.31959511117364
Iteration: 4 || Loss: 206.8043632181291
Iteration: 5 || Loss: 202.2243050703774
Iteration: 6 || Loss: 199.0894340288567
Iteration: 7 || Loss: 196.42152251216189
Iteration: 8 || Loss: 194.10531246851795
Iteration: 9 || Loss: 191.279341731037
Iteration: 10 || Loss: 184.24274025539665
Iteration: 11 || Loss: 176.9700805468033
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.84472
Epoch 20 loss:176.9700805468033
MSE loss S23.72015102067377
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.84472
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1432.8771231936078
Iteration: 2 || Loss: 1432.8174847614891
Iteration: 3 || Loss: 1432.757953409672
Iteration: 4 || Loss: 1432.6983087457825
Iteration: 5 || Loss: 1432.6384718279019
Iteration: 6 || Loss: 1432.6384718279019
saving ADAM checkpoint...
Sum of params:-97.8448
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1432.6384718279019
Iteration: 2 || Loss: 1413.5718161254738
Iteration: 3 || Loss: 1389.9913148754406
Iteration: 4 || Loss: 1347.3640405066014
Iteration: 5 || Loss: 1331.2652428700312
Iteration: 6 || Loss: 1323.7035528711226
Iteration: 7 || Loss: 1316.101861294231
Iteration: 8 || Loss: 1306.0071632872546
Iteration: 9 || Loss: 1294.2769203286325
Iteration: 10 || Loss: 1282.2930581221465
Iteration: 11 || Loss: 1273.3727381192691
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.88916
Epoch 20 loss:1273.3727381192691
MSE loss S92.06536099968672
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:250.03590891120334
MSE loss S - interpolation32.31275695112228
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5700.377427169285
MSE loss S - extrapolation889.1288638941352
waveform batch: 2/2
Test loss - extrapolation:2302.861904160841
MSE loss S - extrapolation98.04182872996022
Epoch 20 mean train loss:53.04453784406198
Epoch 20 mean test loss - interpolation:41.672651485200554
Epoch 20 mean test loss - extrapolation:666.9366109441771
Start training epoch 21
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.88916
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 681.4144923157013
Iteration: 2 || Loss: 681.0787305774338
Iteration: 3 || Loss: 680.7419015757331
Iteration: 4 || Loss: 680.4064976482592
Iteration: 5 || Loss: 680.0708299747423
Iteration: 6 || Loss: 680.0708299747423
saving ADAM checkpoint...
Sum of params:-97.88907
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 680.0708299747423
Iteration: 2 || Loss: 346.0259432600725
Iteration: 3 || Loss: 237.2317360809949
Iteration: 4 || Loss: 173.91568120330203
Iteration: 5 || Loss: 148.88466240847643
Iteration: 6 || Loss: 130.8590868702203
Iteration: 7 || Loss: 119.58803984814276
Iteration: 8 || Loss: 106.54923689575307
Iteration: 9 || Loss: 98.40053349974758
Iteration: 10 || Loss: 90.74612273138214
Iteration: 11 || Loss: 86.51731877628285
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.843056
Epoch 21 loss:86.51731877628285
MSE loss S19.436659276444363
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.843056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 266.29947132771315
Iteration: 2 || Loss: 266.1710247725142
Iteration: 3 || Loss: 266.04386503678666
Iteration: 4 || Loss: 265.9157737711868
Iteration: 5 || Loss: 265.7882896182672
Iteration: 6 || Loss: 265.7882896182672
saving ADAM checkpoint...
Sum of params:-97.843124
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 265.7882896182672
Iteration: 2 || Loss: 220.15267090223253
Iteration: 3 || Loss: 210.39019182781124
Iteration: 4 || Loss: 202.19981859858393
Iteration: 5 || Loss: 197.70102065694107
Iteration: 6 || Loss: 194.69698118630757
Iteration: 7 || Loss: 192.17100074580293
Iteration: 8 || Loss: 189.83768083015784
Iteration: 9 || Loss: 187.1057212440386
Iteration: 10 || Loss: 180.37415941778428
Iteration: 11 || Loss: 173.37658857077093
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.87955
Epoch 21 loss:173.37658857077093
MSE loss S23.32929812714852
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.87955
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1414.078525693348
Iteration: 2 || Loss: 1414.0186478582543
Iteration: 3 || Loss: 1413.9593242336948
Iteration: 4 || Loss: 1413.9004819272525
Iteration: 5 || Loss: 1413.8404757294809
Iteration: 6 || Loss: 1413.8404757294809
saving ADAM checkpoint...
Sum of params:-97.87962
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1413.8404757294809
Iteration: 2 || Loss: 1395.3852639941401
Iteration: 3 || Loss: 1372.1839084512822
Iteration: 4 || Loss: 1330.37467208075
Iteration: 5 || Loss: 1314.1637325781267
Iteration: 6 || Loss: 1306.9779907068096
Iteration: 7 || Loss: 1299.4085472555228
Iteration: 8 || Loss: 1289.5770607987674
Iteration: 9 || Loss: 1278.1442506426952
Iteration: 10 || Loss: 1266.4925156086151
Iteration: 11 || Loss: 1257.8305660356968
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.92179
Epoch 21 loss:1257.8305660356968
MSE loss S90.91418505704844
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:245.2316571925508
MSE loss S - interpolation31.829488853948384
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5649.508413971686
MSE loss S - extrapolation882.5510421333362
waveform batch: 2/2
Test loss - extrapolation:2287.2027655880097
MSE loss S - extrapolation96.81576904914722
Epoch 21 mean train loss:52.33532666837071
Epoch 21 mean test loss - interpolation:40.87194286542513
Epoch 21 mean test loss - extrapolation:661.3925982966413
Start training epoch 22
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.92179
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 663.3387146743338
Iteration: 2 || Loss: 663.006639350519
Iteration: 3 || Loss: 662.6753174499668
Iteration: 4 || Loss: 662.3429498056522
Iteration: 5 || Loss: 662.011913000012
Iteration: 6 || Loss: 662.011913000012
saving ADAM checkpoint...
Sum of params:-97.92172
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 662.011913000012
Iteration: 2 || Loss: 336.4668947760229
Iteration: 3 || Loss: 230.7877445931919
Iteration: 4 || Loss: 170.2244103982104
Iteration: 5 || Loss: 145.98293791695917
Iteration: 6 || Loss: 128.30185525948775
Iteration: 7 || Loss: 117.5673765817359
Iteration: 8 || Loss: 104.75581511631236
Iteration: 9 || Loss: 96.83726237353783
Iteration: 10 || Loss: 89.40153938432582
Iteration: 11 || Loss: 85.21275540887221
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.8762
Epoch 22 loss:85.21275540887221
MSE loss S19.18396638577079
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.8762
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 260.62242280692516
Iteration: 2 || Loss: 260.4955082291936
Iteration: 3 || Loss: 260.36975128782325
Iteration: 4 || Loss: 260.2437780931645
Iteration: 5 || Loss: 260.11753529814234
Iteration: 6 || Loss: 260.11753529814234
saving ADAM checkpoint...
Sum of params:-97.87628
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 260.11753529814234
Iteration: 2 || Loss: 215.50023836444325
Iteration: 3 || Loss: 205.86919405226223
Iteration: 4 || Loss: 197.96108199066012
Iteration: 5 || Loss: 193.55552468290145
Iteration: 6 || Loss: 190.65336861860658
Iteration: 7 || Loss: 188.23979150119345
Iteration: 8 || Loss: 185.90532222181406
Iteration: 9 || Loss: 183.25616961723244
Iteration: 10 || Loss: 176.7938667513453
Iteration: 11 || Loss: 170.0629178965933
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.91336
Epoch 22 loss:170.0629178965933
MSE loss S22.968820307524748
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.91336
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1395.980722954098
Iteration: 2 || Loss: 1395.9216076543162
Iteration: 3 || Loss: 1395.8624943054308
Iteration: 4 || Loss: 1395.8031436023575
Iteration: 5 || Loss: 1395.7437775425371
Iteration: 6 || Loss: 1395.7437775425371
saving ADAM checkpoint...
Sum of params:-97.913414
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1395.7437775425371
Iteration: 2 || Loss: 1377.8683368532616
Iteration: 3 || Loss: 1355.0140312140736
Iteration: 4 || Loss: 1314.0204793856262
Iteration: 5 || Loss: 1297.721810293864
Iteration: 6 || Loss: 1290.8841592415504
Iteration: 7 || Loss: 1283.3430430889084
Iteration: 8 || Loss: 1273.7761420387487
Iteration: 9 || Loss: 1262.6301328123245
Iteration: 10 || Loss: 1251.2906516972578
Iteration: 11 || Loss: 1242.8836716509436
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.95366
Epoch 22 loss:1242.8836716509436
MSE loss S89.81814769166067
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:240.74740642991262
MSE loss S - interpolation31.37277395895281
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5599.526313151389
MSE loss S - extrapolation875.7447087370533
waveform batch: 2/2
Test loss - extrapolation:2271.843249600811
MSE loss S - extrapolation95.63535940781489
Epoch 22 mean train loss:51.660667067462384
Epoch 22 mean test loss - interpolation:40.12456773831877
Epoch 22 mean test loss - extrapolation:655.9474635626833
Start training epoch 23
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.95366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 646.5400463124422
Iteration: 2 || Loss: 646.2123771484443
Iteration: 3 || Loss: 645.8840259310624
Iteration: 4 || Loss: 645.5569561494715
Iteration: 5 || Loss: 645.2289561512235
Iteration: 6 || Loss: 645.2289561512235
saving ADAM checkpoint...
Sum of params:-97.95358
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 645.2289561512235
Iteration: 2 || Loss: 327.4801059708158
Iteration: 3 || Loss: 224.8232967674228
Iteration: 4 || Loss: 166.75947858855608
Iteration: 5 || Loss: 143.2687705518207
Iteration: 6 || Loss: 125.91210975989102
Iteration: 7 || Loss: 115.66866527149338
Iteration: 8 || Loss: 103.0665489588128
Iteration: 9 || Loss: 95.37946105832165
Iteration: 10 || Loss: 88.15388015799822
Iteration: 11 || Loss: 84.01043832850762
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.90861
Epoch 23 loss:84.01043832850762
MSE loss S18.945254971762836
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.90861
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 255.34366341504915
Iteration: 2 || Loss: 255.2187039139998
Iteration: 3 || Loss: 255.0933276568266
Iteration: 4 || Loss: 254.9683598167142
Iteration: 5 || Loss: 254.8447418614377
Iteration: 6 || Loss: 254.8447418614377
saving ADAM checkpoint...
Sum of params:-97.908676
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 254.8447418614377
Iteration: 2 || Loss: 211.19404079938732
Iteration: 3 || Loss: 201.69349433121516
Iteration: 4 || Loss: 194.0274822733286
Iteration: 5 || Loss: 189.73034187382493
Iteration: 6 || Loss: 186.9032699958287
Iteration: 7 || Loss: 184.54965942721907
Iteration: 8 || Loss: 182.2578462783472
Iteration: 9 || Loss: 179.68360730812
Iteration: 10 || Loss: 173.46139066076327
Iteration: 11 || Loss: 166.99161996723623
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.946304
Epoch 23 loss:166.99161996723623
MSE loss S22.63250704368509
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.946304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1378.5126936494726
Iteration: 2 || Loss: 1378.4540218917593
Iteration: 3 || Loss: 1378.3954057086553
Iteration: 4 || Loss: 1378.3364156276027
Iteration: 5 || Loss: 1378.2778344261505
Iteration: 6 || Loss: 1378.2778344261505
saving ADAM checkpoint...
Sum of params:-97.946396
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1378.2778344261505
Iteration: 2 || Loss: 1360.9540075678333
Iteration: 3 || Loss: 1338.4208222115549
Iteration: 4 || Loss: 1298.236643173976
Iteration: 5 || Loss: 1281.875807941986
Iteration: 6 || Loss: 1275.3653162102378
Iteration: 7 || Loss: 1267.8497141419168
Iteration: 8 || Loss: 1258.5520726905743
Iteration: 9 || Loss: 1247.6870737057532
Iteration: 10 || Loss: 1236.64134718228
Iteration: 11 || Loss: 1228.48069780618
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.98493
Epoch 23 loss:1228.48069780618
MSE loss S88.77566151809991
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:236.55493217427892
MSE loss S - interpolation30.94182179124516
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5550.592979111457
MSE loss S - extrapolation868.8180997390834
waveform batch: 2/2
Test loss - extrapolation:2256.7370665816275
MSE loss S - extrapolation94.49941037434701
Epoch 23 mean train loss:51.0166467621353
Epoch 23 mean test loss - interpolation:39.425822029046486
Epoch 23 mean test loss - extrapolation:650.6108371410904
Start training epoch 24
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-97.98493
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 630.9672155761353
Iteration: 2 || Loss: 630.6420038447129
Iteration: 3 || Loss: 630.3186356961922
Iteration: 4 || Loss: 629.9938690684439
Iteration: 5 || Loss: 629.6707344996346
Iteration: 6 || Loss: 629.6707344996346
saving ADAM checkpoint...
Sum of params:-97.98486
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 629.6707344996346
Iteration: 2 || Loss: 319.0110279320755
Iteration: 3 || Loss: 219.28261427586838
Iteration: 4 || Loss: 163.4992927479975
Iteration: 5 || Loss: 140.72126073582083
Iteration: 6 || Loss: 123.6734730958575
Iteration: 7 || Loss: 113.87716369349826
Iteration: 8 || Loss: 101.47114649819446
Iteration: 9 || Loss: 94.01343110406658
Iteration: 10 || Loss: 86.99237060423128
Iteration: 11 || Loss: 82.91150446504987
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.94033
Epoch 24 loss:82.91150446504987
MSE loss S18.722789117920765
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.94033
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 250.47786839858045
Iteration: 2 || Loss: 250.354082168233
Iteration: 3 || Loss: 250.23055495548564
Iteration: 4 || Loss: 250.10754276826987
Iteration: 5 || Loss: 249.9844867454554
Iteration: 6 || Loss: 249.9844867454554
saving ADAM checkpoint...
Sum of params:-97.9404
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 249.9844867454554
Iteration: 2 || Loss: 207.21204358185562
Iteration: 3 || Loss: 197.83414989315608
Iteration: 4 || Loss: 190.38420621142433
Iteration: 5 || Loss: 186.1942553379999
Iteration: 6 || Loss: 183.425408781667
Iteration: 7 || Loss: 181.10634773996892
Iteration: 8 || Loss: 178.87219110921677
Iteration: 9 || Loss: 176.36669160079379
Iteration: 10 || Loss: 170.35928541143824
Iteration: 11 || Loss: 164.13621781014842
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.97856
Epoch 24 loss:164.13621781014842
MSE loss S22.31886524164767
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-97.97856
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1361.680027461261
Iteration: 2 || Loss: 1361.6223237688143
Iteration: 3 || Loss: 1361.5639681823977
Iteration: 4 || Loss: 1361.505724894526
Iteration: 5 || Loss: 1361.447385151742
Iteration: 6 || Loss: 1361.447385151742
saving ADAM checkpoint...
Sum of params:-97.978645
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1361.447385151742
Iteration: 2 || Loss: 1344.6411623039698
Iteration: 3 || Loss: 1322.3858495282107
Iteration: 4 || Loss: 1282.982703235952
Iteration: 5 || Loss: 1266.59954263353
Iteration: 6 || Loss: 1260.39119767452
Iteration: 7 || Loss: 1252.896899974072
Iteration: 8 || Loss: 1243.8637438704793
Iteration: 9 || Loss: 1233.2695355443418
Iteration: 10 || Loss: 1222.4986198346537
Iteration: 11 || Loss: 1214.580017418301
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.01568
Epoch 24 loss:1214.580017418301
MSE loss S87.77839853382144
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:232.58047763549249
MSE loss S - interpolation30.526186181932534
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5502.544222603018
MSE loss S - extrapolation861.7720683347054
waveform batch: 2/2
Test loss - extrapolation:2241.902602227348
MSE loss S - extrapolation93.39956682793114
Epoch 24 mean train loss:50.400956541155146
Epoch 24 mean test loss - interpolation:38.763412939248745
Epoch 24 mean test loss - extrapolation:645.3705687358638
Start training epoch 25
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.01568
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 616.3408279872143
Iteration: 2 || Loss: 616.0202481542801
Iteration: 3 || Loss: 615.6992915554356
Iteration: 4 || Loss: 615.3790013864681
Iteration: 5 || Loss: 615.0585532726257
Iteration: 6 || Loss: 615.0585532726257
saving ADAM checkpoint...
Sum of params:-98.0156
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 615.0585532726257
Iteration: 2 || Loss: 310.9279287642342
Iteration: 3 || Loss: 214.06968835083825
Iteration: 4 || Loss: 160.40068164988733
Iteration: 5 || Loss: 138.31046477197643
Iteration: 6 || Loss: 121.5583478719014
Iteration: 7 || Loss: 112.17138710251017
Iteration: 8 || Loss: 99.95568172951238
Iteration: 9 || Loss: 92.72102249708041
Iteration: 10 || Loss: 85.90040658358572
Iteration: 11 || Loss: 81.88638367513752
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-97.97155
Epoch 25 loss:81.88638367513752
MSE loss S18.51149320849281
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-97.97155
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 245.97490519465603
Iteration: 2 || Loss: 245.85263267677172
Iteration: 3 || Loss: 245.73033131798874
Iteration: 4 || Loss: 245.6082848116623
Iteration: 5 || Loss: 245.48691076247871
Iteration: 6 || Loss: 245.48691076247871
saving ADAM checkpoint...
Sum of params:-97.97162
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 245.48691076247871
Iteration: 2 || Loss: 203.51836624243873
Iteration: 3 || Loss: 194.25690964353953
Iteration: 4 || Loss: 186.9825401206763
Iteration: 5 || Loss: 182.90781359225446
Iteration: 6 || Loss: 180.17980244122862
Iteration: 7 || Loss: 177.86497689673115
Iteration: 8 || Loss: 175.71083627850672
Iteration: 9 || Loss: 173.26744416325928
Iteration: 10 || Loss: 167.45946489273607
Iteration: 11 || Loss: 161.4736123363527
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.0102
Epoch 25 loss:161.4736123363527
MSE loss S22.022983098400182
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.0102
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1345.397796762618
Iteration: 2 || Loss: 1345.3395602324163
Iteration: 3 || Loss: 1345.2812506190278
Iteration: 4 || Loss: 1345.2238077881245
Iteration: 5 || Loss: 1345.1659071772535
Iteration: 6 || Loss: 1345.1659071772535
saving ADAM checkpoint...
Sum of params:-98.01028
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1345.1659071772535
Iteration: 2 || Loss: 1328.8459690663542
Iteration: 3 || Loss: 1306.8429657220602
Iteration: 4 || Loss: 1268.1945850889888
Iteration: 5 || Loss: 1251.8187994130794
Iteration: 6 || Loss: 1245.895744862382
Iteration: 7 || Loss: 1238.4252050287216
Iteration: 8 || Loss: 1229.6602379921958
Iteration: 9 || Loss: 1219.3327019854491
Iteration: 10 || Loss: 1208.8231034008372
Iteration: 11 || Loss: 1201.1350732259323
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.04592
Epoch 25 loss:1201.1350732259323
MSE loss S86.82714475948639
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:228.84116783383098
MSE loss S - interpolation30.131659106004072
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5455.680174755436
MSE loss S - extrapolation854.7354313344995
waveform batch: 2/2
Test loss - extrapolation:2227.2488113378295
MSE loss S - extrapolation92.33586518250725
Epoch 25 mean train loss:49.81017480129043
Epoch 25 mean test loss - interpolation:38.14019463897183
Epoch 25 mean test loss - extrapolation:640.2440821744389
Start training epoch 26
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.04592
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 602.774460996827
Iteration: 2 || Loss: 602.4562734802057
Iteration: 3 || Loss: 602.1392449775007
Iteration: 4 || Loss: 601.8213476719409
Iteration: 5 || Loss: 601.5046425499901
Iteration: 6 || Loss: 601.5046425499901
saving ADAM checkpoint...
Sum of params:-98.04585
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 601.5046425499901
Iteration: 2 || Loss: 303.2637501462045
Iteration: 3 || Loss: 209.18301344023553
Iteration: 4 || Loss: 157.46215785775584
Iteration: 5 || Loss: 136.03149373516686
Iteration: 6 || Loss: 119.56263911050893
Iteration: 7 || Loss: 110.54881720846424
Iteration: 8 || Loss: 98.52335620337637
Iteration: 9 || Loss: 91.50324018678135
Iteration: 10 || Loss: 84.87700574333589
Iteration: 11 || Loss: 80.93624812887465
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.0023
Epoch 26 loss:80.93624812887465
MSE loss S18.31340808949709
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.0023
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 241.76583461626788
Iteration: 2 || Loss: 241.64451741325885
Iteration: 3 || Loss: 241.5232807800375
Iteration: 4 || Loss: 241.40337878615063
Iteration: 5 || Loss: 241.2827185111309
Iteration: 6 || Loss: 241.2827185111309
saving ADAM checkpoint...
Sum of params:-98.00237
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 241.2827185111309
Iteration: 2 || Loss: 200.07316682275658
Iteration: 3 || Loss: 190.9251113888023
Iteration: 4 || Loss: 183.81184519894452
Iteration: 5 || Loss: 179.849870824659
Iteration: 6 || Loss: 177.15298852405752
Iteration: 7 || Loss: 174.80934730248134
Iteration: 8 || Loss: 172.76036972235556
Iteration: 9 || Loss: 170.37412963905948
Iteration: 10 || Loss: 164.74246901210992
Iteration: 11 || Loss: 158.98296734875606
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.041306
Epoch 26 loss:158.98296734875606
MSE loss S21.74386451819575
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.041306
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1329.6836423801485
Iteration: 2 || Loss: 1329.6256970158056
Iteration: 3 || Loss: 1329.5680735428964
Iteration: 4 || Loss: 1329.5112024960285
Iteration: 5 || Loss: 1329.4538204776775
Iteration: 6 || Loss: 1329.4538204776775
saving ADAM checkpoint...
Sum of params:-98.0414
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1329.4538204776775
Iteration: 2 || Loss: 1313.587698417942
Iteration: 3 || Loss: 1291.7991127089954
Iteration: 4 || Loss: 1253.8639084102374
Iteration: 5 || Loss: 1237.526993490045
Iteration: 6 || Loss: 1231.8669045617928
Iteration: 7 || Loss: 1224.4182622686758
Iteration: 8 || Loss: 1215.9152918186824
Iteration: 9 || Loss: 1205.8437744311977
Iteration: 10 || Loss: 1195.579230865766
Iteration: 11 || Loss: 1188.1132293764347
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.07582
Epoch 26 loss:1188.1132293764347
MSE loss S85.91435714356956
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:225.27466498146848
MSE loss S - interpolation29.749957255761235
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5409.720355366675
MSE loss S - extrapolation847.667705512079
waveform batch: 2/2
Test loss - extrapolation:2212.8192158428806
MSE loss S - extrapolation91.30441862864025
Epoch 26 mean train loss:49.24249809841605
Epoch 26 mean test loss - interpolation:37.54577749691141
Epoch 26 mean test loss - extrapolation:635.2116309341296
Start training epoch 27
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.07582
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 589.9993866040792
Iteration: 2 || Loss: 589.6845638387193
Iteration: 3 || Loss: 589.3702746215464
Iteration: 4 || Loss: 589.055466614017
Iteration: 5 || Loss: 588.7418720606495
Iteration: 6 || Loss: 588.7418720606495
saving ADAM checkpoint...
Sum of params:-98.07574
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 588.7418720606495
Iteration: 2 || Loss: 295.9172765765296
Iteration: 3 || Loss: 204.547069546566
Iteration: 4 || Loss: 154.64335213342437
Iteration: 5 || Loss: 133.8510901526004
Iteration: 6 || Loss: 117.6568803697265
Iteration: 7 || Loss: 108.98668287381828
Iteration: 8 || Loss: 97.15501211098754
Iteration: 9 || Loss: 90.3405367754239
Iteration: 10 || Loss: 83.90642447404949
Iteration: 11 || Loss: 80.04714400994054
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.03267
Epoch 27 loss:80.04714400994054
MSE loss S18.12740632160863
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.03267
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 237.83371041122373
Iteration: 2 || Loss: 237.71417965264826
Iteration: 3 || Loss: 237.59379449954375
Iteration: 4 || Loss: 237.4750294105593
Iteration: 5 || Loss: 237.35542521115738
Iteration: 6 || Loss: 237.35542521115738
saving ADAM checkpoint...
Sum of params:-98.03274
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 237.35542521115738
Iteration: 2 || Loss: 196.83451490122923
Iteration: 3 || Loss: 187.8030960137397
Iteration: 4 || Loss: 180.834484784624
Iteration: 5 || Loss: 176.98696917459174
Iteration: 6 || Loss: 174.3138430354388
Iteration: 7 || Loss: 171.91134371448254
Iteration: 8 || Loss: 169.9903214639373
Iteration: 9 || Loss: 167.65731141648686
Iteration: 10 || Loss: 162.1921536803637
Iteration: 11 || Loss: 156.6473012008593
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.071945
Epoch 27 loss:156.6473012008593
MSE loss S21.478625365069238
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.071945
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1314.5001638979168
Iteration: 2 || Loss: 1314.4427805060527
Iteration: 3 || Loss: 1314.3853869614431
Iteration: 4 || Loss: 1314.3284094183884
Iteration: 5 || Loss: 1314.2715515263658
Iteration: 6 || Loss: 1314.2715515263658
saving ADAM checkpoint...
Sum of params:-98.072044
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1314.2715515263658
Iteration: 2 || Loss: 1298.8227745415436
Iteration: 3 || Loss: 1277.2024291108962
Iteration: 4 || Loss: 1239.925195556913
Iteration: 5 || Loss: 1223.6640226325885
Iteration: 6 || Loss: 1218.250793492358
Iteration: 7 || Loss: 1210.8240002327857
Iteration: 8 || Loss: 1202.583226748285
Iteration: 9 || Loss: 1192.7629172589423
Iteration: 10 || Loss: 1182.7334124329918
Iteration: 11 || Loss: 1175.4773136542472
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.10535
Epoch 27 loss:1175.4773136542472
MSE loss S85.03579189301249
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:221.8675178850062
MSE loss S - interpolation29.37921340004643
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5364.704302619022
MSE loss S - extrapolation840.6027981035952
waveform batch: 2/2
Test loss - extrapolation:2198.56183354086
MSE loss S - extrapolation90.29658959954313
Epoch 27 mean train loss:48.69557789189817
Epoch 27 mean test loss - interpolation:36.97791964750103
Epoch 27 mean test loss - extrapolation:630.2721780133235
Start training epoch 28
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.10535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 577.9699005629687
Iteration: 2 || Loss: 577.6579664191497
Iteration: 3 || Loss: 577.3464318830917
Iteration: 4 || Loss: 577.0348006867024
Iteration: 5 || Loss: 576.7233077288998
Iteration: 6 || Loss: 576.7233077288998
saving ADAM checkpoint...
Sum of params:-98.105255
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 576.7233077288998
Iteration: 2 || Loss: 288.8684925394205
Iteration: 3 || Loss: 200.14167366910357
Iteration: 4 || Loss: 151.93983679859022
Iteration: 5 || Loss: 131.76894906005464
Iteration: 6 || Loss: 115.83945821164454
Iteration: 7 || Loss: 107.48152324370922
Iteration: 8 || Loss: 95.85037814687053
Iteration: 9 || Loss: 89.23071314465066
Iteration: 10 || Loss: 82.9867153566167
Iteration: 11 || Loss: 79.20985070228863
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.06275
Epoch 28 loss:79.20985070228863
MSE loss S17.95124421406838
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.06275
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 234.15264533497432
Iteration: 2 || Loss: 234.0343147836142
Iteration: 3 || Loss: 233.91544108019488
Iteration: 4 || Loss: 233.79712542274459
Iteration: 5 || Loss: 233.67846271397406
Iteration: 6 || Loss: 233.67846271397406
saving ADAM checkpoint...
Sum of params:-98.06279
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 233.67846271397406
Iteration: 2 || Loss: 193.79984372528952
Iteration: 3 || Loss: 184.88472514468975
Iteration: 4 || Loss: 178.03901866127174
Iteration: 5 || Loss: 174.30435793333137
Iteration: 6 || Loss: 171.64879863639132
Iteration: 7 || Loss: 169.1562467815259
Iteration: 8 || Loss: 167.38807851577195
Iteration: 9 || Loss: 165.10557590706316
Iteration: 10 || Loss: 159.80012614602356
Iteration: 11 || Loss: 154.45651873352446
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.10219
Epoch 28 loss:154.45651873352446
MSE loss S21.22747227969605
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.10219
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1299.8054143943089
Iteration: 2 || Loss: 1299.7491081742576
Iteration: 3 || Loss: 1299.6925430080753
Iteration: 4 || Loss: 1299.6356235110568
Iteration: 5 || Loss: 1299.578462201983
Iteration: 6 || Loss: 1299.578462201983
saving ADAM checkpoint...
Sum of params:-98.10229
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1299.578462201983
Iteration: 2 || Loss: 1284.5066829057205
Iteration: 3 || Loss: 1263.0147281281386
Iteration: 4 || Loss: 1226.344305574769
Iteration: 5 || Loss: 1210.1876646891117
Iteration: 6 || Loss: 1205.0066696314368
Iteration: 7 || Loss: 1197.603438228529
Iteration: 8 || Loss: 1189.6260449797433
Iteration: 9 || Loss: 1180.053248246625
Iteration: 10 || Loss: 1170.2495844700245
Iteration: 11 || Loss: 1163.1922109681038
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.1346
Epoch 28 loss:1163.1922109681038
MSE loss S84.18953308663242
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:218.61376909759792
MSE loss S - interpolation29.020941575673795
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5320.686324394628
MSE loss S - extrapolation833.5899409080695
waveform batch: 2/2
Test loss - extrapolation:2184.4365460168992
MSE loss S - extrapolation89.31148258381033
Epoch 28 mean train loss:48.16753725530748
Epoch 28 mean test loss - interpolation:36.43562818293299
Epoch 28 mean test loss - extrapolation:625.4269058676273
Start training epoch 29
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.1346
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 566.6546340042465
Iteration: 2 || Loss: 566.3444148640153
Iteration: 3 || Loss: 566.0358890328346
Iteration: 4 || Loss: 565.7259185755895
Iteration: 5 || Loss: 565.4180302316855
Iteration: 6 || Loss: 565.4180302316855
saving ADAM checkpoint...
Sum of params:-98.13451
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 565.4180302316855
Iteration: 2 || Loss: 282.11044891220024
Iteration: 3 || Loss: 195.9549446551716
Iteration: 4 || Loss: 149.35109374053565
Iteration: 5 || Loss: 129.7799638262224
Iteration: 6 || Loss: 114.10675341397067
Iteration: 7 || Loss: 106.03373916135045
Iteration: 8 || Loss: 94.60528906085376
Iteration: 9 || Loss: 88.16993262376539
Iteration: 10 || Loss: 82.11312759189563
Iteration: 11 || Loss: 78.42012259109053
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.09253
Epoch 29 loss:78.42012259109053
MSE loss S17.78416790043473
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.09253
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 230.67146642314492
Iteration: 2 || Loss: 230.5536272218903
Iteration: 3 || Loss: 230.43569851703273
Iteration: 4 || Loss: 230.31898881948467
Iteration: 5 || Loss: 230.2017275579056
Iteration: 6 || Loss: 230.2017275579056
saving ADAM checkpoint...
Sum of params:-98.092606
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 230.2017275579056
Iteration: 2 || Loss: 190.9402101598205
Iteration: 3 || Loss: 182.14577851574538
Iteration: 4 || Loss: 175.41620100987885
Iteration: 5 || Loss: 171.7874987467473
Iteration: 6 || Loss: 169.14642819009822
Iteration: 7 || Loss: 166.55423869801652
Iteration: 8 || Loss: 164.94104833011284
Iteration: 9 || Loss: 162.70712241397163
Iteration: 10 || Loss: 157.5546159090102
Iteration: 11 || Loss: 152.40020900830118
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.1321
Epoch 29 loss:152.40020900830118
MSE loss S20.98948816096348
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.1321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1285.5520386166118
Iteration: 2 || Loss: 1285.4955526635704
Iteration: 3 || Loss: 1285.438705304011
Iteration: 4 || Loss: 1285.3818098097681
Iteration: 5 || Loss: 1285.3258557963545
Iteration: 6 || Loss: 1285.3258557963545
saving ADAM checkpoint...
Sum of params:-98.132195
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1285.3258557963545
Iteration: 2 || Loss: 1270.598370669347
Iteration: 3 || Loss: 1249.1944332687308
Iteration: 4 || Loss: 1213.0907910063106
Iteration: 5 || Loss: 1197.0581151274605
Iteration: 6 || Loss: 1192.0945277012597
Iteration: 7 || Loss: 1184.7233309085584
Iteration: 8 || Loss: 1177.009044380444
Iteration: 9 || Loss: 1167.6818984274914
Iteration: 10 || Loss: 1158.097548490688
Iteration: 11 || Loss: 1151.2261151993632
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.16362
Epoch 29 loss:1151.2261151993632
MSE loss S83.37190376284755
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:215.5005365999289
MSE loss S - interpolation28.674169098993012
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5277.62280476203
MSE loss S - extrapolation826.641500686884
waveform batch: 2/2
Test loss - extrapolation:2170.4147793818624
MSE loss S - extrapolation88.34418387452497
Epoch 29 mean train loss:47.65677402754327
Epoch 29 mean test loss - interpolation:35.91675609998815
Epoch 29 mean test loss - extrapolation:620.6697986786577
Start training epoch 30
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.16362
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 555.9962395139372
Iteration: 2 || Loss: 555.6896074482352
Iteration: 3 || Loss: 555.3829422359548
Iteration: 4 || Loss: 555.0760588988969
Iteration: 5 || Loss: 554.7699623155531
Iteration: 6 || Loss: 554.7699623155531
saving ADAM checkpoint...
Sum of params:-98.16353
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 554.7699623155531
Iteration: 2 || Loss: 275.63720372912036
Iteration: 3 || Loss: 191.9690838321684
Iteration: 4 || Loss: 146.869872331307
Iteration: 5 || Loss: 127.87804785453444
Iteration: 6 || Loss: 112.45229648103741
Iteration: 7 || Loss: 104.63956416712399
Iteration: 8 || Loss: 93.41965774830572
Iteration: 9 || Loss: 87.15500693161327
Iteration: 10 || Loss: 81.28289219681159
Iteration: 11 || Loss: 77.67140127079415
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.12208
Epoch 30 loss:77.67140127079415
MSE loss S17.626362340209795
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.12208
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 227.401866182357
Iteration: 2 || Loss: 227.28556105093386
Iteration: 3 || Loss: 227.16910249306275
Iteration: 4 || Loss: 227.05275867763308
Iteration: 5 || Loss: 226.9363066162048
Iteration: 6 || Loss: 226.9363066162048
saving ADAM checkpoint...
Sum of params:-98.12216
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 226.9363066162048
Iteration: 2 || Loss: 188.23824686585445
Iteration: 3 || Loss: 179.56671887441047
Iteration: 4 || Loss: 172.9418378321432
Iteration: 5 || Loss: 169.41937687323306
Iteration: 6 || Loss: 166.79126309364474
Iteration: 7 || Loss: 164.05023371120092
Iteration: 8 || Loss: 162.6345930440759
Iteration: 9 || Loss: 160.44845356965658
Iteration: 10 || Loss: 155.44533406562485
Iteration: 11 || Loss: 150.46157414871968
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.16174
Epoch 30 loss:150.46157414871968
MSE loss S20.763176073532176
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.16174
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1271.732281865042
Iteration: 2 || Loss: 1271.6755748980856
Iteration: 3 || Loss: 1271.6198956789576
Iteration: 4 || Loss: 1271.5639710717303
Iteration: 5 || Loss: 1271.5076308995883
Iteration: 6 || Loss: 1271.5076308995883
saving ADAM checkpoint...
Sum of params:-98.16182
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1271.5076308995883
Iteration: 2 || Loss: 1257.0913534245076
Iteration: 3 || Loss: 1235.7517016044312
Iteration: 4 || Loss: 1200.1727310815108
Iteration: 5 || Loss: 1184.2758074630528
Iteration: 6 || Loss: 1179.5139116479716
Iteration: 7 || Loss: 1172.175093325232
Iteration: 8 || Loss: 1164.7190142197628
Iteration: 9 || Loss: 1155.6292972279414
Iteration: 10 || Loss: 1146.2572343342204
Iteration: 11 || Loss: 1139.5597454364654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.19242
Epoch 30 loss:1139.5597454364654
MSE loss S82.58129256663744
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:212.51440240913243
MSE loss S - interpolation28.339363751080214
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5235.445523338451
MSE loss S - extrapolation819.7692629272192
waveform batch: 2/2
Test loss - extrapolation:2156.5180706704414
MSE loss S - extrapolation87.39868686125104
Epoch 30 mean train loss:47.161817960551005
Epoch 30 mean test loss - interpolation:35.41906706818874
Epoch 30 mean test loss - extrapolation:615.9969661674077
Start training epoch 31
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.19242
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 545.9234473162908
Iteration: 2 || Loss: 545.6180009918174
Iteration: 3 || Loss: 545.3146632192621
Iteration: 4 || Loss: 545.0094609037145
Iteration: 5 || Loss: 544.7060970677629
Iteration: 6 || Loss: 544.7060970677629
saving ADAM checkpoint...
Sum of params:-98.192345
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 544.7060970677629
Iteration: 2 || Loss: 269.4280289169983
Iteration: 3 || Loss: 188.15912749503195
Iteration: 4 || Loss: 144.47821747982044
Iteration: 5 || Loss: 126.04526932652722
Iteration: 6 || Loss: 110.86093174961049
Iteration: 7 || Loss: 103.28663521836165
Iteration: 8 || Loss: 92.28142331328182
Iteration: 9 || Loss: 86.17626540872678
Iteration: 10 || Loss: 80.48737138186715
Iteration: 11 || Loss: 76.95568474854731
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.151474
Epoch 31 loss:76.95568474854731
MSE loss S17.474914051664612
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.151474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 224.2927186100167
Iteration: 2 || Loss: 224.176616388221
Iteration: 3 || Loss: 224.06075086878033
Iteration: 4 || Loss: 223.94572401382842
Iteration: 5 || Loss: 223.8309687718009
Iteration: 6 || Loss: 223.8309687718009
saving ADAM checkpoint...
Sum of params:-98.15155
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 223.8309687718009
Iteration: 2 || Loss: 185.68411413812152
Iteration: 3 || Loss: 177.1367132330539
Iteration: 4 || Loss: 170.60744263112284
Iteration: 5 || Loss: 167.18550573244022
Iteration: 6 || Loss: 164.56744770475913
Iteration: 7 || Loss: 161.69576915150188
Iteration: 8 || Loss: 160.45232738573588
Iteration: 9 || Loss: 158.31534797131616
Iteration: 10 || Loss: 153.46382460127327
Iteration: 11 || Loss: 148.6361876616579
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.19113
Epoch 31 loss:148.6361876616579
MSE loss S20.54884808009858
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.19113
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1258.2895066566257
Iteration: 2 || Loss: 1258.2329899906422
Iteration: 3 || Loss: 1258.1770843481838
Iteration: 4 || Loss: 1258.1211274388588
Iteration: 5 || Loss: 1258.0649119686232
Iteration: 6 || Loss: 1258.0649119686232
saving ADAM checkpoint...
Sum of params:-98.19122
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1258.0649119686232
Iteration: 2 || Loss: 1243.9223374299554
Iteration: 3 || Loss: 1222.6135388617606
Iteration: 4 || Loss: 1187.523081632441
Iteration: 5 || Loss: 1171.7715659670891
Iteration: 6 || Loss: 1167.2024037677693
Iteration: 7 || Loss: 1159.904818718998
Iteration: 8 || Loss: 1152.710174161189
Iteration: 9 || Loss: 1143.8610693083858
Iteration: 10 || Loss: 1134.6966629465198
Iteration: 11 || Loss: 1128.1586075913985
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.22109
Epoch 31 loss:1128.1586075913985
MSE loss S81.81578288631805
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:209.66111294442928
MSE loss S - interpolation28.017600654522475
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5194.244985924798
MSE loss S - extrapolation813.0166497008676
waveform batch: 2/2
Test loss - extrapolation:2142.658731995076
MSE loss S - extrapolation86.46594344205792
Epoch 31 mean train loss:46.681051034538065
Epoch 31 mean test loss - interpolation:34.943518824071546
Epoch 31 mean test loss - extrapolation:611.4086431599895
Start training epoch 32
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.22109
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 536.4547096021495
Iteration: 2 || Loss: 536.1530305725515
Iteration: 3 || Loss: 535.8504449808473
Iteration: 4 || Loss: 535.5486457117632
Iteration: 5 || Loss: 535.2460812501149
Iteration: 6 || Loss: 535.2460812501149
saving ADAM checkpoint...
Sum of params:-98.22099
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 535.2460812501149
Iteration: 2 || Loss: 263.50836663082686
Iteration: 3 || Loss: 184.5387694973299
Iteration: 4 || Loss: 142.19351094339456
Iteration: 5 || Loss: 124.29811695572957
Iteration: 6 || Loss: 109.34446002921517
Iteration: 7 || Loss: 101.98525944919714
Iteration: 8 || Loss: 91.19992446781497
Iteration: 9 || Loss: 85.24044514190848
Iteration: 10 || Loss: 79.73151143185781
Iteration: 11 || Loss: 76.27708278333284
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.18069
Epoch 32 loss:76.27708278333284
MSE loss S17.33275095563232
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.18069
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 221.39324271280535
Iteration: 2 || Loss: 221.27919928819492
Iteration: 3 || Loss: 221.16428041512899
Iteration: 4 || Loss: 221.049807671696
Iteration: 5 || Loss: 220.93510320309292
Iteration: 6 || Loss: 220.93510320309292
saving ADAM checkpoint...
Sum of params:-98.180756
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 220.93510320309292
Iteration: 2 || Loss: 183.27411626857514
Iteration: 3 || Loss: 174.85028338255518
Iteration: 4 || Loss: 168.4087877405352
Iteration: 5 || Loss: 165.0821874795198
Iteration: 6 || Loss: 162.4738923367292
Iteration: 7 || Loss: 159.44989064314828
Iteration: 8 || Loss: 158.39050368260976
Iteration: 9 || Loss: 156.30622659951533
Iteration: 10 || Loss: 151.606095723689
Iteration: 11 || Loss: 146.90958914264863
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.22031
Epoch 32 loss:146.90958914264863
MSE loss S20.344474329392156
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.22031
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1245.1908881934723
Iteration: 2 || Loss: 1245.1346671950769
Iteration: 3 || Loss: 1245.078255196759
Iteration: 4 || Loss: 1245.0225742670548
Iteration: 5 || Loss: 1244.9671872013676
Iteration: 6 || Loss: 1244.9671872013676
saving ADAM checkpoint...
Sum of params:-98.22041
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1244.9671872013676
Iteration: 2 || Loss: 1231.0765698309585
Iteration: 3 || Loss: 1209.7998228107047
Iteration: 4 || Loss: 1175.1729225908346
Iteration: 5 || Loss: 1159.5660684725178
Iteration: 6 || Loss: 1155.1729160934608
Iteration: 7 || Loss: 1147.9198108520575
Iteration: 8 || Loss: 1140.9792271773715
Iteration: 9 || Loss: 1132.3631202937886
Iteration: 10 || Loss: 1123.4007875979728
Iteration: 11 || Loss: 1117.0094169208442
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.24958
Epoch 32 loss:1117.0094169208442
MSE loss S81.07475232502429
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:206.9270734532262
MSE loss S - interpolation27.709682974481794
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5153.990377863011
MSE loss S - extrapolation806.4087938258754
waveform batch: 2/2
Test loss - extrapolation:2128.8850894148063
MSE loss S - extrapolation85.55656551259739
Epoch 32 mean train loss:46.21365823609744
Epoch 32 mean test loss - interpolation:34.4878455755377
Epoch 32 mean test loss - extrapolation:606.9062889398182
Start training epoch 33
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.24958
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 527.5528666418444
Iteration: 2 || Loss: 527.2517168962045
Iteration: 3 || Loss: 526.952249859656
Iteration: 4 || Loss: 526.6514832399998
Iteration: 5 || Loss: 526.3519438539234
Iteration: 6 || Loss: 526.3519438539234
saving ADAM checkpoint...
Sum of params:-98.24949
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 526.3519438539234
Iteration: 2 || Loss: 257.8408271423275
Iteration: 3 || Loss: 181.07251941600848
Iteration: 4 || Loss: 139.98919497875468
Iteration: 5 || Loss: 122.60994915454378
Iteration: 6 || Loss: 107.88059688214175
Iteration: 7 || Loss: 100.72001168542084
Iteration: 8 || Loss: 90.16008035277754
Iteration: 9 || Loss: 84.33426851377786
Iteration: 10 || Loss: 79.00345619758399
Iteration: 11 || Loss: 75.62440387567061
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.20978
Epoch 33 loss:75.62440387567061
MSE loss S17.19541845056284
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.20978
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 218.61164937526434
Iteration: 2 || Loss: 218.49731289802224
Iteration: 3 || Loss: 218.38373252361632
Iteration: 4 || Loss: 218.27067608332337
Iteration: 5 || Loss: 218.15717482984962
Iteration: 6 || Loss: 218.15717482984962
saving ADAM checkpoint...
Sum of params:-98.20991
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 218.15717482984962
Iteration: 2 || Loss: 180.97351443169035
Iteration: 3 || Loss: 172.67920702709378
Iteration: 4 || Loss: 166.32191405532046
Iteration: 5 || Loss: 163.08643695769575
Iteration: 6 || Loss: 160.48867108965499
Iteration: 7 || Loss: 157.3437657627202
Iteration: 8 || Loss: 156.42761336476315
Iteration: 9 || Loss: 154.40248281224376
Iteration: 10 || Loss: 149.85235088613058
Iteration: 11 || Loss: 145.26952639229378
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.24946
Epoch 33 loss:145.26952639229378
MSE loss S20.149865272730075
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.24946
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1232.4064651091146
Iteration: 2 || Loss: 1232.3504583354315
Iteration: 3 || Loss: 1232.2946218981306
Iteration: 4 || Loss: 1232.2395682873182
Iteration: 5 || Loss: 1232.1837416748106
Iteration: 6 || Loss: 1232.1837416748106
saving ADAM checkpoint...
Sum of params:-98.24952
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1232.1837416748106
Iteration: 2 || Loss: 1218.5241386779815
Iteration: 3 || Loss: 1197.2715091028872
Iteration: 4 || Loss: 1163.0953727883793
Iteration: 5 || Loss: 1147.6274218220549
Iteration: 6 || Loss: 1143.399058649103
Iteration: 7 || Loss: 1136.195899986149
Iteration: 8 || Loss: 1129.5044995669793
Iteration: 9 || Loss: 1121.1185914780501
Iteration: 10 || Loss: 1112.3516791323218
Iteration: 11 || Loss: 1106.0892358912847
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.278015
Epoch 33 loss:1106.0892358912847
MSE loss S80.35565664032224
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:204.3070183926102
MSE loss S - interpolation27.414430366225478
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5114.58567224502
MSE loss S - extrapolation799.929713376951
waveform batch: 2/2
Test loss - extrapolation:2115.154703450379
MSE loss S - extrapolation84.66323320980638
Epoch 33 mean train loss:45.7580402123879
Epoch 33 mean test loss - interpolation:34.0511697321017
Epoch 33 mean test loss - extrapolation:602.4783646412833
Start training epoch 34
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.278015
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 519.1692410162987
Iteration: 2 || Loss: 518.8697229851352
Iteration: 3 || Loss: 518.5718258128505
Iteration: 4 || Loss: 518.2729716719458
Iteration: 5 || Loss: 517.975433916956
Iteration: 6 || Loss: 517.975433916956
saving ADAM checkpoint...
Sum of params:-98.27794
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 517.975433916956
Iteration: 2 || Loss: 252.43139405130376
Iteration: 3 || Loss: 177.76430279718807
Iteration: 4 || Loss: 137.86951441527918
Iteration: 5 || Loss: 120.98559184748976
Iteration: 6 || Loss: 106.47358822805779
Iteration: 7 || Loss: 99.49538489119898
Iteration: 8 || Loss: 89.16920532523774
Iteration: 9 || Loss: 83.46205509998859
Iteration: 10 || Loss: 78.30561825320036
Iteration: 11 || Loss: 74.99769055358469
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.23884
Epoch 34 loss:74.99769055358469
MSE loss S17.06439592775261
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.23884
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 215.98258714018283
Iteration: 2 || Loss: 215.86984393036616
Iteration: 3 || Loss: 215.75685295741732
Iteration: 4 || Loss: 215.6436498494335
Iteration: 5 || Loss: 215.5312313857526
Iteration: 6 || Loss: 215.5312313857526
saving ADAM checkpoint...
Sum of params:-98.23899
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 215.5312313857526
Iteration: 2 || Loss: 178.79462141793974
Iteration: 3 || Loss: 170.62543787040903
Iteration: 4 || Loss: 164.34183340088717
Iteration: 5 || Loss: 161.19520915568984
Iteration: 6 || Loss: 158.60738255615328
Iteration: 7 || Loss: 155.34467650984644
Iteration: 8 || Loss: 154.54642131512486
Iteration: 9 || Loss: 152.59749059278658
Iteration: 10 || Loss: 148.19474122074516
Iteration: 11 || Loss: 143.70418604718455
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.27842
Epoch 34 loss:143.70418604718455
MSE loss S19.96246421975162
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.27842
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1219.9062281525682
Iteration: 2 || Loss: 1219.851135897681
Iteration: 3 || Loss: 1219.7958470789026
Iteration: 4 || Loss: 1219.7398222976467
Iteration: 5 || Loss: 1219.6842743244765
Iteration: 6 || Loss: 1219.6842743244765
saving ADAM checkpoint...
Sum of params:-98.2785
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1219.6842743244765
Iteration: 2 || Loss: 1206.2436418285504
Iteration: 3 || Loss: 1185.0286714407173
Iteration: 4 || Loss: 1151.2886111757998
Iteration: 5 || Loss: 1135.9556370767875
Iteration: 6 || Loss: 1131.8803038282435
Iteration: 7 || Loss: 1124.7259261766862
Iteration: 8 || Loss: 1118.2747572482842
Iteration: 9 || Loss: 1110.1130300723266
Iteration: 10 || Loss: 1101.5343408283059
Iteration: 11 || Loss: 1095.383915465466
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.306366
Epoch 34 loss:1095.383915465466
MSE loss S79.65759648031101
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:201.79324443321093
MSE loss S - interpolation27.131753350147513
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5076.011659351761
MSE loss S - extrapolation793.5947877742788
waveform batch: 2/2
Test loss - extrapolation:2101.479224286928
MSE loss S - extrapolation83.79112931166952
Epoch 34 mean train loss:45.31330317469777
Epoch 34 mean test loss - interpolation:33.632207405535155
Epoch 34 mean test loss - extrapolation:598.1242403032242
Start training epoch 35
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.306366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 511.2931018440068
Iteration: 2 || Loss: 510.99542494267683
Iteration: 3 || Loss: 510.699412787098
Iteration: 4 || Loss: 510.4019580244901
Iteration: 5 || Loss: 510.1063291961664
Iteration: 6 || Loss: 510.1063291961664
saving ADAM checkpoint...
Sum of params:-98.3063
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 510.1063291961664
Iteration: 2 || Loss: 247.263357440982
Iteration: 3 || Loss: 174.60148342887396
Iteration: 4 || Loss: 135.82573358888027
Iteration: 5 || Loss: 119.41578001643349
Iteration: 6 || Loss: 105.11294547641047
Iteration: 7 || Loss: 98.30476549590094
Iteration: 8 || Loss: 88.21849715507373
Iteration: 9 || Loss: 82.61643244245177
Iteration: 10 || Loss: 77.63187453052456
Iteration: 11 || Loss: 74.39322885139084
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.267784
Epoch 35 loss:74.39322885139084
MSE loss S16.937719101295375
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.267784
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 213.45276525266374
Iteration: 2 || Loss: 213.34069078139837
Iteration: 3 || Loss: 213.2286944125674
Iteration: 4 || Loss: 213.11632136101832
Iteration: 5 || Loss: 213.0046948211907
Iteration: 6 || Loss: 213.0046948211907
saving ADAM checkpoint...
Sum of params:-98.267944
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 213.0046948211907
Iteration: 2 || Loss: 176.71462473091518
Iteration: 3 || Loss: 168.67400111083742
Iteration: 4 || Loss: 162.46168316399638
Iteration: 5 || Loss: 159.3970181587741
Iteration: 6 || Loss: 156.8168424780079
Iteration: 7 || Loss: 153.4694302994032
Iteration: 8 || Loss: 152.7191588931628
Iteration: 9 || Loss: 150.8805294105103
Iteration: 10 || Loss: 146.62220003180232
Iteration: 11 || Loss: 142.20928285506696
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.30729
Epoch 35 loss:142.20928285506696
MSE loss S19.783086783274115
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.30729
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1207.6408112503634
Iteration: 2 || Loss: 1207.5852432389527
Iteration: 3 || Loss: 1207.529549633759
Iteration: 4 || Loss: 1207.4738287246516
Iteration: 5 || Loss: 1207.418369390273
Iteration: 6 || Loss: 1207.418369390273
saving ADAM checkpoint...
Sum of params:-98.3073
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1207.418369390273
Iteration: 2 || Loss: 1194.184447737377
Iteration: 3 || Loss: 1173.0152837059038
Iteration: 4 || Loss: 1139.7069187961943
Iteration: 5 || Loss: 1124.5057549751466
Iteration: 6 || Loss: 1120.575065039479
Iteration: 7 || Loss: 1113.4766791630595
Iteration: 8 || Loss: 1107.2609438557868
Iteration: 9 || Loss: 1099.3236362805803
Iteration: 10 || Loss: 1090.9274939575885
Iteration: 11 || Loss: 1084.8688772068758
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.33455
Epoch 35 loss:1084.8688772068758
MSE loss S78.97939971519415
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:199.3810714596768
MSE loss S - interpolation26.86062478849645
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5038.2590969620305
MSE loss S - extrapolation787.4168902141064
waveform batch: 2/2
Test loss - extrapolation:2087.8010085376873
MSE loss S - extrapolation82.9346486843348
Epoch 35 mean train loss:44.878323755632195
Epoch 35 mean test loss - interpolation:33.2301785766128
Epoch 35 mean test loss - extrapolation:593.8383421249765
Start training epoch 36
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.33455
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 503.91560096044975
Iteration: 2 || Loss: 503.6193872234376
Iteration: 3 || Loss: 503.32531178521106
Iteration: 4 || Loss: 503.02949293527485
Iteration: 5 || Loss: 502.73607037921977
Iteration: 6 || Loss: 502.73607037921977
saving ADAM checkpoint...
Sum of params:-98.33448
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 502.73607037921977
Iteration: 2 || Loss: 242.33025114470937
Iteration: 3 || Loss: 171.58207068751307
Iteration: 4 || Loss: 133.85865204706704
Iteration: 5 || Loss: 117.90542437313724
Iteration: 6 || Loss: 103.80233687035599
Iteration: 7 || Loss: 97.14748430086021
Iteration: 8 || Loss: 87.31289873126192
Iteration: 9 || Loss: 81.80057004120184
Iteration: 10 || Loss: 76.98517666111685
Iteration: 11 || Loss: 73.81219799204364
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.29661
Epoch 36 loss:73.81219799204364
MSE loss S16.81655356288272
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.29661
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 211.07591567688883
Iteration: 2 || Loss: 210.96421991082795
Iteration: 3 || Loss: 210.85251843972935
Iteration: 4 || Loss: 210.74105791602065
Iteration: 5 || Loss: 210.63074972666368
Iteration: 6 || Loss: 210.63074972666368
saving ADAM checkpoint...
Sum of params:-98.29676
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 210.63074972666368
Iteration: 2 || Loss: 174.73962748803794
Iteration: 3 || Loss: 166.81979518778994
Iteration: 4 || Loss: 160.67395026990522
Iteration: 5 || Loss: 157.68621073143544
Iteration: 6 || Loss: 155.11549143366793
Iteration: 7 || Loss: 151.71284425312433
Iteration: 8 || Loss: 150.89005591762506
Iteration: 9 || Loss: 149.24553904232747
Iteration: 10 || Loss: 145.12811569569715
Iteration: 11 || Loss: 140.77541547831234
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.33604
Epoch 36 loss:140.77541547831234
MSE loss S19.609762446207828
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.33604
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1195.5949363107247
Iteration: 2 || Loss: 1195.539867874589
Iteration: 3 || Loss: 1195.484270200058
Iteration: 4 || Loss: 1195.4289822432704
Iteration: 5 || Loss: 1195.3734983083168
Iteration: 6 || Loss: 1195.3734983083168
saving ADAM checkpoint...
Sum of params:-98.336044
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1195.3734983083168
Iteration: 2 || Loss: 1182.344874315307
Iteration: 3 || Loss: 1161.238184109358
Iteration: 4 || Loss: 1128.3613684447741
Iteration: 5 || Loss: 1113.2865225882076
Iteration: 6 || Loss: 1109.4890005038135
Iteration: 7 || Loss: 1102.4461290284266
Iteration: 8 || Loss: 1096.454457532687
Iteration: 9 || Loss: 1088.7361109161718
Iteration: 10 || Loss: 1080.5184155903912
Iteration: 11 || Loss: 1074.5337960852842
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.36267
Epoch 36 loss:1074.5337960852842
MSE loss S78.31767618407443
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:197.04635849098395
MSE loss S - interpolation26.59744475382218
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:5001.154028317469
MSE loss S - extrapolation781.3610749310552
waveform batch: 2/2
Test loss - extrapolation:2074.1618649230727
MSE loss S - extrapolation82.09631633593858
Epoch 36 mean train loss:44.45246239847035
Epoch 36 mean test loss - interpolation:32.84105974849732
Epoch 36 mean test loss - extrapolation:589.6096577700451
Start training epoch 37
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.36267
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 496.9435701618091
Iteration: 2 || Loss: 496.6499045634033
Iteration: 3 || Loss: 496.35676486321586
Iteration: 4 || Loss: 496.0635506575593
Iteration: 5 || Loss: 495.7707146453148
Iteration: 6 || Loss: 495.7707146453148
saving ADAM checkpoint...
Sum of params:-98.36259
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 495.7707146453148
Iteration: 2 || Loss: 237.59675261579576
Iteration: 3 || Loss: 168.68290916575285
Iteration: 4 || Loss: 131.95372354554544
Iteration: 5 || Loss: 116.44277186891017
Iteration: 6 || Loss: 102.52961958882501
Iteration: 7 || Loss: 96.01600330304393
Iteration: 8 || Loss: 86.44199662111635
Iteration: 9 || Loss: 81.00360728261602
Iteration: 10 || Loss: 76.35779648920506
Iteration: 11 || Loss: 73.24724708265116
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.32539
Epoch 37 loss:73.24724708265116
MSE loss S16.69839382548604
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.32539
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 208.780869844116
Iteration: 2 || Loss: 208.67067539490282
Iteration: 3 || Loss: 208.55991581497722
Iteration: 4 || Loss: 208.44934087017924
Iteration: 5 || Loss: 208.33886161614723
Iteration: 6 || Loss: 208.33886161614723
saving ADAM checkpoint...
Sum of params:-98.32555
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 208.33886161614723
Iteration: 2 || Loss: 172.84627887441465
Iteration: 3 || Loss: 165.0485328896744
Iteration: 4 || Loss: 158.9619530178887
Iteration: 5 || Loss: 156.04894451334525
Iteration: 6 || Loss: 153.48687774650503
Iteration: 7 || Loss: 150.0620902068854
Iteration: 8 || Loss: 148.91794054122158
Iteration: 9 || Loss: 147.6627071328022
Iteration: 10 || Loss: 143.69830917020474
Iteration: 11 || Loss: 139.39618732907184
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.364716
Epoch 37 loss:139.39618732907184
MSE loss S19.442365055189857
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.364716
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1183.7592102316826
Iteration: 2 || Loss: 1183.704134639937
Iteration: 3 || Loss: 1183.6493945915745
Iteration: 4 || Loss: 1183.5944029094567
Iteration: 5 || Loss: 1183.5389631524101
Iteration: 6 || Loss: 1183.5389631524101
saving ADAM checkpoint...
Sum of params:-98.36473
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1183.5389631524101
Iteration: 2 || Loss: 1170.7100301455312
Iteration: 3 || Loss: 1149.676091210089
Iteration: 4 || Loss: 1117.224515475535
Iteration: 5 || Loss: 1102.271715434534
Iteration: 6 || Loss: 1098.599830279562
Iteration: 7 || Loss: 1091.6154847637563
Iteration: 8 || Loss: 1085.840040223811
Iteration: 9 || Loss: 1078.336093632604
Iteration: 10 || Loss: 1070.2922165736438
Iteration: 11 || Loss: 1064.3637205914883
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.39072
Epoch 37 loss:1064.3637205914883
MSE loss S77.67300779928806
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:194.79011847612495
MSE loss S - interpolation26.34222034593752
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4964.756612728482
MSE loss S - extrapolation775.4564124698104
waveform batch: 2/2
Test loss - extrapolation:2060.521576835243
MSE loss S - extrapolation81.27520187216982
Epoch 37 mean train loss:44.03472948286935
Epoch 37 mean test loss - interpolation:32.46501974602082
Epoch 37 mean test loss - extrapolation:585.4398491303104
Start training epoch 38
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.39072
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 490.4128626974494
Iteration: 2 || Loss: 490.12116507681816
Iteration: 3 || Loss: 489.8292658038039
Iteration: 4 || Loss: 489.53769629112094
Iteration: 5 || Loss: 489.2459302011489
Iteration: 6 || Loss: 489.2459302011489
saving ADAM checkpoint...
Sum of params:-98.390625
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 489.2459302011489
Iteration: 2 || Loss: 233.05773081852433
Iteration: 3 || Loss: 165.9023762797026
Iteration: 4 || Loss: 130.10883434028054
Iteration: 5 || Loss: 115.02947421262189
Iteration: 6 || Loss: 101.29529453244484
Iteration: 7 || Loss: 94.90602461376501
Iteration: 8 || Loss: 85.60920812607105
Iteration: 9 || Loss: 80.22870891948372
Iteration: 10 || Loss: 75.7520958721814
Iteration: 11 || Loss: 72.70016657079965
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.354126
Epoch 38 loss:72.70016657079965
MSE loss S16.58423445227664
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.354126
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 206.59243732687193
Iteration: 2 || Loss: 206.48270242649426
Iteration: 3 || Loss: 206.37250218753502
Iteration: 4 || Loss: 206.26266918149895
Iteration: 5 || Loss: 206.15337682371776
Iteration: 6 || Loss: 206.15337682371776
saving ADAM checkpoint...
Sum of params:-98.35427
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 206.15337682371776
Iteration: 2 || Loss: 171.03735467301692
Iteration: 3 || Loss: 163.35662801461913
Iteration: 4 || Loss: 157.32431187889156
Iteration: 5 || Loss: 154.48155347536996
Iteration: 6 || Loss: 151.92843747110328
Iteration: 7 || Loss: 148.50976863438598
Iteration: 8 || Loss: 146.79889542846448
Iteration: 9 || Loss: 145.67164801416052
Iteration: 10 || Loss: 142.3143484599154
Iteration: 11 || Loss: 138.06571532806785
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.39337
Epoch 38 loss:138.06571532806785
MSE loss S19.28153058215477
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.39337
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1172.1291197701323
Iteration: 2 || Loss: 1172.075022185974
Iteration: 3 || Loss: 1172.020110130783
Iteration: 4 || Loss: 1171.9654335764808
Iteration: 5 || Loss: 1171.9104396999826
Iteration: 6 || Loss: 1171.9104396999826
saving ADAM checkpoint...
Sum of params:-98.39337
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1171.9104396999826
Iteration: 2 || Loss: 1159.2825501673306
Iteration: 3 || Loss: 1138.3349059806644
Iteration: 4 || Loss: 1106.2900308279918
Iteration: 5 || Loss: 1091.4687117756168
Iteration: 6 || Loss: 1087.9131909825285
Iteration: 7 || Loss: 1080.984485971995
Iteration: 8 || Loss: 1075.4138845153332
Iteration: 9 || Loss: 1068.1172936948597
Iteration: 10 || Loss: 1060.239723213503
Iteration: 11 || Loss: 1054.3535372401336
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.41865
Epoch 38 loss:1054.3535372401336
MSE loss S77.04246499169822
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:192.58574584295633
MSE loss S - interpolation26.09101623724041
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4928.809254547849
MSE loss S - extrapolation769.6367303215659
waveform batch: 2/2
Test loss - extrapolation:2046.9387874975528
MSE loss S - extrapolation80.47292612445484
Epoch 38 mean train loss:43.62480755651728
Epoch 38 mean test loss - interpolation:32.097624307159386
Epoch 38 mean test loss - extrapolation:581.3123368371168
Start training epoch 39
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.41865
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 484.1961765774729
Iteration: 2 || Loss: 483.90482250249363
Iteration: 3 || Loss: 483.61474493593767
Iteration: 4 || Loss: 483.324090230677
Iteration: 5 || Loss: 483.03477673192515
Iteration: 6 || Loss: 483.03477673192515
saving ADAM checkpoint...
Sum of params:-98.418564
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 483.03477673192515
Iteration: 2 || Loss: 228.67689181408917
Iteration: 3 || Loss: 163.21562543144205
Iteration: 4 || Loss: 128.30798952141896
Iteration: 5 || Loss: 113.64973988158592
Iteration: 6 || Loss: 100.08625335444532
Iteration: 7 || Loss: 93.8055224666725
Iteration: 8 || Loss: 84.80432540191998
Iteration: 9 || Loss: 79.4641086739435
Iteration: 10 || Loss: 75.16077656378783
Iteration: 11 || Loss: 72.1628869262923
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.38286
Epoch 39 loss:72.1628869262923
MSE loss S16.47181533204461
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.38286
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 204.4790975802832
Iteration: 2 || Loss: 204.3698110401173
Iteration: 3 || Loss: 204.26071770030228
Iteration: 4 || Loss: 204.15205084541182
Iteration: 5 || Loss: 204.0429451628303
Iteration: 6 || Loss: 204.0429451628303
saving ADAM checkpoint...
Sum of params:-98.38302
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 204.0429451628303
Iteration: 2 || Loss: 169.29796026479949
Iteration: 3 || Loss: 161.72691607330088
Iteration: 4 || Loss: 155.74485587454453
Iteration: 5 || Loss: 152.96746827286796
Iteration: 6 || Loss: 150.42268771123997
Iteration: 7 || Loss: 147.03589001248682
Iteration: 8 || Loss: 145.50452454744723
Iteration: 9 || Loss: 144.5953331695392
Iteration: 10 || Loss: 140.98850658629118
Iteration: 11 || Loss: 136.7719367327738
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.42195
Epoch 39 loss:136.7719367327738
MSE loss S19.117665635319643
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.42195
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1160.5809963659851
Iteration: 2 || Loss: 1160.5262310034095
Iteration: 3 || Loss: 1160.4721866776251
Iteration: 4 || Loss: 1160.417972651398
Iteration: 5 || Loss: 1160.3636987602551
Iteration: 6 || Loss: 1160.3636987602551
saving ADAM checkpoint...
Sum of params:-98.42195
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1160.3636987602551
Iteration: 2 || Loss: 1147.9224407595
Iteration: 3 || Loss: 1127.0534928760305
Iteration: 4 || Loss: 1095.4853341404291
Iteration: 5 || Loss: 1080.7572608434998
Iteration: 6 || Loss: 1077.3178950731062
Iteration: 7 || Loss: 1070.4673122670647
Iteration: 8 || Loss: 1065.1013987151155
Iteration: 9 || Loss: 1058.0292157387432
Iteration: 10 || Loss: 1050.3286813358488
Iteration: 11 || Loss: 1044.470909201265
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.446594
Epoch 39 loss:1044.470909201265
MSE loss S76.42277042325404
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:190.46363945666718
MSE loss S - interpolation25.84692714178984
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4893.716084938657
MSE loss S - extrapolation764.0180686171022
waveform batch: 2/2
Test loss - extrapolation:2033.2645599011182
MSE loss S - extrapolation79.67708210129831
Epoch 39 mean train loss:43.220887340011416
Epoch 39 mean test loss - interpolation:31.74393990944453
Epoch 39 mean test loss - extrapolation:577.2483870699813
Start training epoch 40
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.446594
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 478.4852201964418
Iteration: 2 || Loss: 478.1964101276708
Iteration: 3 || Loss: 477.9071102259818
Iteration: 4 || Loss: 477.6182633521768
Iteration: 5 || Loss: 477.3288535054035
Iteration: 6 || Loss: 477.3288535054035
saving ADAM checkpoint...
Sum of params:-98.44654
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 477.3288535054035
Iteration: 2 || Loss: 224.52972274690214
Iteration: 3 || Loss: 160.67790025692545
Iteration: 4 || Loss: 126.59850419381301
Iteration: 5 || Loss: 112.35574352069058
Iteration: 6 || Loss: 98.9477708497567
Iteration: 7 || Loss: 92.74223824111083
Iteration: 8 || Loss: 84.04795725231511
Iteration: 9 || Loss: 78.72913780102961
Iteration: 10 || Loss: 74.59710591053265
Iteration: 11 || Loss: 71.64645431362968
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.41156
Epoch 40 loss:71.64645431362968
MSE loss S16.363688569300773
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.41156
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 202.4712081999936
Iteration: 2 || Loss: 202.36236764432414
Iteration: 3 || Loss: 202.25424663580367
Iteration: 4 || Loss: 202.14670469548548
Iteration: 5 || Loss: 202.0377639222647
Iteration: 6 || Loss: 202.0377639222647
saving ADAM checkpoint...
Sum of params:-98.41173
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 202.0377639222647
Iteration: 2 || Loss: 167.6397779528002
Iteration: 3 || Loss: 160.1772256616734
Iteration: 4 || Loss: 154.23893237394415
Iteration: 5 || Loss: 151.5250113202943
Iteration: 6 || Loss: 148.990063908982
Iteration: 7 || Loss: 145.6555179984942
Iteration: 8 || Loss: 144.63782998231878
Iteration: 9 || Loss: 143.38037579807
Iteration: 10 || Loss: 139.72307725158737
Iteration: 11 || Loss: 135.53705831686656
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.45056
Epoch 40 loss:135.53705831686656
MSE loss S18.965943813377866
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.45056
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1149.2356621564413
Iteration: 2 || Loss: 1149.181321015995
Iteration: 3 || Loss: 1149.127695388927
Iteration: 4 || Loss: 1149.0736332543281
Iteration: 5 || Loss: 1149.0194671894328
Iteration: 6 || Loss: 1149.0194671894328
saving ADAM checkpoint...
Sum of params:-98.45057
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1149.0194671894328
Iteration: 2 || Loss: 1136.7742866484023
Iteration: 3 || Loss: 1116.016280233181
Iteration: 4 || Loss: 1084.8296647603534
Iteration: 5 || Loss: 1070.2571205978086
Iteration: 6 || Loss: 1066.921052802002
Iteration: 7 || Loss: 1060.1337752719076
Iteration: 8 || Loss: 1054.9591787327986
Iteration: 9 || Loss: 1048.0974267296504
Iteration: 10 || Loss: 1040.5647315168637
Iteration: 11 || Loss: 1034.7317120935895
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.474396
Epoch 40 loss:1034.7317120935895
MSE loss S75.81496221776428
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:188.36346356243683
MSE loss S - interpolation25.602792206474987
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4858.892079574854
MSE loss S - extrapolation758.4475541721894
waveform batch: 2/2
Test loss - extrapolation:2019.6721577916815
MSE loss S - extrapolation78.89957855621091
Epoch 40 mean train loss:42.8246629215202
Epoch 40 mean test loss - interpolation:31.393910593739474
Epoch 40 mean test loss - extrapolation:573.2136864472112
Start training epoch 41
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.474396
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 472.97276707692197
Iteration: 2 || Loss: 472.68462447838186
Iteration: 3 || Loss: 472.3964738310235
Iteration: 4 || Loss: 472.1086711665874
Iteration: 5 || Loss: 471.8210410756458
Iteration: 6 || Loss: 471.8210410756458
saving ADAM checkpoint...
Sum of params:-98.47432
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 471.8210410756458
Iteration: 2 || Loss: 220.48798507066232
Iteration: 3 || Loss: 158.19082436788787
Iteration: 4 || Loss: 124.89978483805741
Iteration: 5 || Loss: 111.07091866339651
Iteration: 6 || Loss: 97.8138207223041
Iteration: 7 || Loss: 91.66515977729047
Iteration: 8 || Loss: 83.30658278470626
Iteration: 9 || Loss: 77.99054169497443
Iteration: 10 || Loss: 74.04081481983054
Iteration: 11 || Loss: 71.1320272426662
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.4403
Epoch 41 loss:71.1320272426662
MSE loss S16.255061025905494
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.4403
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 200.5017926166949
Iteration: 2 || Loss: 200.39416320982824
Iteration: 3 || Loss: 200.28661060443977
Iteration: 4 || Loss: 200.17900124215046
Iteration: 5 || Loss: 200.0712365982716
Iteration: 6 || Loss: 200.0712365982716
saving ADAM checkpoint...
Sum of params:-98.44045
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 200.0712365982716
Iteration: 2 || Loss: 166.03356593548523
Iteration: 3 || Loss: 158.67353309843656
Iteration: 4 || Loss: 152.77385072221242
Iteration: 5 || Loss: 150.11916372478646
Iteration: 6 || Loss: 147.59359832074765
Iteration: 7 || Loss: 144.32220523836568
Iteration: 8 || Loss: 143.5010585267858
Iteration: 9 || Loss: 142.06945794659427
Iteration: 10 || Loss: 138.481305170291
Iteration: 11 || Loss: 134.33178940120175
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.47913
Epoch 41 loss:134.33178940120175
MSE loss S18.816315150005913
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.47913
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1138.0642728455416
Iteration: 2 || Loss: 1138.010227842305
Iteration: 3 || Loss: 1137.9565826135185
Iteration: 4 || Loss: 1137.902916424119
Iteration: 5 || Loss: 1137.84944547479
Iteration: 6 || Loss: 1137.84944547479
saving ADAM checkpoint...
Sum of params:-98.479164
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1137.84944547479
Iteration: 2 || Loss: 1125.7883443380122
Iteration: 3 || Loss: 1105.1335542484796
Iteration: 4 || Loss: 1074.3424541966897
Iteration: 5 || Loss: 1059.9027745500298
Iteration: 6 || Loss: 1056.6684445797862
Iteration: 7 || Loss: 1049.9525935337117
Iteration: 8 || Loss: 1044.9628776336551
Iteration: 9 || Loss: 1038.3094937733242
Iteration: 10 || Loss: 1030.9444366356136
Iteration: 11 || Loss: 1025.1338276516224
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.50217
Epoch 41 loss:1025.1338276516224
MSE loss S75.22006229122242
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:186.3113849737695
MSE loss S - interpolation25.36255333007466
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4824.681803026238
MSE loss S - extrapolation753.0283865382477
waveform batch: 2/2
Test loss - extrapolation:2006.1228995845095
MSE loss S - extrapolation78.14296054983556
Epoch 41 mean train loss:42.434401527430694
Epoch 41 mean test loss - interpolation:31.05189749562825
Epoch 41 mean test loss - extrapolation:569.2337252175623
Start training epoch 42
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.50217
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 467.83475180309676
Iteration: 2 || Loss: 467.5479704313854
Iteration: 3 || Loss: 467.2603297353175
Iteration: 4 || Loss: 466.97421070895325
Iteration: 5 || Loss: 466.68720894501325
Iteration: 6 || Loss: 466.68720894501325
saving ADAM checkpoint...
Sum of params:-98.50208
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 466.68720894501325
Iteration: 2 || Loss: 216.60694516373067
Iteration: 3 || Loss: 155.79294855031094
Iteration: 4 || Loss: 123.24846021131854
Iteration: 5 || Loss: 109.82995865992712
Iteration: 6 || Loss: 96.71549158417328
Iteration: 7 || Loss: 90.59362478906584
Iteration: 8 || Loss: 82.59148682947243
Iteration: 9 || Loss: 77.2615259904214
Iteration: 10 || Loss: 73.4961260993508
Iteration: 11 || Loss: 70.62319063371608
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.46902
Epoch 42 loss:70.62319063371608
MSE loss S16.14746144734788
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.46902
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 198.59342100046217
Iteration: 2 || Loss: 198.48604756555193
Iteration: 3 || Loss: 198.37893662316034
Iteration: 4 || Loss: 198.27197010212348
Iteration: 5 || Loss: 198.16566695431615
Iteration: 6 || Loss: 198.16566695431615
saving ADAM checkpoint...
Sum of params:-98.46917
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 198.16566695431615
Iteration: 2 || Loss: 164.47320008341757
Iteration: 3 || Loss: 157.2099826698451
Iteration: 4 || Loss: 151.34644817540087
Iteration: 5 || Loss: 148.74780617858195
Iteration: 6 || Loss: 146.2337003971969
Iteration: 7 || Loss: 143.0376313397749
Iteration: 8 || Loss: 142.29404228369557
Iteration: 9 || Loss: 140.78456005494294
Iteration: 10 || Loss: 137.26515429984238
Iteration: 11 || Loss: 133.1530057427939
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.50771
Epoch 42 loss:133.1530057427939
MSE loss S18.66808159927661
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.50771
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1127.0446814025418
Iteration: 2 || Loss: 1126.9910694179036
Iteration: 3 || Loss: 1126.9372281103704
Iteration: 4 || Loss: 1126.8838242922338
Iteration: 5 || Loss: 1126.8304556145426
Iteration: 6 || Loss: 1126.8304556145426
saving ADAM checkpoint...
Sum of params:-98.50772
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1126.8304556145426
Iteration: 2 || Loss: 1114.957940018499
Iteration: 3 || Loss: 1094.4153333264564
Iteration: 4 || Loss: 1064.019580257439
Iteration: 5 || Loss: 1049.7098429047487
Iteration: 6 || Loss: 1046.5737041010902
Iteration: 7 || Loss: 1039.9296754721033
Iteration: 8 || Loss: 1035.1160737889704
Iteration: 9 || Loss: 1028.6661749080813
Iteration: 10 || Loss: 1021.4659235252979
Iteration: 11 || Loss: 1015.6803872450483
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.5298
Epoch 42 loss:1015.6803872450483
MSE loss S74.6361360007473
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:184.29589745565218
MSE loss S - interpolation25.124950955591277
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4790.927569314755
MSE loss S - extrapolation747.7071309403834
waveform batch: 2/2
Test loss - extrapolation:1992.6746497268111
MSE loss S - extrapolation77.40959435604458
Epoch 42 mean train loss:42.050227021433045
Epoch 42 mean test loss - interpolation:30.715982909275365
Epoch 42 mean test loss - extrapolation:565.3001849201305
Start training epoch 43
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.5298
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 462.99455549597735
Iteration: 2 || Loss: 462.7076879550773
Iteration: 3 || Loss: 462.4227623557997
Iteration: 4 || Loss: 462.1362289891667
Iteration: 5 || Loss: 461.85108504411363
Iteration: 6 || Loss: 461.85108504411363
saving ADAM checkpoint...
Sum of params:-98.52972
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 461.85108504411363
Iteration: 2 || Loss: 212.8785251797531
Iteration: 3 || Loss: 153.4686142478207
Iteration: 4 || Loss: 121.62660241333745
Iteration: 5 || Loss: 108.61510842131362
Iteration: 6 || Loss: 95.64007161193734
Iteration: 7 || Loss: 89.51448845997723
Iteration: 8 || Loss: 81.8908511503201
Iteration: 9 || Loss: 76.52943606769193
Iteration: 10 || Loss: 72.95495998966959
Iteration: 11 || Loss: 70.11194240707918
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.497696
Epoch 43 loss:70.11194240707918
MSE loss S16.039091899883793
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.497696
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 196.6979367909238
Iteration: 2 || Loss: 196.59093421799892
Iteration: 3 || Loss: 196.48465237840946
Iteration: 4 || Loss: 196.3788486029646
Iteration: 5 || Loss: 196.2732806713816
Iteration: 6 || Loss: 196.2732806713816
saving ADAM checkpoint...
Sum of params:-98.497856
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 196.2732806713816
Iteration: 2 || Loss: 162.941167963319
Iteration: 3 || Loss: 155.76996396856316
Iteration: 4 || Loss: 149.94370380083274
Iteration: 5 || Loss: 147.39797388604762
Iteration: 6 || Loss: 144.8969132663626
Iteration: 7 || Loss: 141.77845276781505
Iteration: 8 || Loss: 141.04809981764825
Iteration: 9 || Loss: 139.5216013814326
Iteration: 10 || Loss: 136.06404230975494
Iteration: 11 || Loss: 131.99424204516845
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.53622
Epoch 43 loss:131.99424204516845
MSE loss S18.52089613704796
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.53622
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1116.1946028892437
Iteration: 2 || Loss: 1116.1412352868738
Iteration: 3 || Loss: 1116.0877985884108
Iteration: 4 || Loss: 1116.0350529481573
Iteration: 5 || Loss: 1115.9819806483295
Iteration: 6 || Loss: 1115.9819806483295
saving ADAM checkpoint...
Sum of params:-98.536224
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1115.9819806483295
Iteration: 2 || Loss: 1104.2972640189928
Iteration: 3 || Loss: 1083.8699711252161
Iteration: 4 || Loss: 1053.8610026277447
Iteration: 5 || Loss: 1039.6818802472487
Iteration: 6 || Loss: 1036.639104167004
Iteration: 7 || Loss: 1030.0680921330957
Iteration: 8 || Loss: 1025.420901083692
Iteration: 9 || Loss: 1019.1691192627278
Iteration: 10 || Loss: 1012.1311868431474
Iteration: 11 || Loss: 1006.3791154801113
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.55729
Epoch 43 loss:1006.3791154801113
MSE loss S74.06257926092374
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:182.30992553449198
MSE loss S - interpolation24.889397087540488
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4757.576125735284
MSE loss S - extrapolation742.4616895350217
waveform batch: 2/2
Test loss - extrapolation:1979.3715128245656
MSE loss S - extrapolation76.70247382493011
Epoch 43 mean train loss:41.67190689421927
Epoch 43 mean test loss - interpolation:30.384987589081998
Epoch 43 mean test loss - extrapolation:561.4123032133208
Start training epoch 44
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.55729
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 458.3991350978268
Iteration: 2 || Loss: 458.1138428576623
Iteration: 3 || Loss: 457.82859321860656
Iteration: 4 || Loss: 457.54395955658447
Iteration: 5 || Loss: 457.2594227386891
Iteration: 6 || Loss: 457.2594227386891
saving ADAM checkpoint...
Sum of params:-98.55722
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 457.2594227386891
Iteration: 2 || Loss: 209.28558540636445
Iteration: 3 || Loss: 151.20372098381728
Iteration: 4 || Loss: 120.02974588284897
Iteration: 5 || Loss: 107.42318425583971
Iteration: 6 || Loss: 94.58656010667208
Iteration: 7 || Loss: 88.42731963222704
Iteration: 8 || Loss: 81.19813648966816
Iteration: 9 || Loss: 75.79216335662036
Iteration: 10 || Loss: 72.41115957470682
Iteration: 11 || Loss: 69.59467039499732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.526344
Epoch 44 loss:69.59467039499732
MSE loss S15.928917900489054
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.526344
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 194.81860928166904
Iteration: 2 || Loss: 194.7126537423667
Iteration: 3 || Loss: 194.60681571867747
Iteration: 4 || Loss: 194.50195804085538
Iteration: 5 || Loss: 194.3970442302086
Iteration: 6 || Loss: 194.3970442302086
saving ADAM checkpoint...
Sum of params:-98.52652
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 194.3970442302086
Iteration: 2 || Loss: 161.4278083940409
Iteration: 3 || Loss: 154.3458285649175
Iteration: 4 || Loss: 148.5547929313661
Iteration: 5 || Loss: 146.06019423218217
Iteration: 6 || Loss: 143.57356696514228
Iteration: 7 || Loss: 140.53380416524618
Iteration: 8 || Loss: 139.79299566809212
Iteration: 9 || Loss: 138.2732100324858
Iteration: 10 || Loss: 134.87175245217705
Iteration: 11 || Loss: 130.85027738550698
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.56466
Epoch 44 loss:130.85027738550698
MSE loss S18.374887161951232
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.56466
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1105.5169250964132
Iteration: 2 || Loss: 1105.4636907997738
Iteration: 3 || Loss: 1105.4103035198837
Iteration: 4 || Loss: 1105.3576487206808
Iteration: 5 || Loss: 1105.3050881540169
Iteration: 6 || Loss: 1105.3050881540169
saving ADAM checkpoint...
Sum of params:-98.56465
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1105.3050881540169
Iteration: 2 || Loss: 1093.80311045156
Iteration: 3 || Loss: 1073.4941308168877
Iteration: 4 || Loss: 1043.8639414770175
Iteration: 5 || Loss: 1029.818808772188
Iteration: 6 || Loss: 1026.866724873706
Iteration: 7 || Loss: 1020.3706941596035
Iteration: 8 || Loss: 1015.8819752366635
Iteration: 9 || Loss: 1009.823179959472
Iteration: 10 || Loss: 1002.9430720968945
Iteration: 11 || Loss: 997.2345812542045
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.58467
Epoch 44 loss:997.2345812542045
MSE loss S73.49910064987739
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:180.35094384987744
MSE loss S - interpolation24.65583095965406
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4724.526104176183
MSE loss S - extrapolation737.2536853025913
waveform batch: 2/2
Test loss - extrapolation:1966.245178583904
MSE loss S - extrapolation76.01980860864498
Epoch 44 mean train loss:41.299294104645135
Epoch 44 mean test loss - interpolation:30.05849064164624
Epoch 44 mean test loss - extrapolation:557.5642735633406
Start training epoch 45
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.58467
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 454.0023393861636
Iteration: 2 || Loss: 453.71874169799423
Iteration: 3 || Loss: 453.4343518963249
Iteration: 4 || Loss: 453.15044801180386
Iteration: 5 || Loss: 452.86700904441165
Iteration: 6 || Loss: 452.86700904441165
saving ADAM checkpoint...
Sum of params:-98.584595
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 452.86700904441165
Iteration: 2 || Loss: 205.83143512458798
Iteration: 3 || Loss: 148.99609070223778
Iteration: 4 || Loss: 118.45689008584355
Iteration: 5 || Loss: 106.24907112186652
Iteration: 6 || Loss: 93.55315734389589
Iteration: 7 || Loss: 87.3318403448731
Iteration: 8 || Loss: 80.50857699050259
Iteration: 9 || Loss: 75.04503590743694
Iteration: 10 || Loss: 71.86060990805507
Iteration: 11 || Loss: 69.06943038512355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.554886
Epoch 45 loss:69.06943038512355
MSE loss S15.816511732766104
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.554886
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 192.9479163367913
Iteration: 2 || Loss: 192.8433421396822
Iteration: 3 || Loss: 192.738786072078
Iteration: 4 || Loss: 192.63405101192876
Iteration: 5 || Loss: 192.52937551801384
Iteration: 6 || Loss: 192.52937551801384
saving ADAM checkpoint...
Sum of params:-98.55504
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 192.52937551801384
Iteration: 2 || Loss: 159.92987062159494
Iteration: 3 || Loss: 152.93133055896422
Iteration: 4 || Loss: 147.1783547555441
Iteration: 5 || Loss: 144.7297995844529
Iteration: 6 || Loss: 142.2598291168204
Iteration: 7 || Loss: 139.29128482942073
Iteration: 8 || Loss: 138.53403903838247
Iteration: 9 || Loss: 137.0345620691194
Iteration: 10 || Loss: 133.68370546757538
Iteration: 11 || Loss: 129.71433949149906
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.59297
Epoch 45 loss:129.71433949149906
MSE loss S18.227232180155713
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.59297
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1095.0055525144942
Iteration: 2 || Loss: 1094.953439046661
Iteration: 3 || Loss: 1094.9007600248165
Iteration: 4 || Loss: 1094.8485496290136
Iteration: 5 || Loss: 1094.796277378757
Iteration: 6 || Loss: 1094.796277378757
saving ADAM checkpoint...
Sum of params:-98.59297
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1094.796277378757
Iteration: 2 || Loss: 1083.4845116857891
Iteration: 3 || Loss: 1063.302088597098
Iteration: 4 || Loss: 1034.0483928999352
Iteration: 5 || Loss: 1020.1337627019886
Iteration: 6 || Loss: 1017.2693912266054
Iteration: 7 || Loss: 1010.8481140453864
Iteration: 8 || Loss: 1006.5083378840781
Iteration: 9 || Loss: 1000.6360371645941
Iteration: 10 || Loss: 993.9077369624904
Iteration: 11 || Loss: 988.256403268687
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.61186
Epoch 45 loss:988.256403268687
MSE loss S72.9458705643386
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:178.4299609028198
MSE loss S - interpolation24.426851980534188
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4691.921628038476
MSE loss S - extrapolation732.1159442668568
waveform batch: 2/2
Test loss - extrapolation:1953.3348891790833
MSE loss S - extrapolation75.37107512558637
Epoch 45 mean train loss:40.932419763631366
Epoch 45 mean test loss - interpolation:29.738326817136635
Epoch 45 mean test loss - extrapolation:553.7713764347967
Start training epoch 46
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.61186
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 449.84773644486506
Iteration: 2 || Loss: 449.56523482135253
Iteration: 3 || Loss: 449.2813817284023
Iteration: 4 || Loss: 448.9993870606336
Iteration: 5 || Loss: 448.71621773692306
Iteration: 6 || Loss: 448.71621773692306
saving ADAM checkpoint...
Sum of params:-98.61178
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 448.71621773692306
Iteration: 2 || Loss: 202.51656007499122
Iteration: 3 || Loss: 146.83865731996212
Iteration: 4 || Loss: 116.90392732516986
Iteration: 5 || Loss: 105.08597119618435
Iteration: 6 || Loss: 92.53496643146906
Iteration: 7 || Loss: 86.22913712699915
Iteration: 8 || Loss: 79.81275568615199
Iteration: 9 || Loss: 74.2911455914371
Iteration: 10 || Loss: 71.29667906972365
Iteration: 11 || Loss: 68.53220257596001
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.58333
Epoch 46 loss:68.53220257596001
MSE loss S15.701671930840252
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.58333
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 191.07675583111293
Iteration: 2 || Loss: 190.9731275847293
Iteration: 3 || Loss: 190.86902703152725
Iteration: 4 || Loss: 190.76533732095862
Iteration: 5 || Loss: 190.661558257086
Iteration: 6 || Loss: 190.661558257086
saving ADAM checkpoint...
Sum of params:-98.58347
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 190.661558257086
Iteration: 2 || Loss: 158.4290083871744
Iteration: 3 || Loss: 151.51236030068767
Iteration: 4 || Loss: 145.80005180656795
Iteration: 5 || Loss: 143.39675063852025
Iteration: 6 || Loss: 140.94633346461265
Iteration: 7 || Loss: 138.05254423175583
Iteration: 8 || Loss: 137.27539071905014
Iteration: 9 || Loss: 135.79863758184098
Iteration: 10 || Loss: 132.49285922878119
Iteration: 11 || Loss: 128.58080909747994
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.62111
Epoch 46 loss:128.58080909747994
MSE loss S18.07858965813502
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.62111
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1084.6752370772983
Iteration: 2 || Loss: 1084.6231078009428
Iteration: 3 || Loss: 1084.570778306069
Iteration: 4 || Loss: 1084.518590823457
Iteration: 5 || Loss: 1084.4666363793867
Iteration: 6 || Loss: 1084.4666363793867
saving ADAM checkpoint...
Sum of params:-98.62109
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1084.4666363793867
Iteration: 2 || Loss: 1073.345994237141
Iteration: 3 || Loss: 1053.3006148412767
Iteration: 4 || Loss: 1024.418255209841
Iteration: 5 || Loss: 1010.6377302659032
Iteration: 6 || Loss: 1007.8574449016204
Iteration: 7 || Loss: 1001.5123006288435
Iteration: 8 || Loss: 997.3096354791504
Iteration: 9 || Loss: 991.6162270576905
Iteration: 10 || Loss: 985.0338612305212
Iteration: 11 || Loss: 979.4528922769218
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.63885
Epoch 46 loss:979.4528922769218
MSE loss S72.40207465301987
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:176.53169567282757
MSE loss S - interpolation24.200209586758657
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4659.521202920117
MSE loss S - extrapolation726.9645106401869
waveform batch: 2/2
Test loss - extrapolation:1940.6900511221397
MSE loss S - extrapolation74.75318317720135
Epoch 46 mean train loss:40.57123806725386
Epoch 46 mean test loss - interpolation:29.421949278804593
Epoch 46 mean test loss - extrapolation:550.0176045035214
Start training epoch 47
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.63885
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 445.80187835957395
Iteration: 2 || Loss: 445.5193357351005
Iteration: 3 || Loss: 445.2373298888132
Iteration: 4 || Loss: 444.9553837748252
Iteration: 5 || Loss: 444.6740268778458
Iteration: 6 || Loss: 444.6740268778458
saving ADAM checkpoint...
Sum of params:-98.63874
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 444.6740268778458
Iteration: 2 || Loss: 199.31155465226922
Iteration: 3 || Loss: 144.71632379123164
Iteration: 4 || Loss: 115.3650117841876
Iteration: 5 || Loss: 103.92804299766605
Iteration: 6 || Loss: 91.5274473795772
Iteration: 7 || Loss: 85.12148983979958
Iteration: 8 || Loss: 79.10271590103955
Iteration: 9 || Loss: 73.52118107949883
Iteration: 10 || Loss: 70.71220605460019
Iteration: 11 || Loss: 67.98049020146482
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.61154
Epoch 47 loss:67.98049020146482
MSE loss S15.583336781983341
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.61154
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 189.18179915900896
Iteration: 2 || Loss: 189.0787136508719
Iteration: 3 || Loss: 188.97546169378688
Iteration: 4 || Loss: 188.87234053126258
Iteration: 5 || Loss: 188.77008269646606
Iteration: 6 || Loss: 188.77008269646606
saving ADAM checkpoint...
Sum of params:-98.6117
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 188.77008269646606
Iteration: 2 || Loss: 156.9242362303527
Iteration: 3 || Loss: 150.08662767253097
Iteration: 4 || Loss: 144.41805418917508
Iteration: 5 || Loss: 142.0570573037052
Iteration: 6 || Loss: 139.62761068571186
Iteration: 7 || Loss: 136.80036985861474
Iteration: 8 || Loss: 136.00825690834068
Iteration: 9 || Loss: 134.55918975244026
Iteration: 10 || Loss: 131.2950954882899
Iteration: 11 || Loss: 127.44603855585805
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.649086
Epoch 47 loss:127.44603855585805
MSE loss S17.928448865309726
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.649086
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1074.5215158814913
Iteration: 2 || Loss: 1074.4698336114889
Iteration: 3 || Loss: 1074.4181961451604
Iteration: 4 || Loss: 1074.366502225484
Iteration: 5 || Loss: 1074.314793518409
Iteration: 6 || Loss: 1074.314793518409
saving ADAM checkpoint...
Sum of params:-98.64904
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1074.314793518409
Iteration: 2 || Loss: 1063.3891805565104
Iteration: 3 || Loss: 1043.4890993719189
Iteration: 4 || Loss: 1014.979628507403
Iteration: 5 || Loss: 1001.3343185987148
Iteration: 6 || Loss: 998.6363550606605
Iteration: 7 || Loss: 992.3667193693389
Iteration: 8 || Loss: 988.2923040796297
Iteration: 9 || Loss: 982.7708263651696
Iteration: 10 || Loss: 976.3259565216823
Iteration: 11 || Loss: 970.8283311128962
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.6656
Epoch 47 loss:970.8283311128962
MSE loss S71.86715650966576
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:174.6655262271261
MSE loss S - interpolation23.977671045235777
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4627.324700867706
MSE loss S - extrapolation721.7898457327206
waveform batch: 2/2
Test loss - extrapolation:1928.3216777991436
MSE loss S - extrapolation74.16644924447633
Epoch 47 mean train loss:40.215684823111
Epoch 47 mean test loss - interpolation:29.11092103785435
Epoch 47 mean test loss - extrapolation:546.3038648889042
Start training epoch 48
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.6656
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 441.8613019738479
Iteration: 2 || Loss: 441.5799615041374
Iteration: 3 || Loss: 441.298909924012
Iteration: 4 || Loss: 441.0181033447948
Iteration: 5 || Loss: 440.7372870825667
Iteration: 6 || Loss: 440.7372870825667
saving ADAM checkpoint...
Sum of params:-98.66552
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 440.7372870825667
Iteration: 2 || Loss: 196.2300236709669
Iteration: 3 || Loss: 142.64113637543926
Iteration: 4 || Loss: 113.84910002077888
Iteration: 5 || Loss: 102.77738976096474
Iteration: 6 || Loss: 90.53287260765336
Iteration: 7 || Loss: 84.01754362482221
Iteration: 8 || Loss: 78.37907525239747
Iteration: 9 || Loss: 72.74570794152349
Iteration: 10 || Loss: 70.10737497786462
Iteration: 11 || Loss: 67.41601871004362
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.6396
Epoch 48 loss:67.41601871004362
MSE loss S15.462442715122453
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.6396
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 187.2825964133429
Iteration: 2 || Loss: 187.18029336734466
Iteration: 3 || Loss: 187.0776410343684
Iteration: 4 || Loss: 186.97547901946078
Iteration: 5 || Loss: 186.8741629133641
Iteration: 6 || Loss: 186.8741629133641
saving ADAM checkpoint...
Sum of params:-98.63974
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 186.8741629133641
Iteration: 2 || Loss: 155.41272478110363
Iteration: 3 || Loss: 148.65154400470976
Iteration: 4 || Loss: 143.03239915468626
Iteration: 5 || Loss: 140.71137175815224
Iteration: 6 || Loss: 138.3053336599741
Iteration: 7 || Loss: 135.54501880879351
Iteration: 8 || Loss: 134.73985491188586
Iteration: 9 || Loss: 133.3171092317381
Iteration: 10 || Loss: 130.09144125292067
Iteration: 11 || Loss: 126.30897098572021
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.676796
Epoch 48 loss:126.30897098572021
MSE loss S17.775790428857366
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.676796
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1064.5541783757142
Iteration: 2 || Loss: 1064.502878954472
Iteration: 3 || Loss: 1064.4513190285882
Iteration: 4 || Loss: 1064.3998810722012
Iteration: 5 || Loss: 1064.3495685005303
Iteration: 6 || Loss: 1064.3495685005303
saving ADAM checkpoint...
Sum of params:-98.676735
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1064.3495685005303
Iteration: 2 || Loss: 1053.623413325353
Iteration: 3 || Loss: 1033.8774583557733
Iteration: 4 || Loss: 1005.7396103272832
Iteration: 5 || Loss: 992.2335746997146
Iteration: 6 || Loss: 989.6127546265477
Iteration: 7 || Loss: 983.4189138191318
Iteration: 8 || Loss: 979.4601308004144
Iteration: 9 || Loss: 974.1034030155561
Iteration: 10 || Loss: 967.7879376390947
Iteration: 11 || Loss: 962.3851910685908
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.69212
Epoch 48 loss:962.3851910685908
MSE loss S71.33976760436249
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:172.8282373255621
MSE loss S - interpolation23.758528556732266
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4595.270604730655
MSE loss S - extrapolation716.5605783789626
waveform batch: 2/2
Test loss - extrapolation:1916.256868651058
MSE loss S - extrapolation73.6103894830324
Epoch 48 mean train loss:39.86586830221912
Epoch 48 mean test loss - interpolation:28.804706220927017
Epoch 48 mean test loss - extrapolation:542.6272894484761
Start training epoch 49
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.69212
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 437.9709942353839
Iteration: 2 || Loss: 437.69025322060753
Iteration: 3 || Loss: 437.41055795776884
Iteration: 4 || Loss: 437.1303511687194
Iteration: 5 || Loss: 436.85080635694464
Iteration: 6 || Loss: 436.85080635694464
saving ADAM checkpoint...
Sum of params:-98.69205
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 436.85080635694464
Iteration: 2 || Loss: 193.25725190978523
Iteration: 3 || Loss: 140.6005586245006
Iteration: 4 || Loss: 112.35096446120006
Iteration: 5 || Loss: 101.62786842928605
Iteration: 6 || Loss: 89.54543314191079
Iteration: 7 || Loss: 82.92091772027716
Iteration: 8 || Loss: 77.63348996763709
Iteration: 9 || Loss: 71.96005829794181
Iteration: 10 || Loss: 69.47449554127545
Iteration: 11 || Loss: 66.83681265515152
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.66731
Epoch 49 loss:66.83681265515152
MSE loss S15.338219536318405
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.66731
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 185.37587430387148
Iteration: 2 || Loss: 185.274278726375
Iteration: 3 || Loss: 185.17259456569528
Iteration: 4 || Loss: 185.0720166832728
Iteration: 5 || Loss: 184.97095191676848
Iteration: 6 || Loss: 184.97095191676848
saving ADAM checkpoint...
Sum of params:-98.66748
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 184.97095191676848
Iteration: 2 || Loss: 153.89256804118455
Iteration: 3 || Loss: 147.2046427614335
Iteration: 4 || Loss: 141.63804951307242
Iteration: 5 || Loss: 139.35641062782676
Iteration: 6 || Loss: 136.9760161543497
Iteration: 7 || Loss: 134.28418755148925
Iteration: 8 || Loss: 133.4671813479428
Iteration: 9 || Loss: 132.06946752398656
Iteration: 10 || Loss: 128.88041545371155
Iteration: 11 || Loss: 125.16621653790376
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.70419
Epoch 49 loss:125.16621653790376
MSE loss S17.620828568332634
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.70419
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1054.7643035402748
Iteration: 2 || Loss: 1054.7139311742112
Iteration: 3 || Loss: 1054.66293472808
Iteration: 4 || Loss: 1054.6123594864205
Iteration: 5 || Loss: 1054.561681282305
Iteration: 6 || Loss: 1054.561681282305
saving ADAM checkpoint...
Sum of params:-98.704124
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1054.561681282305
Iteration: 2 || Loss: 1044.042369796105
Iteration: 3 || Loss: 1024.4666431185838
Iteration: 4 || Loss: 996.7036257810677
Iteration: 5 || Loss: 983.3387108161437
Iteration: 6 || Loss: 980.7930322722729
Iteration: 7 || Loss: 974.6745391659124
Iteration: 8 || Loss: 970.8194751333513
Iteration: 9 || Loss: 965.6196106824027
Iteration: 10 || Loss: 959.4243495745706
Iteration: 11 || Loss: 954.1252402920724
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.7184
Epoch 49 loss:954.1252402920724
MSE loss S70.82001142920896
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:171.02074608401867
MSE loss S - interpolation23.54321984530192
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4563.279452680773
MSE loss S - extrapolation711.2532566328774
waveform batch: 2/2
Test loss - extrapolation:1904.5016222625663
MSE loss S - extrapolation73.08549908007126
Epoch 49 mean train loss:39.521664465004406
Epoch 49 mean test loss - interpolation:28.50345768066978
Epoch 49 mean test loss - extrapolation:538.9817562452782
Start training epoch 50
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.7184
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 434.0974793577387
Iteration: 2 || Loss: 433.817897442371
Iteration: 3 || Loss: 433.5391815991599
Iteration: 4 || Loss: 433.2607085815659
Iteration: 5 || Loss: 432.9816927853333
Iteration: 6 || Loss: 432.9816927853333
saving ADAM checkpoint...
Sum of params:-98.71833
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 432.9816927853333
Iteration: 2 || Loss: 190.38705223250454
Iteration: 3 || Loss: 138.59720380568066
Iteration: 4 || Loss: 110.87279862045126
Iteration: 5 || Loss: 100.48044616422203
Iteration: 6 || Loss: 88.56495426243666
Iteration: 7 || Loss: 81.8355268008304
Iteration: 8 || Loss: 76.8655862054013
Iteration: 9 || Loss: 71.16944639017005
Iteration: 10 || Loss: 68.81147210854519
Iteration: 11 || Loss: 66.24454576694417
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.69479
Epoch 50 loss:66.24454576694417
MSE loss S15.2113757479451
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.69479
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 183.4637075786589
Iteration: 2 || Loss: 183.3626530728208
Iteration: 3 || Loss: 183.26300106177735
Iteration: 4 || Loss: 183.16240799339812
Iteration: 5 || Loss: 183.06222846203207
Iteration: 6 || Loss: 183.06222846203207
saving ADAM checkpoint...
Sum of params:-98.69496
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 183.06222846203207
Iteration: 2 || Loss: 152.3685657625281
Iteration: 3 || Loss: 145.7484922144937
Iteration: 4 || Loss: 140.2378307485299
Iteration: 5 || Loss: 137.9942311570743
Iteration: 6 || Loss: 135.64136436330665
Iteration: 7 || Loss: 133.01687194372082
Iteration: 8 || Loss: 132.19081860314648
Iteration: 9 || Loss: 130.81633061122793
Iteration: 10 || Loss: 127.66144361264669
Iteration: 11 || Loss: 124.01746076072642
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.73131
Epoch 50 loss:124.01746076072642
MSE loss S17.463293677466915
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.73131
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1045.1683784858496
Iteration: 2 || Loss: 1045.117768278298
Iteration: 3 || Loss: 1045.0673163884758
Iteration: 4 || Loss: 1045.017143558542
Iteration: 5 || Loss: 1044.9668675400012
Iteration: 6 || Loss: 1044.9668675400012
saving ADAM checkpoint...
Sum of params:-98.73125
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1044.9668675400012
Iteration: 2 || Loss: 1034.6578643803362
Iteration: 3 || Loss: 1015.2621542229398
Iteration: 4 || Loss: 987.8771977573933
Iteration: 5 || Loss: 974.6533580431386
Iteration: 6 || Loss: 972.1781633247825
Iteration: 7 || Loss: 966.1343351593574
Iteration: 8 || Loss: 962.3704526384744
Iteration: 9 || Loss: 957.3207939439418
Iteration: 10 || Loss: 951.2362536466687
Iteration: 11 || Loss: 946.0482499558528
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.74442
Epoch 50 loss:946.0482499558528
MSE loss S70.30725260525193
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:169.2364174256568
MSE loss S - interpolation23.330401981342824
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4531.29827167647
MSE loss S - extrapolation705.842053431314
waveform batch: 2/2
Test loss - extrapolation:1893.0664677333368
MSE loss S - extrapolation72.58899001773472
Epoch 50 mean train loss:39.18311229253529
Epoch 50 mean test loss - interpolation:28.2060695709428
Epoch 50 mean test loss - extrapolation:535.3637282841506
Start training epoch 51
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.74442
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 430.18084878454204
Iteration: 2 || Loss: 429.90320969138435
Iteration: 3 || Loss: 429.62498092422743
Iteration: 4 || Loss: 429.34769761073386
Iteration: 5 || Loss: 429.06988893980207
Iteration: 6 || Loss: 429.06988893980207
saving ADAM checkpoint...
Sum of params:-98.74433
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 429.06988893980207
Iteration: 2 || Loss: 187.59169065603902
Iteration: 3 || Loss: 136.6205805970296
Iteration: 4 || Loss: 109.41449559639523
Iteration: 5 || Loss: 99.33511775331898
Iteration: 6 || Loss: 87.58896685964645
Iteration: 7 || Loss: 80.76905957758211
Iteration: 8 || Loss: 76.07343629211621
Iteration: 9 || Loss: 70.37688130282606
Iteration: 10 || Loss: 68.1119353106287
Iteration: 11 || Loss: 65.63890837361554
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.721985
Epoch 51 loss:65.63890837361554
MSE loss S15.081463323978912
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.721985
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 181.5442012745453
Iteration: 2 || Loss: 181.44459902260525
Iteration: 3 || Loss: 181.34478700118225
Iteration: 4 || Loss: 181.24593784066735
Iteration: 5 || Loss: 181.14629395975007
Iteration: 6 || Loss: 181.14629395975007
saving ADAM checkpoint...
Sum of params:-98.722145
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 181.14629395975007
Iteration: 2 || Loss: 150.83961898120802
Iteration: 3 || Loss: 144.28081902425902
Iteration: 4 || Loss: 138.83069501081235
Iteration: 5 || Loss: 136.62502242505607
Iteration: 6 || Loss: 134.30051679055546
Iteration: 7 || Loss: 131.7469072231913
Iteration: 8 || Loss: 130.9122282234635
Iteration: 9 || Loss: 129.55770740648762
Iteration: 10 || Loss: 126.43562034530878
Iteration: 11 || Loss: 122.8624827428192
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.75813
Epoch 51 loss:122.8624827428192
MSE loss S17.302383377944086
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.75813
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1035.7516069616065
Iteration: 2 || Loss: 1035.7017285905897
Iteration: 3 || Loss: 1035.6518247143322
Iteration: 4 || Loss: 1035.601884356407
Iteration: 5 || Loss: 1035.5522726934435
Iteration: 6 || Loss: 1035.5522726934435
saving ADAM checkpoint...
Sum of params:-98.758064
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1035.5522726934435
Iteration: 2 || Loss: 1025.4630845873755
Iteration: 3 || Loss: 1006.2605276072845
Iteration: 4 || Loss: 979.2528962179254
Iteration: 5 || Loss: 966.1760187993752
Iteration: 6 || Loss: 963.767356444156
Iteration: 7 || Loss: 957.7974787842144
Iteration: 8 || Loss: 954.1125838939327
Iteration: 9 || Loss: 949.2071884203649
Iteration: 10 || Loss: 943.2236084395768
Iteration: 11 || Loss: 938.1516913022875
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.77024
Epoch 51 loss:938.1516913022875
MSE loss S69.80077570131945
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:167.48481534281987
MSE loss S - interpolation23.121166557613705
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4499.352323571768
MSE loss S - extrapolation700.3326493461659
waveform batch: 2/2
Test loss - extrapolation:1881.9366119181086
MSE loss S - extrapolation72.11944943862767
Epoch 51 mean train loss:38.85010629030077
Epoch 51 mean test loss - interpolation:27.91413589046998
Epoch 51 mean test loss - extrapolation:531.7740779574897
Start training epoch 52
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.77024
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 426.24944658113947
Iteration: 2 || Loss: 425.9729595042504
Iteration: 3 || Loss: 425.69656406495335
Iteration: 4 || Loss: 425.4200955794136
Iteration: 5 || Loss: 425.1440467143541
Iteration: 6 || Loss: 425.1440467143541
saving ADAM checkpoint...
Sum of params:-98.77015
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 425.1440467143541
Iteration: 2 || Loss: 184.89288796764214
Iteration: 3 || Loss: 134.68623962379192
Iteration: 4 || Loss: 107.98227496669881
Iteration: 5 || Loss: 98.19628118286045
Iteration: 6 || Loss: 86.62021543832627
Iteration: 7 || Loss: 79.7230130821037
Iteration: 8 || Loss: 75.25855915668178
Iteration: 9 || Loss: 69.58866406768543
Iteration: 10 || Loss: 67.37030923426676
Iteration: 11 || Loss: 65.02185587759355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.7488
Epoch 52 loss:65.02185587759355
MSE loss S14.94896465516955
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.7488
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 179.63521233791676
Iteration: 2 || Loss: 179.53611726530897
Iteration: 3 || Loss: 179.4374481654995
Iteration: 4 || Loss: 179.3388780586795
Iteration: 5 || Loss: 179.24086088856697
Iteration: 6 || Loss: 179.24086088856697
saving ADAM checkpoint...
Sum of params:-98.748955
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 179.24086088856697
Iteration: 2 || Loss: 149.31790888183326
Iteration: 3 || Loss: 142.80769850018234
Iteration: 4 || Loss: 137.41893513446533
Iteration: 5 || Loss: 135.25172657438313
Iteration: 6 || Loss: 132.95619656167167
Iteration: 7 || Loss: 130.47826374357683
Iteration: 8 || Loss: 129.63401035297005
Iteration: 9 || Loss: 128.29555704074306
Iteration: 10 || Loss: 125.20560388648241
Iteration: 11 || Loss: 121.70286690258229
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.78462
Epoch 52 loss:121.70286690258229
MSE loss S17.139586964215198
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.78462
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1026.5325664450088
Iteration: 2 || Loss: 1026.482994690249
Iteration: 3 || Loss: 1026.4340601044626
Iteration: 4 || Loss: 1026.3849661293998
Iteration: 5 || Loss: 1026.3351551878736
Iteration: 6 || Loss: 1026.3351551878736
saving ADAM checkpoint...
Sum of params:-98.78456
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1026.3351551878736
Iteration: 2 || Loss: 1016.4679567393329
Iteration: 3 || Loss: 997.4659980427778
Iteration: 4 || Loss: 970.8374254037846
Iteration: 5 || Loss: 957.9059631301801
Iteration: 6 || Loss: 955.5595161976646
Iteration: 7 || Loss: 949.6631344872759
Iteration: 8 || Loss: 946.0443572422619
Iteration: 9 || Loss: 941.27676779836
Iteration: 10 || Loss: 935.3844131562411
Iteration: 11 || Loss: 930.4307962397937
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.79575
Epoch 52 loss:930.4307962397937
MSE loss S69.2991094784899
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:165.753600243949
MSE loss S - interpolation22.91310091254906
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4467.276122553901
MSE loss S - extrapolation694.674540217022
waveform batch: 2/2
Test loss - extrapolation:1871.1147411757047
MSE loss S - extrapolation71.6727859683987
Epoch 52 mean train loss:38.52260410413689
Epoch 52 mean test loss - interpolation:27.625600040658167
Epoch 52 mean test loss - extrapolation:528.1992386441337
Start training epoch 53
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.79575
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 422.19948931119694
Iteration: 2 || Loss: 421.923851653138
Iteration: 3 || Loss: 421.64891988499323
Iteration: 4 || Loss: 421.3741879271509
Iteration: 5 || Loss: 421.09946075059065
Iteration: 6 || Loss: 421.09946075059065
saving ADAM checkpoint...
Sum of params:-98.79566
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 421.09946075059065
Iteration: 2 || Loss: 182.2576930183722
Iteration: 3 || Loss: 132.78143646654985
Iteration: 4 || Loss: 106.57432824400703
Iteration: 5 || Loss: 97.06213565526342
Iteration: 6 || Loss: 85.65617846766757
Iteration: 7 || Loss: 78.69953856815962
Iteration: 8 || Loss: 74.42115640508064
Iteration: 9 || Loss: 68.807187336943
Iteration: 10 || Loss: 66.57278572099582
Iteration: 11 || Loss: 64.3928294138936
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.77523
Epoch 53 loss:64.3928294138936
MSE loss S14.813000739170226
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.77523
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 177.74005738357337
Iteration: 2 || Loss: 177.64244240155125
Iteration: 3 || Loss: 177.54465643819304
Iteration: 4 || Loss: 177.44669320607557
Iteration: 5 || Loss: 177.3496814806383
Iteration: 6 || Loss: 177.3496814806383
saving ADAM checkpoint...
Sum of params:-98.77537
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 177.3496814806383
Iteration: 2 || Loss: 147.81667457485415
Iteration: 3 || Loss: 141.3338078884722
Iteration: 4 || Loss: 136.0057445743627
Iteration: 5 || Loss: 133.87781454959895
Iteration: 6 || Loss: 131.6100166642632
Iteration: 7 || Loss: 129.21119922917197
Iteration: 8 || Loss: 128.3569842773602
Iteration: 9 || Loss: 127.03088594945517
Iteration: 10 || Loss: 123.97225298825002
Iteration: 11 || Loss: 120.53849488638227
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.81077
Epoch 53 loss:120.53849488638227
MSE loss S16.974194666437782
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.81077
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1017.4977536796029
Iteration: 2 || Loss: 1017.4486154267537
Iteration: 3 || Loss: 1017.4003102364289
Iteration: 4 || Loss: 1017.3513059622402
Iteration: 5 || Loss: 1017.3026561703522
Iteration: 6 || Loss: 1017.3026561703522
saving ADAM checkpoint...
Sum of params:-98.81071
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1017.3026561703522
Iteration: 2 || Loss: 1007.6620754371206
Iteration: 3 || Loss: 988.8715598420222
Iteration: 4 || Loss: 962.619697743115
Iteration: 5 || Loss: 949.8368030174254
Iteration: 6 || Loss: 947.5479243350211
Iteration: 7 || Loss: 941.7247862720817
Iteration: 8 || Loss: 938.1586411702134
Iteration: 9 || Loss: 933.5263948670128
Iteration: 10 || Loss: 927.7155122724464
Iteration: 11 || Loss: 922.8818985581916
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.82103
Epoch 53 loss:922.8818985581916
MSE loss S68.80341369768053
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:164.0475199038941
MSE loss S - interpolation22.707287304113997
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4435.148549672983
MSE loss S - extrapolation688.8986002704994
waveform batch: 2/2
Test loss - extrapolation:1860.5861310741063
MSE loss S - extrapolation71.25119853421117
Epoch 53 mean train loss:38.20045596063681
Epoch 53 mean test loss - interpolation:27.341253317315687
Epoch 53 mean test loss - extrapolation:524.6445567289242
Start training epoch 54
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.82103
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 418.0762199851769
Iteration: 2 || Loss: 417.80198368055187
Iteration: 3 || Loss: 417.5286380027092
Iteration: 4 || Loss: 417.2553975478121
Iteration: 5 || Loss: 416.98235875349616
Iteration: 6 || Loss: 416.98235875349616
saving ADAM checkpoint...
Sum of params:-98.82094
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 416.98235875349616
Iteration: 2 || Loss: 179.69464164338285
Iteration: 3 || Loss: 130.91095508507485
Iteration: 4 || Loss: 105.19060063382656
Iteration: 5 || Loss: 95.93570762092203
Iteration: 6 || Loss: 84.69814975941415
Iteration: 7 || Loss: 77.69840547946998
Iteration: 8 || Loss: 73.56105540292158
Iteration: 9 || Loss: 68.03682734155707
Iteration: 10 || Loss: 65.70688809135746
Iteration: 11 || Loss: 63.74890743135364
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.80119
Epoch 54 loss:63.74890743135364
MSE loss S14.671653452387144
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.80119
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 175.90738195068127
Iteration: 2 || Loss: 175.8100344134691
Iteration: 3 || Loss: 175.71379047203678
Iteration: 4 || Loss: 175.61710097780667
Iteration: 5 || Loss: 175.5208181943591
Iteration: 6 || Loss: 175.5208181943591
saving ADAM checkpoint...
Sum of params:-98.80135
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 175.5208181943591
Iteration: 2 || Loss: 146.36700715406943
Iteration: 3 || Loss: 139.8680303604672
Iteration: 4 || Loss: 134.59184673689174
Iteration: 5 || Loss: 132.5038794601815
Iteration: 6 || Loss: 130.25964605624856
Iteration: 7 || Loss: 127.95035355053373
Iteration: 8 || Loss: 127.08195559423284
Iteration: 9 || Loss: 125.76243859869773
Iteration: 10 || Loss: 122.73687810001597
Iteration: 11 || Loss: 119.37042324142531
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.83663
Epoch 54 loss:119.37042324142531
MSE loss S16.806132990172557
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.83663
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 1008.6548809349497
Iteration: 2 || Loss: 1008.6067303485286
Iteration: 3 || Loss: 1008.5584386307314
Iteration: 4 || Loss: 1008.509997773896
Iteration: 5 || Loss: 1008.4620873272539
Iteration: 6 || Loss: 1008.4620873272539
saving ADAM checkpoint...
Sum of params:-98.83655
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 1008.4620873272539
Iteration: 2 || Loss: 999.0503931106913
Iteration: 3 || Loss: 980.4796260655987
Iteration: 4 || Loss: 954.5972973675943
Iteration: 5 || Loss: 941.96198852152
Iteration: 6 || Loss: 939.725832594149
Iteration: 7 || Loss: 933.9761850012189
Iteration: 8 || Loss: 930.4512661343397
Iteration: 9 || Loss: 925.9522513139706
Iteration: 10 || Loss: 920.2123498176635
Iteration: 11 || Loss: 915.4970512813173
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.846
Epoch 54 loss:915.4970512813173
MSE loss S68.31321633109565
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:162.36607181590318
MSE loss S - interpolation22.503459996367603
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4402.902985467095
MSE loss S - extrapolation682.983313715412
waveform batch: 2/2
Test loss - extrapolation:1850.3277959430054
MSE loss S - extrapolation70.84995025656869
Epoch 54 mean train loss:37.883323515658496
Epoch 54 mean test loss - interpolation:27.061011969317196
Epoch 54 mean test loss - extrapolation:521.1025651175083
Start training epoch 55
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.846
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 413.84144626348206
Iteration: 2 || Loss: 413.5689961709454
Iteration: 3 || Loss: 413.29759811972485
Iteration: 4 || Loss: 413.02567293280254
Iteration: 5 || Loss: 412.7545702441014
Iteration: 6 || Loss: 412.7545702441014
saving ADAM checkpoint...
Sum of params:-98.845924
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 412.7545702441014
Iteration: 2 || Loss: 177.19029400588224
Iteration: 3 || Loss: 129.07348331827808
Iteration: 4 || Loss: 103.8367492158193
Iteration: 5 || Loss: 94.8202911270034
Iteration: 6 || Loss: 83.74707319107844
Iteration: 7 || Loss: 76.72643889310868
Iteration: 8 || Loss: 72.68543500142763
Iteration: 9 || Loss: 67.2835471878776
Iteration: 10 || Loss: 64.77362149083854
Iteration: 11 || Loss: 63.08314482030276
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.82648
Epoch 55 loss:63.08314482030276
MSE loss S14.519877258579342
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.82648
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 174.35523298490807
Iteration: 2 || Loss: 174.25881582761505
Iteration: 3 || Loss: 174.163445088669
Iteration: 4 || Loss: 174.0678096517252
Iteration: 5 || Loss: 173.97224537870022
Iteration: 6 || Loss: 173.97224537870022
saving ADAM checkpoint...
Sum of params:-98.82663
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 173.97224537870022
Iteration: 2 || Loss: 145.05762554925263
Iteration: 3 || Loss: 138.44107891118117
Iteration: 4 || Loss: 133.18559527800448
Iteration: 5 || Loss: 131.13219685299558
Iteration: 6 || Loss: 128.906016360082
Iteration: 7 || Loss: 126.70418063961345
Iteration: 8 || Loss: 125.81065056463169
Iteration: 9 || Loss: 124.48994340472802
Iteration: 10 || Loss: 121.5013440816653
Iteration: 11 || Loss: 118.19324303376213
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.86215
Epoch 55 loss:118.19324303376213
MSE loss S16.633053875388363
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.86215
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 999.995192718948
Iteration: 2 || Loss: 999.9481543496976
Iteration: 3 || Loss: 999.9008342966397
Iteration: 4 || Loss: 999.8529625469039
Iteration: 5 || Loss: 999.8053631897118
Iteration: 6 || Loss: 999.8053631897118
saving ADAM checkpoint...
Sum of params:-98.862076
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 999.8053631897118
Iteration: 2 || Loss: 990.637522627374
Iteration: 3 || Loss: 972.3071518514525
Iteration: 4 || Loss: 946.7866422271188
Iteration: 5 || Loss: 934.2934202639215
Iteration: 6 || Loss: 932.1027441303452
Iteration: 7 || Loss: 926.425500759594
Iteration: 8 || Loss: 922.9268125280197
Iteration: 9 || Loss: 918.5582694292078
Iteration: 10 || Loss: 912.8732350212807
Iteration: 11 || Loss: 908.2756089420814
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.87073
Epoch 55 loss:908.2756089420814
MSE loss S67.83124612493816
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:160.71422097742902
MSE loss S - interpolation22.303365101753784
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4370.5456607949745
MSE loss S - extrapolation676.9462986012722
waveform batch: 2/2
Test loss - extrapolation:1840.3407847319008
MSE loss S - extrapolation70.47572775757385
Epoch 55 mean train loss:37.570758510211945
Epoch 55 mean test loss - interpolation:26.78570349623817
Epoch 55 mean test loss - extrapolation:517.573870460573
Start training epoch 56
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.87073
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 409.52921797667506
Iteration: 2 || Loss: 409.25977321267766
Iteration: 3 || Loss: 408.98909524421646
Iteration: 4 || Loss: 408.71973970038977
Iteration: 5 || Loss: 408.4495536343372
Iteration: 6 || Loss: 408.4495536343372
saving ADAM checkpoint...
Sum of params:-98.87063
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 408.4495536343372
Iteration: 2 || Loss: 174.7683103672514
Iteration: 3 || Loss: 127.28088617094694
Iteration: 4 || Loss: 102.51422847113716
Iteration: 5 || Loss: 93.71940342547798
Iteration: 6 || Loss: 82.80213042881164
Iteration: 7 || Loss: 75.77746745245508
Iteration: 8 || Loss: 71.79219772136153
Iteration: 9 || Loss: 66.54788671432075
Iteration: 10 || Loss: 63.8336555283107
Iteration: 11 || Loss: 62.37876438537892
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.85082
Epoch 56 loss:62.37876438537892
MSE loss S14.349488736376163
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.85082
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 173.7280185158757
Iteration: 2 || Loss: 173.632307519405
Iteration: 3 || Loss: 173.53590458156532
Iteration: 4 || Loss: 173.4407134440038
Iteration: 5 || Loss: 173.34519892008674
Iteration: 6 || Loss: 173.34519892008674
saving ADAM checkpoint...
Sum of params:-98.85097
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 173.34519892008674
Iteration: 2 || Loss: 143.9954607211696
Iteration: 3 || Loss: 137.08681438821938
Iteration: 4 || Loss: 131.78256161014437
Iteration: 5 || Loss: 129.73458420104777
Iteration: 6 || Loss: 127.53929035131637
Iteration: 7 || Loss: 125.47370446256078
Iteration: 8 || Loss: 124.53098040099194
Iteration: 9 || Loss: 123.2026420331086
Iteration: 10 || Loss: 120.25566686828851
Iteration: 11 || Loss: 116.9803857573861
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.887314
Epoch 56 loss:116.9803857573861
MSE loss S16.444383991565438
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.887314
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 991.5097964275305
Iteration: 2 || Loss: 991.4626741766818
Iteration: 3 || Loss: 991.415903177094
Iteration: 4 || Loss: 991.3697085801966
Iteration: 5 || Loss: 991.3235954416208
Iteration: 6 || Loss: 991.3235954416208
saving ADAM checkpoint...
Sum of params:-98.88723
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 991.3235954416208
Iteration: 2 || Loss: 982.4537277704524
Iteration: 3 || Loss: 964.440497404812
Iteration: 4 || Loss: 939.2740299240302
Iteration: 5 || Loss: 926.9014071955651
Iteration: 6 || Loss: 924.7386279231766
Iteration: 7 || Loss: 919.11808885846
Iteration: 8 || Loss: 915.6275708735861
Iteration: 9 || Loss: 911.3812987738455
Iteration: 10 || Loss: 905.7147223565003
Iteration: 11 || Loss: 901.2376445772869
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.895134
Epoch 56 loss:901.2376445772869
MSE loss S67.36734894575488
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:159.0953306107089
MSE loss S - interpolation22.113454947147904
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4337.937786951806
MSE loss S - extrapolation670.7805111230174
waveform batch: 2/2
Test loss - extrapolation:1830.7343596981784
MSE loss S - extrapolation70.15393171147522
Epoch 56 mean train loss:37.261958438622486
Epoch 56 mean test loss - interpolation:26.515888435118153
Epoch 56 mean test loss - extrapolation:514.056012220832
Start training epoch 57
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.895134
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 405.1183125361714
Iteration: 2 || Loss: 404.8500628733849
Iteration: 3 || Loss: 404.5818191430174
Iteration: 4 || Loss: 404.31399005527584
Iteration: 5 || Loss: 404.0464164481323
Iteration: 6 || Loss: 404.0464164481323
saving ADAM checkpoint...
Sum of params:-98.895065
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 404.0464164481323
Iteration: 2 || Loss: 172.45353024914948
Iteration: 3 || Loss: 125.53465995657123
Iteration: 4 || Loss: 101.22714241464918
Iteration: 5 || Loss: 92.6316221089701
Iteration: 6 || Loss: 81.84740878614065
Iteration: 7 || Loss: 74.84030698721452
Iteration: 8 || Loss: 70.88218079814274
Iteration: 9 || Loss: 65.81931789848797
Iteration: 10 || Loss: 63.038957118644404
Iteration: 11 || Loss: 61.676911798922156
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.87472
Epoch 57 loss:61.676911798922156
MSE loss S14.206464439987421
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.87472
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 174.63729530096646
Iteration: 2 || Loss: 174.53775404960396
Iteration: 3 || Loss: 174.4387870125268
Iteration: 4 || Loss: 174.34020723743376
Iteration: 5 || Loss: 174.24143116270577
Iteration: 6 || Loss: 174.24143116270577
saving ADAM checkpoint...
Sum of params:-98.87486
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 174.24143116270577
Iteration: 2 || Loss: 142.722169203069
Iteration: 3 || Loss: 135.65670418275602
Iteration: 4 || Loss: 130.3093421719491
Iteration: 5 || Loss: 128.20501752517657
Iteration: 6 || Loss: 126.1633162972565
Iteration: 7 || Loss: 124.19663888423587
Iteration: 8 || Loss: 123.21442030261849
Iteration: 9 || Loss: 121.89319149667418
Iteration: 10 || Loss: 118.96793100409461
Iteration: 11 || Loss: 115.63740142119822
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.91196
Epoch 57 loss:115.63740142119822
MSE loss S16.198592744013418
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.91196
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 983.3005528853336
Iteration: 2 || Loss: 983.2555410874571
Iteration: 3 || Loss: 983.2103567589775
Iteration: 4 || Loss: 983.165240120858
Iteration: 5 || Loss: 983.1202661603254
Iteration: 6 || Loss: 983.1202661603254
saving ADAM checkpoint...
Sum of params:-98.91188
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 983.1202661603254
Iteration: 2 || Loss: 974.7404623437853
Iteration: 3 || Loss: 957.440997080519
Iteration: 4 || Loss: 932.5829512025309
Iteration: 5 || Loss: 920.180118966116
Iteration: 6 || Loss: 917.9719234656033
Iteration: 7 || Loss: 912.2707413455486
Iteration: 8 || Loss: 908.7384711330119
Iteration: 9 || Loss: 904.59790425648
Iteration: 10 || Loss: 898.8276743957148
Iteration: 11 || Loss: 894.5064922611085
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.91935
Epoch 57 loss:894.5064922611085
MSE loss S66.93850346628861
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:157.4804377770205
MSE loss S - interpolation21.95538692126509
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4303.87379245842
MSE loss S - extrapolation664.22492680325
waveform batch: 2/2
Test loss - extrapolation:1822.3371131961244
MSE loss S - extrapolation69.9694107558467
Epoch 57 mean train loss:36.959338120042375
Epoch 57 mean test loss - interpolation:26.24673962950342
Epoch 57 mean test loss - extrapolation:510.51757547121207
Start training epoch 58
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.91935
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 400.0879697296272
Iteration: 2 || Loss: 399.8227412273095
Iteration: 3 || Loss: 399.55726719187095
Iteration: 4 || Loss: 399.2921735119388
Iteration: 5 || Loss: 399.0268868130593
Iteration: 6 || Loss: 399.0268868130593
saving ADAM checkpoint...
Sum of params:-98.91926
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 399.0268868130593
Iteration: 2 || Loss: 170.51064980351128
Iteration: 3 || Loss: 123.94122885252622
Iteration: 4 || Loss: 100.0214339583277
Iteration: 5 || Loss: 91.55259698917115
Iteration: 6 || Loss: 80.82832393977515
Iteration: 7 || Loss: 73.90435592101502
Iteration: 8 || Loss: 70.00662696446477
Iteration: 9 || Loss: 65.04604973684623
Iteration: 10 || Loss: 62.54894318766075
Iteration: 11 || Loss: 61.05844992543008
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.900696
Epoch 58 loss:61.05844992543008
MSE loss S14.127544425525874
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.900696
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 172.68396389450288
Iteration: 2 || Loss: 172.5828306650252
Iteration: 3 || Loss: 172.48171958876884
Iteration: 4 || Loss: 172.38144402028064
Iteration: 5 || Loss: 172.28030187197606
Iteration: 6 || Loss: 172.28030187197606
saving ADAM checkpoint...
Sum of params:-98.90084
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 172.28030187197606
Iteration: 2 || Loss: 140.0801517694917
Iteration: 3 || Loss: 133.69265587339655
Iteration: 4 || Loss: 128.58260556759004
Iteration: 5 || Loss: 126.51267897222021
Iteration: 6 || Loss: 124.69287748536834
Iteration: 7 || Loss: 122.702666723805
Iteration: 8 || Loss: 121.78648326922165
Iteration: 9 || Loss: 120.50063990703525
Iteration: 10 || Loss: 117.51779960857067
Iteration: 11 || Loss: 114.20227915571022
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.93578
Epoch 58 loss:114.20227915571022
MSE loss S15.951659149961138
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.93578
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 975.5920294156078
Iteration: 2 || Loss: 975.5487286044847
Iteration: 3 || Loss: 975.5051990013899
Iteration: 4 || Loss: 975.4612976375781
Iteration: 5 || Loss: 975.417771685401
Iteration: 6 || Loss: 975.417771685401
saving ADAM checkpoint...
Sum of params:-98.93574
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 975.417771685401
Iteration: 2 || Loss: 967.4553781611751
Iteration: 3 || Loss: 950.9221760057385
Iteration: 4 || Loss: 926.3808947320348
Iteration: 5 || Loss: 913.9271095112142
Iteration: 6 || Loss: 911.6400744728811
Iteration: 7 || Loss: 905.7839456979159
Iteration: 8 || Loss: 902.1488202887493
Iteration: 9 || Loss: 898.1053652794959
Iteration: 10 || Loss: 892.2397637315908
Iteration: 11 || Loss: 888.0635906976574
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.9434
Epoch 58 loss:888.0635906976574
MSE loss S66.46254804061738
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:155.6256781960854
MSE loss S - interpolation21.743706654404914
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4266.844865159106
MSE loss S - extrapolation656.5135760227604
waveform batch: 2/2
Test loss - extrapolation:1815.1527309341031
MSE loss S - extrapolation69.7607501089904
Epoch 58 mean train loss:36.666355854441306
Epoch 58 mean test loss - interpolation:25.9376130326809
Epoch 58 mean test loss - extrapolation:506.83313300776746
Start training epoch 59
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.9434
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 393.29329345358565
Iteration: 2 || Loss: 393.0310534821117
Iteration: 3 || Loss: 392.7698801330986
Iteration: 4 || Loss: 392.50846195221357
Iteration: 5 || Loss: 392.2471548686411
Iteration: 6 || Loss: 392.2471548686411
saving ADAM checkpoint...
Sum of params:-98.943306
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 392.2471548686411
Iteration: 2 || Loss: 168.27763175183927
Iteration: 3 || Loss: 122.20853826239433
Iteration: 4 || Loss: 98.72524664491561
Iteration: 5 || Loss: 90.37970847057886
Iteration: 6 || Loss: 79.75426301153563
Iteration: 7 || Loss: 72.95335275647903
Iteration: 8 || Loss: 69.07799779269595
Iteration: 9 || Loss: 64.25444091299303
Iteration: 10 || Loss: 62.24387843514507
Iteration: 11 || Loss: 60.34686278306716
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.92616
Epoch 59 loss:60.34686278306716
MSE loss S13.995894189992367
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.92616
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 168.9336303360622
Iteration: 2 || Loss: 168.8344981664816
Iteration: 3 || Loss: 168.73497636662196
Iteration: 4 || Loss: 168.63604918251025
Iteration: 5 || Loss: 168.53742939897398
Iteration: 6 || Loss: 168.53742939897398
saving ADAM checkpoint...
Sum of params:-98.92633
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 168.53742939897398
Iteration: 2 || Loss: 137.66830327027782
Iteration: 3 || Loss: 131.68137564314256
Iteration: 4 || Loss: 126.81214949565155
Iteration: 5 || Loss: 124.84535817027997
Iteration: 6 || Loss: 123.09228655744406
Iteration: 7 || Loss: 121.18253132825802
Iteration: 8 || Loss: 120.28077882986781
Iteration: 9 || Loss: 119.00730119507139
Iteration: 10 || Loss: 115.99863505240054
Iteration: 11 || Loss: 112.82156632940507
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.95933
Epoch 59 loss:112.82156632940507
MSE loss S15.753756849180068
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.95933
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 968.0113949778183
Iteration: 2 || Loss: 967.9681121430059
Iteration: 3 || Loss: 967.9251467950854
Iteration: 4 || Loss: 967.8821615329023
Iteration: 5 || Loss: 967.8389669823645
Iteration: 6 || Loss: 967.8389669823645
saving ADAM checkpoint...
Sum of params:-98.95927
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 967.8389669823645
Iteration: 2 || Loss: 960.0874063550272
Iteration: 3 || Loss: 943.7163123579885
Iteration: 4 || Loss: 919.6102783575344
Iteration: 5 || Loss: 907.3607131333865
Iteration: 6 || Loss: 905.0882828675858
Iteration: 7 || Loss: 899.2856516398025
Iteration: 8 || Loss: 895.5631019678691
Iteration: 9 || Loss: 891.6100919780705
Iteration: 10 || Loss: 885.7970407448687
Iteration: 11 || Loss: 881.684835663431
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.96671
Epoch 59 loss:881.684835663431
MSE loss S65.96927493658958
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:153.77881280550952
MSE loss S - interpolation21.483571215083156
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4230.858498383085
MSE loss S - extrapolation648.771282557046
waveform batch: 2/2
Test loss - extrapolation:1807.2607266150494
MSE loss S - extrapolation69.46840827995918
Epoch 59 mean train loss:36.37425050951391
Epoch 59 mean test loss - interpolation:25.629802134251587
Epoch 59 mean test loss - extrapolation:503.17660208317784
Start training epoch 60
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.96671
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 387.01746880198624
Iteration: 2 || Loss: 386.75932318913357
Iteration: 3 || Loss: 386.49966899577026
Iteration: 4 || Loss: 386.2416105722553
Iteration: 5 || Loss: 385.98258184454505
Iteration: 6 || Loss: 385.98258184454505
saving ADAM checkpoint...
Sum of params:-98.96666
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 385.98258184454505
Iteration: 2 || Loss: 165.49289128554904
Iteration: 3 || Loss: 120.24050096238082
Iteration: 4 || Loss: 97.30324911138862
Iteration: 5 || Loss: 89.17159430317152
Iteration: 6 || Loss: 78.73677836541485
Iteration: 7 || Loss: 72.01510849483377
Iteration: 8 || Loss: 68.072851042319
Iteration: 9 || Loss: 63.53420272202189
Iteration: 10 || Loss: 61.77245188556914
Iteration: 11 || Loss: 59.65292486040424
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.950485
Epoch 60 loss:59.65292486040424
MSE loss S13.850321755716847
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.950485
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 166.22548212851987
Iteration: 2 || Loss: 166.128415368748
Iteration: 3 || Loss: 166.03039406433643
Iteration: 4 || Loss: 165.9327824870383
Iteration: 5 || Loss: 165.83528753991686
Iteration: 6 || Loss: 165.83528753991686
saving ADAM checkpoint...
Sum of params:-98.950645
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 165.83528753991686
Iteration: 2 || Loss: 135.8453830232523
Iteration: 3 || Loss: 130.00438926989506
Iteration: 4 || Loss: 125.27586988446863
Iteration: 5 || Loss: 123.3783855598695
Iteration: 6 || Loss: 121.65178731198222
Iteration: 7 || Loss: 119.83557335878946
Iteration: 8 || Loss: 118.92508216929718
Iteration: 9 || Loss: 117.6464359730375
Iteration: 10 || Loss: 114.64687107073307
Iteration: 11 || Loss: 111.57105088141705
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.98284
Epoch 60 loss:111.57105088141705
MSE loss S15.57077874856136
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-98.98284
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 960.2914789364875
Iteration: 2 || Loss: 960.2489333405236
Iteration: 3 || Loss: 960.2065835529161
Iteration: 4 || Loss: 960.1642830250358
Iteration: 5 || Loss: 960.1214239235079
Iteration: 6 || Loss: 960.1214239235079
saving ADAM checkpoint...
Sum of params:-98.98276
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 960.1214239235079
Iteration: 2 || Loss: 952.5648324836535
Iteration: 3 || Loss: 936.3580459638044
Iteration: 4 || Loss: 912.6768339880506
Iteration: 5 || Loss: 900.6525459546616
Iteration: 6 || Loss: 898.4009252682943
Iteration: 7 || Loss: 892.7202990642799
Iteration: 8 || Loss: 888.9498787021697
Iteration: 9 || Loss: 885.1215921415063
Iteration: 10 || Loss: 879.3602774954131
Iteration: 11 || Loss: 875.3092592561966
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.98988
Epoch 60 loss:875.3092592561966
MSE loss S65.47954774165188
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:152.07419721068018
MSE loss S - interpolation21.237493968493265
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4195.9910081646485
MSE loss S - extrapolation641.2575606726682
waveform batch: 2/2
Test loss - extrapolation:1798.8941325775297
MSE loss S - extrapolation69.14628308329833
Epoch 60 mean train loss:36.087352930966134
Epoch 60 mean test loss - interpolation:25.345699535113365
Epoch 60 mean test loss - extrapolation:499.57376172851485
Start training epoch 61
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-98.98988
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 381.31925799460834
Iteration: 2 || Loss: 381.06302445818545
Iteration: 3 || Loss: 380.8063592806044
Iteration: 4 || Loss: 380.5502014380932
Iteration: 5 || Loss: 380.29440574308694
Iteration: 6 || Loss: 380.29440574308694
saving ADAM checkpoint...
Sum of params:-98.9898
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 380.29440574308694
Iteration: 2 || Loss: 162.90071530981342
Iteration: 3 || Loss: 118.41601443701009
Iteration: 4 || Loss: 95.99245041254903
Iteration: 5 || Loss: 88.06327191731575
Iteration: 6 || Loss: 77.80586974533477
Iteration: 7 || Loss: 71.15647443480059
Iteration: 8 || Loss: 67.11530658668293
Iteration: 9 || Loss: 62.87590472911631
Iteration: 10 || Loss: 61.2452005195119
Iteration: 11 || Loss: 59.012288930059654
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.974396
Epoch 61 loss:59.012288930059654
MSE loss S13.710072056452093
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.974396
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 164.034930616256
Iteration: 2 || Loss: 163.938098610427
Iteration: 3 || Loss: 163.84176897500882
Iteration: 4 || Loss: 163.74515907308512
Iteration: 5 || Loss: 163.6494710476994
Iteration: 6 || Loss: 163.6494710476994
saving ADAM checkpoint...
Sum of params:-98.97455
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 163.6494710476994
Iteration: 2 || Loss: 134.26193340037952
Iteration: 3 || Loss: 128.51824267058126
Iteration: 4 || Loss: 123.89782633601801
Iteration: 5 || Loss: 122.05529994739585
Iteration: 6 || Loss: 120.3506177020904
Iteration: 7 || Loss: 118.61181681178277
Iteration: 8 || Loss: 117.69054850383225
Iteration: 9 || Loss: 116.40536792950438
Iteration: 10 || Loss: 113.41426510904122
Iteration: 11 || Loss: 110.41013985869988
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.00617
Epoch 61 loss:110.41013985869988
MSE loss S15.395857191749341
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.00617
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 952.6106779966073
Iteration: 2 || Loss: 952.5683819671301
Iteration: 3 || Loss: 952.5263338470447
Iteration: 4 || Loss: 952.4842875885851
Iteration: 5 || Loss: 952.4421909627424
Iteration: 6 || Loss: 952.4421909627424
saving ADAM checkpoint...
Sum of params:-99.0061
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 952.4421909627424
Iteration: 2 || Loss: 945.0761718738836
Iteration: 3 || Loss: 929.047428194509
Iteration: 4 || Loss: 905.7640252897498
Iteration: 5 || Loss: 893.9645229027018
Iteration: 6 || Loss: 891.7263843808776
Iteration: 7 || Loss: 886.168273407393
Iteration: 8 || Loss: 882.3495584901764
Iteration: 9 || Loss: 878.6610551007157
Iteration: 10 || Loss: 872.9565829234558
Iteration: 11 || Loss: 868.9719516826189
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.01289
Epoch 61 loss:868.9719516826189
MSE loss S64.98660270750484
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:150.41795061062177
MSE loss S - interpolation20.9956550194216
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4161.630403844219
MSE loss S - extrapolation633.8204528159505
waveform batch: 2/2
Test loss - extrapolation:1790.4216057559634
MSE loss S - extrapolation68.80725262469008
Epoch 61 mean train loss:35.80670277487511
Epoch 61 mean test loss - interpolation:25.06965843510363
Epoch 61 mean test loss - extrapolation:496.0043341333485
Start training epoch 62
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.01289
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 375.7193985961578
Iteration: 2 || Loss: 375.46562924594383
Iteration: 3 || Loss: 375.21187943135703
Iteration: 4 || Loss: 374.958074624214
Iteration: 5 || Loss: 374.70477540301476
Iteration: 6 || Loss: 374.70477540301476
saving ADAM checkpoint...
Sum of params:-99.01281
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 374.70477540301476
Iteration: 2 || Loss: 160.38538010203024
Iteration: 3 || Loss: 116.64534067739747
Iteration: 4 || Loss: 94.72591103389863
Iteration: 5 || Loss: 86.99040536035841
Iteration: 6 || Loss: 76.90813560653656
Iteration: 7 || Loss: 70.34441557169006
Iteration: 8 || Loss: 66.20263529253904
Iteration: 9 || Loss: 62.22817854890344
Iteration: 10 || Loss: 60.681379238841636
Iteration: 11 || Loss: 58.39021535668158
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-98.99811
Epoch 62 loss:58.39021535668158
MSE loss S13.57062480412
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-98.99811
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 162.07382187431554
Iteration: 2 || Loss: 161.9792290696588
Iteration: 3 || Loss: 161.88318011298813
Iteration: 4 || Loss: 161.78792996299455
Iteration: 5 || Loss: 161.69234669327696
Iteration: 6 || Loss: 161.69234669327696
saving ADAM checkpoint...
Sum of params:-98.99827
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 161.69234669327696
Iteration: 2 || Loss: 132.78146160175956
Iteration: 3 || Loss: 127.12277119813575
Iteration: 4 || Loss: 122.5957701654375
Iteration: 5 || Loss: 120.79861458646103
Iteration: 6 || Loss: 119.11416916512867
Iteration: 7 || Loss: 117.43454179951917
Iteration: 8 || Loss: 116.51098028522934
Iteration: 9 || Loss: 115.22416877022768
Iteration: 10 || Loss: 112.24547022697809
Iteration: 11 || Loss: 109.30121671202535
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.029366
Epoch 62 loss:109.30121671202535
MSE loss S15.225110662584454
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.029366
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 944.9949319634933
Iteration: 2 || Loss: 944.9536361011984
Iteration: 3 || Loss: 944.9116781811877
Iteration: 4 || Loss: 944.8702526505672
Iteration: 5 || Loss: 944.8292420879815
Iteration: 6 || Loss: 944.8292420879815
saving ADAM checkpoint...
Sum of params:-99.0293
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 944.8292420879815
Iteration: 2 || Loss: 937.653462633263
Iteration: 3 || Loss: 921.8116246809678
Iteration: 4 || Loss: 898.9124508267465
Iteration: 5 || Loss: 887.3296802159324
Iteration: 6 || Loss: 885.0955025600367
Iteration: 7 || Loss: 879.6682368749656
Iteration: 8 || Loss: 875.8026324013191
Iteration: 9 || Loss: 872.2598956110005
Iteration: 10 || Loss: 866.6133975393811
Iteration: 11 || Loss: 862.6948040199411
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.035835
Epoch 62 loss:862.6948040199411
MSE loss S64.49703371726342
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:148.8020834403209
MSE loss S - interpolation20.759344968039603
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4127.805042819683
MSE loss S - extrapolation626.5046845142142
waveform batch: 2/2
Test loss - extrapolation:1781.9118702966464
MSE loss S - extrapolation68.46017213601573
Epoch 62 mean train loss:35.530559865125795
Epoch 62 mean test loss - interpolation:24.800347240053483
Epoch 62 mean test loss - extrapolation:492.47640942636076
Start training epoch 63
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.035835
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 370.2083429496573
Iteration: 2 || Loss: 369.95630497905535
Iteration: 3 || Loss: 369.70517999168476
Iteration: 4 || Loss: 369.4537973590972
Iteration: 5 || Loss: 369.2029271275502
Iteration: 6 || Loss: 369.2029271275502
saving ADAM checkpoint...
Sum of params:-99.035774
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 369.2029271275502
Iteration: 2 || Loss: 157.9431270482061
Iteration: 3 || Loss: 114.92121687442071
Iteration: 4 || Loss: 93.49970445241962
Iteration: 5 || Loss: 85.94866686368015
Iteration: 6 || Loss: 76.03803022342935
Iteration: 7 || Loss: 69.57050414431977
Iteration: 8 || Loss: 65.34036599352999
Iteration: 9 || Loss: 61.577849422936126
Iteration: 10 || Loss: 60.09999359335568
Iteration: 11 || Loss: 57.783013851063
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.02166
Epoch 63 loss:57.783013851063
MSE loss S13.431890647694432
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.02166
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 160.24691167525293
Iteration: 2 || Loss: 160.15251684532487
Iteration: 3 || Loss: 160.05772865416375
Iteration: 4 || Loss: 159.96344606921355
Iteration: 5 || Loss: 159.86886178633017
Iteration: 6 || Loss: 159.86886178633017
saving ADAM checkpoint...
Sum of params:-99.02182
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 159.86886178633017
Iteration: 2 || Loss: 131.3704329077436
Iteration: 3 || Loss: 125.7913602421413
Iteration: 4 || Loss: 121.3496846100134
Iteration: 5 || Loss: 119.59074644748425
Iteration: 6 || Loss: 117.9229590845353
Iteration: 7 || Loss: 116.28366795041666
Iteration: 8 || Loss: 115.36806800917614
Iteration: 9 || Loss: 114.0855078420358
Iteration: 10 || Loss: 111.1240623341
Iteration: 11 || Loss: 108.23429634939339
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.05237
Epoch 63 loss:108.23429634939339
MSE loss S15.06100370515866
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.05237
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 937.4365121955502
Iteration: 2 || Loss: 937.3956806535506
Iteration: 3 || Loss: 937.3545358414375
Iteration: 4 || Loss: 937.3138946543163
Iteration: 5 || Loss: 937.2733458858163
Iteration: 6 || Loss: 937.2733458858163
saving ADAM checkpoint...
Sum of params:-99.05229
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 937.2733458858163
Iteration: 2 || Loss: 930.2788653249258
Iteration: 3 || Loss: 914.6102447377419
Iteration: 4 || Loss: 892.0923307792182
Iteration: 5 || Loss: 880.7382068778963
Iteration: 6 || Loss: 878.501825914469
Iteration: 7 || Loss: 873.2241686707541
Iteration: 8 || Loss: 869.3089283118615
Iteration: 9 || Loss: 865.9168994634556
Iteration: 10 || Loss: 860.3364118978288
Iteration: 11 || Loss: 856.4812029770682
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.0586
Epoch 63 loss:856.4812029770682
MSE loss S64.01214946663526
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:147.2251294845277
MSE loss S - interpolation20.527394679233222
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4094.549576710018
MSE loss S - extrapolation619.3237844260781
waveform batch: 2/2
Test loss - extrapolation:1773.3541290771886
MSE loss S - extrapolation68.10639250018939
Epoch 63 mean train loss:35.258569419914636
Epoch 63 mean test loss - interpolation:24.537521580754618
Epoch 63 mean test loss - extrapolation:488.9919754822672
Start training epoch 64
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.0586
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 364.82062322448496
Iteration: 2 || Loss: 364.5711159747429
Iteration: 3 || Loss: 364.32246614535615
Iteration: 4 || Loss: 364.07348477467997
Iteration: 5 || Loss: 363.82532689824876
Iteration: 6 || Loss: 363.82532689824876
saving ADAM checkpoint...
Sum of params:-99.05851
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 363.82532689824876
Iteration: 2 || Loss: 155.55977061104812
Iteration: 3 || Loss: 113.23354055264676
Iteration: 4 || Loss: 92.30309105669397
Iteration: 5 || Loss: 84.9307918087909
Iteration: 6 || Loss: 75.19431878293854
Iteration: 7 || Loss: 68.82922989008965
Iteration: 8 || Loss: 64.53536616936613
Iteration: 9 || Loss: 60.91753576769371
Iteration: 10 || Loss: 59.51005561122112
Iteration: 11 || Loss: 57.18969093702062
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.04506
Epoch 64 loss:57.18969093702062
MSE loss S13.29297850757346
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.04506
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 158.4979435573239
Iteration: 2 || Loss: 158.4035721108708
Iteration: 3 || Loss: 158.31020957144707
Iteration: 4 || Loss: 158.21698273356833
Iteration: 5 || Loss: 158.12339606389628
Iteration: 6 || Loss: 158.12339606389628
saving ADAM checkpoint...
Sum of params:-99.04521
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 158.12339606389628
Iteration: 2 || Loss: 130.01804479935876
Iteration: 3 || Loss: 124.51988052975642
Iteration: 4 || Loss: 120.15658996794939
Iteration: 5 || Loss: 118.42985513108351
Iteration: 6 || Loss: 116.7748854408121
Iteration: 7 || Loss: 115.15289190434264
Iteration: 8 || Loss: 114.25816390863854
Iteration: 9 || Loss: 112.98786320894808
Iteration: 10 || Loss: 110.04692515965432
Iteration: 11 || Loss: 107.20590535474119
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.07522
Epoch 64 loss:107.20590535474119
MSE loss S14.902113605179022
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.07522
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 929.9439099150287
Iteration: 2 || Loss: 929.9038123380617
Iteration: 3 || Loss: 929.8637452977522
Iteration: 4 || Loss: 929.8233515759379
Iteration: 5 || Loss: 929.7830711308912
Iteration: 6 || Loss: 929.7830711308912
saving ADAM checkpoint...
Sum of params:-99.07516
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 929.7830711308912
Iteration: 2 || Loss: 922.9613442253066
Iteration: 3 || Loss: 907.4538821328706
Iteration: 4 || Loss: 885.3061055814222
Iteration: 5 || Loss: 874.1915748144705
Iteration: 6 || Loss: 871.9464545135643
Iteration: 7 || Loss: 866.8350216837381
Iteration: 8 || Loss: 862.871681861104
Iteration: 9 || Loss: 859.6310952550353
Iteration: 10 || Loss: 854.1238529155689
Iteration: 11 || Loss: 850.32628188967
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.08127
Epoch 64 loss:850.32628188967
MSE loss S63.53223316661562
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:145.69068838030924
MSE loss S - interpolation20.30066681087349
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4061.88913343733
MSE loss S - extrapolation612.2896290807967
waveform batch: 2/2
Test loss - extrapolation:1764.7353329778384
MSE loss S - extrapolation67.74636232382053
Epoch 64 mean train loss:34.990409592463166
Epoch 64 mean test loss - interpolation:24.281781396718205
Epoch 64 mean test loss - extrapolation:485.5520388679308
Start training epoch 65
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.08127
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 359.5761375031231
Iteration: 2 || Loss: 359.32929401877317
Iteration: 3 || Loss: 359.08288274766073
Iteration: 4 || Loss: 358.83656637371183
Iteration: 5 || Loss: 358.59051160924867
Iteration: 6 || Loss: 358.59051160924867
saving ADAM checkpoint...
Sum of params:-99.08117
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 358.59051160924867
Iteration: 2 || Loss: 153.23530940729435
Iteration: 3 || Loss: 111.58744385638259
Iteration: 4 || Loss: 91.13938118146298
Iteration: 5 || Loss: 83.93989987581489
Iteration: 6 || Loss: 74.37589448401309
Iteration: 7 || Loss: 68.11672755126573
Iteration: 8 || Loss: 63.78311485725179
Iteration: 9 || Loss: 60.24847253161112
Iteration: 10 || Loss: 58.91968562669547
Iteration: 11 || Loss: 56.61268337292165
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.06829
Epoch 65 loss:56.61268337292165
MSE loss S13.155124514042202
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.06829
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 156.81084146611823
Iteration: 2 || Loss: 156.7177821736356
Iteration: 3 || Loss: 156.6253909297538
Iteration: 4 || Loss: 156.53281160372669
Iteration: 5 || Loss: 156.4397449990192
Iteration: 6 || Loss: 156.4397449990192
saving ADAM checkpoint...
Sum of params:-99.06844
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 156.4397449990192
Iteration: 2 || Loss: 128.72062568187698
Iteration: 3 || Loss: 123.30294591425394
Iteration: 4 || Loss: 119.01394143012138
Iteration: 5 || Loss: 117.31503673865423
Iteration: 6 || Loss: 115.66926187496355
Iteration: 7 || Loss: 114.04721293141954
Iteration: 8 || Loss: 113.18041154247275
Iteration: 9 || Loss: 111.9294833727692
Iteration: 10 || Loss: 109.01137195198717
Iteration: 11 || Loss: 106.21566342329436
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.097916
Epoch 65 loss:106.21566342329436
MSE loss S14.749683820396829
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.097916
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 922.528130219542
Iteration: 2 || Loss: 922.4880265403083
Iteration: 3 || Loss: 922.4480684627413
Iteration: 4 || Loss: 922.4086661378692
Iteration: 5 || Loss: 922.3685685862011
Iteration: 6 || Loss: 922.3685685862011
saving ADAM checkpoint...
Sum of params:-99.09786
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 922.3685685862011
Iteration: 2 || Loss: 915.7072035329445
Iteration: 3 || Loss: 900.3367411177554
Iteration: 4 || Loss: 878.5544876472243
Iteration: 5 || Loss: 867.6850897153847
Iteration: 6 || Loss: 865.4236517757985
Iteration: 7 || Loss: 860.4960699022438
Iteration: 8 || Loss: 856.4809320959724
Iteration: 9 || Loss: 853.3953897780193
Iteration: 10 || Loss: 847.9707381690563
Iteration: 11 || Loss: 844.2265819309107
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.103775
Epoch 65 loss:844.2265819309107
MSE loss S63.05515414981046
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:144.19369308376312
MSE loss S - interpolation20.077828852229622
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:4029.769051331579
MSE loss S - extrapolation605.3849909081881
waveform batch: 2/2
Test loss - extrapolation:1756.06595699688
MSE loss S - extrapolation67.37877275810291
Epoch 65 mean train loss:34.72603202507334
Epoch 65 mean test loss - interpolation:24.03228218062719
Epoch 65 mean test loss - extrapolation:482.15291736070486
Start training epoch 66
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.103775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 354.4551757066755
Iteration: 2 || Loss: 354.21093612538374
Iteration: 3 || Loss: 353.9670674286446
Iteration: 4 || Loss: 353.72295105310377
Iteration: 5 || Loss: 353.478817451625
Iteration: 6 || Loss: 353.478817451625
saving ADAM checkpoint...
Sum of params:-99.10368
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 353.478817451625
Iteration: 2 || Loss: 150.97057057862625
Iteration: 3 || Loss: 109.98041711886428
Iteration: 4 || Loss: 90.00339992182552
Iteration: 5 || Loss: 82.97107084196372
Iteration: 6 || Loss: 73.58051433532422
Iteration: 7 || Loss: 67.42857995485892
Iteration: 8 || Loss: 63.08210484717014
Iteration: 9 || Loss: 59.576896436402464
Iteration: 10 || Loss: 58.32991631697807
Iteration: 11 || Loss: 56.05035224922556
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.09138
Epoch 66 loss:56.05035224922556
MSE loss S13.018604113849115
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.09138
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 155.1506497576988
Iteration: 2 || Loss: 155.0583334510761
Iteration: 3 || Loss: 154.96634431223433
Iteration: 4 || Loss: 154.87504166597753
Iteration: 5 || Loss: 154.78326145441937
Iteration: 6 || Loss: 154.78326145441937
saving ADAM checkpoint...
Sum of params:-99.09155
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 154.78326145441937
Iteration: 2 || Loss: 127.46935143868764
Iteration: 3 || Loss: 122.1325425302777
Iteration: 4 || Loss: 117.91578329366276
Iteration: 5 || Loss: 116.24260447376852
Iteration: 6 || Loss: 114.6020920734881
Iteration: 7 || Loss: 112.96916903037932
Iteration: 8 || Loss: 112.13078695805763
Iteration: 9 || Loss: 110.90748500703653
Iteration: 10 || Loss: 108.01414882674567
Iteration: 11 || Loss: 105.26062240709378
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.12049
Epoch 66 loss:105.26062240709378
MSE loss S14.603478675929955
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.12049
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 915.1918796785304
Iteration: 2 || Loss: 915.1523676258468
Iteration: 3 || Loss: 915.1126974639837
Iteration: 4 || Loss: 915.073513256988
Iteration: 5 || Loss: 915.0345965524398
Iteration: 6 || Loss: 915.0345965524398
saving ADAM checkpoint...
Sum of params:-99.12042
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 915.0345965524398
Iteration: 2 || Loss: 908.5231187385814
Iteration: 3 || Loss: 893.2735437754997
Iteration: 4 || Loss: 871.8433107317802
Iteration: 5 || Loss: 861.225926528334
Iteration: 6 || Loss: 858.9409474964535
Iteration: 7 || Loss: 854.2089637255544
Iteration: 8 || Loss: 850.1362928956243
Iteration: 9 || Loss: 847.2091488445078
Iteration: 10 || Loss: 841.8762329477871
Iteration: 11 || Loss: 838.1814466339999
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.12619
Epoch 66 loss:838.1814466339999
MSE loss S62.58145152903836
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:142.7366547144526
MSE loss S - interpolation19.859817604576374
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3998.267113239512
MSE loss S - extrapolation598.6387439834525
waveform batch: 2/2
Test loss - extrapolation:1747.3515395135323
MSE loss S - extrapolation67.00638123469128
Epoch 66 mean train loss:34.46525590656273
Epoch 66 mean test loss - interpolation:23.789442452408768
Epoch 66 mean test loss - extrapolation:478.80155439608706
Start training epoch 67
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.12619
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 349.4930151245937
Iteration: 2 || Loss: 349.25117616434585
Iteration: 3 || Loss: 349.00926475106604
Iteration: 4 || Loss: 348.7674004284347
Iteration: 5 || Loss: 348.5263801367488
Iteration: 6 || Loss: 348.5263801367488
saving ADAM checkpoint...
Sum of params:-99.12612
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 348.5263801367488
Iteration: 2 || Loss: 148.77077110076186
Iteration: 3 || Loss: 108.41538457314145
Iteration: 4 || Loss: 88.89758798488512
Iteration: 5 || Loss: 82.02488849850548
Iteration: 6 || Loss: 72.80741251634305
Iteration: 7 || Loss: 66.76210705027236
Iteration: 8 || Loss: 62.4239243638929
Iteration: 9 || Loss: 58.914403652935654
Iteration: 10 || Loss: 57.74244113162051
Iteration: 11 || Loss: 55.50305220642998
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.11433
Epoch 67 loss:55.50305220642998
MSE loss S12.884902523107359
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.11433
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 153.52905841360018
Iteration: 2 || Loss: 153.43792379266992
Iteration: 3 || Loss: 153.34696037136462
Iteration: 4 || Loss: 153.25594859163797
Iteration: 5 || Loss: 153.16551205798848
Iteration: 6 || Loss: 153.16551205798848
saving ADAM checkpoint...
Sum of params:-99.114494
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 153.16551205798848
Iteration: 2 || Loss: 126.26161223568894
Iteration: 3 || Loss: 121.00225021848586
Iteration: 4 || Loss: 116.85519731635668
Iteration: 5 || Loss: 115.2074346536015
Iteration: 6 || Loss: 113.57042269443933
Iteration: 7 || Loss: 111.92496207301927
Iteration: 8 || Loss: 111.1070818951911
Iteration: 9 || Loss: 109.91771827290047
Iteration: 10 || Loss: 107.05138721220021
Iteration: 11 || Loss: 104.33641667612083
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.14291
Epoch 67 loss:104.33641667612083
MSE loss S14.462167794300802
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.14291
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 907.9495855585536
Iteration: 2 || Loss: 907.9102395081941
Iteration: 3 || Loss: 907.8712402629285
Iteration: 4 || Loss: 907.8319337380104
Iteration: 5 || Loss: 907.7929707340411
Iteration: 6 || Loss: 907.7929707340411
saving ADAM checkpoint...
Sum of params:-99.142845
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 907.7929707340411
Iteration: 2 || Loss: 901.4235092027664
Iteration: 3 || Loss: 886.2838088384536
Iteration: 4 || Loss: 865.1935837177892
Iteration: 5 || Loss: 854.8247681836259
Iteration: 6 || Loss: 852.5017399634972
Iteration: 7 || Loss: 847.97895278082
Iteration: 8 || Loss: 843.8443853157805
Iteration: 9 || Loss: 841.0757537865785
Iteration: 10 || Loss: 835.841639034658
Iteration: 11 || Loss: 832.1932992869351
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.14855
Epoch 67 loss:832.1932992869351
MSE loss S62.10973605873473
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:141.31014174675337
MSE loss S - interpolation19.64567589954023
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3967.1953695063166
MSE loss S - extrapolation591.9949112743627
waveform batch: 2/2
Test loss - extrapolation:1738.634927793571
MSE loss S - extrapolation66.62885123746045
Epoch 67 mean train loss:34.20802648860296
Epoch 67 mean test loss - interpolation:23.551690291125563
Epoch 67 mean test loss - extrapolation:475.485858108324
Start training epoch 68
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.14855
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 344.60633956922936
Iteration: 2 || Loss: 344.3667247434052
Iteration: 3 || Loss: 344.1269127803986
Iteration: 4 || Loss: 343.8877263979475
Iteration: 5 || Loss: 343.64845340741135
Iteration: 6 || Loss: 343.64845340741135
saving ADAM checkpoint...
Sum of params:-99.14847
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 343.64845340741135
Iteration: 2 || Loss: 146.6376326989369
Iteration: 3 || Loss: 106.89388066242672
Iteration: 4 || Loss: 87.81948882766025
Iteration: 5 || Loss: 81.09934460029885
Iteration: 6 || Loss: 72.05342250265323
Iteration: 7 || Loss: 66.11118339103234
Iteration: 8 || Loss: 61.7941520772452
Iteration: 9 || Loss: 58.268591674544766
Iteration: 10 || Loss: 57.15450174563985
Iteration: 11 || Loss: 54.967000069128744
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.13714
Epoch 68 loss:54.967000069128744
MSE loss S12.754282908402619
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.13714
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 151.9478204922075
Iteration: 2 || Loss: 151.85761119402355
Iteration: 3 || Loss: 151.76791037791884
Iteration: 4 || Loss: 151.67792669347352
Iteration: 5 || Loss: 151.58810314295582
Iteration: 6 || Loss: 151.58810314295582
saving ADAM checkpoint...
Sum of params:-99.13731
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 151.58810314295582
Iteration: 2 || Loss: 125.08561661385757
Iteration: 3 || Loss: 119.89954480261908
Iteration: 4 || Loss: 115.82219820848954
Iteration: 5 || Loss: 114.19905664283479
Iteration: 6 || Loss: 112.56621249994659
Iteration: 7 || Loss: 110.91482932457679
Iteration: 8 || Loss: 110.10499104079449
Iteration: 9 || Loss: 108.95379589630572
Iteration: 10 || Loss: 106.11362668026442
Iteration: 11 || Loss: 103.43374555437141
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.165215
Epoch 68 loss:103.43374555437141
MSE loss S14.323469125417677
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.165215
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 900.8144642760203
Iteration: 2 || Loss: 900.7756902125898
Iteration: 3 || Loss: 900.737197376432
Iteration: 4 || Loss: 900.6987816331853
Iteration: 5 || Loss: 900.6601641292941
Iteration: 6 || Loss: 900.6601641292941
saving ADAM checkpoint...
Sum of params:-99.16514
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 900.6601641292941
Iteration: 2 || Loss: 894.4297270412144
Iteration: 3 || Loss: 879.4016792245199
Iteration: 4 || Loss: 858.6416390064951
Iteration: 5 || Loss: 848.5104533922109
Iteration: 6 || Loss: 846.1250680223378
Iteration: 7 || Loss: 841.8237037752259
Iteration: 8 || Loss: 837.6185715629916
Iteration: 9 || Loss: 835.0075360635014
Iteration: 10 || Loss: 829.8742402192537
Iteration: 11 || Loss: 826.2693813277521
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.17082
Epoch 68 loss:826.2693813277521
MSE loss S61.64024817718639
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:139.9054171364503
MSE loss S - interpolation19.43510374749457
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3936.44153775279
MSE loss S - extrapolation585.4262154390352
waveform batch: 2/2
Test loss - extrapolation:1729.976217865139
MSE loss S - extrapolation66.2508745615825
Epoch 68 mean train loss:33.95414230866387
Epoch 68 mean test loss - interpolation:23.317569522741717
Epoch 68 mean test loss - extrapolation:472.2014796348274
Start training epoch 69
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.17082
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 339.73862992921306
Iteration: 2 || Loss: 339.50144622469855
Iteration: 3 || Loss: 339.2638661063829
Iteration: 4 || Loss: 339.0272440486441
Iteration: 5 || Loss: 338.78996334101856
Iteration: 6 || Loss: 338.78996334101856
saving ADAM checkpoint...
Sum of params:-99.170746
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 338.78996334101856
Iteration: 2 || Loss: 144.56650454933083
Iteration: 3 || Loss: 105.41186694154744
Iteration: 4 || Loss: 86.7669406137018
Iteration: 5 || Loss: 80.18999488289923
Iteration: 6 || Loss: 71.31191759889941
Iteration: 7 || Loss: 65.47164970073905
Iteration: 8 || Loss: 61.183692510352216
Iteration: 9 || Loss: 57.64209897343598
Iteration: 10 || Loss: 56.564937131464475
Iteration: 11 || Loss: 54.43820211743587
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.159805
Epoch 69 loss:54.43820211743587
MSE loss S12.626470918474835
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.159805
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 150.40479249646407
Iteration: 2 || Loss: 150.31553709927044
Iteration: 3 || Loss: 150.22666412661513
Iteration: 4 || Loss: 150.13751357891664
Iteration: 5 || Loss: 150.04854511943606
Iteration: 6 || Loss: 150.04854511943606
saving ADAM checkpoint...
Sum of params:-99.159996
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 150.04854511943606
Iteration: 2 || Loss: 123.93256149658686
Iteration: 3 || Loss: 118.81401039598278
Iteration: 4 || Loss: 114.8059595157959
Iteration: 5 || Loss: 113.20784336786863
Iteration: 6 || Loss: 111.57973204141479
Iteration: 7 || Loss: 109.92861853261337
Iteration: 8 || Loss: 109.11577778681952
Iteration: 9 || Loss: 108.00644328530913
Iteration: 10 || Loss: 105.19313547600409
Iteration: 11 || Loss: 102.5473716873787
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.18741
Epoch 69 loss:102.5473716873787
MSE loss S14.187044589104561
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.18741
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 893.8005380418015
Iteration: 2 || Loss: 893.7622663227789
Iteration: 3 || Loss: 893.7247267164877
Iteration: 4 || Loss: 893.6863935655663
Iteration: 5 || Loss: 893.6483577922747
Iteration: 6 || Loss: 893.6483577922747
saving ADAM checkpoint...
Sum of params:-99.187325
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 893.6483577922747
Iteration: 2 || Loss: 887.5516769299969
Iteration: 3 || Loss: 872.6395994949753
Iteration: 4 || Loss: 852.1872120223491
Iteration: 5 || Loss: 842.2843336139516
Iteration: 6 || Loss: 839.8067002894905
Iteration: 7 || Loss: 835.7457010875047
Iteration: 8 || Loss: 831.4577305132443
Iteration: 9 || Loss: 829.005759919032
Iteration: 10 || Loss: 823.9774980634032
Iteration: 11 || Loss: 820.41132430229
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.19304
Epoch 69 loss:820.41132430229
MSE loss S61.17300680645906
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:138.5256449541408
MSE loss S - interpolation19.228049290406574
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3905.9773872335804
MSE loss S - extrapolation578.9219208911486
waveform batch: 2/2
Test loss - extrapolation:1721.3685119549245
MSE loss S - extrapolation65.87178248107323
Epoch 69 mean train loss:33.70334131403809
Epoch 69 mean test loss - interpolation:23.0876074923568
Epoch 69 mean test loss - extrapolation:468.9454915990421
Start training epoch 70
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.19304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 334.89784239433135
Iteration: 2 || Loss: 334.66320828439854
Iteration: 3 || Loss: 334.4273966462434
Iteration: 4 || Loss: 334.19274763626134
Iteration: 5 || Loss: 333.95748106033756
Iteration: 6 || Loss: 333.95748106033756
saving ADAM checkpoint...
Sum of params:-99.19286
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 333.95748106033756
Iteration: 2 || Loss: 142.51537689610186
Iteration: 3 || Loss: 103.94980150941119
Iteration: 4 || Loss: 85.73538132297473
Iteration: 5 || Loss: 79.29704009103386
Iteration: 6 || Loss: 70.58466107499582
Iteration: 7 || Loss: 64.84083300366962
Iteration: 8 || Loss: 60.583375987233765
Iteration: 9 || Loss: 57.02948368239494
Iteration: 10 || Loss: 55.96539490898866
Iteration: 11 || Loss: 53.916823224300025
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.18231
Epoch 70 loss:53.916823224300025
MSE loss S12.50075305269971
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.18231
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 148.8926811199513
Iteration: 2 || Loss: 148.80467829597472
Iteration: 3 || Loss: 148.7162956806691
Iteration: 4 || Loss: 148.62813918782354
Iteration: 5 || Loss: 148.5401626780457
Iteration: 6 || Loss: 148.5401626780457
saving ADAM checkpoint...
Sum of params:-99.18248
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 148.5401626780457
Iteration: 2 || Loss: 122.80255114964129
Iteration: 3 || Loss: 117.74782557678745
Iteration: 4 || Loss: 113.80760388994346
Iteration: 5 || Loss: 112.23346878021549
Iteration: 6 || Loss: 110.61092395723814
Iteration: 7 || Loss: 108.9611588857957
Iteration: 8 || Loss: 108.1320896537836
Iteration: 9 || Loss: 107.07549406291251
Iteration: 10 || Loss: 104.29003451819064
Iteration: 11 || Loss: 101.67784118415643
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.20944
Epoch 70 loss:101.67784118415643
MSE loss S14.052774942588048
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.20944
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 886.8727695724102
Iteration: 2 || Loss: 886.8349503994281
Iteration: 3 || Loss: 886.7978767571642
Iteration: 4 || Loss: 886.7601969230742
Iteration: 5 || Loss: 886.7223919288616
Iteration: 6 || Loss: 886.7223919288616
saving ADAM checkpoint...
Sum of params:-99.209366
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 886.7223919288616
Iteration: 2 || Loss: 880.758258990747
Iteration: 3 || Loss: 865.9704419878004
Iteration: 4 || Loss: 845.8188750285236
Iteration: 5 || Loss: 836.1368448976046
Iteration: 6 || Loss: 833.537614621853
Iteration: 7 || Loss: 829.7389433892474
Iteration: 8 || Loss: 825.3677354167108
Iteration: 9 || Loss: 823.0668827925003
Iteration: 10 || Loss: 818.1484069064666
Iteration: 11 || Loss: 814.616893814355
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.21514
Epoch 70 loss:814.616893814355
MSE loss S60.70842559588225
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:137.17017749541426
MSE loss S - interpolation19.024640591734318
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3875.8208685441687
MSE loss S - extrapolation572.4895914681563
waveform batch: 2/2
Test loss - extrapolation:1712.813641513465
MSE loss S - extrapolation65.49258233603666
Epoch 70 mean train loss:33.455570973200395
Epoch 70 mean test loss - interpolation:22.86169624923571
Epoch 70 mean test loss - extrapolation:465.7195425048028
Start training epoch 71
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.21514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 330.1009111039087
Iteration: 2 || Loss: 329.86720414875043
Iteration: 3 || Loss: 329.6337911636641
Iteration: 4 || Loss: 329.40038986519386
Iteration: 5 || Loss: 329.167112569766
Iteration: 6 || Loss: 329.167112569766
saving ADAM checkpoint...
Sum of params:-99.21498
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 329.167112569766
Iteration: 2 || Loss: 140.57670069877446
Iteration: 3 || Loss: 102.5568323862742
Iteration: 4 || Loss: 84.73691931496128
Iteration: 5 || Loss: 78.425290796659
Iteration: 6 || Loss: 69.87344371520791
Iteration: 7 || Loss: 64.22308798762543
Iteration: 8 || Loss: 59.998855415561785
Iteration: 9 || Loss: 56.4488379239989
Iteration: 10 || Loss: 55.38607620527947
Iteration: 11 || Loss: 53.40392113013833
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.204704
Epoch 71 loss:53.40392113013833
MSE loss S12.378785440989452
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.204704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 147.4333010624976
Iteration: 2 || Loss: 147.34602254428594
Iteration: 3 || Loss: 147.25801202843812
Iteration: 4 || Loss: 147.1706881703957
Iteration: 5 || Loss: 147.08403104808394
Iteration: 6 || Loss: 147.08403104808394
saving ADAM checkpoint...
Sum of params:-99.20485
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 147.08403104808394
Iteration: 2 || Loss: 121.69333242572442
Iteration: 3 || Loss: 116.69594182678813
Iteration: 4 || Loss: 112.8242004694335
Iteration: 5 || Loss: 111.27493581524632
Iteration: 6 || Loss: 109.66030725952953
Iteration: 7 || Loss: 108.0218903660018
Iteration: 8 || Loss: 107.17402930953358
Iteration: 9 || Loss: 106.1614906172387
Iteration: 10 || Loss: 103.40165825900564
Iteration: 11 || Loss: 100.82333782324692
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.23137
Epoch 71 loss:100.82333782324692
MSE loss S13.919547061567162
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.23137
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 880.051990393999
Iteration: 2 || Loss: 880.0148787060499
Iteration: 3 || Loss: 879.9779008489689
Iteration: 4 || Loss: 879.9406013919734
Iteration: 5 || Loss: 879.9033630528977
Iteration: 6 || Loss: 879.9033630528977
saving ADAM checkpoint...
Sum of params:-99.23131
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 879.9033630528977
Iteration: 2 || Loss: 874.0715170575273
Iteration: 3 || Loss: 859.4160418732163
Iteration: 4 || Loss: 839.5487471465851
Iteration: 5 || Loss: 830.0771359782958
Iteration: 6 || Loss: 827.3050989896258
Iteration: 7 || Loss: 823.8068074403071
Iteration: 8 || Loss: 819.3432938755597
Iteration: 9 || Loss: 817.1859366331385
Iteration: 10 || Loss: 812.386727702198
Iteration: 11 || Loss: 808.8851763440687
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.23717
Epoch 71 loss:808.8851763440687
MSE loss S60.245187882305686
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:135.83259839895416
MSE loss S - interpolation18.82348157713547
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3845.885437648683
MSE loss S - extrapolation566.0992571778692
waveform batch: 2/2
Test loss - extrapolation:1704.3210459782129
MSE loss S - extrapolation65.11226082021179
Epoch 71 mean train loss:33.21077363094668
Epoch 71 mean test loss - interpolation:22.638766399825695
Epoch 71 mean test loss - extrapolation:462.51720696890794
Start training epoch 72
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.23717
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 325.30346643410104
Iteration: 2 || Loss: 325.07121827669823
Iteration: 3 || Loss: 324.8395453673983
Iteration: 4 || Loss: 324.60836212396424
Iteration: 5 || Loss: 324.3765797877843
Iteration: 6 || Loss: 324.3765797877843
saving ADAM checkpoint...
Sum of params:-99.237
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 324.3765797877843
Iteration: 2 || Loss: 138.685932804466
Iteration: 3 || Loss: 101.20192005891299
Iteration: 4 || Loss: 83.76257819228113
Iteration: 5 || Loss: 77.57071216666735
Iteration: 6 || Loss: 69.1749959220851
Iteration: 7 || Loss: 63.61371662275119
Iteration: 8 || Loss: 59.42086211651906
Iteration: 9 || Loss: 55.88605273093937
Iteration: 10 || Loss: 54.820567607277724
Iteration: 11 || Loss: 52.89769661509526
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.226944
Epoch 72 loss:52.89769661509526
MSE loss S12.259238502690764
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.226944
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 146.00293253794229
Iteration: 2 || Loss: 145.91633320085234
Iteration: 3 || Loss: 145.82991121519999
Iteration: 4 || Loss: 145.74363645465715
Iteration: 5 || Loss: 145.65681317111023
Iteration: 6 || Loss: 145.65681317111023
saving ADAM checkpoint...
Sum of params:-99.22712
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 145.65681317111023
Iteration: 2 || Loss: 120.6017423043422
Iteration: 3 || Loss: 115.65831785365758
Iteration: 4 || Loss: 111.85491453522522
Iteration: 5 || Loss: 110.33025235065118
Iteration: 6 || Loss: 108.72464858386599
Iteration: 7 || Loss: 107.101106871391
Iteration: 8 || Loss: 106.2302059169167
Iteration: 9 || Loss: 105.26174884531355
Iteration: 10 || Loss: 102.52716236131609
Iteration: 11 || Loss: 99.98589612750057
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.2532
Epoch 72 loss:99.98589612750057
MSE loss S13.789805565898885
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.2532
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 873.3041568122778
Iteration: 2 || Loss: 873.2672473933236
Iteration: 3 || Loss: 873.2306067649464
Iteration: 4 || Loss: 873.1937698031527
Iteration: 5 || Loss: 873.1568297077886
Iteration: 6 || Loss: 873.1568297077886
saving ADAM checkpoint...
Sum of params:-99.253136
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 873.1568297077886
Iteration: 2 || Loss: 867.4518328088301
Iteration: 3 || Loss: 852.935575497388
Iteration: 4 || Loss: 833.3513359876265
Iteration: 5 || Loss: 824.084519730347
Iteration: 6 || Loss: 821.0946697566978
Iteration: 7 || Loss: 817.9377230484571
Iteration: 8 || Loss: 813.3868910303142
Iteration: 9 || Loss: 811.348934062601
Iteration: 10 || Loss: 806.6871863978874
Iteration: 11 || Loss: 803.2114097801104
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.259125
Epoch 72 loss:803.2114097801104
MSE loss S59.78400801870431
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:134.51997116950835
MSE loss S - interpolation18.62517083196274
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3816.264641248509
MSE loss S - extrapolation559.7788032799431
waveform batch: 2/2
Test loss - extrapolation:1695.8571605813415
MSE loss S - extrapolation64.73004892659606
Epoch 72 mean train loss:32.96879319043815
Epoch 72 mean test loss - interpolation:22.41999519491806
Epoch 72 mean test loss - extrapolation:459.34348348582085
Start training epoch 73
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.259125
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 320.57156167587647
Iteration: 2 || Loss: 320.34145458077893
Iteration: 3 || Loss: 320.11171979036453
Iteration: 4 || Loss: 319.88206443010137
Iteration: 5 || Loss: 319.65180115013527
Iteration: 6 || Loss: 319.65180115013527
saving ADAM checkpoint...
Sum of params:-99.25898
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 319.65180115013527
Iteration: 2 || Loss: 136.85196250424002
Iteration: 3 || Loss: 99.8864471154943
Iteration: 4 || Loss: 82.81124308630861
Iteration: 5 || Loss: 76.73332179178705
Iteration: 6 || Loss: 68.49135101794012
Iteration: 7 || Loss: 63.01382900882344
Iteration: 8 || Loss: 58.85160601884893
Iteration: 9 || Loss: 55.34211942077488
Iteration: 10 || Loss: 54.27845827278537
Iteration: 11 || Loss: 52.40075363622732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.24911
Epoch 73 loss:52.40075363622732
MSE loss S12.142278610469287
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.24911
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 144.60923814171352
Iteration: 2 || Loss: 144.52324915245188
Iteration: 3 || Loss: 144.4372868252421
Iteration: 4 || Loss: 144.35160636964642
Iteration: 5 || Loss: 144.26613831437714
Iteration: 6 || Loss: 144.26613831437714
saving ADAM checkpoint...
Sum of params:-99.24932
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 144.26613831437714
Iteration: 2 || Loss: 119.52569520506474
Iteration: 3 || Loss: 114.63487145157718
Iteration: 4 || Loss: 110.90264315881541
Iteration: 5 || Loss: 109.40362801991844
Iteration: 6 || Loss: 107.80983575998815
Iteration: 7 || Loss: 106.20598613240296
Iteration: 8 || Loss: 105.32275867956535
Iteration: 9 || Loss: 104.38191263142292
Iteration: 10 || Loss: 101.67102470241194
Iteration: 11 || Loss: 99.16914983616023
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.274956
Epoch 73 loss:99.16914983616023
MSE loss S13.662141836068226
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.274956
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 866.6190051596944
Iteration: 2 || Loss: 866.5824378588167
Iteration: 3 || Loss: 866.5458206372235
Iteration: 4 || Loss: 866.5093762434865
Iteration: 5 || Loss: 866.4730627034297
Iteration: 6 || Loss: 866.4730627034297
saving ADAM checkpoint...
Sum of params:-99.27489
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 866.4730627034297
Iteration: 2 || Loss: 860.8943257404975
Iteration: 3 || Loss: 846.5298662207126
Iteration: 4 || Loss: 827.2215471270754
Iteration: 5 || Loss: 818.1497935886209
Iteration: 6 || Loss: 814.8663711502273
Iteration: 7 || Loss: 812.1200532461174
Iteration: 8 || Loss: 807.4919484561378
Iteration: 9 || Loss: 805.5368476508911
Iteration: 10 || Loss: 801.0421907009109
Iteration: 11 || Loss: 797.5879395698065
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.28112
Epoch 73 loss:797.5879395698065
MSE loss S59.32405944799359
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:133.229735734641
MSE loss S - interpolation18.42950509150078
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3786.937535617832
MSE loss S - extrapolation553.5242268253006
waveform batch: 2/2
Test loss - extrapolation:1687.4078690909266
MSE loss S - extrapolation64.3433283833165
Epoch 73 mean train loss:32.729580794558416
Epoch 73 mean test loss - interpolation:22.2049559557735
Epoch 73 mean test loss - extrapolation:456.1954503923966
Start training epoch 74
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.28112
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 315.88515619468455
Iteration: 2 || Loss: 315.65703252212944
Iteration: 3 || Loss: 315.42848153742153
Iteration: 4 || Loss: 315.2005829093496
Iteration: 5 || Loss: 314.97284618502164
Iteration: 6 || Loss: 314.97284618502164
saving ADAM checkpoint...
Sum of params:-99.280945
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 314.97284618502164
Iteration: 2 || Loss: 135.0639814786149
Iteration: 3 || Loss: 98.60969057905007
Iteration: 4 || Loss: 81.88573083185133
Iteration: 5 || Loss: 75.91646175021052
Iteration: 6 || Loss: 67.8247961595071
Iteration: 7 || Loss: 62.4257734819746
Iteration: 8 || Loss: 58.292505310314
Iteration: 9 || Loss: 54.81576266196068
Iteration: 10 || Loss: 53.762133055536935
Iteration: 11 || Loss: 51.91420457685521
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.271194
Epoch 74 loss:51.91420457685521
MSE loss S12.028265732412557
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.271194
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 143.26207401900595
Iteration: 2 || Loss: 143.17673688507912
Iteration: 3 || Loss: 143.09117349924983
Iteration: 4 || Loss: 143.00634677549866
Iteration: 5 || Loss: 142.92094498008947
Iteration: 6 || Loss: 142.92094498008947
saving ADAM checkpoint...
Sum of params:-99.27143
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 142.92094498008947
Iteration: 2 || Loss: 118.48329064693742
Iteration: 3 || Loss: 113.63956196684931
Iteration: 4 || Loss: 109.97513588560888
Iteration: 5 || Loss: 108.5002971758589
Iteration: 6 || Loss: 106.91811263521657
Iteration: 7 || Loss: 105.33408872099339
Iteration: 8 || Loss: 104.44046364454226
Iteration: 9 || Loss: 103.52346098472809
Iteration: 10 || Loss: 100.83570319277574
Iteration: 11 || Loss: 98.3764641290156
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.29671
Epoch 74 loss:98.3764641290156
MSE loss S13.538634287519365
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.29671
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 859.9840539105761
Iteration: 2 || Loss: 859.9476663853148
Iteration: 3 || Loss: 859.911545999185
Iteration: 4 || Loss: 859.8754561797547
Iteration: 5 || Loss: 859.8395071904707
Iteration: 6 || Loss: 859.8395071904707
saving ADAM checkpoint...
Sum of params:-99.296646
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 859.8395071904707
Iteration: 2 || Loss: 854.3838210778698
Iteration: 3 || Loss: 840.1716504360295
Iteration: 4 || Loss: 821.1395455485795
Iteration: 5 || Loss: 812.2598733744708
Iteration: 6 || Loss: 808.6403385030588
Iteration: 7 || Loss: 806.3444477377082
Iteration: 8 || Loss: 801.6624744644585
Iteration: 9 || Loss: 799.7249167888752
Iteration: 10 || Loss: 795.444955213465
Iteration: 11 || Loss: 792.008750045214
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.30303
Epoch 74 loss:792.008750045214
MSE loss S58.86597042511073
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:131.9661103699858
MSE loss S - interpolation18.236771427538024
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3758.0147104409957
MSE loss S - extrapolation547.3700427170393
waveform batch: 2/2
Test loss - extrapolation:1678.9370540095915
MSE loss S - extrapolation63.95148158687894
Epoch 74 mean train loss:32.49308340520982
Epoch 74 mean test loss - interpolation:21.99435172833097
Epoch 74 mean test loss - extrapolation:453.07931370421556
Start training epoch 75
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.30303
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 311.30081701545345
Iteration: 2 || Loss: 311.0744423003066
Iteration: 3 || Loss: 310.847854732193
Iteration: 4 || Loss: 310.62177276539444
Iteration: 5 || Loss: 310.39497022681
Iteration: 6 || Loss: 310.39497022681
saving ADAM checkpoint...
Sum of params:-99.30288
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 310.39497022681
Iteration: 2 || Loss: 133.32076610013826
Iteration: 3 || Loss: 97.36761822301258
Iteration: 4 || Loss: 80.98334825595978
Iteration: 5 || Loss: 75.11949872020209
Iteration: 6 || Loss: 67.1747198487454
Iteration: 7 || Loss: 61.84852619683547
Iteration: 8 || Loss: 57.74465599920375
Iteration: 9 || Loss: 54.306314577483946
Iteration: 10 || Loss: 53.267940826211024
Iteration: 11 || Loss: 51.440432338593716
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.293205
Epoch 75 loss:51.440432338593716
MSE loss S11.917408681326707
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.293205
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 141.95781732002408
Iteration: 2 || Loss: 141.8726525857079
Iteration: 3 || Loss: 141.78759248563262
Iteration: 4 || Loss: 141.702918536646
Iteration: 5 || Loss: 141.61879075820042
Iteration: 6 || Loss: 141.61879075820042
saving ADAM checkpoint...
Sum of params:-99.29345
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 141.61879075820042
Iteration: 2 || Loss: 117.47623646857951
Iteration: 3 || Loss: 112.67491364790504
Iteration: 4 || Loss: 109.07433130786197
Iteration: 5 || Loss: 107.621828336136
Iteration: 6 || Loss: 106.05222536764052
Iteration: 7 || Loss: 104.48726387192056
Iteration: 8 || Loss: 103.57904703746341
Iteration: 9 || Loss: 102.68924816776821
Iteration: 10 || Loss: 100.024407164601
Iteration: 11 || Loss: 97.61257665271478
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.318405
Epoch 75 loss:97.61257665271478
MSE loss S13.420128231819904
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.318405
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 853.3858079706858
Iteration: 2 || Loss: 853.3503085566969
Iteration: 3 || Loss: 853.3145613342732
Iteration: 4 || Loss: 853.2788445922301
Iteration: 5 || Loss: 853.2431727868778
Iteration: 6 || Loss: 853.2431727868778
saving ADAM checkpoint...
Sum of params:-99.31834
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 853.2431727868778
Iteration: 2 || Loss: 847.9043943554677
Iteration: 3 || Loss: 833.8438443798949
Iteration: 4 || Loss: 815.0863645019663
Iteration: 5 || Loss: 806.3981060490709
Iteration: 6 || Loss: 802.3750257914826
Iteration: 7 || Loss: 800.5927450153538
Iteration: 8 || Loss: 795.8856913144034
Iteration: 9 || Loss: 793.859582107752
Iteration: 10 || Loss: 789.8839672262031
Iteration: 11 || Loss: 786.4662533023499
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.3249
Epoch 75 loss:786.4662533023499
MSE loss S58.410353094538536
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:130.7326489835414
MSE loss S - interpolation18.047715690344123
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3729.586444360192
MSE loss S - extrapolation541.3512024817057
waveform batch: 2/2
Test loss - extrapolation:1670.4120195428652
MSE loss S - extrapolation63.553681772762815
Epoch 75 mean train loss:32.25928490667788
Epoch 75 mean test loss - interpolation:21.78877483059023
Epoch 75 mean test loss - extrapolation:449.99987199192145
Start training epoch 76
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.3249
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 306.8519230757156
Iteration: 2 || Loss: 306.62772864471145
Iteration: 3 || Loss: 306.4021001556845
Iteration: 4 || Loss: 306.17835491243784
Iteration: 5 || Loss: 305.953167924772
Iteration: 6 || Loss: 305.953167924772
saving ADAM checkpoint...
Sum of params:-99.32475
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 305.953167924772
Iteration: 2 || Loss: 131.6151806873643
Iteration: 3 || Loss: 96.16220637247855
Iteration: 4 || Loss: 80.10583191170221
Iteration: 5 || Loss: 74.34490552344116
Iteration: 6 || Loss: 66.54545268436469
Iteration: 7 || Loss: 61.28860641899236
Iteration: 8 || Loss: 57.21343781303192
Iteration: 9 || Loss: 53.81751135145362
Iteration: 10 || Loss: 52.80399232813395
Iteration: 11 || Loss: 50.98118760745519
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.31514
Epoch 76 loss:50.98118760745519
MSE loss S11.81021636964242
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.31514
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 140.7104756876606
Iteration: 2 || Loss: 140.62551197216044
Iteration: 3 || Loss: 140.5413285896716
Iteration: 4 || Loss: 140.4571564436369
Iteration: 5 || Loss: 140.3733512469911
Iteration: 6 || Loss: 140.3733512469911
saving ADAM checkpoint...
Sum of params:-99.3154
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 140.3733512469911
Iteration: 2 || Loss: 116.50416026425678
Iteration: 3 || Loss: 111.7409206606268
Iteration: 4 || Loss: 108.20424410127556
Iteration: 5 || Loss: 106.77296610368855
Iteration: 6 || Loss: 105.21621556985289
Iteration: 7 || Loss: 103.67042849950751
Iteration: 8 || Loss: 102.7533322110252
Iteration: 9 || Loss: 101.88326724814357
Iteration: 10 || Loss: 99.23875504650645
Iteration: 11 || Loss: 96.8774547530746
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.340065
Epoch 76 loss:96.8774547530746
MSE loss S13.306017644391238
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.340065
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 846.8201440282779
Iteration: 2 || Loss: 846.7845703006269
Iteration: 3 || Loss: 846.7488792077016
Iteration: 4 || Loss: 846.7133495591867
Iteration: 5 || Loss: 846.6778234399978
Iteration: 6 || Loss: 846.6778234399978
saving ADAM checkpoint...
Sum of params:-99.34
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 846.6778234399978
Iteration: 2 || Loss: 841.4539298940273
Iteration: 3 || Loss: 827.551540944993
Iteration: 4 || Loss: 809.0710276968454
Iteration: 5 || Loss: 800.568280438702
Iteration: 6 || Loss: 796.1312032125077
Iteration: 7 || Loss: 794.8491385871258
Iteration: 8 || Loss: 790.1675795216967
Iteration: 9 || Loss: 787.885039656188
Iteration: 10 || Loss: 784.3533957379376
Iteration: 11 || Loss: 780.9573909920599
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.34671
Epoch 76 loss:780.9573909920599
MSE loss S57.95752583277142
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:129.52325169661526
MSE loss S - interpolation17.862496353731295
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3701.593507027438
MSE loss S - extrapolation535.4535785742285
waveform batch: 2/2
Test loss - extrapolation:1661.8483488122088
MSE loss S - extrapolation63.15008000506366
Epoch 76 mean train loss:32.02813908112378
Epoch 76 mean test loss - interpolation:21.587208616102544
Epoch 76 mean test loss - extrapolation:446.9534879866372
Start training epoch 77
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.34671
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 302.49428174968216
Iteration: 2 || Loss: 302.27168947998496
Iteration: 3 || Loss: 302.0480555429095
Iteration: 4 || Loss: 301.82574399318963
Iteration: 5 || Loss: 301.60254388600333
Iteration: 6 || Loss: 301.60254388600333
saving ADAM checkpoint...
Sum of params:-99.346565
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 301.60254388600333
Iteration: 2 || Loss: 129.93760057804172
Iteration: 3 || Loss: 94.98610160863602
Iteration: 4 || Loss: 79.24931327802462
Iteration: 5 || Loss: 73.58931894765105
Iteration: 6 || Loss: 65.93334728415694
Iteration: 7 || Loss: 60.741317964914025
Iteration: 8 || Loss: 56.69608006833025
Iteration: 9 || Loss: 53.34642267564219
Iteration: 10 || Loss: 52.36276985592272
Iteration: 11 || Loss: 50.534626974008496
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.33704
Epoch 77 loss:50.534626974008496
MSE loss S11.706047646046116
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.33704
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 139.50637628249902
Iteration: 2 || Loss: 139.4218951246155
Iteration: 3 || Loss: 139.3384265039204
Iteration: 4 || Loss: 139.25471653316566
Iteration: 5 || Loss: 139.17087167114923
Iteration: 6 || Loss: 139.17087167114923
saving ADAM checkpoint...
Sum of params:-99.3373
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 139.17087167114923
Iteration: 2 || Loss: 115.56509723258921
Iteration: 3 || Loss: 110.83628205757091
Iteration: 4 || Loss: 107.36133302063517
Iteration: 5 || Loss: 105.95021328966028
Iteration: 6 || Loss: 104.40772816261335
Iteration: 7 || Loss: 102.88012483772235
Iteration: 8 || Loss: 101.95536994100445
Iteration: 9 || Loss: 101.10302746159182
Iteration: 10 || Loss: 98.47952114946797
Iteration: 11 || Loss: 96.17158640580958
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.361694
Epoch 77 loss:96.17158640580958
MSE loss S13.197165799579029
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.361694
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 840.282332805622
Iteration: 2 || Loss: 840.2469622653954
Iteration: 3 || Loss: 840.2115888521627
Iteration: 4 || Loss: 840.1768396906335
Iteration: 5 || Loss: 840.1417260808736
Iteration: 6 || Loss: 840.1417260808736
saving ADAM checkpoint...
Sum of params:-99.36164
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 840.1417260808736
Iteration: 2 || Loss: 835.0275193538898
Iteration: 3 || Loss: 821.2862308725714
Iteration: 4 || Loss: 803.0804607030846
Iteration: 5 || Loss: 794.7617730076774
Iteration: 6 || Loss: 789.9841559748672
Iteration: 7 || Loss: 789.0102668190283
Iteration: 8 || Loss: 784.500651713202
Iteration: 9 || Loss: 781.6627875012007
Iteration: 10 || Loss: 778.8338670921963
Iteration: 11 || Loss: 775.4790055654745
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.36854
Epoch 77 loss:775.4790055654745
MSE loss S57.50928148402126
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:128.33168637321128
MSE loss S - interpolation17.6818305751695
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3674.006891889311
MSE loss S - extrapolation529.673591862917
waveform batch: 2/2
Test loss - extrapolation:1653.2531363126218
MSE loss S - extrapolation62.740852238948506
Epoch 77 mean train loss:31.799490308458367
Epoch 77 mean test loss - interpolation:21.388614395535214
Epoch 77 mean test loss - extrapolation:443.9383356834944
Start training epoch 78
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.36854
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 298.17444928097524
Iteration: 2 || Loss: 297.95343490198525
Iteration: 3 || Loss: 297.73158121715113
Iteration: 4 || Loss: 297.51057607830745
Iteration: 5 || Loss: 297.2890648336094
Iteration: 6 || Loss: 297.2890648336094
saving ADAM checkpoint...
Sum of params:-99.36838
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 297.2890648336094
Iteration: 2 || Loss: 128.23940350739068
Iteration: 3 || Loss: 93.83354169881899
Iteration: 4 || Loss: 78.41110029156472
Iteration: 5 || Loss: 72.84957602057538
Iteration: 6 || Loss: 65.33850222189513
Iteration: 7 || Loss: 60.207833383156824
Iteration: 8 || Loss: 56.192495445520514
Iteration: 9 || Loss: 52.89321944668177
Iteration: 10 || Loss: 51.940921141270344
Iteration: 11 || Loss: 50.10147205880637
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.358925
Epoch 78 loss:50.10147205880637
MSE loss S11.604949491016399
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.358925
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 138.35960132622247
Iteration: 2 || Loss: 138.27540954250375
Iteration: 3 || Loss: 138.19194802109266
Iteration: 4 || Loss: 138.108748245122
Iteration: 5 || Loss: 138.02578061528544
Iteration: 6 || Loss: 138.02578061528544
saving ADAM checkpoint...
Sum of params:-99.35916
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 138.02578061528544
Iteration: 2 || Loss: 114.66055761579459
Iteration: 3 || Loss: 109.96224095977999
Iteration: 4 || Loss: 106.5467934638261
Iteration: 5 || Loss: 105.15388187218218
Iteration: 6 || Loss: 103.62658801592232
Iteration: 7 || Loss: 102.11630767713963
Iteration: 8 || Loss: 101.18499739177055
Iteration: 9 || Loss: 100.34905732651445
Iteration: 10 || Loss: 97.74505599070919
Iteration: 11 || Loss: 95.4927360235454
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.38336
Epoch 78 loss:95.4927360235454
MSE loss S13.092597398192426
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.38336
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 833.769804581709
Iteration: 2 || Loss: 833.7347677112663
Iteration: 3 || Loss: 833.6999510308314
Iteration: 4 || Loss: 833.66533395779
Iteration: 5 || Loss: 833.6308279206057
Iteration: 6 || Loss: 833.6308279206057
saving ADAM checkpoint...
Sum of params:-99.383286
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 833.6308279206057
Iteration: 2 || Loss: 828.6256224815347
Iteration: 3 || Loss: 815.0512709132109
Iteration: 4 || Loss: 797.1251931774462
Iteration: 5 || Loss: 788.9854998439101
Iteration: 6 || Loss: 784.0941845210108
Iteration: 7 || Loss: 780.8223660413172
Iteration: 8 || Loss: 778.8796342978304
Iteration: 9 || Loss: 775.1961592446565
Iteration: 10 || Loss: 773.2352654258684
Iteration: 11 || Loss: 770.0297845324831
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.39007
Epoch 78 loss:770.0297845324831
MSE loss S57.08646264573481
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:127.1410546853398
MSE loss S - interpolation17.512710783554482
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3646.72380942912
MSE loss S - extrapolation524.0333478996379
waveform batch: 2/2
Test loss - extrapolation:1644.6078669304186
MSE loss S - extrapolation62.35270458428067
Epoch 78 mean train loss:31.573241124649478
Epoch 78 mean test loss - interpolation:21.190175780889966
Epoch 78 mean test loss - extrapolation:440.94430636329486
Start training epoch 79
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.39007
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 293.8254384428857
Iteration: 2 || Loss: 293.6064420895009
Iteration: 3 || Loss: 293.3864148525807
Iteration: 4 || Loss: 293.16678360491534
Iteration: 5 || Loss: 292.94695447739025
Iteration: 6 || Loss: 292.94695447739025
saving ADAM checkpoint...
Sum of params:-99.389915
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 292.94695447739025
Iteration: 2 || Loss: 126.35141036113866
Iteration: 3 || Loss: 92.66445219577533
Iteration: 4 || Loss: 77.56515275241493
Iteration: 5 || Loss: 72.1011581502842
Iteration: 6 || Loss: 64.74786273716718
Iteration: 7 || Loss: 59.680682247026574
Iteration: 8 || Loss: 55.69814696061772
Iteration: 9 || Loss: 52.45113995133361
Iteration: 10 || Loss: 51.52725265161658
Iteration: 11 || Loss: 49.67823748513942
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.380775
Epoch 79 loss:49.67823748513942
MSE loss S11.50572882254974
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.380775
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 137.23923138793288
Iteration: 2 || Loss: 137.1555392613967
Iteration: 3 || Loss: 137.07272179481112
Iteration: 4 || Loss: 136.9899745096343
Iteration: 5 || Loss: 136.90691568050892
Iteration: 6 || Loss: 136.90691568050892
saving ADAM checkpoint...
Sum of params:-99.38101
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 136.90691568050892
Iteration: 2 || Loss: 113.78372802926536
Iteration: 3 || Loss: 109.11531218433446
Iteration: 4 || Loss: 105.75876480527356
Iteration: 5 || Loss: 104.38324491498327
Iteration: 6 || Loss: 102.8716807748201
Iteration: 7 || Loss: 101.37768362408187
Iteration: 8 || Loss: 100.43257362132694
Iteration: 9 || Loss: 99.62057140633245
Iteration: 10 || Loss: 97.03703420446345
Iteration: 11 || Loss: 94.8449253556394
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.405045
Epoch 79 loss:94.8449253556394
MSE loss S12.993602872128578
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.405045
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 827.2763301968305
Iteration: 2 || Loss: 827.2416990576219
Iteration: 3 || Loss: 827.2073936663892
Iteration: 4 || Loss: 827.1729814442107
Iteration: 5 || Loss: 827.1388987693117
Iteration: 6 || Loss: 827.1388987693117
saving ADAM checkpoint...
Sum of params:-99.40497
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 827.1388987693117
Iteration: 2 || Loss: 822.2365965947401
Iteration: 3 || Loss: 808.8349684978178
Iteration: 4 || Loss: 791.1817579026119
Iteration: 5 || Loss: 783.2210311992991
Iteration: 6 || Loss: 778.5292653165781
Iteration: 7 || Loss: 777.4922959356315
Iteration: 8 || Loss: 773.3223417979904
Iteration: 9 || Loss: 769.048740048892
Iteration: 10 || Loss: 765.6157095984364
Iteration: 11 || Loss: 763.9418193329731
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.41026
Epoch 79 loss:763.9418193329731
MSE loss S57.17705826379489
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:126.22873990545698
MSE loss S - interpolation17.66350930467945
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3619.540513084413
MSE loss S - extrapolation519.8769633989946
waveform batch: 2/2
Test loss - extrapolation:1632.5710445826319
MSE loss S - extrapolation62.21794821044682
Epoch 79 mean train loss:31.326378695646618
Epoch 79 mean test loss - interpolation:21.03812331757616
Epoch 79 mean test loss - extrapolation:437.6759631389204
Start training epoch 80
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.41026
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 286.76986433080765
Iteration: 2 || Loss: 286.5499616381209
Iteration: 3 || Loss: 286.33127940199915
Iteration: 4 || Loss: 286.111413884787
Iteration: 5 || Loss: 285.8930363904725
Iteration: 6 || Loss: 285.8930363904725
saving ADAM checkpoint...
Sum of params:-99.41009
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 285.8930363904725
Iteration: 2 || Loss: 119.28099528330411
Iteration: 3 || Loss: 91.04798310765602
Iteration: 4 || Loss: 76.87667077657949
Iteration: 5 || Loss: 71.28431611816987
Iteration: 6 || Loss: 64.1217842010157
Iteration: 7 || Loss: 59.60365445289585
Iteration: 8 || Loss: 55.57469203055523
Iteration: 9 || Loss: 52.48312466002098
Iteration: 10 || Loss: 51.54490877050202
Iteration: 11 || Loss: 49.65150336684732
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.40435
Epoch 80 loss:49.65150336684732
MSE loss S11.489592926004919
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.40435
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 137.1409042679339
Iteration: 2 || Loss: 137.05738987342627
Iteration: 3 || Loss: 136.9741145256104
Iteration: 4 || Loss: 136.89092797240005
Iteration: 5 || Loss: 136.80845045879863
Iteration: 6 || Loss: 136.80845045879863
saving ADAM checkpoint...
Sum of params:-99.404594
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 136.80845045879863
Iteration: 2 || Loss: 113.68204118234335
Iteration: 3 || Loss: 109.03515560093598
Iteration: 4 || Loss: 105.71881118787815
Iteration: 5 || Loss: 104.34455645314844
Iteration: 6 || Loss: 102.837720409572
Iteration: 7 || Loss: 101.30962366807681
Iteration: 8 || Loss: 100.37438236995783
Iteration: 9 || Loss: 99.49976386180516
Iteration: 10 || Loss: 96.81750048943795
Iteration: 11 || Loss: 94.61186475601863
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.42835
Epoch 80 loss:94.61186475601863
MSE loss S12.953166508909245
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.42835
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 820.197899364895
Iteration: 2 || Loss: 820.1632826382995
Iteration: 3 || Loss: 820.1287703680639
Iteration: 4 || Loss: 820.0943819885442
Iteration: 5 || Loss: 820.0602236927946
Iteration: 6 || Loss: 820.0602236927946
saving ADAM checkpoint...
Sum of params:-99.42829
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 820.0602236927946
Iteration: 2 || Loss: 815.1686821229027
Iteration: 3 || Loss: 801.6390522572794
Iteration: 4 || Loss: 784.1922969925486
Iteration: 5 || Loss: 776.4664045396613
Iteration: 6 || Loss: 772.0524297603125
Iteration: 7 || Loss: 771.1779320986079
Iteration: 8 || Loss: 766.9098364491392
Iteration: 9 || Loss: 763.1727603932893
Iteration: 10 || Loss: 761.4640281628987
Iteration: 11 || Loss: 758.5388719303962
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.43673
Epoch 80 loss:758.5388719303962
MSE loss S55.987169393224825
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:125.23266735292066
MSE loss S - interpolation17.125436386512444
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3598.255111724915
MSE loss S - extrapolation514.1467337361545
waveform batch: 2/2
Test loss - extrapolation:1624.9779423004406
MSE loss S - extrapolation61.24421116169506
Epoch 80 mean train loss:31.131111725974556
Epoch 80 mean test loss - interpolation:20.872111225486776
Epoch 80 mean test loss - extrapolation:435.2694211687796
Start training epoch 81
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.43673
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 289.0996876260281
Iteration: 2 || Loss: 288.8824433869986
Iteration: 3 || Loss: 288.66440327603743
Iteration: 4 || Loss: 288.4471255127659
Iteration: 5 || Loss: 288.22952752372885
Iteration: 6 || Loss: 288.22952752372885
saving ADAM checkpoint...
Sum of params:-99.43657
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 288.22952752372885
Iteration: 2 || Loss: 125.16650459547726
Iteration: 3 || Loss: 90.87566664117595
Iteration: 4 || Loss: 76.1738941758516
Iteration: 5 || Loss: 70.92446923790934
Iteration: 6 || Loss: 63.74208107476362
Iteration: 7 || Loss: 58.76578852776911
Iteration: 8 || Loss: 54.849375766677646
Iteration: 9 || Loss: 51.72552696810319
Iteration: 10 || Loss: 50.84958400471544
Iteration: 11 || Loss: 48.94781509242431
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.426155
Epoch 81 loss:48.94781509242431
MSE loss S11.335076054896982
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.426155
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 135.52391664125318
Iteration: 2 || Loss: 135.44069382543688
Iteration: 3 || Loss: 135.3577112167656
Iteration: 4 || Loss: 135.2749879002672
Iteration: 5 || Loss: 135.1928046418067
Iteration: 6 || Loss: 135.1928046418067
saving ADAM checkpoint...
Sum of params:-99.42639
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 135.1928046418067
Iteration: 2 || Loss: 112.27637910697919
Iteration: 3 || Loss: 107.63973513585475
Iteration: 4 || Loss: 104.39284978532119
Iteration: 5 || Loss: 103.04828003658585
Iteration: 6 || Loss: 101.58049850437908
Iteration: 7 || Loss: 100.11345440780536
Iteration: 8 || Loss: 99.1590278052125
Iteration: 9 || Loss: 98.38407929472591
Iteration: 10 || Loss: 95.83942037795154
Iteration: 11 || Loss: 93.74981793245324
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.45008
Epoch 81 loss:93.74981793245324
MSE loss S12.823408914939215
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.45008
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 813.9994979769627
Iteration: 2 || Loss: 813.9658252455494
Iteration: 3 || Loss: 813.9321155958762
Iteration: 4 || Loss: 813.8984965987117
Iteration: 5 || Loss: 813.8645865925313
Iteration: 6 || Loss: 813.8645865925313
saving ADAM checkpoint...
Sum of params:-99.450035
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 813.8645865925313
Iteration: 2 || Loss: 809.1636648740786
Iteration: 3 || Loss: 796.1080813512402
Iteration: 4 || Loss: 778.9468335857913
Iteration: 5 || Loss: 771.2956979020862
Iteration: 6 || Loss: 767.6306678317912
Iteration: 7 || Loss: 766.1751808448599
Iteration: 8 || Loss: 761.8107789740739
Iteration: 9 || Loss: 759.0922558758199
Iteration: 10 || Loss: 756.6658371930391
Iteration: 11 || Loss: 753.3852331717594
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.458
Epoch 81 loss:753.3852331717594
MSE loss S55.616783540363855
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:123.9220830387423
MSE loss S - interpolation16.961986938089836
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3569.43910856534
MSE loss S - extrapolation508.2942373161277
waveform batch: 2/2
Test loss - extrapolation:1617.0777794111964
MSE loss S - extrapolation60.86892891981182
Epoch 81 mean train loss:30.899409179194375
Epoch 81 mean test loss - interpolation:20.65368050645705
Epoch 81 mean test loss - extrapolation:432.20974066471143
Start training epoch 82
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.458
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 283.4369276156379
Iteration: 2 || Loss: 283.22207544995547
Iteration: 3 || Loss: 283.0064403837101
Iteration: 4 || Loss: 282.7917264598903
Iteration: 5 || Loss: 282.5764174167496
Iteration: 6 || Loss: 282.5764174167496
saving ADAM checkpoint...
Sum of params:-99.45785
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 282.5764174167496
Iteration: 2 || Loss: 123.10569059025839
Iteration: 3 || Loss: 89.68849848029414
Iteration: 4 || Loss: 75.3102859227103
Iteration: 5 || Loss: 70.15684665679899
Iteration: 6 || Loss: 63.15108861515035
Iteration: 7 || Loss: 58.23040504075324
Iteration: 8 || Loss: 54.34425951187193
Iteration: 9 || Loss: 51.28625487629032
Iteration: 10 || Loss: 50.42028697501486
Iteration: 11 || Loss: 48.511864838366726
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.44765
Epoch 82 loss:48.511864838366726
MSE loss S11.237215523938973
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.44765
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 134.30435226926713
Iteration: 2 || Loss: 134.22148292787026
Iteration: 3 || Loss: 134.1394777559767
Iteration: 4 || Loss: 134.05748353011865
Iteration: 5 || Loss: 133.97550197718792
Iteration: 6 || Loss: 133.97550197718792
saving ADAM checkpoint...
Sum of params:-99.447876
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 133.97550197718792
Iteration: 2 || Loss: 111.37906985021675
Iteration: 3 || Loss: 106.76356709481824
Iteration: 4 || Loss: 103.57063841300739
Iteration: 5 || Loss: 102.23709645833
Iteration: 6 || Loss: 100.78202969132319
Iteration: 7 || Loss: 99.33128162289141
Iteration: 8 || Loss: 98.34145729959684
Iteration: 9 || Loss: 97.60857312819316
Iteration: 10 || Loss: 95.09387280841165
Iteration: 11 || Loss: 93.08245611556494
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.471535
Epoch 82 loss:93.08245611556494
MSE loss S12.72766023518232
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.471535
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 807.6200291338257
Iteration: 2 || Loss: 807.5864203147552
Iteration: 3 || Loss: 807.5529125017362
Iteration: 4 || Loss: 807.5197491052173
Iteration: 5 || Loss: 807.486795105829
Iteration: 6 || Loss: 807.486795105829
saving ADAM checkpoint...
Sum of params:-99.47148
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 807.486795105829
Iteration: 2 || Loss: 802.8938564106387
Iteration: 3 || Loss: 790.0718041052377
Iteration: 4 || Loss: 773.2187113753533
Iteration: 5 || Loss: 765.7408456854611
Iteration: 6 || Loss: 762.7384696997751
Iteration: 7 || Loss: 760.7588227440466
Iteration: 8 || Loss: 756.5105407089482
Iteration: 9 || Loss: 754.4413083298821
Iteration: 10 || Loss: 751.4731216278627
Iteration: 11 || Loss: 748.137598504523
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.47963
Epoch 82 loss:748.137598504523
MSE loss S55.20089135666349
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:122.844046177801
MSE loss S - interpolation16.799093548229333
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3543.6411783393237
MSE loss S - extrapolation503.068098523326
waveform batch: 2/2
Test loss - extrapolation:1608.3879049033426
MSE loss S - extrapolation60.45396339569525
Epoch 82 mean train loss:30.68041101580878
Epoch 82 mean test loss - interpolation:20.474007696300166
Epoch 82 mean test loss - extrapolation:429.33575693688886
Start training epoch 83
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.47963
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 279.58512052013134
Iteration: 2 || Loss: 279.3706600984169
Iteration: 3 || Loss: 279.1567890543673
Iteration: 4 || Loss: 278.9430206420046
Iteration: 5 || Loss: 278.729812721863
Iteration: 6 || Loss: 278.729812721863
saving ADAM checkpoint...
Sum of params:-99.47943
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 278.729812721863
Iteration: 2 || Loss: 121.59482908298718
Iteration: 3 || Loss: 88.6610227190591
Iteration: 4 || Loss: 74.5406333217384
Iteration: 5 || Loss: 69.49065468520674
Iteration: 6 || Loss: 62.625966626650694
Iteration: 7 || Loss: 57.75414653370861
Iteration: 8 || Loss: 53.90234732023141
Iteration: 9 || Loss: 50.89596574450878
Iteration: 10 || Loss: 50.03498438955076
Iteration: 11 || Loss: 48.12520878768547
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.4692
Epoch 83 loss:48.12520878768547
MSE loss S11.146444140989974
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.4692
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 133.30681856240264
Iteration: 2 || Loss: 133.22448578842304
Iteration: 3 || Loss: 133.14263992933815
Iteration: 4 || Loss: 133.06100335127962
Iteration: 5 || Loss: 132.97896720260718
Iteration: 6 || Loss: 132.97896720260718
saving ADAM checkpoint...
Sum of params:-99.46947
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 132.97896720260718
Iteration: 2 || Loss: 110.60210927938603
Iteration: 3 || Loss: 106.00404488269866
Iteration: 4 || Loss: 102.86333622362716
Iteration: 5 || Loss: 101.53915367140844
Iteration: 6 || Loss: 100.10065146704676
Iteration: 7 || Loss: 98.65911053799279
Iteration: 8 || Loss: 97.62780782422958
Iteration: 9 || Loss: 96.94120578588054
Iteration: 10 || Loss: 94.44646471056843
Iteration: 11 || Loss: 92.49728040399668
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.49309
Epoch 83 loss:92.49728040399668
MSE loss S12.64039508451359
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.49309
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 801.2823522873443
Iteration: 2 || Loss: 801.2489509515615
Iteration: 3 || Loss: 801.2157671157986
Iteration: 4 || Loss: 801.1827004708956
Iteration: 5 || Loss: 801.1498082932817
Iteration: 6 || Loss: 801.1498082932817
saving ADAM checkpoint...
Sum of params:-99.493034
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 801.1498082932817
Iteration: 2 || Loss: 796.6544761581417
Iteration: 3 || Loss: 784.019002571804
Iteration: 4 || Loss: 767.4502686493901
Iteration: 5 || Loss: 760.1430604414627
Iteration: 6 || Loss: 757.61022131038
Iteration: 7 || Loss: 755.2787962222786
Iteration: 8 || Loss: 751.1670190839453
Iteration: 9 || Loss: 749.4457522407316
Iteration: 10 || Loss: 746.213530543501
Iteration: 11 || Loss: 742.8697417107569
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.501305
Epoch 83 loss:742.8697417107569
MSE loss S54.770216585733195
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:121.80694192056788
MSE loss S - interpolation16.636814289742592
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3518.536020446988
MSE loss S - extrapolation497.97777154457117
waveform batch: 2/2
Test loss - extrapolation:1599.5526723279413
MSE loss S - extrapolation60.01857586759223
Epoch 83 mean train loss:30.465249341463416
Epoch 83 mean test loss - interpolation:20.301156986761313
Epoch 83 mean test loss - extrapolation:426.5073910645774
Start training epoch 84
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.501305
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 276.0226528928861
Iteration: 2 || Loss: 275.81006007236067
Iteration: 3 || Loss: 275.59753283575077
Iteration: 4 || Loss: 275.3849425483689
Iteration: 5 || Loss: 275.17257114960995
Iteration: 6 || Loss: 275.17257114960995
saving ADAM checkpoint...
Sum of params:-99.501114
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 275.17257114960995
Iteration: 2 || Loss: 120.19317438820408
Iteration: 3 || Loss: 87.6822395088017
Iteration: 4 || Loss: 73.80033126860275
Iteration: 5 || Loss: 68.85006873264236
Iteration: 6 || Loss: 62.11736219883103
Iteration: 7 || Loss: 57.29103772861488
Iteration: 8 || Loss: 53.47555778487436
Iteration: 9 || Loss: 50.52507156149884
Iteration: 10 || Loss: 49.66425990535812
Iteration: 11 || Loss: 47.753518401447415
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.49087
Epoch 84 loss:47.753518401447415
MSE loss S11.059014350923936
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.49087
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 132.38332172212364
Iteration: 2 || Loss: 132.30124221129955
Iteration: 3 || Loss: 132.21977243481678
Iteration: 4 || Loss: 132.1380597024019
Iteration: 5 || Loss: 132.056451754285
Iteration: 6 || Loss: 132.056451754285
saving ADAM checkpoint...
Sum of params:-99.49113
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 132.056451754285
Iteration: 2 || Loss: 109.86204433952744
Iteration: 3 || Loss: 105.2766699036732
Iteration: 4 || Loss: 102.18740962553535
Iteration: 5 || Loss: 100.87186982801296
Iteration: 6 || Loss: 99.45319725398178
Iteration: 7 || Loss: 98.02116419229355
Iteration: 8 || Loss: 96.94819676899334
Iteration: 9 || Loss: 96.30862439729417
Iteration: 10 || Loss: 93.83509394833806
Iteration: 11 || Loss: 91.9448957455891
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.51474
Epoch 84 loss:91.9448957455891
MSE loss S12.557947648837635
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.51474
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 794.991801522927
Iteration: 2 || Loss: 794.9589459650691
Iteration: 3 || Loss: 794.9260395817388
Iteration: 4 || Loss: 794.8933638718211
Iteration: 5 || Loss: 794.8609915352172
Iteration: 6 || Loss: 794.8609915352172
saving ADAM checkpoint...
Sum of params:-99.51469
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 794.8609915352172
Iteration: 2 || Loss: 790.4594093013774
Iteration: 3 || Loss: 778.004367659743
Iteration: 4 || Loss: 761.7086270390928
Iteration: 5 || Loss: 754.566299941509
Iteration: 6 || Loss: 752.393122648574
Iteration: 7 || Loss: 749.807234393834
Iteration: 8 || Loss: 745.8403359189406
Iteration: 9 || Loss: 744.3071159456746
Iteration: 10 || Loss: 740.958737944175
Iteration: 11 || Loss: 737.6193514864646
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.52304
Epoch 84 loss:737.6193514864646
MSE loss S54.33760855102271
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:120.78873193197762
MSE loss S - interpolation16.476386546650275
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3493.8918302409484
MSE loss S - extrapolation493.0106696559999
waveform batch: 2/2
Test loss - extrapolation:1590.680046318645
MSE loss S - extrapolation59.574924815009325
Epoch 84 mean train loss:30.252336745982795
Epoch 84 mean test loss - interpolation:20.13145532199627
Epoch 84 mean test loss - extrapolation:423.71432304663284
Start training epoch 85
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.52304
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 272.58062578015875
Iteration: 2 || Loss: 272.3688535117515
Iteration: 3 || Loss: 272.15749135579983
Iteration: 4 || Loss: 271.9463001931942
Iteration: 5 || Loss: 271.7356378223433
Iteration: 6 || Loss: 271.7356378223433
saving ADAM checkpoint...
Sum of params:-99.52287
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 271.7356378223433
Iteration: 2 || Loss: 118.84667784596998
Iteration: 3 || Loss: 86.73144096229663
Iteration: 4 || Loss: 73.07486347760796
Iteration: 5 || Loss: 68.22406660921676
Iteration: 6 || Loss: 61.619872057523146
Iteration: 7 || Loss: 56.836192555852904
Iteration: 8 || Loss: 53.058173185821985
Iteration: 9 || Loss: 50.16719291852728
Iteration: 10 || Loss: 49.30115517827725
Iteration: 11 || Loss: 47.39053885373112
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.512566
Epoch 85 loss:47.39053885373112
MSE loss S10.973584430349598
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.512566
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 131.49151479751177
Iteration: 2 || Loss: 131.40970394688895
Iteration: 3 || Loss: 131.32799961826495
Iteration: 4 || Loss: 131.2471394337362
Iteration: 5 || Loss: 131.16552246975462
Iteration: 6 || Loss: 131.16552246975462
saving ADAM checkpoint...
Sum of params:-99.51281
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 131.16552246975462
Iteration: 2 || Loss: 109.14248706778334
Iteration: 3 || Loss: 104.56894444587523
Iteration: 4 || Loss: 101.52956633440327
Iteration: 5 || Loss: 100.22116455406757
Iteration: 6 || Loss: 98.82290800183738
Iteration: 7 || Loss: 97.40093541519083
Iteration: 8 || Loss: 96.28134268001897
Iteration: 9 || Loss: 95.69274201879217
Iteration: 10 || Loss: 93.24354815521401
Iteration: 11 || Loss: 91.40796982586944
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.53642
Epoch 85 loss:91.40796982586944
MSE loss S12.478146397043897
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.53642
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 788.7609344051593
Iteration: 2 || Loss: 788.7281531410472
Iteration: 3 || Loss: 788.6957631214691
Iteration: 4 || Loss: 788.6631311116406
Iteration: 5 || Loss: 788.6306805757417
Iteration: 6 || Loss: 788.6306805757417
saving ADAM checkpoint...
Sum of params:-99.53637
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 788.6306805757417
Iteration: 2 || Loss: 784.3211233031907
Iteration: 3 || Loss: 772.0436656187838
Iteration: 4 || Loss: 756.0149299544141
Iteration: 5 || Loss: 749.031710801917
Iteration: 6 || Loss: 747.12226827121
Iteration: 7 || Loss: 744.3683070362874
Iteration: 8 || Loss: 740.5489383391623
Iteration: 9 || Loss: 739.114169426461
Iteration: 10 || Loss: 735.7300694598323
Iteration: 11 || Loss: 732.4009616469061
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.54482
Epoch 85 loss:732.4009616469061
MSE loss S53.9062919014801
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:119.7846015287936
MSE loss S - interpolation16.318221173609498
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3469.5001244054392
MSE loss S - extrapolation488.11450770434146
waveform batch: 2/2
Test loss - extrapolation:1581.8172645944298
MSE loss S - extrapolation59.12718445788371
Epoch 85 mean train loss:30.04136104574161
Epoch 85 mean test loss - interpolation:19.964100254798932
Epoch 85 mean test loss - extrapolation:420.9431157499891
Start training epoch 86
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.54482
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 269.17909518384687
Iteration: 2 || Loss: 268.96868442878355
Iteration: 3 || Loss: 268.7588102826215
Iteration: 4 || Loss: 268.54899617525433
Iteration: 5 || Loss: 268.339315339738
Iteration: 6 || Loss: 268.339315339738
saving ADAM checkpoint...
Sum of params:-99.544655
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 268.339315339738
Iteration: 2 || Loss: 117.54689433901422
Iteration: 3 || Loss: 85.80781266108531
Iteration: 4 || Loss: 72.35998822838908
Iteration: 5 || Loss: 67.60770248664147
Iteration: 6 || Loss: 61.13177043467248
Iteration: 7 || Loss: 56.387957741497786
Iteration: 8 || Loss: 52.647919940974525
Iteration: 9 || Loss: 49.81807759388145
Iteration: 10 || Loss: 48.942350719361535
Iteration: 11 || Loss: 47.03362569642865
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.534256
Epoch 86 loss:47.03362569642865
MSE loss S10.889841739778879
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.534256
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 130.61706693328404
Iteration: 2 || Loss: 130.53569379283843
Iteration: 3 || Loss: 130.4544241831396
Iteration: 4 || Loss: 130.37301390356285
Iteration: 5 || Loss: 130.29166661087925
Iteration: 6 || Loss: 130.29166661087925
saving ADAM checkpoint...
Sum of params:-99.534515
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 130.29166661087925
Iteration: 2 || Loss: 108.43601920569056
Iteration: 3 || Loss: 103.87186136838575
Iteration: 4 || Loss: 100.88261259691876
Iteration: 5 || Loss: 99.57997623430218
Iteration: 6 || Loss: 98.20343344517879
Iteration: 7 || Loss: 96.79221385616692
Iteration: 8 || Loss: 95.61820917043838
Iteration: 9 || Loss: 95.08428692395286
Iteration: 10 || Loss: 92.66411846798673
Iteration: 11 || Loss: 90.88341408112181
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.55811
Epoch 86 loss:90.88341408112181
MSE loss S12.401120455739555
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.55811
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 782.5774453341706
Iteration: 2 || Loss: 782.5451100316133
Iteration: 3 || Loss: 782.5127616317008
Iteration: 4 || Loss: 782.4808275752705
Iteration: 5 || Loss: 782.4486087845215
Iteration: 6 || Loss: 782.4486087845215
saving ADAM checkpoint...
Sum of params:-99.55804
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 782.4486087845215
Iteration: 2 || Loss: 778.2286719431632
Iteration: 3 || Loss: 766.1331665826705
Iteration: 4 || Loss: 750.3656621835121
Iteration: 5 || Loss: 743.5385696224063
Iteration: 6 || Loss: 741.8257198753057
Iteration: 7 || Loss: 738.965270982045
Iteration: 8 || Loss: 735.2946377689701
Iteration: 9 || Loss: 733.9079073531753
Iteration: 10 || Loss: 730.5340956869974
Iteration: 11 || Loss: 727.2169105520278
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.5666
Epoch 86 loss:727.2169105520278
MSE loss S53.4782772549785
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:118.79550095528306
MSE loss S - interpolation16.162650151275315
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3445.4197405936047
MSE loss S - extrapolation483.30887917342125
waveform batch: 2/2
Test loss - extrapolation:1572.960944372989
MSE loss S - extrapolation58.678393218277954
Epoch 86 mean train loss:29.832205183778562
Epoch 86 mean test loss - interpolation:19.799250159213845
Epoch 86 mean test loss - extrapolation:418.1983904138828
Start training epoch 87
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.5666
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 265.8422434124214
Iteration: 2 || Loss: 265.6326313619154
Iteration: 3 || Loss: 265.4241277977565
Iteration: 4 || Loss: 265.21512539848135
Iteration: 5 || Loss: 265.0069755963256
Iteration: 6 || Loss: 265.0069755963256
saving ADAM checkpoint...
Sum of params:-99.56644
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 265.0069755963256
Iteration: 2 || Loss: 116.27895603171322
Iteration: 3 || Loss: 84.90376432740354
Iteration: 4 || Loss: 71.65454357575476
Iteration: 5 || Loss: 67.00102870003295
Iteration: 6 || Loss: 60.65183372705057
Iteration: 7 || Loss: 55.94617375344352
Iteration: 8 || Loss: 52.24583875420212
Iteration: 9 || Loss: 49.477303953043666
Iteration: 10 || Loss: 48.58791553600313
Iteration: 11 || Loss: 46.68294123796085
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.55594
Epoch 87 loss:46.68294123796085
MSE loss S10.807619617791891
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.55594
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 129.7679333278594
Iteration: 2 || Loss: 129.6864112667805
Iteration: 3 || Loss: 129.60588715684023
Iteration: 4 || Loss: 129.52451894409407
Iteration: 5 || Loss: 129.44344420578412
Iteration: 6 || Loss: 129.44344420578412
saving ADAM checkpoint...
Sum of params:-99.55618
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 129.44344420578412
Iteration: 2 || Loss: 107.74687485546455
Iteration: 3 || Loss: 103.19037628424249
Iteration: 4 || Loss: 100.2479843033006
Iteration: 5 || Loss: 98.94970925157673
Iteration: 6 || Loss: 97.59556831614483
Iteration: 7 || Loss: 96.19514917181124
Iteration: 8 || Loss: 94.962588186666
Iteration: 9 || Loss: 94.4804065997611
Iteration: 10 || Loss: 92.09669930129978
Iteration: 11 || Loss: 90.36787510127657
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.57986
Epoch 87 loss:90.36787510127657
MSE loss S12.325536772861891
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.57986
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 776.4554736418634
Iteration: 2 || Loss: 776.4232170117243
Iteration: 3 || Loss: 776.3912100178786
Iteration: 4 || Loss: 776.3593812696804
Iteration: 5 || Loss: 776.3278681507722
Iteration: 6 || Loss: 776.3278681507722
saving ADAM checkpoint...
Sum of params:-99.57979
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 776.3278681507722
Iteration: 2 || Loss: 772.1970850377096
Iteration: 3 || Loss: 760.2861582125709
Iteration: 4 || Loss: 744.7740239522951
Iteration: 5 || Loss: 738.0963759238604
Iteration: 6 || Loss: 736.52798107848
Iteration: 7 || Loss: 733.6045717150793
Iteration: 8 || Loss: 730.0810923296082
Iteration: 9 || Loss: 728.7126370141559
Iteration: 10 || Loss: 725.3758814202905
Iteration: 11 || Loss: 722.0712802265475
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.58838
Epoch 87 loss:722.0712802265475
MSE loss S53.05243604790834
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:117.81600085798497
MSE loss S - interpolation16.00861900184629
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3421.514776273583
MSE loss S - extrapolation478.54577645162607
waveform batch: 2/2
Test loss - extrapolation:1564.1409312287938
MSE loss S - extrapolation58.22861482010545
Epoch 87 mean train loss:29.624899881578788
Epoch 87 mean test loss - interpolation:19.636000142997496
Epoch 87 mean test loss - extrapolation:415.4713089585314
Start training epoch 88
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.58838
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 262.5233863253272
Iteration: 2 || Loss: 262.315992887399
Iteration: 3 || Loss: 262.10801789318253
Iteration: 4 || Loss: 261.90153192697875
Iteration: 5 || Loss: 261.69354382239175
Iteration: 6 || Loss: 261.69354382239175
saving ADAM checkpoint...
Sum of params:-99.5882
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 261.69354382239175
Iteration: 2 || Loss: 115.04480388879513
Iteration: 3 || Loss: 84.02031445493051
Iteration: 4 || Loss: 70.95482993707527
Iteration: 5 || Loss: 66.39951649504917
Iteration: 6 || Loss: 60.17776822399582
Iteration: 7 || Loss: 55.50792266837399
Iteration: 8 || Loss: 51.8484045129601
Iteration: 9 || Loss: 49.142049461276166
Iteration: 10 || Loss: 48.235795082885204
Iteration: 11 || Loss: 46.33603203510149
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.57761
Epoch 88 loss:46.33603203510149
MSE loss S10.726436901708777
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.57761
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 128.92957970321842
Iteration: 2 || Loss: 128.8481334715117
Iteration: 3 || Loss: 128.76682605875038
Iteration: 4 || Loss: 128.68637776914431
Iteration: 5 || Loss: 128.60607898146958
Iteration: 6 || Loss: 128.60607898146958
saving ADAM checkpoint...
Sum of params:-99.57785
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 128.60607898146958
Iteration: 2 || Loss: 107.06821657125116
Iteration: 3 || Loss: 102.5185242746122
Iteration: 4 || Loss: 99.62221560040769
Iteration: 5 || Loss: 98.32669576090683
Iteration: 6 || Loss: 96.99579134545071
Iteration: 7 || Loss: 95.60659310995673
Iteration: 8 || Loss: 94.31695060651644
Iteration: 9 || Loss: 93.86936583027453
Iteration: 10 || Loss: 91.53816894960372
Iteration: 11 || Loss: 89.85940438673396
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.60155
Epoch 88 loss:89.85940438673396
MSE loss S12.25155334412863
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.60155
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 770.3915729436982
Iteration: 2 || Loss: 770.3597218838817
Iteration: 3 || Loss: 770.3282595873255
Iteration: 4 || Loss: 770.2969514235384
Iteration: 5 || Loss: 770.2653999357965
Iteration: 6 || Loss: 770.2653999357965
saving ADAM checkpoint...
Sum of params:-99.60149
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 770.2653999357965
Iteration: 2 || Loss: 766.2230178985641
Iteration: 3 || Loss: 754.4968968485842
Iteration: 4 || Loss: 739.2353545324138
Iteration: 5 || Loss: 732.7041253785036
Iteration: 6 || Loss: 731.2408138675092
Iteration: 7 || Loss: 728.2883345163158
Iteration: 8 || Loss: 724.9084359446152
Iteration: 9 || Loss: 723.5398697588317
Iteration: 10 || Loss: 720.2567757904027
Iteration: 11 || Loss: 716.9659323804706
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.61014
Epoch 88 loss:716.9659323804706
MSE loss S52.63075879797272
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:116.8483095971547
MSE loss S - interpolation15.856935432456622
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3397.801208370072
MSE loss S - extrapolation473.833531061877
waveform batch: 2/2
Test loss - extrapolation:1555.3554558483117
MSE loss S - extrapolation57.78029614178765
Epoch 88 mean train loss:29.419357544907104
Epoch 88 mean test loss - interpolation:19.47471826619245
Epoch 88 mean test loss - extrapolation:412.76305535153193
Start training epoch 89
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.61014
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 259.2293357209642
Iteration: 2 || Loss: 259.0226547766161
Iteration: 3 || Loss: 258.8170196162977
Iteration: 4 || Loss: 258.610306153442
Iteration: 5 || Loss: 258.4053163708106
Iteration: 6 || Loss: 258.4053163708106
saving ADAM checkpoint...
Sum of params:-99.609985
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 258.4053163708106
Iteration: 2 || Loss: 113.83621339249348
Iteration: 3 || Loss: 83.15518736844639
Iteration: 4 || Loss: 70.26166714191685
Iteration: 5 || Loss: 65.80442730693898
Iteration: 6 || Loss: 59.710416864870595
Iteration: 7 || Loss: 55.07445499841764
Iteration: 8 || Loss: 51.456330290577704
Iteration: 9 || Loss: 48.81102058065076
Iteration: 10 || Loss: 47.88589373031663
Iteration: 11 || Loss: 45.99300158591918
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.59927
Epoch 89 loss:45.99300158591918
MSE loss S10.645980518497591
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.59927
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 128.08908734927695
Iteration: 2 || Loss: 128.00798541011275
Iteration: 3 || Loss: 127.92697607740966
Iteration: 4 || Loss: 127.84638804040823
Iteration: 5 || Loss: 127.76609404984679
Iteration: 6 || Loss: 127.76609404984679
saving ADAM checkpoint...
Sum of params:-99.599525
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 127.76609404984679
Iteration: 2 || Loss: 106.39988092272405
Iteration: 3 || Loss: 101.85605767999806
Iteration: 4 || Loss: 99.00631287455673
Iteration: 5 || Loss: 97.71166880528875
Iteration: 6 || Loss: 96.40389207256729
Iteration: 7 || Loss: 95.02639680028749
Iteration: 8 || Loss: 93.68440122559808
Iteration: 9 || Loss: 93.2182989687918
Iteration: 10 || Loss: 90.98761324486476
Iteration: 11 || Loss: 89.35508026326819
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.62321
Epoch 89 loss:89.35508026326819
MSE loss S12.178500087408498
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.62321
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 764.3951204687801
Iteration: 2 || Loss: 764.3636694735118
Iteration: 3 || Loss: 764.3323244764691
Iteration: 4 || Loss: 764.3010418255378
Iteration: 5 || Loss: 764.2700119491817
Iteration: 6 || Loss: 764.2700119491817
saving ADAM checkpoint...
Sum of params:-99.62313
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 764.2700119491817
Iteration: 2 || Loss: 760.3118807295954
Iteration: 3 || Loss: 748.7658689983836
Iteration: 4 || Loss: 733.7546947668608
Iteration: 5 || Loss: 727.3615847512906
Iteration: 6 || Loss: 725.9736676268666
Iteration: 7 || Loss: 723.0187433973281
Iteration: 8 || Loss: 719.7750760020973
Iteration: 9 || Loss: 718.3978140189331
Iteration: 10 || Loss: 715.1780213890943
Iteration: 11 || Loss: 711.9014891516272
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.63187
Epoch 89 loss:711.9014891516272
MSE loss S52.21311127214138
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:115.89429885938219
MSE loss S - interpolation15.708050310852233
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3374.271187980241
MSE loss S - extrapolation469.16920504038654
waveform batch: 2/2
Test loss - extrapolation:1546.613692283642
MSE loss S - extrapolation57.33501761896214
Epoch 89 mean train loss:29.215502448303948
Epoch 89 mean test loss - interpolation:19.315716476563697
Epoch 89 mean test loss - extrapolation:410.07374002199026
Start training epoch 90
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.63187
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 255.9688222297018
Iteration: 2 || Loss: 255.76381200942203
Iteration: 3 || Loss: 255.55856891879006
Iteration: 4 || Loss: 255.3537272355716
Iteration: 5 || Loss: 255.14910682544598
Iteration: 6 || Loss: 255.14910682544598
saving ADAM checkpoint...
Sum of params:-99.63171
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 255.14910682544598
Iteration: 2 || Loss: 112.66461758586028
Iteration: 3 || Loss: 82.314162690038
Iteration: 4 || Loss: 69.5773335078357
Iteration: 5 || Loss: 65.21552923044332
Iteration: 6 || Loss: 59.249102881968255
Iteration: 7 || Loss: 54.64439007575926
Iteration: 8 || Loss: 51.069288666952026
Iteration: 9 || Loss: 48.48478933223356
Iteration: 10 || Loss: 47.53857393838335
Iteration: 11 || Loss: 45.6535007273233
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.62094
Epoch 90 loss:45.6535007273233
MSE loss S10.566693217849794
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.62094
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 127.26784042677417
Iteration: 2 || Loss: 127.18711663269407
Iteration: 3 || Loss: 127.10681244301979
Iteration: 4 || Loss: 127.02641197526776
Iteration: 5 || Loss: 126.94594493025762
Iteration: 6 || Loss: 126.94594493025762
saving ADAM checkpoint...
Sum of params:-99.6212
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 126.94594493025762
Iteration: 2 || Loss: 105.74133458341427
Iteration: 3 || Loss: 101.20122339783987
Iteration: 4 || Loss: 98.39588646839034
Iteration: 5 || Loss: 97.10149955780899
Iteration: 6 || Loss: 95.81810800919737
Iteration: 7 || Loss: 94.4534069390644
Iteration: 8 || Loss: 93.07495236931933
Iteration: 9 || Loss: 92.39646498161737
Iteration: 10 || Loss: 90.4397414841922
Iteration: 11 || Loss: 88.85638681026089
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.64478
Epoch 90 loss:88.85638681026089
MSE loss S12.105363639722253
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.64478
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 758.4148770530992
Iteration: 2 || Loss: 758.3837062843714
Iteration: 3 || Loss: 758.3526454051624
Iteration: 4 || Loss: 758.3218568029952
Iteration: 5 || Loss: 758.2913830799745
Iteration: 6 || Loss: 758.2913830799745
saving ADAM checkpoint...
Sum of params:-99.64471
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 758.2913830799745
Iteration: 2 || Loss: 754.4154389783681
Iteration: 3 || Loss: 743.0396198892079
Iteration: 4 || Loss: 728.3188912238617
Iteration: 5 || Loss: 722.0427342480964
Iteration: 6 || Loss: 720.712644748869
Iteration: 7 || Loss: 717.7763813454715
Iteration: 8 || Loss: 714.6602027808956
Iteration: 9 || Loss: 713.2774924513884
Iteration: 10 || Loss: 710.1320428057724
Iteration: 11 || Loss: 706.8711843912382
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.65369
Epoch 90 loss:706.8711843912382
MSE loss S51.80131991137715
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:114.970426081014
MSE loss S - interpolation15.564645294471413
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3351.208898715527
MSE loss S - extrapolation464.6411255506243
waveform batch: 2/2
Test loss - extrapolation:1537.8548074753423
MSE loss S - extrapolation56.891672271706945
Epoch 90 mean train loss:29.013140411338703
Epoch 90 mean test loss - interpolation:19.161737680169
Epoch 90 mean test loss - extrapolation:407.4219755159058
Start training epoch 91
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.65369
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 252.87080575416846
Iteration: 2 || Loss: 252.66671963872756
Iteration: 3 || Loss: 252.46360990728317
Iteration: 4 || Loss: 252.2594339634526
Iteration: 5 || Loss: 252.05640456186745
Iteration: 6 || Loss: 252.05640456186745
saving ADAM checkpoint...
Sum of params:-99.65352
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 252.05640456186745
Iteration: 2 || Loss: 111.54894149063348
Iteration: 3 || Loss: 81.51337320456574
Iteration: 4 || Loss: 68.91681410664239
Iteration: 5 || Loss: 64.64728921799274
Iteration: 6 || Loss: 58.806566813858616
Iteration: 7 || Loss: 54.22910935527415
Iteration: 8 || Loss: 50.69507111438578
Iteration: 9 || Loss: 48.16555032094433
Iteration: 10 || Loss: 47.19858016693879
Iteration: 11 || Loss: 45.32243717175375
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.64258
Epoch 91 loss:45.32243717175375
MSE loss S10.489164010431216
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.64258
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 126.47306610679001
Iteration: 2 || Loss: 126.39263574580661
Iteration: 3 || Loss: 126.3122495512838
Iteration: 4 || Loss: 126.23170683652172
Iteration: 5 || Loss: 126.15206294174531
Iteration: 6 || Loss: 126.15206294174531
saving ADAM checkpoint...
Sum of params:-99.64282
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 126.15206294174531
Iteration: 2 || Loss: 105.10135386751715
Iteration: 3 || Loss: 100.56404039213001
Iteration: 4 || Loss: 97.80131827257269
Iteration: 5 || Loss: 96.50515111976144
Iteration: 6 || Loss: 95.24692577812743
Iteration: 7 || Loss: 93.89505026189586
Iteration: 8 || Loss: 92.50043501710459
Iteration: 9 || Loss: 90.6661995755879
Iteration: 10 || Loss: 89.77199725420503
Iteration: 11 || Loss: 88.36122598900495
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.6662
Epoch 91 loss:88.36122598900495
MSE loss S12.030252938512197
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.6662
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 752.7503960749228
Iteration: 2 || Loss: 752.7192905301798
Iteration: 3 || Loss: 752.6885745033147
Iteration: 4 || Loss: 752.6581990433054
Iteration: 5 || Loss: 752.6271894069886
Iteration: 6 || Loss: 752.6271894069886
saving ADAM checkpoint...
Sum of params:-99.66612
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 752.6271894069886
Iteration: 2 || Loss: 748.8134710093144
Iteration: 3 || Loss: 737.5440963048568
Iteration: 4 || Loss: 723.2575621115811
Iteration: 5 || Loss: 716.8581715301373
Iteration: 6 || Loss: 715.5705864410553
Iteration: 7 || Loss: 712.6165479412499
Iteration: 8 || Loss: 709.5999150406653
Iteration: 9 || Loss: 708.224005852271
Iteration: 10 || Loss: 705.1579260802173
Iteration: 11 || Loss: 701.8970023763666
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.67543
Epoch 91 loss:701.8970023763666
MSE loss S51.38076500603233
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:113.9913569006785
MSE loss S - interpolation15.411004649274215
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3327.230537743726
MSE loss S - extrapolation459.7903763546417
waveform batch: 2/2
Test loss - extrapolation:1529.3452897371787
MSE loss S - extrapolation56.44190419618522
Epoch 91 mean train loss:28.813126397831905
Epoch 91 mean test loss - interpolation:18.998559483446417
Epoch 91 mean test loss - extrapolation:404.71465229007543
Start training epoch 92
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.67543
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 249.29466746062357
Iteration: 2 || Loss: 249.092383810276
Iteration: 3 || Loss: 248.89053399069988
Iteration: 4 || Loss: 248.68833869469034
Iteration: 5 || Loss: 248.48703978317846
Iteration: 6 || Loss: 248.48703978317846
saving ADAM checkpoint...
Sum of params:-99.67528
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 248.48703978317846
Iteration: 2 || Loss: 110.35921516183707
Iteration: 3 || Loss: 80.66785539655115
Iteration: 4 || Loss: 68.20998130412362
Iteration: 5 || Loss: 64.03488991171956
Iteration: 6 || Loss: 58.33367116715142
Iteration: 7 || Loss: 53.78492252848154
Iteration: 8 || Loss: 50.298909373607316
Iteration: 9 || Loss: 47.832424131918074
Iteration: 10 || Loss: 46.844031239877125
Iteration: 11 || Loss: 44.978812439008124
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.66417
Epoch 92 loss:44.978812439008124
MSE loss S10.408509258799597
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.66417
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 125.63640055581968
Iteration: 2 || Loss: 125.55662042732834
Iteration: 3 || Loss: 125.47634514851366
Iteration: 4 || Loss: 125.39609773760563
Iteration: 5 || Loss: 125.31633202888375
Iteration: 6 || Loss: 125.31633202888375
saving ADAM checkpoint...
Sum of params:-99.66442
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 125.31633202888375
Iteration: 2 || Loss: 104.44708410746772
Iteration: 3 || Loss: 99.91322626734595
Iteration: 4 || Loss: 97.19316698650684
Iteration: 5 || Loss: 95.8935729369759
Iteration: 6 || Loss: 94.66069332356778
Iteration: 7 || Loss: 93.32364628146651
Iteration: 8 || Loss: 91.94045796552759
Iteration: 9 || Loss: 90.90898715637081
Iteration: 10 || Loss: 89.36428978670818
Iteration: 11 || Loss: 87.86689042016356
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.68826
Epoch 92 loss:87.86689042016356
MSE loss S11.966267314352624
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.68826
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 746.6005183228534
Iteration: 2 || Loss: 746.5700545768536
Iteration: 3 || Loss: 746.5397223565217
Iteration: 4 || Loss: 746.5097625578893
Iteration: 5 || Loss: 746.4796489471668
Iteration: 6 || Loss: 746.4796489471668
saving ADAM checkpoint...
Sum of params:-99.68818
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 746.4796489471668
Iteration: 2 || Loss: 742.7863339776419
Iteration: 3 || Loss: 731.8374489242104
Iteration: 4 || Loss: 717.4248438616295
Iteration: 5 || Loss: 711.5885719186258
Iteration: 6 || Loss: 710.3288653655494
Iteration: 7 || Loss: 707.4779445966254
Iteration: 8 || Loss: 704.6248169736931
Iteration: 9 || Loss: 703.1846082744967
Iteration: 10 || Loss: 700.1739592161197
Iteration: 11 || Loss: 696.9550561104205
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.697075
Epoch 92 loss:696.9550561104205
MSE loss S50.98849314332596
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:113.13182210994074
MSE loss S - interpolation15.278976258174033
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3305.1577484207014
MSE loss S - extrapolation455.55550127461413
waveform batch: 2/2
Test loss - extrapolation:1520.6007703366922
MSE loss S - extrapolation56.023878006818784
Epoch 92 mean train loss:28.613819274813522
Epoch 92 mean test loss - interpolation:18.855303684990123
Epoch 92 mean test loss - extrapolation:402.14654322978276
Start training epoch 93
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.697075
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 246.60156287147092
Iteration: 2 || Loss: 246.40025366965634
Iteration: 3 || Loss: 246.19985292630957
Iteration: 4 || Loss: 245.9983957979638
Iteration: 5 || Loss: 245.79840008421542
Iteration: 6 || Loss: 245.79840008421542
saving ADAM checkpoint...
Sum of params:-99.6969
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 245.79840008421542
Iteration: 2 || Loss: 109.35663303076102
Iteration: 3 || Loss: 79.93703950949148
Iteration: 4 || Loss: 67.585719618103
Iteration: 5 || Loss: 63.49319537896347
Iteration: 6 || Loss: 57.91168480050658
Iteration: 7 || Loss: 53.38669439586304
Iteration: 8 || Loss: 49.94332938602497
Iteration: 9 || Loss: 47.521657711246405
Iteration: 10 || Loss: 46.51253328731184
Iteration: 11 || Loss: 44.65722458571938
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.6858
Epoch 93 loss:44.65722458571938
MSE loss S10.333298723337116
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.6858
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 124.83973947134275
Iteration: 2 || Loss: 124.75973267033189
Iteration: 3 || Loss: 124.6798018979753
Iteration: 4 || Loss: 124.59975112185789
Iteration: 5 || Loss: 124.5206976218017
Iteration: 6 || Loss: 124.5206976218017
saving ADAM checkpoint...
Sum of params:-99.68606
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 124.5206976218017
Iteration: 2 || Loss: 103.82021790478406
Iteration: 3 || Loss: 99.29025326129766
Iteration: 4 || Loss: 96.61141526776338
Iteration: 5 || Loss: 95.30742413025395
Iteration: 6 || Loss: 94.10097300574661
Iteration: 7 || Loss: 92.77870916623985
Iteration: 8 || Loss: 91.43425184681594
Iteration: 9 || Loss: 90.94910120955502
Iteration: 10 || Loss: 88.86109518017817
Iteration: 11 || Loss: 87.38227626406102
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.71002
Epoch 93 loss:87.38227626406102
MSE loss S11.897479555098046
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.71002
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 741.0049736448195
Iteration: 2 || Loss: 740.975050720472
Iteration: 3 || Loss: 740.9455467526086
Iteration: 4 || Loss: 740.9153568359718
Iteration: 5 || Loss: 740.8855381818221
Iteration: 6 || Loss: 740.8855381818221
saving ADAM checkpoint...
Sum of params:-99.70995
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 740.8855381818221
Iteration: 2 || Loss: 737.2684546093499
Iteration: 3 || Loss: 726.4857244523995
Iteration: 4 || Loss: 712.2995907289331
Iteration: 5 || Loss: 706.5007487281299
Iteration: 6 || Loss: 705.2614557860871
Iteration: 7 || Loss: 702.4291300204302
Iteration: 8 || Loss: 699.6822331105636
Iteration: 9 || Loss: 698.2189365231715
Iteration: 10 || Loss: 695.2788713475122
Iteration: 11 || Loss: 692.0720551819442
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.71876
Epoch 93 loss:692.0720551819442
MSE loss S50.573740330989935
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:112.16506259661713
MSE loss S - interpolation15.126031413003574
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3281.376538324903
MSE loss S - extrapolation450.72791392674117
waveform batch: 2/2
Test loss - extrapolation:1512.2366942860094
MSE loss S - extrapolation55.58533711182544
Epoch 93 mean train loss:28.41763986316292
Epoch 93 mean test loss - interpolation:18.69417709943619
Epoch 93 mean test loss - extrapolation:399.46776938424273
Start training epoch 94
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.71876
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 243.04946496314372
Iteration: 2 || Loss: 242.84939707963366
Iteration: 3 || Loss: 242.6506483398943
Iteration: 4 || Loss: 242.45116285998284
Iteration: 5 || Loss: 242.25245553492408
Iteration: 6 || Loss: 242.25245553492408
saving ADAM checkpoint...
Sum of params:-99.7186
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 242.25245553492408
Iteration: 2 || Loss: 108.19600695781659
Iteration: 3 || Loss: 79.111196149438
Iteration: 4 || Loss: 66.87537558146623
Iteration: 5 || Loss: 62.8698109033706
Iteration: 6 || Loss: 57.43570288061792
Iteration: 7 || Loss: 52.939128282785425
Iteration: 8 || Loss: 49.54852107086169
Iteration: 9 || Loss: 47.18503485902205
Iteration: 10 || Loss: 46.15587346376791
Iteration: 11 || Loss: 44.31365751199511
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.70739
Epoch 94 loss:44.31365751199511
MSE loss S10.25221525413952
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.70739
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 123.98639016538435
Iteration: 2 || Loss: 123.9068382388927
Iteration: 3 || Loss: 123.82728797475843
Iteration: 4 || Loss: 123.7477202643363
Iteration: 5 || Loss: 123.66856026057464
Iteration: 6 || Loss: 123.66856026057464
saving ADAM checkpoint...
Sum of params:-99.70764
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 123.66856026057464
Iteration: 2 || Loss: 103.17097676632031
Iteration: 3 || Loss: 98.64581973991923
Iteration: 4 || Loss: 96.00669280286198
Iteration: 5 || Loss: 94.69610972342223
Iteration: 6 || Loss: 93.51600835077112
Iteration: 7 || Loss: 92.21018016435143
Iteration: 8 || Loss: 90.92732884306771
Iteration: 9 || Loss: 90.50335961715393
Iteration: 10 || Loss: 88.32905577005667
Iteration: 11 || Loss: 86.8812983512147
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.73155
Epoch 94 loss:86.8812983512147
MSE loss S11.826104663372632
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.73155
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 735.2563657167078
Iteration: 2 || Loss: 735.226738686262
Iteration: 3 || Loss: 735.1970853197927
Iteration: 4 || Loss: 735.1673483274095
Iteration: 5 || Loss: 735.1378076776834
Iteration: 6 || Loss: 735.1378076776834
saving ADAM checkpoint...
Sum of params:-99.731476
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 735.1378076776834
Iteration: 2 || Loss: 731.6017210474589
Iteration: 3 || Loss: 721.0067771844249
Iteration: 4 || Loss: 707.0489610879716
Iteration: 5 || Loss: 701.3882641600209
Iteration: 6 || Loss: 700.1738525440779
Iteration: 7 || Loss: 697.3968167248847
Iteration: 8 || Loss: 694.752496154299
Iteration: 9 || Loss: 693.2805069147192
Iteration: 10 || Loss: 690.4131310614326
Iteration: 11 || Loss: 687.2211417859709
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.74048
Epoch 94 loss:687.2211417859709
MSE loss S50.17981876219726
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:111.28436497582074
MSE loss S - interpolation14.990494230549022
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3258.755712172283
MSE loss S - extrapolation446.2796659408319
waveform batch: 2/2
Test loss - extrapolation:1503.7161945006674
MSE loss S - extrapolation55.16078944700796
Epoch 94 mean train loss:28.22124474652347
Epoch 94 mean test loss - interpolation:18.547394162636788
Epoch 94 mean test loss - extrapolation:396.87265888941255
Start training epoch 95
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.74048
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 240.0161916847255
Iteration: 2 || Loss: 239.81812548591896
Iteration: 3 || Loss: 239.62038539470265
Iteration: 4 || Loss: 239.42240140006243
Iteration: 5 || Loss: 239.22493220649181
Iteration: 6 || Loss: 239.22493220649181
saving ADAM checkpoint...
Sum of params:-99.74031
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 239.22493220649181
Iteration: 2 || Loss: 107.17823156964816
Iteration: 3 || Loss: 78.3831968258611
Iteration: 4 || Loss: 66.2401965754277
Iteration: 5 || Loss: 62.30667904139577
Iteration: 6 || Loss: 57.01056562615996
Iteration: 7 || Loss: 52.53411820298073
Iteration: 8 || Loss: 49.18839356099659
Iteration: 9 || Loss: 46.86871807787907
Iteration: 10 || Loss: 45.8209572052345
Iteration: 11 || Loss: 43.99044819852711
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.728935
Epoch 95 loss:43.99044819852711
MSE loss S10.17598252000895
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.728935
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 123.18219448843892
Iteration: 2 || Loss: 123.10272648003641
Iteration: 3 || Loss: 123.02328581565382
Iteration: 4 || Loss: 122.94425695067692
Iteration: 5 || Loss: 122.86534836041285
Iteration: 6 || Loss: 122.86534836041285
saving ADAM checkpoint...
Sum of params:-99.72921
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 122.86534836041285
Iteration: 2 || Loss: 102.55078229408807
Iteration: 3 || Loss: 98.02883782975096
Iteration: 4 || Loss: 95.43030502139611
Iteration: 5 || Loss: 94.11216126350986
Iteration: 6 || Loss: 92.95918617862498
Iteration: 7 || Loss: 91.66951187924144
Iteration: 8 || Loss: 90.46292463348605
Iteration: 9 || Loss: 90.00647633042276
Iteration: 10 || Loss: 87.81760424295179
Iteration: 11 || Loss: 86.39736045423984
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.7531
Epoch 95 loss:86.39736045423984
MSE loss S11.756657782747467
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.7531
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 729.5819415079677
Iteration: 2 || Loss: 729.5524765692966
Iteration: 3 || Loss: 729.5230570993359
Iteration: 4 || Loss: 729.4937999015707
Iteration: 5 || Loss: 729.4648129750137
Iteration: 6 || Loss: 729.4648129750137
saving ADAM checkpoint...
Sum of params:-99.75303
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 729.4648129750137
Iteration: 2 || Loss: 726.0060594050723
Iteration: 3 || Loss: 715.5911376132249
Iteration: 4 || Loss: 701.8425015139147
Iteration: 5 || Loss: 696.3087352253157
Iteration: 6 || Loss: 695.116797450354
Iteration: 7 || Loss: 692.3966663963771
Iteration: 8 || Loss: 689.8509588913104
Iteration: 9 || Loss: 688.3704082291259
Iteration: 10 || Loss: 685.575500415299
Iteration: 11 || Loss: 682.4038351589982
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.76218
Epoch 95 loss:682.4038351589982
MSE loss S49.786120737512036
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:110.41100964353518
MSE loss S - interpolation14.855965908367327
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3236.2548330430927
MSE loss S - extrapolation441.846404085495
waveform batch: 2/2
Test loss - extrapolation:1495.2543143470903
MSE loss S - extrapolation54.73871026061523
Epoch 95 mean train loss:28.027298062474657
Epoch 95 mean test loss - interpolation:18.401834940589197
Epoch 95 mean test loss - extrapolation:394.2924289491819
Start training epoch 96
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.76218
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 236.99039202509329
Iteration: 2 || Loss: 236.79329981670622
Iteration: 3 || Loss: 236.59721852129204
Iteration: 4 || Loss: 236.4006642759741
Iteration: 5 || Loss: 236.20446424428624
Iteration: 6 || Loss: 236.20446424428624
saving ADAM checkpoint...
Sum of params:-99.76204
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 236.20446424428624
Iteration: 2 || Loss: 106.17969346875883
Iteration: 3 || Loss: 77.67188587260114
Iteration: 4 || Loss: 65.61031093774234
Iteration: 5 || Loss: 61.7384335776906
Iteration: 6 || Loss: 56.58788910268911
Iteration: 7 || Loss: 52.129908269920264
Iteration: 8 || Loss: 48.83099642065968
Iteration: 9 || Loss: 46.5519479068374
Iteration: 10 || Loss: 45.48718958504976
Iteration: 11 || Loss: 43.66897670964691
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.75055
Epoch 96 loss:43.66897670964691
MSE loss S10.100044515429602
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.75055
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 122.3872752025208
Iteration: 2 || Loss: 122.30791501798794
Iteration: 3 || Loss: 122.2289828692734
Iteration: 4 || Loss: 122.1507269650954
Iteration: 5 || Loss: 122.07158418724175
Iteration: 6 || Loss: 122.07158418724175
saving ADAM checkpoint...
Sum of params:-99.75079
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 122.07158418724175
Iteration: 2 || Loss: 101.9369041067804
Iteration: 3 || Loss: 97.41804958832667
Iteration: 4 || Loss: 94.85819890607489
Iteration: 5 || Loss: 93.53060921393063
Iteration: 6 || Loss: 92.40731735365806
Iteration: 7 || Loss: 91.13466785917159
Iteration: 8 || Loss: 90.00674087599931
Iteration: 9 || Loss: 89.49083835271634
Iteration: 10 || Loss: 87.31114389504826
Iteration: 11 || Loss: 85.91620280667593
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.774666
Epoch 96 loss:85.91620280667593
MSE loss S11.687432729996129
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.774666
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 723.9912720196685
Iteration: 2 || Loss: 723.9625087901156
Iteration: 3 || Loss: 723.9332934713224
Iteration: 4 || Loss: 723.9043909370137
Iteration: 5 || Loss: 723.8757461451163
Iteration: 6 || Loss: 723.8757461451163
saving ADAM checkpoint...
Sum of params:-99.77458
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 723.8757461451163
Iteration: 2 || Loss: 720.4932951377714
Iteration: 3 || Loss: 710.2614378373712
Iteration: 4 || Loss: 696.7039034809436
Iteration: 5 || Loss: 691.2841349680546
Iteration: 6 || Loss: 690.1137993854107
Iteration: 7 || Loss: 687.4493408272766
Iteration: 8 || Loss: 684.9971602957876
Iteration: 9 || Loss: 683.506501745576
Iteration: 10 || Loss: 680.7808879004573
Iteration: 11 || Loss: 677.6288849006556
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.78387
Epoch 96 loss:677.6288849006556
MSE loss S49.39469277902319
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:109.53684729218061
MSE loss S - interpolation14.72120613267409
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3213.7213288710677
MSE loss S - extrapolation437.38560663655676
waveform batch: 2/2
Test loss - extrapolation:1486.8755213807551
MSE loss S - extrapolation54.31982599910222
Epoch 96 mean train loss:27.8349677385165
Epoch 96 mean test loss - interpolation:18.256141215363435
Epoch 96 mean test loss - extrapolation:391.7164041876519
Start training epoch 97
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.78387
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 233.91272109576244
Iteration: 2 || Loss: 233.71737445674975
Iteration: 3 || Loss: 233.52236660168307
Iteration: 4 || Loss: 233.3272611787746
Iteration: 5 || Loss: 233.13233796888062
Iteration: 6 || Loss: 233.13233796888062
saving ADAM checkpoint...
Sum of params:-99.783714
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 233.13233796888062
Iteration: 2 || Loss: 105.18391163793693
Iteration: 3 || Loss: 76.96503973842431
Iteration: 4 || Loss: 64.97833302088743
Iteration: 5 || Loss: 61.15617678964285
Iteration: 6 || Loss: 56.162316249829935
Iteration: 7 || Loss: 51.72318427142649
Iteration: 8 || Loss: 48.472797219506745
Iteration: 9 || Loss: 46.23126530331251
Iteration: 10 || Loss: 45.15237575182888
Iteration: 11 || Loss: 43.347435710254786
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.77211
Epoch 97 loss:43.347435710254786
MSE loss S10.02334011742533
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.77211
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 121.58250857379004
Iteration: 2 || Loss: 121.50367407522543
Iteration: 3 || Loss: 121.42499106285926
Iteration: 4 || Loss: 121.3461661427921
Iteration: 5 || Loss: 121.26777933513596
Iteration: 6 || Loss: 121.26777933513596
saving ADAM checkpoint...
Sum of params:-99.772354
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 121.26777933513596
Iteration: 2 || Loss: 101.32756198524973
Iteration: 3 || Loss: 96.81257889553788
Iteration: 4 || Loss: 94.29036458376598
Iteration: 5 || Loss: 92.95198562012727
Iteration: 6 || Loss: 91.85685987026571
Iteration: 7 || Loss: 90.60182788436148
Iteration: 8 || Loss: 89.55367636612208
Iteration: 9 || Loss: 88.96832512424474
Iteration: 10 || Loss: 86.80852939090302
Iteration: 11 || Loss: 85.43645459048386
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.79624
Epoch 97 loss:85.43645459048386
MSE loss S11.618575997302568
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.79624
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 718.4574027197455
Iteration: 2 || Loss: 718.4289436416236
Iteration: 3 || Loss: 718.4003685107893
Iteration: 4 || Loss: 718.371876749042
Iteration: 5 || Loss: 718.343046066407
Iteration: 6 || Loss: 718.343046066407
saving ADAM checkpoint...
Sum of params:-99.79618
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 718.343046066407
Iteration: 2 || Loss: 715.0364565009
Iteration: 3 || Loss: 704.9935241265064
Iteration: 4 || Loss: 691.6168627140628
Iteration: 5 || Loss: 686.3025294993541
Iteration: 6 || Loss: 685.1551629797731
Iteration: 7 || Loss: 682.5481565567062
Iteration: 8 || Loss: 680.183476519474
Iteration: 9 || Loss: 678.6854732254095
Iteration: 10 || Loss: 676.0288286859954
Iteration: 11 || Loss: 672.8951225524622
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.805534
Epoch 97 loss:672.8951225524622
MSE loss S49.005144957304175
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:108.66958590315073
MSE loss S - interpolation14.586949872935204
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3191.2130856029207
MSE loss S - extrapolation432.9123301359356
waveform batch: 2/2
Test loss - extrapolation:1478.5562493823068
MSE loss S - extrapolation53.902063723225965
Epoch 97 mean train loss:27.644103891489685
Epoch 97 mean test loss - interpolation:18.111597650525123
Epoch 97 mean test loss - extrapolation:389.14744458210225
Start training epoch 98
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.805534
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 230.82221500234184
Iteration: 2 || Loss: 230.6285648928885
Iteration: 3 || Loss: 230.43468530879036
Iteration: 4 || Loss: 230.24142188124776
Iteration: 5 || Loss: 230.04783179795766
Iteration: 6 || Loss: 230.04783179795766
saving ADAM checkpoint...
Sum of params:-99.805374
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 230.04783179795766
Iteration: 2 || Loss: 104.20395774176683
Iteration: 3 || Loss: 76.27181324740172
Iteration: 4 || Loss: 64.35190966673598
Iteration: 5 || Loss: 60.561131240901375
Iteration: 6 || Loss: 55.73889333683647
Iteration: 7 || Loss: 51.31763486869433
Iteration: 8 || Loss: 48.11755682357239
Iteration: 9 || Loss: 45.90962765110543
Iteration: 10 || Loss: 44.818788382065016
Iteration: 11 || Loss: 43.02752463158889
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.79368
Epoch 98 loss:43.02752463158889
MSE loss S9.946962489124857
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.79368
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 120.78381300269605
Iteration: 2 || Loss: 120.70527470382112
Iteration: 3 || Loss: 120.62709512285208
Iteration: 4 || Loss: 120.54873917431206
Iteration: 5 || Loss: 120.47039540377136
Iteration: 6 || Loss: 120.47039540377136
saving ADAM checkpoint...
Sum of params:-99.79395
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 120.47039540377136
Iteration: 2 || Loss: 100.72268757183555
Iteration: 3 || Loss: 96.21304000573775
Iteration: 4 || Loss: 93.72659924761602
Iteration: 5 || Loss: 92.3756440853536
Iteration: 6 || Loss: 91.31045138900029
Iteration: 7 || Loss: 90.07305745163654
Iteration: 8 || Loss: 89.09867268328729
Iteration: 9 || Loss: 88.4453277209191
Iteration: 10 || Loss: 86.30997487710842
Iteration: 11 || Loss: 84.9584291491051
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.81782
Epoch 98 loss:84.9584291491051
MSE loss S11.54991770505121
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.81782
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 712.9882153572598
Iteration: 2 || Loss: 712.9600047669676
Iteration: 3 || Loss: 712.9315679575827
Iteration: 4 || Loss: 712.9039052522342
Iteration: 5 || Loss: 712.8757600376414
Iteration: 6 || Loss: 712.8757600376414
saving ADAM checkpoint...
Sum of params:-99.81775
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 712.8757600376414
Iteration: 2 || Loss: 709.6441937534305
Iteration: 3 || Loss: 699.7902152783407
Iteration: 4 || Loss: 686.5807150271733
Iteration: 5 || Loss: 681.3639713299718
Iteration: 6 || Loss: 680.2414026890666
Iteration: 7 || Loss: 677.6937614649604
Iteration: 8 || Loss: 675.4124108968217
Iteration: 9 || Loss: 673.9085960373596
Iteration: 10 || Loss: 671.3183496617646
Iteration: 11 || Loss: 668.2026389385861
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.82724
Epoch 98 loss:668.2026389385861
MSE loss S48.618775627162535
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:107.8102677507024
MSE loss S - interpolation14.45410691652996
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3168.7791599588268
MSE loss S - extrapolation428.44540544718006
waveform batch: 2/2
Test loss - extrapolation:1470.305153487041
MSE loss S - extrapolation53.489907777968966
Epoch 98 mean train loss:27.45477905928552
Epoch 98 mean test loss - interpolation:17.9683779584504
Epoch 98 mean test loss - extrapolation:386.5903594538224
Start training epoch 99
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.82724
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 227.73457737265238
Iteration: 2 || Loss: 227.5422166731011
Iteration: 3 || Loss: 227.3500905877389
Iteration: 4 || Loss: 227.15774963183068
Iteration: 5 || Loss: 226.9659497065755
Iteration: 6 || Loss: 226.9659497065755
saving ADAM checkpoint...
Sum of params:-99.82709
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 226.9659497065755
Iteration: 2 || Loss: 103.24169853355905
Iteration: 3 || Loss: 75.59296093301042
Iteration: 4 || Loss: 63.73181475062016
Iteration: 5 || Loss: 59.94892353537008
Iteration: 6 || Loss: 55.317365768852454
Iteration: 7 || Loss: 50.912867437070425
Iteration: 8 || Loss: 47.764184669837356
Iteration: 9 || Loss: 45.58591949263168
Iteration: 10 || Loss: 44.485409374553925
Iteration: 11 || Loss: 42.70838775550506
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.815254
Epoch 99 loss:42.70838775550506
MSE loss S9.870715814988301
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.815254
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 119.98146150803166
Iteration: 2 || Loss: 119.90354775394614
Iteration: 3 || Loss: 119.82507190248901
Iteration: 4 || Loss: 119.7470146857912
Iteration: 5 || Loss: 119.66911348027783
Iteration: 6 || Loss: 119.66911348027783
saving ADAM checkpoint...
Sum of params:-99.81549
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 119.66911348027783
Iteration: 2 || Loss: 100.12011921626294
Iteration: 3 || Loss: 95.61505955172267
Iteration: 4 || Loss: 93.16511601041809
Iteration: 5 || Loss: 91.79981862913115
Iteration: 6 || Loss: 90.765563815289
Iteration: 7 || Loss: 89.54694309340407
Iteration: 8 || Loss: 88.63509968558657
Iteration: 9 || Loss: 87.92167947033485
Iteration: 10 || Loss: 85.81286800158001
Iteration: 11 || Loss: 84.47967924629658
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.83941
Epoch 99 loss:84.47967924629658
MSE loss S11.480980652727887
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.83941
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 707.5827110575831
Iteration: 2 || Loss: 707.5547437878643
Iteration: 3 || Loss: 707.5270364276028
Iteration: 4 || Loss: 707.4991404969361
Iteration: 5 || Loss: 707.4712514115735
Iteration: 6 || Loss: 707.4712514115735
saving ADAM checkpoint...
Sum of params:-99.839355
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 707.4712514115735
Iteration: 2 || Loss: 704.3142296031237
Iteration: 3 || Loss: 694.6531556425747
Iteration: 4 || Loss: 681.6008985638327
Iteration: 5 || Loss: 676.4713834197655
Iteration: 6 || Loss: 675.3752706027552
Iteration: 7 || Loss: 672.8890882460473
Iteration: 8 || Loss: 670.6847897178957
Iteration: 9 || Loss: 669.1764117667245
Iteration: 10 || Loss: 666.6504331789835
Iteration: 11 || Loss: 663.552481115847
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.84892
Epoch 99 loss:663.552481115847
MSE loss S48.23499063334302
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:106.95425364480023
MSE loss S - interpolation14.321425949503942
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3146.3364352400486
MSE loss S - extrapolation423.9546024314143
waveform batch: 2/2
Test loss - extrapolation:1462.1269618993729
MSE loss S - extrapolation53.080858144616926
Epoch 99 mean train loss:27.26691545233271
Epoch 99 mean test loss - interpolation:17.825708940800038
Epoch 99 mean test loss - extrapolation:384.03861642828514
Start training epoch 100
waveform batch: 1/3
Using ADAM optimizer
Sum of params:-99.84892
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 224.6193796029701
Iteration: 2 || Loss: 224.4288876388546
Iteration: 3 || Loss: 224.23790332590013
Iteration: 4 || Loss: 224.04726682886601
Iteration: 5 || Loss: 223.85675780363158
Iteration: 6 || Loss: 223.85675780363158
saving ADAM checkpoint...
Sum of params:-99.84875
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 223.85675780363158
Iteration: 2 || Loss: 102.28302519819172
Iteration: 3 || Loss: 74.92179431076377
Iteration: 4 || Loss: 63.116337622230176
Iteration: 5 || Loss: 59.30981767503584
Iteration: 6 || Loss: 54.89512982491328
Iteration: 7 || Loss: 50.50768330794321
Iteration: 8 || Loss: 47.412383648919764
Iteration: 9 || Loss: 45.25977245459328
Iteration: 10 || Loss: 44.15168654607629
Iteration: 11 || Loss: 42.38939431373209
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.83679
Epoch 100 loss:42.38939431373209
MSE loss S9.794043770567491
waveform batch: 2/3
Using ADAM optimizer
Sum of params:-99.83679
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 119.1740986371827
Iteration: 2 || Loss: 119.09651786488543
Iteration: 3 || Loss: 119.0184017811506
Iteration: 4 || Loss: 118.94056163490538
Iteration: 5 || Loss: 118.86298003931445
Iteration: 6 || Loss: 118.86298003931445
saving ADAM checkpoint...
Sum of params:-99.83706
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 118.86298003931445
Iteration: 2 || Loss: 99.51967281914648
Iteration: 3 || Loss: 95.02155759607778
Iteration: 4 || Loss: 92.606592501299
Iteration: 5 || Loss: 91.22697216166395
Iteration: 6 || Loss: 90.22215334275067
Iteration: 7 || Loss: 89.02226536145625
Iteration: 8 || Loss: 88.16480164900454
Iteration: 9 || Loss: 87.39832988681927
Iteration: 10 || Loss: 85.31995995074368
Iteration: 11 || Loss: 84.00274826199085
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.860985
Epoch 100 loss:84.00274826199085
MSE loss S11.412304914928086
waveform batch: 3/3
Using ADAM optimizer
Sum of params:-99.860985
Max iters:5
Choosing ADAM Optimizer.
Iteration: 1 || Loss: 702.2249598647118
Iteration: 2 || Loss: 702.197386258946
Iteration: 3 || Loss: 702.1698854804027
Iteration: 4 || Loss: 702.1420195893753
Iteration: 5 || Loss: 702.1148022533887
Iteration: 6 || Loss: 702.1148022533887
saving ADAM checkpoint...
Sum of params:-99.8609
Switching to BFGS optimizer
Max iters:10
Choosing BFGS Optimizer.
Iteration: 1 || Loss: 702.1148022533887
Iteration: 2 || Loss: 699.0317733503571
Iteration: 3 || Loss: 689.5685133269327
Iteration: 4 || Loss: 676.6667463975216
Iteration: 5 || Loss: 671.6155296051879
Iteration: 6 || Loss: 670.5472016892588
Iteration: 7 || Loss: 668.1269092205928
Iteration: 8 || Loss: 665.9937278475066
Iteration: 9 || Loss: 664.4848442373612
Iteration: 10 || Loss: 662.0226713811496
Iteration: 11 || Loss: 658.9412862880973
saving BFGS checkpoint...
saved trained params to ptrained_BFGS.jld2
Sum of params:-99.87057
Epoch 100 loss:658.9412862880973
MSE loss S47.85436530983213
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing -interpolation:
waveform batch: 1/1
Test loss - interpolation:106.10646638434673
MSE loss S - interpolation14.189938036958035
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Testing - extrapolation:
waveform batch: 1/2
Test loss - extrapolation:3123.9579792192485
MSE loss S - extrapolation419.46512438956984
waveform batch: 2/2
Test loss - extrapolation:1454.0000494476226
MSE loss S - extrapolation52.67414428681297
Epoch 100 mean train loss:27.080463064269665
Epoch 100 mean test loss - interpolation:17.684411064057787
Epoch 100 mean test loss - extrapolation:381.496502388906
